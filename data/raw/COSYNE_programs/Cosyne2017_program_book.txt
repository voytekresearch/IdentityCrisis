Program Summary

Thursday, 23 February
4.00p

Registration opens

5.00p

Welcome reception

5.45p

Opening remarks

6.00p

Session 1: Opening session
Invited speakers: Surya Ganguli, Greg Gage

8.00p

Poster Session I

Friday, 24 February
7.30a

Breakfast

8.30a

Session 2: Sensory processing
Invited speaker: Marria Geffen; 3 accepted talks

10.30a

Session 3: Network dynamics
Invited speaker: Brent Doiron; 2 accepted talks

11.45p

Lunch break

2.00p

Session 4: Memory on multiple timescales
Invited speaker: Elizabeth Phelps; 4 accepted talks

4.15p

Session 5: Reinforcement learning
Invited speaker: Naoshige Uchida; 2 accepted talks

5.30p

Dinner break

8.00p

Poster Session II

Saturday, 25 February
7.30a

Breakfast

8.30a

Session 6: Motivation and decision making
Invited speaker: Kay Tye; 3 accepted talks

10.30a

Session 7: Statistical approaches in neuroscience
Invited speaker: Jonathan Pillow; 2 accepted talks

11.45p

Lunch break

2.00p

Session 8: Circuits and behavior
Invited speaker: Catherine Dulac; 4 accepted talks

4.15p

Session 9: Learning and memory
Invited speaker: Daphna Shohamy; 2 accepted talks

5.30p

Dinner break

8.00p

Poster Session III

COSYNE 2017

i

Sunday, 26 February

ii

7.30a

Breakfast

8.30a

Session 10: Attention and arousal
Invited speaker: Gero Miesenbock; 3 accepted talks

10.30a

Session 11: Sensory-motor integration
Invited speaker: Vanessa Ruta; 2 accepted talks

11.45p

Lunch break

2.00p

Session 12: Learning in networks
Invited speaker: Yoshua Bengio; 3 accepted talks

COSYNE 2017

The MIT Press

Deep Learning
Ian Goodfellow, Yoshua Bengio,
and Aaron Courville
An introduction to a broad range of topics
in deep learning, covering mathematical and conceptual background, deep
learning techniques used in industry, and
research perspectives.
Adaptive Computation and Machine
Learning series | Hardcover | $80 | £66.95

The Distracted Mind
Ancient Brains in a High-Tech World

From Neuron to Cognition
via Computational
Neuroscience
edited by Michael A. Arbib
and James J. Bonaiuto
A comprehensive, integrated, and
accessible textbook presenting core
neuroscientific topics from a computational perspective, tracing a path
from cells and circuits to behavior
and cognition.
Computational Neuroscience series
Hardcover | $115 | £95.95

Hardcover | $27.95 | £22.95

Case Studies in
Neural Data Analysis
A Guide for the Practicing
Neuroscientist
Mark A. Kramer and Uri T. Eden

Neuroplasticity
Moheb Costandi
The real story of how our brains and
nervous systems change throughout our
lifetimes—with or without “brain training.”

Frank H. Guenther
A comprehensive and unified account
of the neural computations underlying
speech production, offering a theoretical
framework bridging the behavioral and
the neurological literatures.
Hardcover | $63 | £52.95

Forthcoming

MATLAB for Brain
and Cognitive Scientists
Mike X Cohen

Adam Gazzaley and Larry D. Rosen
Why our brains aren’t built for media
multitasking, and how we can learn to live
with technology in a more balanced way.

Neural Control of Speech

An introduction to a popular programming language for neuroscience research,
taking the reader from beginning to intermediate and advanced levels of MATLAB
programming.
Hardcover | $50 | £41.95

A practical guide to neural data
analysis techniques that presents
sample datasets and hands-on
methods for analyzing the data.
Computational Neuroscience series
Paperback | $60 | £49.95

The MIT Press Essential Knowledge series
Paperback | $15.95 | £11.95

mitpress.mit.edu

PLEASE VISIT
OUR BOOTH TO
ENJOY A 30%
DISCOUNT ON
ALL BOOKS!

The space of interesting
behavior metrics is vast.
Your R&D budget really isn’t.

Sanworks
Open source behavior measurement technology

http://sanworks.io

What’s new in MATLAB?
Deep Learning

• Convolutional neural networks
• Transfer learning

Big Data

• Tall arrays
• Hadoop® and Spark®

Learn more:
mathworks.com/solutions/neuroscience

Follow us on LinkedIn:

“MATLAB for Neuroscience”
News tailored for neuroscientists

V-Probe

U-Probe

The V-Probe multi-site, linear electrode
is a variation of the U-Probe with the
added beneit of
a conical-shaped
tip
to
likely
minimize trauma
to the brain tissue
upon insertion.

Plexon’s very popular, robust, multiuse, multi-site linear electrode most
often used for
acute
studies
with
larger
animals such as
primates.

Technical Specifications

Technical Specifications

Specifications and
V-Probe Features Options
Application

In vivo; acute

Channel counts

8, 16, 24 or 32

Total probe length

30 to 150mm

Probe OD

8 or 16 channel: 185 μm,
24 channel: 210μm, 32
channel: 238μm

Electrode
construction

15μm Pt/Ir electrode site
diamter, circular shape,
HML insulated (polyimide),
and secured in medicalgrade epoxy

Electrode
configurations

Single-row

Inter-electrode
spacing

50μm, 100μm, 150μm,
200μm along length of
probe

Distance from
tip to the closest
electrode site

300μm

Tip shape

Conical-shaped pencil tip

Fluid capillary ID,
OD

40μm, 60μm

Optic fiber OD

125μm

Lifespan

Robust and reusable
with a minimum of thirty
penetrations, likely many
more

For more information email info@plexon.com or call +1-214-369-4957

Specifications and
U-Probe Features Options
Application

In vivo; acute

Channel counts

8, 16, 24 or 32

Total probe length

30 to 150mm

Probe OD

8 or 16 channel: 185 μm,
24 channel: 210μm, 32
channel: 238μm

Electrode
construction

15μm Pt/Ir electrode site
diamter, circular shape,
HML insulated (polyimide),
and secured in medicalgrade epoxy

Electrode
configurations

Single, stereotrode or
tetrode

Inter-electrode
spacing

50μm, 100μm, 150μm,
200μm; 50μm within
stereotrode or tetrode

Distance from tip
to the closest
electrode site

With a 15° tip on a probe
diameter of:
- 185μm OD is 700μm, and
- 210μm OD is 800μm

Tip shape

Bevelled

Tip specifications

Sharpened (15˚ angle),
semi-sharpened (30˚ angle)
or rounded

Stimulation
Options

Fluid capillaries and optic
ibers (max 4)

Fluid capillary ID,
OD

40μm, 60μm

Optic fiber OD

125μm

Lifespan

Robust and reusable
with a minimum of thirty
penetrations, likely many
more

www.plexon.com

Luncheon for Equality and Diversity in Science
Saturday, February 25th from 12:00 p.m. - 1:45 p.m.
Caffe Molise
55 100 S Street (across the street from the Marriott)
pre-pregistration required (follow link from Cosyne program online)

image: Burlington HS international club

COSYNE 2018
New location, save the date!

MAIN MEETING
Denver CO, Mar 1–4
WORKSHOPS Breckenridge CO, Mar 5–6

About Cosyne

About Cosyne
The annual Cosyne meeting provides an inclusive forum for the exchange of experimental
and theoretical/computational approaches to problems in systems neuroscience.
To encourage interdisciplinary interactions, the main meeting is arranged in a single track.
A set of invited talks are selected by the Executive Committee and Organizing Committee, and additional talks and posters are selected by the Program Committee, based on
submitted abstracts and the occasional odd bribe.
Cosyne topics include (but are not limited to): neural coding, natural scene statistics, dendritic computation, neural basis of persistent activity, nonlinear receptive field mapping,
representations of time and sequence, reward systems, decision-making, synaptic plasticity, map formation and plasticity, population coding, attention, and computation with spiking
networks. Participants include pure experimentalists, pure theorists, and everything in between.

Cosyne 2017 Leadership
Organizing Committee
General Chairs
Megan Carey (Champalimaud), Emilio Salinas (Wake Forest University)
Program Chairs
Ilana Witten (Princeton University) and Eric Shea-Brown (University of Washington)
Workshop Chairs
Laura Busse (Ludwig-Maximilians-Universitaet Munich), Alfonso Renart (Champalimaud)
Undergraduate Travel Chairs
Angela Langdon (Princeton University), Robert Wilson (University of Arizona)
Communications Chair
Xaq Pitkow (Rice University)
Publicity Chair
Il Memming Park (Stony Brook University)
Executive Committee
Anne Churchland (Cold Spring Harbor Laboratory)
Zachary Mainen (Champalimaud)
Alexandre Pouget (University of Geneva)
Anthony Zador (Cold Spring Harbor Laboratory)
COSYNE 2017

xiii

About Cosyne
Program Committee
Ilana Witten (Princeton University), co-chair
Eric Shea-Brown (University of Washington), co-chair
Yashar Ahmadian (University of Oregon)
Mark Andermann (Harvard University)
Bruno Averbeck (NIH)
Matthias Bethge (University of Tuebingen)
Bing Brunton (University of Washington)
Tim Buschman (Princeton University)
Daniel Butts (University of Maryland)
Damon Clark (Yale University)
Jeremiah Cohen (Johns Hopkins University)
Saskia DeVries (Allen Institute)
Long Ding (University of Pennsylvania)
Carina Curto (Pennsylvania State University)
Sean Escola (Columbia University)
Allie Fletcher (University of California, Los Angeles)
Lisa Giocomo (Stanford University)
Julijana Gjorgjieva (Max Planck Institute for Brain Research)
Lindsey Glickfeld (Duke University)
Catherine Hartley (New York University)
Alex Huk (University of Texas at Austin)
Santiago Jaramillo (University of Oregon)
Mehrdad Jazayeri (Massachusetts Institute of Technology)
Na Ji (Janelia Farm Research Campus)
Adam Kepecs (Cold Spring Harbor Laboratory)
Matthieu Louis (Centre for Genomic regulation)
Arianna Maffei (Stony Brook University)
Dan O’Connor (Johns Hopkins University)
Srdjan Ostojic (Ecole Normale Superieure)
Liam Paninski (Columbia University)
Hussain Schuler (Johns Hopkins University)
Tatyana Sharpee (Salk Institute for Biological Studies)
Sam Sober (Emory University)
Saori Tanaka (ATR Brain Information Communication Research Laboratory)
Tatjana Tchumatchenko (Max Planck Institute for Brain Research)
Srini Turaga (Janelia Farm Research Campus)
Melissa Warden (Cornell University)
Joel Zylberberg (University of Colorado)

xiv

COSYNE 2017

About Cosyne
Reviewers
Vincent Hakim, Julija Krupic, Dragana Rogulja, Gaby Maimon, Agnieszka Grabska Barwinska, Greg Stephens, Mattia Rigotti, Shih-Chieh Lin, Matt Valley, Gabrielle J Gutierrez,
Betty Hong, Cindy Poo, Farzan Nadim, Kate Wassum, Skyler Jackman, Dmitri Chklovskii,
Leopoldo Petreanu, Bassam Atallah, Alex Kwan, Thomas Naselaris, Spencer Smith, Cristopher Niell, Andrea Giovannucci, Lucas Pinto, Emily Anderson, Thomas A Carlson, Sukbin
Lim, Blake A Richards, Douglas Ruff, Jonas Knöll, Jean-Pascal Pfister, Bilal Haider, Guilhem Ibos, Robert H Cudmore, Christoph Kirst, Yu Hu, Stephen V David, James Jeanne,
Matthieu Gilson, Sarah Muldoon, Duane Nykamp, Amy Ni, Steve Van Hooser, David Cox,
Jan Antolik, Barak A Pearlmutter, Mark Histed, Avishek Adhikari, Ann Kennedy, James
Fitzgerald, Marion Silies, Sandro Romani, Patrick Drew, Robert Guetig, Emre Yaksi, Zachary
P Kilpatrick, Qiaojie Xiong, Jenny Read, Alfredo Fontanini, Luca Mazzucato, Takashi Sato,
Andrew Hires, Michael Graupner, Sunil Gandhi, Chunyu A Duan, Zaneta Navratilova, Giancarlo La Camera, Alla Borisyuk, Ian Fiebelkorn, Yuri Dabaghian, Yuki Sakai, Peter Rudebeck, Vijay Mohan K Namboodiri, Greg D Wayne, Chris Deister, Madineh Sarvestani, Ilya
E Monosov, Jeff Gavornik, Shawn Olsen, Mike Wehr, David Sussillo, David E Carlson, Josh
Merel, Rodica Curtu, Yuwei Cui, Ifat Levy, Rubén Moreno-Bote, Alexander Ecker, Makoto
Fukushima, Shabnam Kadir, Felix Effenberger, Melanie Woodin, Corette J Wierenga, Kenneth Latimer, Matt Smear, Ruben Portugues, Haruo Hosoya, Vince McGinty, Kenway Louie,
Jonathan Mapelli, Arthur Leblois, Michael Crickmore, Ann Hermundstad, Marc Gershow,
Sergei Gepshtein, Tianyi Mao, Kay Tye, Kaoru Amano, Jan Drugowitsch, Aaron Bornstein,
Alexander Vaughan, Daniel Soudry, Guillaume Hennequin, Robert Rosenbaum, Chad Giusti,
Yingxue Wang, David Bulkin, Steve Ramirez, Il Memming Park, Ashesh Dhawale, Prakash
Kara, Kristina Nielsen, Jan-Alexander Heimel, Robb Rutledge, Alex Nectow, Nathan W
Gouwens, Uygar Sümbül, Steve Chang, Robert G K Munn, Eva Pastalkova, Takahiro Doi,
Laureline Logiaco, Athena Akrami, Zengcai Guo, Wieland Brendel, Gordon J Berman,
Nai Ding, Neda Nategh, Caswell Barry, David Rowland, Dori Derdikman, Richard Naud,
David Markowitz, Masahiko Haruno, Gautam Agarwal, Petr Znamenskiy, Saul Kato, Ana
Amador, Anna Levina, Matthias Hennig, David M Schneider, Martha Bagnall, Benjamin
Scholl, Nilay Yapici, John Tuthill, Nicholas Sofroniew, Andrew Leifer, Anita Disney, Ethan
Bromberg-Martin, Till S Hartmann, Wei-Chung Allen Lee, Vincent Bonin, Alberto Bernacchia, Masayoshi Murakami, Johannes Seelig, Quentin Gaudry, Michael Buice, Edward Zagha, Jianing Yu, Robert Ajeman, Hongdian Yang, Sung E Kwon, Jacob Yates, Shushruth
Shushruth, Omri Barak, Robbe Goris, Evan H Feinberg, Evan Schaffer, Jason Wittenbach, Hokto Kazama, Marcus Benna, Akihiro Funamizu, Jesse Goldberg, Tim Hanks,
Biswa Sengupta, Mimi Lilljeholm, Brian Lau, Anne Collins, Harel Shouval, Annabelle C
Singer, Marcel van Gerven, Jeffrey Seely, Mark Eldridge, Jascha Sohl-Dickstein, Pavan
Ramdya, Luke Chang, Daniel LK Yamins, Arash Afraz, Christopher J Rozell, Scott Linderman, federica lucantonio, Bruno Olshausen, Vladimir Itskov, Pedro D Maia, David Rolnick,
Braden Brinkman, Okito Yamashita, Donna J Calu, David Gire, Yeka Aponte, Becket Ebitz, Ritwik Niyogi, Jordan Taylor, Pengcheng Zhou, Merav Stern, Behrad Noudoost, Sungho
Hong, Daniel Durstewitz, David Pfau, Eugenia Chiappe, Tatiana Engel, Hendrikje Nienborg,
Robert Wilson, Mark Brandon, James Murray, Philipp Berens, Cristina Savin, Thaddeus B
Czuba, Takafumi Arakaki, Bart G Borghuis, John D Murray, Heather Read, Ida Momennejad, Samuel Gershman
Special thanks to Daniel Acuna and Konrad Kording for writing and managing the automated software for reviewer abstract assignment.

COSYNE 2017

xv

About Cosyne

Conference Support
Administrative Support, Registration, Hotels
Leslie Weekes, Cosyne

Social media policy
Cosyne encourages the use of social media before, during, and after the conference, so
long as it falls within the following rules:
• Do not capture or share details of any unpublished data presented at the meeting.
• If you are unsure whether data is unpublished, check with the presenter before sharing
the information.
• Respect presenters’ wishes if they indicate the information presented is not to be
shared.
Stay up to date with Cosyne 2017 #cosyne17

Travel Grants
The Cosyne community is committed to bringing talented scientists together at our annual
meeting, regardless of their ability to afford travel. Thus, a number of travel grants are
awarded to students, postdocs, and PIs for travel to the Cosyne meeting. Each award
covers at least $500 towards travel and meeting attendance costs. Four award granting
programs were available for Cosyne 2017.
The generosity of our sponsors helps make these travel grant programs possible. Cosyne
Travel Grant Programs are supported entirely by the following corporations and foundations:

•
•
•
•

xvi

The Gatsby Charitable Foundation
Burroughs Wellcome Fund
National Science Foundation (NSF)
PLoS

COSYNE 2017

About Cosyne

Cosyne Presenters Travel Grant Program
These grants support early career scientists with highly scored abstracts to enable them to
present their work at the meeting.
The 2017 recipients are:
Alia Abbara, Alon Baram, Joao Barbosa, Michael Beyeler, Kathryn Bonnen, Edmund Chong,
Amelia Christensen, SueYeon Chung, Rui Ponte Costa, Eric Drebitz, Lea Duncker, Yi Gu,
Bettina Hein, Brian Hu, Vijay Mohan K Namboodiri, Alex Kell, Matthew Lovett-Barron,
Gonzalo Mena, Timothy Muller, Devika Narain, Nathan Parker, Eugenio Piasini, Nicolas
Schuck, Artur Speiser, Nicholas Wall, Leila Wehbe, G. Elliott Wimmer, Daniel Wood, and
Ali Yousefi.

Cosyne New Attendees Travel Grant Program
These grants help bring scientists that have not previously attended Cosyne to the meeting
for exchange of ideas with the community.
The 2017 recipients are:
Everton J Agnes, Ari Benjamin, Kevin Bolding, Santiago Cadena, Ioana Calangiu, Angus
Chadwick, Xiaomo Chen, Zachary Davis, Dino Dvorak, Erin Hisey, Chengjie Huang, Nataliya Kraynyukova, Md Nasir Uddin Laskar, Jan-Matthis Lueckmann, Mathias Mahn, Ori
Maoz, Mirjana Maras, Morteza Moazami Goudarzi, Max Nolte, Mariann Oemisch, Stefano
Recanatesi, Evan Remington, Rajeev Rikhye, Nishal Shah, Victoria Shavina, Jake Stroud,
Wenbo Tang, Bradley Theilman, Milenna van Dijk, and Jing Wang.

Cosyne Mentorship Travel Grant Program
These grants provide support for early-career scientists of underrepresented minority groups
to attend the meeting. A Cosyne PI must act as a mentor for these trainees and the program
also is meant to recognize these PIs (“Cosyne Mentors”).
The 2017 Cosyne Mentor and mentee are Dmitri Chklovskii and Sreyas Mohan.

Cosyne Undergraduate Travel Grant Program
These grants help bring promising undergraduate students with strong interest in neuroscience to the meeting.
The 2017 recipients are:
Omer Ashmai, Henry Besser, Shannon Brown, Chelsey Campillo Rodriguez, Isabel D’Alessandro,
Jeremy Delahanty, Ahyeon Hwang, Jessica Priest, Brianna Richardson, Virginia Rutten,
Erica Shook, Lillian Tan, Carlos Velazquez, and Michael Wang.

COSYNE 2017

xvii

About Cosyne

xviii

COSYNE 2017

Program

Program

Note: Printed copies of this document do not contain the abstracts; they can be downloaded at:

http://cosyne.org/c/index.php?title=Cosyne2017_Program

Institutions listed in the program are the primary affiliation of the first author. For the complete list, please consult
the abstracts.

Thursday, 23 February
4.00p

Registration opens

5.00p

Welcome reception

5.45p

Opening remarks

Session 1: Opening session
(Chair: Ilana Witten, Eric Shea-Brown)
6.00p

“Deep” neuroscience in high dimensions
Surya Ganguli, Stanford University (invited) . . . . . . . . . . . . . . . . . . . . . . . . . 27

6.45p

Neuroscience for the 99%: How low tech versions of high tech laboratories enable exploratory research experiments and hands-on instruction
Greg Gage, Backyard Brains (invited) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

8.00p

Poster Session I

Friday, 24 February
7.30a

Continental breakfast

Session 2: Sensory processing
(Chair: Damon Clark)
8.30a

Neuronal mechanisms for dynamic auditory perception
Maria Geffen, University of Pennsylvania (invited) . . . . . . . . . . . . . . . . . . . . . 28

9.15a

Olfactory processing channels organize into functional clusters in higher brain regions
J. Jeanne, M. Fisek, R. Wilson, Harvard University . . . . . . . . . . . . . . . . . . . . . 32

9.30a

The significance of nominally non-responsive activity in auditory perception and behavior
M. Insanally, I. Carcea, B. Albanna, R. Froemke, New York University . . . . . . . . . . . 32

9.45a

Domain-specialised CNNs of realistic depth best explain FFA and PPA representations
K. Storrs, J. Mehrer, A. Walther, N. Kriegeskorte, University of Cambridge . . . . . . . . . 33

10.00a

Coffee break

COSYNE 2017

1

Program
Session 3: Network dynamics
(Chair: Maneesh Sahani)
10.30a

Breaking balance: the space and time of shared cortical variability
Brent Doiron, University of Pittsburgh (invited) . . . . . . . . . . . . . . . . . . . . . . . 28

11.15a

High dimensional geometry of cortical population activity
M. Pachitariu, C. Stringer, N. Steinmetz, M. Dipoppa, S. Schroeder, M. Carandini, K.
Harris, University College London . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

11.30a

From dynamics to computations: a theory of recurrent random networks with low-dimensional
structure
F. Mastrogiuseppe, S. Ostojic, Ecole Normale Superieure . . . . . . . . . . . . . . . . . 34

11.45p

Lunch break

12.30p

Wei Ji Ma, Growing up in science: a scientist’s unofficial story.
Have you ever wondered what your advisor was like as a graduate student? What they
struggled with? What they are struggling with now? This informal discussion for students
and postdocs will not be about science, but about becoming and being a scientist. How
do you deal with your own and others’ expectations, with bad advisors, and with impostor syndrome? How do you keep yourself motivated? Wei Ji Ma will tell his “unofficial”
story of struggles, failures, and doubts, and then guide a conversation about the human
factors that are universal undercurrents of working in academia but that too often remain
unspoken. See also the series website at www.growingupinscience.com.

Session 4: Memory on multiple timescales
(Chair: Stephen Lisberger)
2.00p

Mechanisms of fear control in humans
Elizabeth A Phelps, New York University (invited) . . . . . . . . . . . . . . . . . . . . . . 29

2.45p

Posterior Parietal Cortex conveys sensory history that acts as a prior during a parametric
WM task
A. Akrami, C. Kopec, C. Brody, Princeton University . . . . . . . . . . . . . . . . . . . . . 35

3.00p

A cerebellar model for learning Bayesian statistics of time intervals
D. Narain, M. Jazayeri, Massachusetts Institute of Technology . . . . . . . . . . . . . . . 35

3.15p

Discrete oscillatory bursts for flexible read out from working memory
M. Lundqvist, P. Herman, M. Warden, S. L. Brincat, E. K. Miller, Massachusetts Institute
of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

3.30p

Synaptically imprinted memories reignite bump-attractor dynamics prior to stimulus in a
vsWM task
J. Barbosa, C. Constantinidis, A. Compte, IDIBAPS . . . . . . . . . . . . . . . . . . . . . 36

3.45p

Coffee break

Session 5: Reinforcement learning
(Chair: Angela Langdon)

2

4.15p

Homogeneity and heterogeneity of dopamine signals
Naoshige Uchida, Harvard University (invited) . . . . . . . . . . . . . . . . . . . . . . . 29

5.00p

A midbrain-basal ganglia circuit is critical to externally and internally reinforced vocal
learning.
E. Hisey, M. Kearney, R. Mooney, Duke University . . . . . . . . . . . . . . . . . . . . . . 37

5.15p

Choice-selective sequential activity in cortical neurons that project to the nucleus accumbens
N. Parker, M. Murugan, I. Witten, Princeton University . . . . . . . . . . . . . . . . . . . 38

COSYNE 2017

Program
5.30p

Dinner break

5.30p

Simons Foundation Social

8.00p

Poster Session II

Saturday, 25 February
7.30a

Continental breakfast

Session 6: Motivation and decision making
(Chair: Alex Huk)
8.30a

Neural circuits underlying positive and negative valence
Kay Tye, Massachusetts Institute of Technology (invited) . . . . . . . . . . . . . . . . . . 29

9.15a

A pathway for hunger modulation of learned food cue responses in insular cortex
M. L. Andermann, Y. Livneh, R. Ramesh, C. Burgess, K. Levandowski, J. Madara, H.
Fenselau, G. Goldey, N. Jikomes, J. Resch, B. Lowell, Harvard University . . . . . . . . . 38

9.30a

The stabilized supralinear network replicates neural and performance correlates of attention
G. Lindsay, D. B. Rubin, K. D. Miller, Columbia University . . . . . . . . . . . . . . . . . . 39

9.45a

Low-dimensional state-space trajectory of choice at the population level in area MT
Y. Zhao, J. Yates, I. M. Park, Stony Brook University . . . . . . . . . . . . . . . . . . . . . 39

10.00a

Coffee break

Session 7: Statistical approaches in neuroscience
(Chair: Stephanie Palmer)
10.30a

Extracting low-dimensional structure from high-dimensional neural activity
Jonathan Pillow, Princeton University (invited) . . . . . . . . . . . . . . . . . . . . . . . 30

11.15a

Dimension reduction of multi-trial neural data by tensor decomposition
A. Williams, H. Kim, F. Wang, S. Vyas, K. Shenoy, M. Schnitzer, T. Kolda, S. Ganguli,
Stanford University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

11.30a

Low-rank non-stationary population dynamics can account for robustness to optogenetic
stimulation
L. Duncker, D. O’Shea, W. Goo, K. Shenoy, M. Sahani, Gatsby Computational Neuroscience Unit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

11.45p

Lunch break

12.00p

Luncheon for equality and diversity in science

Session 8: Circuits and behavior
(Chair: Santiago Jaramillo)
2.00p

Neuronal representation of social information in the mouse medial amygdala
Catherine Dulac, Harvard University (invited) . . . . . . . . . . . . . . . . . . . . . . . . 30

2.45p

Hippocampal coding arises from probabilistic self-localization across many ambiguous
environments
I. Kanitscheider, I. R. Fiete, University of Texas at Austin . . . . . . . . . . . . . . . . . . 41

3.00p

Conserved circuits for the parallel neuromodulation of brain state
M. Lovett-Barron, W. E. Allen, A. S. Andalman, I. Kauvar, K. Deisseroth, Stanford University 42

3.15p

Paradoxical effects of activating and inactivating cortical interneurons: a cautionary tale
A. Hasenstaub, E. Phillips, Unversity of California, San Francisco . . . . . . . . . . . . . 42

COSYNE 2017

3

Program
3.30p

Distinct eye movement strategies differentially reshape visual space
D. Wood, P. Ramkumar, J. Glaser, P. Lawlor, K. Kording, M. A. Segraves, Northwestern
University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

3.45p

Coffee break

Session 9: Learning and memory
(Chair: Joel Zylberberg)
4.15p

Brain mechanisms for memory guided decision-making in humans
Daphna Shohamy, Columbia University (invited) . . . . . . . . . . . . . . . . . . . . . . 30

5.00p

Optimal degrees of synaptic connectivity
A. Litwin-Kumar, K. Decker Harris, R. Axel, H. Sompolinsky, L. Abbott, Columbia University 43

5.15p

Meta-reinforcement learning: A bridge between prefrontal and dopaminergic function
J. Wang, Z. Kurth-Nelson, D. Tirumala, J. Leibo, H. Soyer, D. Kumaran, M. Botvinick,
DeepMind . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

5.30p

Dinner break

5.30p

Simons Foundation Theory+Experiment Match Making

8.00p

Poster Session III

Sunday, 26 February
7.30a

Continental breakfast

Session 10: Attention and arousal
(Chair: Tatyana Sharpee)
8.30a

Light sleep
Gero Miesenbock, University of Oxford (invited) . . . . . . . . . . . . . . . . . . . . . . 31

9.15a

Thalamic contributions to neocortico-hippocampal interactions during sleep
C. Varela, M. Wilson, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . 44

9.30a

Humans flexibly incorporate attention-dependent uncertainty into perceptual decisions
and confidence
R. Denison, W. Adler, M. Carrasco, W. Ma, New York University . . . . . . . . . . . . . . 45

9.45a

Layer-specific reorganization of neocortical dendritic inhibition during active wakefulness.
R. Tremblay, W. Munoz, D. Levenstein, B. Rudy, The Neuroscience Institute . . . . . . . . 45

10.00a

Coffee break

Session 11: Sensory-motor integration
(Chair: Philip Sabes)

4

10.30a

Circuit mechanisms for flexible sensory processing in Drosophila
Vanessa Ruta, Rockefeller University (invited) . . . . . . . . . . . . . . . . . . . . . . . 31

11.15a

How could neuroscientists understand a microprocessor?
E. Jonas, K. Kording, University of California, Berkeley . . . . . . . . . . . . . . . . . . . 46

11.30a

Quantitatively-tuned internal predictions in a Drosophila visuomotor network
A. Kim, L. Fenk, C. Lyu, G. Maimon, Rockefeller University . . . . . . . . . . . . . . . . . 46

11.45p

Lunch break

COSYNE 2017

Program

Session 12: Learning in networks
(Chair: Julijana Gjorgjieva)
2.00p

Learning nonlinear dynamics in spiking networks with local plasticity rules
A. Alemi, J. Slotine, C. Machens, S. Deneve, Ecole Normale Superieure . . . . . . . . . . 47

2.15p

The emergence of distributed functional networks in the early developing cortex
B. Hein, G. B. Smith, D. E. Whitney, P. Huelsdunk, D. Fitzpatrick, M. Kaschube, FIAS . . . 48

2.30p

Modular structures in recurrent neural networks trained for many cognitive tasks
G. R. Yang, H. F. Song, W. Newsome, X. Wang, New York University . . . . . . . . . . . 48

2.45p

Bridging the gap between deep learning and neuroscience
Yoshua Bengio, University of Montreal (invited) . . . . . . . . . . . . . . . . . . . . . . . 31

3.30p

Closing remarks

COSYNE 2017

5

Posters I

Poster Session I

7:30 pm Thursday 25 February

I-1. Exponential increase of storage capacity in an expansive autoencoder: an analytical lower bound
Alia Abbara, Alireza Alemi, Ecole Normale Superieure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
I-2. Continuous-time point-process GPFA using sparse variational methods
Vincent Adam, Lea Duncker, Maneesh Sahani, Gatsby Computational Neuroscience Unit . . . . . . . . . 49
I-3. A planning game reveals individual differences in strategy that are highly constrained
Gautam Agarwal, Tiago Quendera, Marta Ferreira, Zachary Mainen, Champalimaud Neuroscience Program 50
I-4. Codependent synaptic plasticity for stable dynamics and efficient learning
Everton J Agnes, Tim P Vogels, University of Oxford . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
I-5. C. elegans locomotion is governed by a 6D quasi-Hamiltonian chaotic attractor
Tosif Ahamed, Greg Stephens, Okinawa Institute of Science and Technology . . . . . . . . . . . . . . . . 51
I-6. Observing the observer observing: forgetful world modelling in a self-stimulation task
Sanjeevan Ahilan, Rebecca B Solomon, Kent Conover, Ritwik Niyogi, Peter Shizgal, Peter Dayan, Gatsby
Computational Neuroscience Unit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
I-7. Ultra-fast whole brain imaging during behavior in adult Drosophila
Sophie Aimon, Takeo Katsuki, Logan Grosenick, Michael Broxton, Karl Deisseroth, Terrence Sejnowski,
Ralph Greenspan, University of California, San Diego . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
I-8. All-optical mapping of local circuit connectivity in vivo
Laurence Aitchison, Philippe Castonguay, Jinyao Yan, Adam Packer, Lloyd Russell, Michael Hausser,
Srini Turaga, University of Cambridge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
I-9. STDP under physiological calcium concentration and implications for memory maintenance
Johnatan Aljadeff, Yanis Inglebert, Dominique Debanne, Nicolas Brunel, University of Chicago . . . . . . 54
I-10. Global representations of goal-directed behavior in distinct cell types of mouse neocortex
William E Allen*, Isaac Kauvar*, Michael Z Chen, Ethan Richman, Liqun Luo, Karl Deisseroth, Stanford
University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
I-11. Medial frontal circuits encode reward context and control consummatory behavior
Linda Amarante, Marrcelo Caetano, Nicole Horst, Mark Laubach, American University . . . . . . . . . . . 55
I-12. Somatosensory cortex plays an essential role in forelimb motor adaptation in mice
Mackenzie Amoroso, Alexander Mathis, Naoshige Uchida, Harvard University . . . . . . . . . . . . . . . 55
I-13. A model of high-acuity vision in the presence of fixational eye movements
Alexander Anderson, Kavitha Ratnam, Austin Roorda, Bruno Olshausen, University of California, Berkeley 56
I-14. Cortical gain adaptation to extract signals from background noise
Chris Angeloni, Mark Aizenberg, Maria Geffen, University of Pennsylvania . . . . . . . . . . . . . . . . . 56
I-15. Marked point process filter for clusterless, adaptive encoding for real-time decoding
Kensuke Arai, Daniel Liu, Loren Frank, Uri Eden, Boston University . . . . . . . . . . . . . . . . . . . . . 57
I-16. Specific activity patterns are reinforced via closed-loop pairing with phasic VTA activation
Vivek Athalye, Fernando Santos, Jose M. Carmena, Rui Costa, Champalimaud Neuroscience Programme 57
I-17. Functional organization of the rat whisker pad
Rony Azouz, Erez Gugig, Praveen Kuruppath, Ben-Gurion University of the Negev

. . . . . . . . . . . . 58

I-18. Subspace alignment by inhibitory plasticity recovers excitation-inhibition balance
Itamar Landau, Haim Sompolinsky, Hebrew University of Jerusalem . . . . . . . . . . . . . . . . . . . . 59
I-19. Beyond reward prediction errors: human striatum updates rule values during learning
Ian Ballard, Eric Miller, Steven Piantadosi, Noah Goodman, Samuel McClure, Stanford University

. . . . 59

I-20. Computation of concentration-invariant odor identity in the early olfactory system
Arkarup Banerjee, Honggoo Chae, Dinu F Albeanu, Watson School of Biological Sciences . . . . . . . . 60

6

COSYNE 2017

Posters I
I-21. Geometry of V1’s response to non-Cartesian versus Cartesian stimuli
Faisal Baqai, Janne Kauttonen, Pati Stan, Tai Sing Lee, Sandra Kuhlman, Carnegie Mellon University . . 60
I-22. Intuitive planning: global navigation of cognitive maps with grid-like global representations
Alon Baram, Timothy H Muller, Timothy EJ Behrens, University of Oxford . . . . . . . . . . . . . . . . . . 61
I-23. Cerebellar learning using perturbations
Boris Barbour, Guy Bouvier, Claudia Clopath, Celian Bimbard, Jean-Pierre Nadal, Nicolas Brunel, Vincent
Hakim, Ecole Normale Superieure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
I-24. Noise-limited inference predicts facilitatory normalization in the visual cortex
Gregory Barello, Yashar Ahmadian, University of Oregon . . . . . . . . . . . . . . . . . . . . . . . . . . 62
I-25. Information scaling in large neural ensembles from the macaque prefrontal cortex
Ramon Bartolo, Richard Saunders, Philip Browning, Andrew Mitz, Bruno Averbeck, NIMH/NIH . . . . . . 63
I-26. Pitch recognition with sparse-coding recurrent neural network
Oded Barzelay, Omri Barak, Miriam Furst, Technion - Israel Institute of Technology . . . . . . . . . . . . 63
I-27. Using bayesian inference to estimate receptive fields from a small number of spikes
Giacomo Bassetto, Jakob Macke, research center caesar . . . . . . . . . . . . . . . . . . . . . . . . . . 64
I-28. The carrot or the stick: opposite effects of rewards and punishments on human vigor
Ulrik Beierholm, Benjamin Griffiths, Durham University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
I-29. Stabilizing patterns in time: neural network approach
Nadav Ben-Shushan, Misha Tsodyks, Weizmann Institute of Science . . . . . . . . . . . . . . . . . . . . 65
I-30. Better spike encoding with neural nets and boosted trees
Ari Benjamin, Hugo Fernades, Konrad Kording, Northwestern University . . . . . . . . . . . . . . . . . . 65
I-31. Visibility of eigen-distortions of hierarchical models
Alexander Berardino, Valero Laparra, Johannes Balle, Eero P Simoncelli, New York University . . . . . . 66
I-32. Standardizing and benchmarking data analysis for calcium imaging
Philipp Berens, Lucas Theis, Jasmine T Stone, Nicholas Sofroniew, Andreas Tolias, Matthias Bethge,
Jeremy Freeman, University of Tuebingen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
I-33. Dale’s principle preserves sequentiality in neural circuits
Alberto Bernacchia, Jozsef Fiser, Guillaume Hennequin, Mate Lengyel, University of Cambridge . . . . . 67
I-34. Modeling the perceptual experience of retinal prosthesis patients
Michael Beyeler, Ariel Rokem, Geoffrey M Boynton, Ione Fine, University of Washington

. . . . . . . . . 68

I-35. Extracting stable representations of neural population state from unstable neural recordings
William Bishop, Alan Degenhart, Emily Oby, Aaron Batista, Steven Chase, Byron Yu, Carnegie Mellon
University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
I-36. Activating distinct neuronal subtypes in auditory cortex differentially affects collicular responses
Jennifer Blackwell, Mark Aizenberg, Winnie Rao, Ryan Natan, Maria Geffen, University of Pennsylvania . 69
I-37. Olfactory navigation: information theoretic scene analysis motivating a history-based algorithm
Sebastian Boie, John Crimaldi, Bard Ermentrout, Margaret McHugh, Katherine Nagel, Jonathan Victor,
Cornell University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
I-38. Recurrent circuitry in piriform cortex implements concentration-invariant odor identification
Kevin Bolding, Kevin Franks, Duke University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
I-39. Encoding and decoding in neural populations with non-Gaussian tuning: the example of 3D motion
Kathryn Bonnen, Thaddeus B Czuba, Adam Kohn, Lawrence K Cormack, Alex Huk, University of Texas
at Austin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
I-40. Mapping the latent space of human auditory functional special- ization with binary sparse coding
Moritz Boos, Joerg Luecke, Jochem W. Rieger, Applied Neurocognitive Psychology Lab . . . . . . . . . . 71
I-41. The rates of visual information encoding in parallel retinal bipolar cell pathways
Bart G Borghuis, Charles Ratliff, University of Louisville . . . . . . . . . . . . . . . . . . . . . . . . . . . 72

COSYNE 2017

7

Posters I
I-42. Examining weight perturbations in plastic neural networks
Colin Bredenberg, Brent Doiron, University of Pittsburgh . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
I-43. Explicit calculation of effective interactions in strongly-coupled networks
Braden Brinkman, Fred Rieke, Eric Shea-Brown, Michae Buice, University of Washington . . . . . . . . . 73
I-44. The Allen Brain Observatory
Michael Buice, Saskia DeVries, Jerome Lecoq, David Feng, Allen Institute for Brain Science . . . . . . . 73
I-45. Characterizing nonlinear neuronal computation within a single stage of processing
Daniel A Butts, Gregory E Perrin, Yuwei Cui, Matthew Whiteway, Jonathan Demb, Josh Singer, University
of Maryland . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
I-46. A goal-driven deep learning approach for V1 system identification
Santiago Cadena, Alexander Ecker, George Denfield, Edgar Walker, Andreas Tolias, Matthias Bethge,
University of Tuebingen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
I-47. A principled model of robust neural dynamics in premotor cortex.
Nuno Calaim*, Pierre-Etienne Fiquet*, Sophie Deneve, Christian Machens, Champalimaud Centre for the
Unknown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
I-48. Neural population dynamics of short-term and long-term learning in the primate prefrontal cortex
Ioana Calangiu, Valerio Mante, Institute for Neuroinformatics . . . . . . . . . . . . . . . . . . . . . . . . 76
I-49. Estimating behavioral state for sensorimotor transformations
Adam Calhoun, Pip Coen, Jonathan Pillow, Mala Murthy, Princeton University . . . . . . . . . . . . . . . 76
I-50. Dynamics of cortical activity during behavioral engagement and auditory perception
Ioana Carcea, Michele Insanally, Robert Froemke, New York University . . . . . . . . . . . . . . . . . . . 77
I-51. Sparse synaptic connectivity is required for pattern separation in divergent feedforward networks
Alex Cayco Gajic, Claudia Clopath, R Angus Silver, University College London . . . . . . . . . . . . . . . 77
I-52. Learning modifies the collective dynamics of cortical interneuron networks
Angus Chadwick, Adil Khan, Jasper Poort, Antonin Blot, Thomas Mrsic-Flogel, Sonja B Hofer, Maneesh
Sahani, Gatsby Computational Neuroscience Unit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
I-53. Variational information bottleneck: a general framework for efficient, task-relevant coding
Matthew Chalk, Olivier Marre, Gasper Tkacik, Institute of Science and Technology Austria . . . . . . . . 78
I-54. Continuous-time partitioning of binned spike counts
Adam S Charles, Jonathan Pillow, Princeton University . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
I-55. Articulatory gesture encoding in human sensorimotor cortex during continuous speech production
Josh Chartier, Gopala Krishna Anumanchipalli, Edward Chang, University of California, San Francisco . . 80
I-56. Unsupervised latent variable extraction from neural data to characterize processing across states
Rishidev Chaudhuri, Berk Gercek, Biraj Pandey, Ila Fiete, University of Texas at Austin . . . . . . . . . . 80
I-57. Target masking by natural backgrounds in primate V1 predicts behavioral detection sensitivity
Spencer Chen, Yoon Bai, Stephen Sebastian, Yuzhi Chen, Wilson Geisler, Eyal Seidemann, University of
Texas at Austin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
I-58. Repeated membrane potential patterns in the hippocampus of awake mice are related to spiking
Ilya Kolb, Giovanni Talei Franzesi, Michael Wang, Suhasa Kodandaramaiah, Craig Forest, Edward Boyden, Annabelle C Singer, Georgia Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . 82
I-59. Perturbation of visual salience representation in FEF following inactivation of parietal cortex
Xiaomo Chen, Marc Zirnsak, Stephen Lomber, Tirin Moore, Stanford University . . . . . . . . . . . . . . 82
I-60. Emergence of foveal image sampling from learning to attend in visual scenes
Brian Cheung, Eric Weiss, Bruno Olshausen, University of California, Berkeley . . . . . . . . . . . . . . 83
I-61. Effect of mnemonic strategies on tuning of prefrontal neurons
Feng-Kuei Chiang, Joni Wallis, University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . 83

8

COSYNE 2017

Posters I
I-62. Blind nonnegative source separation using biological neural networks
Dmitri Chklovskii, Cengiz Pehlevan, Sreyas Mohan, Simons Foundation . . . . . . . . . . . . . . . . . . 84
I-63. Predictive coding in area V4 as a mechanism for recognition of partially occluded shapes
Hannah Choi, Anitha Pasupathy, Eric Shea-Brown, University of Washington . . . . . . . . . . . . . . . . 84
I-64. Mapping sensory inputs to behavior using patterned optogenetics
Edmund Chong, Anthony Oganov, Johannes Kappel, Dmitry Rinberg, New York University . . . . . . . . 85
I-65. Topology of stimulus space via directed network persistent homology
Samir Chowdhury, Facundo Memoli, Bowen Dai, Ohio State University . . . . . . . . . . . . . . . . . . . 85
I-66. Model-based analysis of state-dependent modulation in rodent ventral stream
Amelia Christensen, Jonathan Pillow, Stanford University . . . . . . . . . . . . . . . . . . . . . . . . . . 86
I-67. Representation and readout of object manifolds
SueYeon Chung, Daniel Lee, Haim Sompolinsky, Harvard University . . . . . . . . . . . . . . . . . . . . 86
I-68. Mean and variance adaptation in auditory receptor neurons of Drosophila
Jan Clemens, Nofar Ozeri, Mala Murthy, Princeton University . . . . . . . . . . . . . . . . . . . . . . . . 87
I-69. Rate-distortion analysis of R1-R6 fly photoreceptors
Daniel Coca, Carlos Luna Ortiz, Inaki Esnaola, University of Sheffield

. . . . . . . . . . . . . . . . . . . 87

I-70. Variability of V1 population responses to natural images reflects probabilistic inference
Ruben Coen-Cagli, Adam Kohn, Albert Einstein College of Medicine . . . . . . . . . . . . . . . . . . . . 88
I-71. Circuit mechanisms for flexible sensorimotor processing in Drosophila
Raphael Cohn, Aryeh Zolin, Adam Samulak, Ianessa Morantte, Vanessa Ruta, Rockefeller University . . 88
I-72. Macaque orbitofrontal cortex shows context-dependent adaptation to changes in minimum value
Katherine Conen, Camillo Padoa-Schioppa, Washington University in St Louis . . . . . . . . . . . . . . . 89
I-73. Closed-loop optimization of neural dynamics: Towards linking neural state and behavior
Mark Connolly, Robert Gross, Babak Mahmoudi, Emory University . . . . . . . . . . . . . . . . . . . . . 90
I-74. Economic choices reveal risk aversion and probability distortion in rats
Christine Constantinople, Carlos Brody, Princeton University . . . . . . . . . . . . . . . . . . . . . . . . 90
I-75. Statistical long-term synaptic plasticity predicts optimal excitation-inhibition balance
Rui Ponte Costa, Zahid Padamsey, James D’Amour, Nigel Emptage, Robert Froemke, Tim P Vogels,
University of Oxford . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
I-76. Distance covariance analysis
Benjamin Cowley, Joao Semedo, Amin Zandvakili, Matthew Smith, Adam Kohn, Byron Yu, Carnegie
Mellon University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
I-77. Two distinct motion detection algorithms regulate turning and walking speed in Drosophila
Matthew S Creamer, Omer Mano, Damon Clark, Yale University . . . . . . . . . . . . . . . . . . . . . . . 92
I-78. Adaptive neural control counters unexpected reaching dynamics
Frederic Crevecoeur, Philippe Lefevre, Universite Catholique de Louvain . . . . . . . . . . . . . . . . . . 92
I-79. Multisensory control of orientation in tethered flying Drosophila
Timothy Currier, Katherine Nagel, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
I-80. Spontaneous cortical waves in Area MT of the awake marmoset modulate neural and perceptual
sensitivi
Zachary Davis, Lyle Muller, Terrence Sejnowski, Julio Martinez-Trujillo, John Reynolds, Salk Institute for
Biological Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
I-81. Semantic representation in the human brain during reading and listening to stories and passages
Fatma Deniz, Alexander Huth, Jack Gallant, University of California, Berkeley . . . . . . . . . . . . . . . 94
I-82. Grid cells store local positional information
Dori Derdikman, Omri Barak, Kate Jeffery, Revekka Ismakov, Technion - Israel Institute of Technology . . 94

COSYNE 2017

9

Posters I
I-83. Task-relevant motor variability is dynamically regulated by reward history
Ashesh Dhawale, Yohsuke Miyamoto, Maurice Smith, Bence Olveczky, Harvard University . . . . . . . . 95
I-84. Signal transmission between monkey areas V2 and V4 is causally dependent on phase synchronization
Eric Drebitz, Lukas-Paul Rausch, Heiko Stemmann, Andreas Kreiter, University of Bremen . . . . . . . . 95
I-85. The temporal tuning of Drosophila motion detectors is given by the dynamics of their input elements
Michael Drews, Alexander Arenz, Florian Richter, Georg Ammer, Alexander Borst, Max Planck Institute
of Neurobiology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
I-86. Scalable variational inference for low-rank receptive fields with nonstationary smoothness
Lea Duncker, Sneha Ravi, Greg Field, Jonathan Pillow, Gatsby Computational Neuroscience Unit . . . . 97
I-87. Control of recollection by slow gamma dominating medium gamma in hippocampus CA1
Dino Dvorak, Basma Radwan, Fraser T Sparks, Zoe Talbot, Andre Fenton, New York University

. . . . . 97

I-88. Bottom-up salience drives choice during exploration
Becket Ebitz, Tirin Moore, Tim Buschman, Princeton University . . . . . . . . . . . . . . . . . . . . . . . 98
I-89. GSM=SSN: recurrent neural circuits optimised for probabilistic inference
Rodrigo Echeveste, Guillaume Hennequin, Mate Lengyel, University of Cambridge . . . . . . . . . . . . 98
I-90. Learning local statistics in serial electron microscopy recordings of neural tissue
Felix Effenberger, Christopher Hillar, Xiaotang Lu, Richard Schalek, Jeff Lichtman, Frankfurt Institute for
Advanced Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
I-91. Cortical neurons regulate dynamics to integrate sensory input into motor plans
Seth Egger, Chia-Jung Chang, Mehrdad Jazayeri, Massachusetts Institute of Technology . . . . . . . . . 100
I-92. Dynamic integration onset during perceptual decision making in trained recurrent neural networks
Daniel Ehrlich, Hyojung Seo, Daeyeol Lee, John D Murray, Yale University . . . . . . . . . . . . . . . . . 100
I-93. Contribution of cortical On-Off dynamics to spike-count variability and correlations
Tatiana Engel, Nicholas Steinmetz, Tirin Moore, Kwabena Boahen, Stanford University . . . . . . . . . . 101
I-94. Dimensionality and entropy of spontaneous and evoked rate activity
Rainer Engelken, Fred Wolf, Max Planck Institute for Dynamics and Self-Organization . . . . . . . . . . . 101
I-95. Deep learning approach towards generating neuronal morphology
Roozbeh Farhoodi, Pavan Ramkumar, Konrad Kording, Sharif University of Technology . . . . . . . . . . 102
I-96. Cherchez les auxiliaires: interneurons are key for high-capacity attractor networks
Dylan Festa, Guillaume Hennequin, Mate Lengyel, University of Cambridge . . . . . . . . . . . . . . . . 102
I-97. Oscillations in local field potentials and spike times link the attention network to behavior
Ian Fiebelkorn, Mark Pinsk, Sabine Kastner, Princeton University . . . . . . . . . . . . . . . . . . . . . . 103
I-98. Estimates and updates of reward probabilities in frontal and parietal cortex
Nicholas C Foley, Jacqueline Gottlieb, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . 103
I-99. Thalamus relays choice and reward-related signals to layer 1 of visual cortex
Marina Fridman, Tiago Marques, Dmitry Kobak, Christian Machens, Leopoldo Petreanu, Champalimaud
Centre for the Unknown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
I-100. Fast active set methods for online deconvolution of calcium imaging data
Johannes Friedrich, Pengcheng Zhou, Liam Paninski, Columbia University . . . . . . . . . . . . . . . . . 105
I-101. Modeling experience and time pressure in human gameplay
Gianni Galbiati, Yunqi Li, Bas van Opheusden, Weiji Ma, New York University . . . . . . . . . . . . . . . 105
I-102. Population level trial-to-trial variability as a signature of neural dynamics
Aniruddh Galgali, Valerio Mante, ETH Zurich . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
I-103. The development of spatial acuity in mouse visual cortex
Sunil Gandhi, Dario Figueroa Velez, Melissa Davis, University of California, Irvine . . . . . . . . . . . . . 106

10

COSYNE 2017

Posters I
I-104. Robust estimation of calcium transients by modeling contamination
Jeff Gauthier, Adam S Charles, Jonathan Pillow, David Tank, Princeton University . . . . . . . . . . . . . 107
I-105. A biologically plausible neural network for similarity-based clustering
Alex Genkin, Cengiz Pehlevan, Dmitri Chklovskii, New York University . . . . . . . . . . . . . . . . . . . 107
I-106. Behavioral adaptation to variance in sensory input
Marc Gershow, Ruben Gepner, Jason Wolk, Digvijay S Wadekar, New York University . . . . . . . . . . . 108
I-107. Modeling dispersion improves decoding of population neural responses
Abed Ghanbari, Ian Stevenson, University of Connecticut . . . . . . . . . . . . . . . . . . . . . . . . . . 108
I-108. Probabilistic inference over switching network dynamics.
Aram Giahi, Xaq Pitkow, Baylor College of Medicine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
I-109. Learning a non-linear dynamical system in a recurrent spiking neural network
Aditya Gilra, Wulfram Gerstner, Ecole Polytechnique Federale de Lausanne . . . . . . . . . . . . . . . . 109
I-110. CaImAn: An open source toolbox for large scale calcium imaging data analysis on standalone
machines
Andrea Giovannucci, Johannes Friedrich, Ben Deverett, Valentina Staneva, Dmitri Chklovskii, Eftychios
Pnevmatikakis, Simons Foundation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
I-111. Modern machine learning tools for neural decoding
Joshua Glaser, Raeed Chowdhury, Matthew Perich, Lee Miller, Konrad Kording, Northwestern University

110

I-112. Modelling large-scale communities requires effective connectivity
Katharina Glomb, Adrian Ponce-Alvarez, Matthieu Gilson, Petra Ritter, Gustavo Deco, Universitat Pompeu
Fabra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
I-113. Neuronal circuit analysis with spike-triggered non-negative matrix factorization
Tim Gollisch, Jian K. Liu, Helene M Schreyer, Arno Onken, Fernando Rozenblit, Mohammad H Khani,
Stefano Panzeri, University Medical Center Goettingen . . . . . . . . . . . . . . . . . . . . . . . . . . . 112

COSYNE 2017

11

Posters II

Poster Session II

7:30 pm Friday 26 February

II-1. Learning by neural reassociation
Matthew Golub, Patrick Sadtler, Kristin Quick, Stephen Ryu, Elizabeth Tyler-Kabara, Aaron Batista, Steven
Chase, Byron Yu, Carnegie Mellon University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
II-2. Optimized computation of binocular disparity by populations of simple and complex cells
Nuno Goncalves, Andrew Welchman, University of Cambridge . . . . . . . . . . . . . . . . . . . . . . . 113
II-3. Flexible Bayesian inference for mechanistic models of neural dynamics
Pedro J Goncalves, Jan-Matthis Lueckmann, Giacomo Bassetto, Marcel Nonnenmacher, Jakob Macke,
research center caesar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
II-4. Slow gain fluctuations limit temporal integration in visual cortex
Robbe Goris, Corey M Ziemba, J Anthony Movshon, Eero P Simoncelli, University of Texas at Austin . . . 114
II-5. Striatal and orbitofrontal network dynamics differentially encode time
Vishwa Goudar, Konstantin Bakhurin, Dean Buonomano, Sotiris Masmanidis, University of California, Los
Angeles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
II-6. Micro-organization of grid cells in layer II of the medial entorhinal cortex
Yi Gu, Amina Kinkhabwala, Cristina Domnisoru, Jeff Gauthier, David Tank, Princeton University . . . . . 115
II-7. Neural processing of color in the larval zebrafish brain
Drago Guggiana-Nilo, Clemens Riegler, Florian Engert, Harvard University . . . . . . . . . . . . . . . . . 116
II-8. Computational model of spatio-temporal coding in CA3 using speed-dependent theta oscillation
Caroline Haimerl, David Angulo-Garcia, Alessandro Torcini, Rosa Cossart, Arnaud Malvache, New York
University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
II-9. Selective dopamine signaling underlies bidirectional synaptic plasticity in a learning center
Annie Handler, Andrew Siliciano, Raphael Cohn, Ianessa Morantte, Vanessa Ruta, Rockefeller University 117
II-10. Discrete modes of social information processing predict individual behavior of fish in a group
Roy Harpaz, Gasper Tkacik, Elad Schneidman, Weizmann Institute of Science . . . . . . . . . . . . . . 117
II-11. Dissecting sensorimotor signals in area LIP during oculomotor persistent activity
Eric Hart, Leor Katz, Alex Huk, University of Texas at Austin . . . . . . . . . . . . . . . . . . . . . . . . . 118
II-12. An energy-accuracy tradeoff in subneuronal molecular sensing
Sarah Harvey, Subhaneil Lahiri, Surya Ganguli, Stanford University . . . . . . . . . . . . . . . . . . . . . 118
II-13. Robust object learning with cross-cortical column connections
Jeff Hawkins, Yuwei Cui, Subutai Ahmad, Nathanael Romano, Marcus Lewis, Numenta, Inc . . . . . . . . 119
II-14. Predicting neural activity in behaviorally irrelevant dimensions
Jay Hennig, Matthew Golub, Peter Lund, Patrick Sadtler, Kristin Quick, Stephen Ryu, Elizabeth TylerKabara, Aaron Batista, Byron Yu, Steven Chase, Carnegie Mellon University . . . . . . . . . . . . . . . . 120
II-15. Human noise blindness drives suboptimal cognitive inference
Santiago Herce Castanon, Dan Bang, Rani Moran, Jacqueline Ding, Tobias Egner, Christopher Summerfield, University of Oxford . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
II-16. Statistical inference under resource constraints in dynamic environments
Ann Hermundstad, Wiktor Mlynarski, Janelia Research Campus . . . . . . . . . . . . . . . . . . . . . . 121
II-17. Detailed dendritic excitatory/inhibitory balance through heterosynaptic STDP
Naoki Hiratani, Tomoki Fukai, RIKEN Brain Science Institute . . . . . . . . . . . . . . . . . . . . . . . . 121
II-18. Instability of the generalized linear model for spike trains
David Hocker, Il Memming Park, Stony Brook University . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
II-19. Amplitude modulation encoding and decoding in mouse versus primate auditory cortex
Nerissa Hoglen, Elizabeth AK Phillips, Phillip Larimer, Brian J Malone, Andrea Hasenstaub, University of
California, San Francisco . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

12

COSYNE 2017

Posters II
II-20. Proto-object based contour detection and figure-ground segmentation
Brian Hu, Rudiger von der Heydt, Ernst Niebur, Johns Hopkins University . . . . . . . . . . . . . . . . . 123
II-21. Modeling within and across area neuronal variability in the visual system
Chengcheng Huang, Douglas Ruff, Marlene Cohen, Brent Doiron, University of Pittsburgh . . . . . . . . 123
II-22. Serotonergic SK channel modulation promotes adaptive optimized coding of natural stimuli
Chengjie Huang, Diana Martinez, Michael G Metzen, Maurice Chacron, McGill University . . . . . . . . . 124
II-23. Mapping brain-wide corticocortical projections at single-cell resolution by barcoded RNA sequencing
Longwen Huang, Justus Kebschull, Anthony Zador, Cold Spring Harbor Laboratory . . . . . . . . . . . . 124
II-24. Gradient based learning for spiking neural networks
Dongsung Huh, Terrence Sejnowski, Salk Institute for Biological Studies . . . . . . . . . . . . . . . . . . 125
II-25. Probabilistic models for neural populations that naturally capture global coupling and criticality
Jan Humplik, Gasper Tkacik, Institute of Science and Technology Austria . . . . . . . . . . . . . . . . . . 125
II-26. Deep learning in spiking LIF neurons
Eric Hunsberger, Chris Eliasmith, University of Waterloo . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
II-27. Rats decisions flexibly integrate sensory information and recent history of outcomes
Alexandre Hyafil, Ainhoa Hermoso-Mendizabal, Pavel E Rueda-Orozco, Santiago Jaramillo, David Robbe,
Jaime De La Rocha, Idibaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
II-28. Suppression-based visual coding and feature extraction of the deep superior colliculus of the mouse
Shinya Ito, David Feldheim, Alan Litke, University of California, Santa Cruz . . . . . . . . . . . . . . . . . 127
II-29. Extending Levelt’s propositions to perceptual multistability involving interocular grouping
Alain Jacot-Guillarmod, Yunjiao Wang, Claudia Pedroza, Haluk Ogmen, Kresimir Josic, Zachary P Kilpatrick, Lausanne University Hospital . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
II-30. Thalamo-cortical computations underlying decision making, conflict resolution, and confidence
Jorge Jaramillo, Xiao-Jing Wang, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
II-31. A Bayesian learning rule for spiking neurons
Jannes Jegminat, Jean-Pascal Pfister, ETH Zurich . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
II-32. The SOM-PV circuit motif appears designed to scale pyramidal neuron responses
Lei Jin, Bardia F Behabadi, Monika Jadi, Bartlett W Mel, University of Southern California . . . . . . . . . 129
II-33. Neuronal ensemble dynamics in medial orbitofrontal cortex during cue-reward associative learning
Vijay Mohan K Namboodiri, Jose Rodriguez Romaguera, Garret Stuber, University of North Carolina at
Chapel Hill . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
II-34. Sensory processing by deep networks with layer-localized learning
Jonathan Kadmon, Haim Sompolinsky, Hebrew University of Jerusalem . . . . . . . . . . . . . . . . . . 130
II-35. Devaluation modulates representations of expected outcome identity in orbitofrontal cortex
Thorsten Kahnt, James Howard, Northwestern University . . . . . . . . . . . . . . . . . . . . . . . . . . 130
II-36. Coupling between attractor networks naturally generates a discrete grid cell hierarchy
Louis Kang, Vijay Balasubramanian, University of Pennsylvania . . . . . . . . . . . . . . . . . . . . . . . 131
II-37. Evidence that feedback is required for object identity inferences computed by the ventral stream
Kohitij Kar, Jonas Kubilius, Elias B Issa, Kailyn Schmidt, James DiCarlo, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
II-38. Information socialtaxis: efficient collective foraging in groups of information seeking agents
Ehud Karpas, Adi Shklarsh, Elad Schneidman, Weizmann Institute of Science . . . . . . . . . . . . . . . 132
II-39. The branched spline attractor: whole-brain dynamics inform a theory of behavior sequence generation
Saul Kato, University of California, San Francisco . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
II-40. Neuron-level resolution of activity boundaries using calcium imaging
Matthew Kaufman, Simon Musall, Daniel Barabasi, Anne Churchland, Cold Spring Harbor Laboratory . . 133

COSYNE 2017

13

Posters II
II-41. Boundary-tethered grid shifts reproduce effects of environment deformations on grid and place cells
Alex Keinath, Russell Epstein, Vijay Balasubramanian, University of Pennsylvania . . . . . . . . . . . . . 134
II-42. Robustness to real-world background noise increases from primary to non-primary auditory cortex
Alexander Kell, Josh McDermott, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . . 134
II-43. Olfactory perceptual space and predicting olfactory percepts from molecular structure
Daniel Kepple, Alexei Koulakov, Cold Spring Harbor Laboratory . . . . . . . . . . . . . . . . . . . . . . . 135
II-44. Distinct neural encoding schemes emerge for actions generated by the same effector
Preeya Khanna, Vivek Athalye, Rui M Costa, Jose M Carmena, University of California, Berkeley . . . . . 135
II-45. Neural discrimination of sound category utilizes high-order sound statistics in the central auditory
Fatemeh Khatami, Mina Sadeghi, Heather Read, Ian Stevenson, Monty Escabi, University of Connecticut 136
II-46. Metaplasticity for a changing world
Peyman Khorsand, Alireza Soltani, Dartmouth College . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
II-47. Visual response similarity predicts the strength of PV-pyramidal neuron connections in mouse V1
Meanhwan Kim, Petr Znamenskiy, Maria Florencia Iacaruso, Sonja Hofer, Thomas Mrsic-Flogel, University of Basel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
II-48. Human brain networks that dynamically encode mood
Lowry Kirkby, Francisco Luongo, Mor Nahum, Tom Van Vleet, Morgan Lee, Heather Dawes, Edward
Chang, Vikaas Sohal, University of California, San Francisco . . . . . . . . . . . . . . . . . . . . . . . . 137
II-49. Flexible information processing on top of dynamical reference states in neuronal networks
Christoph Kirst, Rockefeller University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
II-50. Dynamic modulation of spike probability and precision in feed-forward hippocampal circuits
Vitaly Klyachko, Sarah Wahlstrom Helgren, Washington University . . . . . . . . . . . . . . . . . . . . . 139
II-51. Efficient spatiotemporal assessment of naturalistic visual selection behavior
Jonas Knoell, Jonathan Pillow, Alex Huk, University of Texas at Austin . . . . . . . . . . . . . . . . . . . 139
II-52. Facilitation of pattern and contour selectivity by excitatory intra-cortical circuits
Erin Koch, Jianzhong Jin, Jose Manuel Alonso, Qasim Zaidi, State University of New York

. . . . . . . . 140

II-53. How neuronal and synaptic nonlinearities determine the stimulus susceptibility of cortical networks
Sara Konrad, Tatjana Tchumatchenko, Max Planck Institute for Brain Research . . . . . . . . . . . . . . 140
II-54. The stabilized supralinear network supports bistable, oscillatory and persistent activity
Nataliya Kraynyukova, Tatjana Tchumatchenko, Max Planck Institute for Brain Research . . . . . . . . . 141
II-55. Dynamics of partially structured recurrent networks with bidirectional correlations
Alexander Kuczala, Tatyana Sharpee, Salk Institute for Biological Studies . . . . . . . . . . . . . . . . . 141
II-56. Low-dimensional geometry of stimuli shapes the information content of a neural code
Alex Kunin, Vladimir Itskov, Pennsylvania State University . . . . . . . . . . . . . . . . . . . . . . . . . . 142
II-57. A complete map of M1 population representations during posture and reaching
Hagai Lalazar, LF Abbott, Eilon Vaadia, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . 142
II-58. Adaptive optimal training of animal behavior
Ji Hyun Bak, Jung Yoon Choi, Athena Akrami, Ilana Witten, Jonathan Pillow, Korea Institute for Advanced
Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
II-59. Dynamic neural representation of reward predictions in rat ventral striatum during learning
Angela Langdon, Yuji Takahashi, Matthew Roesch, Geoffrey Schoenbaum, Yael Niv, Princeton University 143
II-60. Perceptual confirmation bias from approximate online inference
Richard Lange, Ankani Chattoraj, Matthew Hochberg, Jacob Yates, Ralf Haefner, University of Rochester 144
II-61. Deep learning captures V2 selectivity for natural textures
Md Nasir Uddin Laskar, Luis G Sanchez-Giraldo, Odelia Schwartz, University of Miami . . . . . . . . . . 144

14

COSYNE 2017

Posters II
II-62. The influence of trait anxiety on decision making under ambiguity: a computational fMRI study
Emma Lawrance, Jill O’Reilly, Janine Bijsterbosch, Christopher Gagne, Timothy Behrens, Sonia Bishop,
University of Oxford . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
II-63. Flexibility to contingency changes distinguishes habitual and goal-directed strategies in humans
Julie Lee, Mehdi Keramati, University College London . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
II-64. The impact of synaptic unreliability on spiking correlations in recurrent, balanced networks
Michael Leone, Chengcheng Huang, Brent Doiron, Carnegie Mellon University . . . . . . . . . . . . . . 146
II-65. Neurogenetic manipulation of visuomotor circuitry in freely flying Drosophila
Aljoscha Leonhardt, Alex Mauss, Alexander Borst, Max Planck Institute for Neurobiology . . . . . . . . . 147
II-66. Inferring distributions of neural data from incomplete observations
Anna Levina, Viola Priesemann, Institute of Science and Technology Austria . . . . . . . . . . . . . . . . 147
II-67. A unified neural network model for reconsolidation and extinction of fear memory
Ho Ling Li, Wulfram Gerstner, Ecole Polytechnique Federale de Lausanne . . . . . . . . . . . . . . . . . 148
II-68. Uncertainty-dependent extinction of fear memory in an amygdala-mPFC neural circuit model
Yuzhe Li, Ken Nakae, Shin Ishii, Honda Naoki, Kyoto University . . . . . . . . . . . . . . . . . . . . . . . 148
II-69. Human learning in complex environments: episodic memory challenges the model-free–modelbased realm
Vasiliki Liakoni, Marco Lehmann, Wulfram Gerstner, Kerstin Preuschoff, Ecole Polytechnique Federale de
Lausanne . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
II-70. Emergence of V1 recurrent connectivity pattern and Hebbian rule by supervised learning
Fangzhou Liao, Xiaolin Hu, Sen Song, Tsinghua university . . . . . . . . . . . . . . . . . . . . . . . . . 149
II-71. Sensory history affects perception through online updating of prior expectations
Itay Lieder, Vincent Adam, Maneesh Sahani, Merav Ahissar, Hebrew University of Jerusalem . . . . . . . 150
II-72. Recurrent switching linear dynamical systems
Scott Linderman, Andrew Miller, Ryan Adams, David Blei, Matthew Johnson, Liam Paninski, Columbia
University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
II-73. Modeling the spatiotemporal dynamics of human focal seizures
Jyun-you Liou, Catherine Schevon, Laurence Abbott, Columbia University . . . . . . . . . . . . . . . . . 151
II-74. Gain adaptation with and without rate adaptation in cortical area MT
Bing Liu, Matthew Macellaio, Leslie Osborne, University of Chicago . . . . . . . . . . . . . . . . . . . . . 152
II-75. Interrupting behaviour
Kevin Lloyd, Peter Dayan, Gatsby Computational Neuroscience Unit . . . . . . . . . . . . . . . . . . . . 152
II-76. Optimal inference of hidden states by mice in a probabilistic foraging task
Eran Lottem, Pietro Vertechi, Matthijs Oude Lohuis, Dario Sarra, Zachary Mainen, Champalimaud Centre
for the Unknown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
II-77. Can serial dependencies in choices and neural activity explain choice probabilities?
Jan-Matthis Lueckmann, Jakob Macke, Hendrikje Nienborg, research center caesar . . . . . . . . . . . . 153
II-78. Common sound-frequency dependence for linear and nonlinear responses in the auditory midbrain
Dominika Lyzwa, Chen Chen, Monty Escabi, Heather Read, Max Planck Institute for Dynamics and SelfOrganization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
II-79. Biophysical constraints of optogenetic inhibition at presynaptic terminals
Mathias Mahn, Katayun Cohen-Kashi-Malina, Matthias Prigge, Shiri Ron, Rivka Levy, Ilan Lampl, Ofer
Yizhar, Weizmann Institute of Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
II-80. Measuring cross-frequency coupling using mutual information and its application to epilepsy
Rakesh Malladi, Don Johnson, Giridhar Kalamangalam, Nitin Tandon, Behnaam Aazhang, Rice University 155
II-81. A probabilistic model for the neural code using maximum entropy of random projections
Ori Maoz, Gasper Tkacik, Roozbeh Kiani, Elad Schneidman, Weizmann Institute of Science . . . . . . . 156

COSYNE 2017

15

Posters II
II-82. Sparse predictive coding in balanced spiking networks
Mirjana Maras, Sophie Deneve, Ecole Normale Superieure . . . . . . . . . . . . . . . . . . . . . . . . . 156
II-83. Confidence representations across modalities in orbitofrontal cortex
Paul Masset, Torben Ott, Junya Hirokawa, Adam Kepecs, Cold Spring Harbor Laboratory . . . . . . . . . 157
II-84. Boosting olfactory cocktail-party performance by semi-supervised learning in mice
Alexander Mathis, Alan Wei, Alexandra W Ding, Matthias Bethge, Venkatesh N Murthy, Harvard University 158
II-85. Anticipatory neural activity is driven by a cue-triggered acceleration of network dynamics
Luca Mazzucato, Giancarlo La Camera, Alfredo Fontanini, Stony Brook University . . . . . . . . . . . . . 158
II-86. Efficiency despite diversity across retinal ganglion cell types
Lane McIntosh, David B. Kastner, Mihai Manu, Stephen A Baccus, Stanford University . . . . . . . . . . 159
II-87. Efficient multi-dimensional neural coding via conformal diffusion flow
Daniel McNamee, Daniel Wolpert, Mate Lengyel, University of Cambridge . . . . . . . . . . . . . . . . . 159
II-88. Adaptive time-averaging for texture perception
Richard McWalter, Josh McDermott, Technical University of Denmark . . . . . . . . . . . . . . . . . . . . 160
II-89. Sleep restores slow intrinsic timescales of cortical dynamics after sustained wakefulness in rats
Christian Meisel, Dietmar Plenz, NIMH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
II-90. Large-scale spike sorting for the analysis of electrical stimulation
Gonzalo Mena, Lauren Grosberg, Sasidhar Madugula, Pawel Hottowy, Alan Litke, John Cunningham, EJ
Chichilnisky, Liam Paninski, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
II-91. Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed
during cognitive tasks
Thomas Miconi, The Neurosciences Institute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
II-92. The role of orbitofrontal cortex in planning: Decision process, learning process, or both?
Kevin Miller, Matthew Botvinick, Carlos Brody, Princeton University . . . . . . . . . . . . . . . . . . . . . 162
II-93. Motor cortex engages output circuits in a behaviorally-selective manner
Andrew Miri, Claire Warriner, Jeffrey Seely, Gamaleldin Elsayed, Laurence Abbott, John Cunningham,
Mark Churchland, Thomas Jessell, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
II-94. Adaptive compression of statistically homogeneous sensory signals
Wiktor Mlynarski, Josh McDermott, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . 163
II-95. Neural coding of context-specific rules in primate prefrontal cortex, caudate, and hippocampus
Morteza Moazami, Scott L Brincat, Earl K Miller, Massachusetts Institute of Technology . . . . . . . . . . 164
II-96. Decoupling categorical choice from action reveals pure timing signals in striatal populations
Tiago Monteiro, Filipe Rodrigues, Asma Motiwala, Thiago S Gouvea, Joseph Paton, Champalimaud Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
II-97. Emergence of goal-driven multi-agent communication in artificial neural networks
Igor Mordatch, OpenAI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
II-98. Visual modulation of layer 6 neurons in mouse auditory cortex
Ryan Morrill, Andrea Hasenstaub, University of California, San Francisco . . . . . . . . . . . . . . . . . . 165
II-99. Local generation of errors in multi-layer networks through broad category tuning of each neuron
Hesham Mostafa, Gert Cauwenberghs, University of California, San Diego . . . . . . . . . . . . . . . . . 166
II-100. Judgement of time is reflected in dopaminergic reward prediction errors
Asma Motiwala, Sofia Soares, Bassam Atallah, Joseph Paton, Christian Machens, Champalimaud Neuroscience Programme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
II-101. A neural model of motor sequence learning and timing in basal ganglia
James Murray, Sean Escola, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
II-102. Maximal-length orbits in binary neural networks
Samuel P Muscinelli, Wulfram Gerstner, Johanni Brea, Ecole Polytechnique Federale de Lausanne . . . 168

16

COSYNE 2017

Posters II
II-103. Single-trial decision can be predicted from population activity of excitatory and inhibitor neurons
Farzaneh Najafi, Gamaleldin Elsayed, Eftychios Pnevmatikakis, John Cunningham, Anne Churchland,
Cold Spring Harbor Laboratory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
II-104. A computational model providing quantitative description of features of view-tuned face neurons
Yunjun Nam, Takayuki Sato, Yuji Yamauchi, Chia-pei Lin, Chou P Hung, Uchida Go, Manabu Tanifuji,
RIKEN Brain Science Institute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
II-105. Burst ensemble multiplexing: Linking neural codes to dendritic spikes and microcircuits
Richard Naud, Filip Vercruysse, Henning Sprekeler, University of Ottawa . . . . . . . . . . . . . . . . . . 169
II-106. Deciphering neural variability within a neocortical microcircuit
Max Nolte, Michael Reimann, Eilif Muller, Henry Markram, Ecole Polytechnique Federale de Lausanne
II-107. Changing prior probability in perceptual decision-making
Elyse Norton, Luigi Acerbi, Weiji Ma, Michael Landy, New York University

. 170

. . . . . . . . . . . . . . . . . 171

II-108. Linking structure and activity in nonlinear spiking networks
Gabriel Ocker, Kresimir Josic, Eric Shea-Brown, Michael A. Buice, Allen Institute for Brain Science . . . . 171
II-109. Evidence for optimal Bayesian cue combination of landmarks and velocity in the entorhinal cortex
Samuel Ocko, Kiah Hardcastle, Lisa Giocomo, Surya Ganguli, Stanford University . . . . . . . . . . . . . 172
II-110. Feature-specific prediction errors in the macaque fronto-striatal system during reversal learning
Mariann Oemisch, Stephanie Westendorff, Thilo Womelsdorf, York University . . . . . . . . . . . . . . . 172
II-111. Psychophysical reverse correlation reflects both sensory filters and the decision-making process
Gouki Okazawa, Roozbeh Kiani, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
II-112. Functional specialization of information flow between single neurons across behavioral states
Umberto Olcese, Jeroen Bos, Martin Vinck, Laura van Mourik-Donga, Cyriel Pennartz, University of Amsterdam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
II-113. High-dimensional inhibitory activity in visual cortex
Carsen Stringer, Marius Pachitariu, Mario Dipoppa, Matteo Carandini, Kenneth Harris, University College
London . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174

COSYNE 2017

17

Posters III

Poster Session III

7:30 pm Saturday 27 February

III-1. Behavioral discrimination of natural images correlates with average neural activity in higher visual
areas
Douglas Ollerenshaw, Marina Garrett, Peter Groblewski, Justin Kiggins, Derric Williams, Sahar Manavi,
Stefan Mihalas, Shawn Olsen, Allen Institute for Brain Science . . . . . . . . . . . . . . . . . . . . . . . 175
III-2. Adaptation of spontaneous activity in V1 during exposure to a novel stimulus statistics
Gergo Orban, Marcell Stippinger, Andreea Lazar, Mihaly Banyai, Wolf Singer, MTA Wigner RCP . . . . . 175
III-3. Physiological compensation for animal-to-animal variability in morphology
Adriane Otopalik, Eve Marder, Brandeis University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
III-4. Flexible decision-making in rats
Marino Pagan, Diksha Gupta, Alex Piet, Carlos Brody, Princeton University . . . . . . . . . . . . . . . . . 176
III-5. High dimensional chaos and synchronous irregular dynamics in delayed spiking networks.
Agostina Palmigiano, Fred Wolf, Max Planck Institute for Dynamics . . . . . . . . . . . . . . . . . . . . . 177
III-6. Precise estimates of single-trial neural population state in motor cortex via deep learning methods
Chethan Pandarinath, Jasmine Collins, Rafal Jozefowicz, Sergey Stavisky, Jonathan Kao, Mark Churchland, Matthew Kaufman, Stephen Ryu, Jaimie Henderson, Krishna Shenoy, Laurence Abbott, David Sussillo, Emory University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
III-7. Sequence replay in model neural networks without Hebbian plasticity
Richard Pang, Adrienne Fairhall, University of Washington . . . . . . . . . . . . . . . . . . . . . . . . . . 178
III-8. Optimal unsupervised Hebbian learning rules for attractor neural networks
Ulises Pereira, Nicolas Brunel, University of Chicago . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
III-9. Coding of visual stimuli and attentional state across layers of area V4
Warren Pettine, Nicholas Steinmetz, Tirin Moore, Stanford University . . . . . . . . . . . . . . . . . . . . 179
III-10. Synaptic inhibition shapes forward suppression in the auditory cortex of awake mice
Elizabeth AK Phillips, Andrea Hasenstaub, University of California, San Francisco . . . . . . . . . . . . . 180
III-11. Diverse timescales of population coding in cortex
Eugenio Piasini, Caroline Runyan, Stefano Panzeri, Christopher Harvey, Istituto Italiano di Tecnologia . . 180
III-12. Rats can optimally discount evidence for decision-making in a dynamic environment
Alex Piet, Carlos Brody, Ahmed El Hady, Princeton University . . . . . . . . . . . . . . . . . . . . . . . . 181
III-13. A mechanism for self-organized error-correction of grid cells by border cells
Eli Pollock, Niral Desai, Xuexin Wei, Vijay Balasubramanian, Massachusetts Institute of Technology . . . 181
III-14. Time-warped PCA: simultaneous alignment and dimensionality reduction of neural data
Ben Poole, Alexander Williams, Niru Maheswaranathan, Byron Yu, Gopal Santhanam, Stephen Ryu,
Stephen A. Baccus, Krishna Shenoy, Surya Ganguli, Stanford University . . . . . . . . . . . . . . . . . . 182
III-15. Linking normative models and methods for neural systems identification
Johannes Burge, Priyank Jaini, University of Pennsylvania . . . . . . . . . . . . . . . . . . . . . . . . . . 183
III-16. Optimal population coding depends on the source of noise
Kai Roeth, Julijana Gjorgjieva, Max Planck Institute for Brain Research . . . . . . . . . . . . . . . . . . . 183
III-17. Neural implementation of change rate inference in dynamic environments
Adrian Radillo, Alan Veliz-Cuba, Kresimir Josic, Zachary P Kilpatrick, University of Houston . . . . . . . . 184
III-18. Hunger-dependent enhancement of food cue responses in mouse postrhinal cortex and lateral
amygdala
Rohan Ramesh, Christian Burgess, Arthur Sugden, Kirsten Levandowski, Margaret Minnig, Henning
Fenselau, Bradford Lowell, Mark L Andermann, Harvard University . . . . . . . . . . . . . . . . . . . . . 184
III-19. What can we learn about natural vision from color tuning curves?
Pavan Ramkumar, Hugo Fernades, Matthew Smith, Konrad Kording, Northwestern University . . . . . . . 185

18

COSYNE 2017

Posters III
III-20. Spatiotemporal characteristics of brain regions modulated by the difficulty of mental arithmetic
Michael Randazzo, Youssef Ezzyat, Michael Kahana, University of Pennsylvania . . . . . . . . . . . . . 186
III-21. The hippocampus as a predictive recurrent neural network
stefano recanatesi, Greg D Wayne, Mattia Rigotti, weizmann institute of science . . . . . . . . . . . . . . 186
III-22. Quantifying the information rate of sensory feedback
Julien Rechenmann, Joseph E O’Doherty, Philip Sabes, University of California, San Francisco . . . . . . 187
III-23. Evidence for a dynamical network underlying self-timed actions in macaque frontal cortex
Evan Remington, Mehrdad Jazayeri, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . 187
III-24. An inhibitory neural circuit for reliable coding in mouse visual cortex
Rajeev V Rikhye, Ming Hu, Mriganka Sur, Massachusetts Institute of Technology

. . . . . . . . . . . . . 188

III-25. Closed-loop stimulation reveals modulation of somatosensory cortex excitability by active sensing
Jason Ritt, Smrithi Sunil, Joseph Schroeder, Vincent Mariano, Gregory Telian, Boston University . . . . . 188
III-26. Markov transitions between attractor states in a recurrent neural network
David Rolnick, Jeremy Bernstein, Ishita Dasgupta, Haim Sompolinsky, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
III-27. 3D rotations with recurrent neural networks
Herve Rouault, Alon Rubin, Sandro Romani, Janelia Research Campus . . . . . . . . . . . . . . . . . . 189
III-28. Balanced excitation and inhibition are needed for robust neuronal selectivity and associative memory
Ran Rubin, Laurence Abbott, Haim Sompolinsky, Columbia University . . . . . . . . . . . . . . . . . . . 190
III-29. To see or not to see: Cortico-collicular circuit dynamics in visual perception
Sarah Ruediger, Massimo Scanziani, Howard Hughes Medical Institute . . . . . . . . . . . . . . . . . . . 191
III-30. Limits on fast, high-dimensional information processing in recurrent circuits
Virginia Rutten, Guillaume Hennequin, University of Cambridge . . . . . . . . . . . . . . . . . . . . . . . 191
III-31. Diverse weighting of shared input noise prevents information saturation in a population code
Pratik Sachdeva, Michael DeWeese, University of California, Berkeley . . . . . . . . . . . . . . . . . . . 192
III-32. Coordinated activation of value-related computations across human orbitofrontal cortex
Ignacio Saez, Jack Lin, Edward Chang, Josef Parvizi, Gerwin Schalk, Robert Knight, Ming Hsu, University
of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
III-33. Latent manifold analysis reveals parallels in hierarchical patterning of birdsong and human speech
Tim Sainburg, Brad Theilman, Marvin Thielk, Timothy Gentner, University of California, San Diego . . . . 193
III-34. Complementary organization of responses to apparent motion in Drosophila elementary motion
detectors
Emilio Salazar, Bara Badwan, Margarida Agrochao, Damon Clark, Yale University . . . . . . . . . . . . . 193
III-35. Predictive information in retinal ganglion cell responses to natural movies
Jared Salisbury, Olivier Marre, Michael Berry, Stephanie Palmer, University of Chicago . . . . . . . . . . 194
III-36. Flexible normalization in deep convolutional neural networks
Luis G Sanchez-Giraldo, Odelia Schwartz, University of Miami . . . . . . . . . . . . . . . . . . . . . . . 194
III-37. Confidence results from internal monitoring of information integration
Hannah Schewe, Windy Torgerud, Dominic Mussack, Ganesh Rakate, Paul Schrater, University of Minnesota . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
III-38. Perceptual categorization of infant vocalizations in the auditory cortex of maternal mice
Jennifer Schiavo, Robert Froemke, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
III-39. Synaptic architecture of visual space in ferret visual cortex
Benjamin Scholl, Daniel Wilson, David Fitzpatrick, Max Planck Florida Institute for Neuroscience . . . . . 196

COSYNE 2017

19

Posters III
III-40. Inter-areal synchronization in visual cortex during a divided attention paradigm
Marieke Scholvinck, Jarrod Dowdall, Georgios Spyropoulos, Pascal Fries, Ernst Strungmann Institute for
Neuroscience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
III-41. Task states are represented in OFC during task performance and replayed in hippocampus at rest
Nicolas Schuck, Yael Niv, Princeton University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
III-42. Neural noise improves path representation in a simulated network of grid, place, and time cells
David Schwartz, O Ozan Koyluoglu, University of Arizona . . . . . . . . . . . . . . . . . . . . . . . . . . 198
III-43. A miniature ultra-widefield microscope for imaging global brain dynamics in GCaMP6 transgenic
rats
Benjamin Scott, Stephan Thiberge, Caiying Guo, Dougal Tervo, Carlos Brody, Alla Karpova, David Tank,
Princeton University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
III-44. Population coupling in the mouse visual cortex
Madineh Sedigh-Sarvestani, Philip Mardoum, Max Nolte, University of Pennsylvania . . . . . . . . . . . . 199
III-45. Cortical spatial representations for solving the cocktail party problem
Kamal Sen, Howard Gritton, Jun Ma, Nicholas James, Xue Han, Boston University . . . . . . . . . . . . 199
III-46. A biologically plausible neural network for whitening-free ICA
Anirvan Sengupta, Dmitri Chklovskii, Department of Physics and Astronomy . . . . . . . . . . . . . . . . 200
III-47. Model-based inference of nonlinear subunits underlying responses of primate retinal ganglion cells
Nishal Shah, Nora Brackbill, Colleen Rhoades, Alexandra Tikidji-Hamburyan, Georges Goetz, Alexander
Sher, Alan Litke, Liam Paninski, Eero P Simoncelli, EJ Chichilnisky, Stanford University . . . . . . . . . . 200
III-48. Learning nonlinear models for visual computation in populations of retinal ganglion cells
Nishal Shah, Nora Brackbill, Colleen Rhoades, Alexandra Tikidji-Hamburyan, Georges Goetz, Alexander
Sher, Alan Litke, Vineet Gupta, Yoram Singer, EJ Chichilnisky, Jon Shlens, Stanford University . . . . . . 201
III-49. Neural adaptation may explain anomalous diffusion in fixational eye motion
Nimrod Shaham, Nadav Ben-Shushan, Yoram Burak, Hebrew University of Jerusalem

. . . . . . . . . . 202

III-50. Fine-grained characterization and modeling of decision behavior with optimal motion-pulse sequences
Victoria Shavina, Valerio Mante, ETH Zurich . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
III-51. Can you teach an old monkey a new trick?
Noa Shinitski, Yingzhuo Zhang, Daniel Gray, Sara Burke, Anne Smith, Carol Barnes, Demba Ba, Technische Universitaet Berlin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
III-52. Orthogonal spatial features and homogeneous temporal dynamics in larval Drosophila olfactory
coding
Guangwei Si, Jessleen Kanwal, Yu Hu, Matthew E Berck, Christopher Tabone, Aravinthan Samuel, Harvard University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
III-53. Distributed temporal processing in human prefrontal cortical neurons during cognitive control
Elliot Smith, Guillermo Horga, Charles Mikell, Mark Yates, Garrett Banks, Yagna Pathak, Seth Pullman,
Qiping Yu, Catherine Schevon, Shraddha Shrinivasan, Guy McKhann, Matthew Botvinick, Sameer Sheth,
Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
III-54. Learning Bayesian prior and loss function in interval timing
Hansem Sohn, Mehrdad Jazayeri, Massachusetts Institute of Technology

. . . . . . . . . . . . . . . . . 205

III-55. Uncovering the whole-brain electrical networks that underlie human cognition
Ethan Solomon, James Kragel, Michael Sperling, Ashwini Sharan, Gregory Worrell, Michal Kucewicz,
Robert Gross, Bradley Lega, Kathryn Davis, Joel Stein, Barbara Jobst, Kareem Zaghloul, Sameer Sheth,
Daniel Rizzuto, Michael Kahana, University of Pennsylvania . . . . . . . . . . . . . . . . . . . . . . . . . 206
III-56. A systematic study of variation of response sparseness across rat visual cortical areas
Liviu Soltuzu, Davide Zoccolan, International School for Advanced Studies . . . . . . . . . . . . . . . . . 207

20

COSYNE 2017

Posters III
III-57. Two-photon microscopy simulation for optics optimization and benchmarking
Alexander Song, Adam S Charles, Jeff Gauthier, Sue Ann Koay, David Tank, Jonathan Pillow, Princeton
University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
III-58. Differences between “statistical“ and “dynamical“ criticality in multi-neuron activity
Martino Sorbaro Sindaci, Gerrit Hilgen, Hayder Ibraheem Hussein A Amin, Alessandro Maccione, Luca
Berdondini, Evelyne Sernagor, Matthias Hennig, University of Edinburgh . . . . . . . . . . . . . . . . . . 208
III-59. Amortized inference for fast spike prediction from calcium imaging data
Artur Speiser, Srini Turaga, Evan Archer, Jakob Macke, research center caesar . . . . . . . . . . . . . . 208
III-60. Predictive plasticity in dendrites: from a computational principle to experimental data
Dominik Spicher, Claudia Clopath, Walter Senn, University of Bern . . . . . . . . . . . . . . . . . . . . . 209
III-61. Superior limb-movement decoding from cortex with a new, unsupervised-learning algorithm
Joseph Makin, Joseph E O’Doherty, Philip Sabes, University of California, San Francisco . . . . . . . . . 210
III-62. Dopamine reward prediction errors reflect hidden state inference across time
Clara Starkweather, Naoshige Uchida, Samuel Gershman, Harvard University . . . . . . . . . . . . . . . 210
III-63. Exactly solvable nonlinear recurrent networks for context-dependent information processing
Christopher Stock, Subhaneil Lahiri, Alexander Williams, Surya Ganguli, Stanford University . . . . . . . 211
III-64. Distinct roles of basolateral amygdala and orbitofrontal cortex in value-learning under uncertainty.
Alexandra Stolyarova, Alicia Izquierdo, University of California, Los Angeles . . . . . . . . . . . . . . . . 211
III-65. How much to gain: targeted gain modulation facilitates learning in recurrent motor circuits
Jake Stroud, Guillaume Hennequin, Mason Porter, Tim P Vogels, University of Oxford . . . . . . . . . . . 212
III-66. Two-photon imaging of functional diversity of trigeminal meningeal afferents in awake mice
Arthur Sugden, Jun Zhao, Helaine Gariepy, Kirsten Levandowski, Dan Levy, Mark L Andermann, Harvard
University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
III-67. Central circuits encoding wind direction in the fruit fly
Marie Suver, Katherine Nagel, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
III-68. Optimal policy leads to “irrational” choice behavior under multiple alternatives
Satohiro Tajima, Jan Drugowitsch, Alexandre Pouget, University of Geneva . . . . . . . . . . . . . . . . 214
III-69. Task demand modulates collective attractor dynamics in visual cortex
Satohiro Tajima, Kowa Koida, Chihiro I Tajima, Hideyuki Suzuki, Kazuyuki Aihara, Hidehiko Komatsu,
University of Geneva . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
III-70. Coordination in the hippocampal-prefrontal network during awake and sleep sharp wave ripple
events
Wenbo Tang, Justin Shin, Loren Frank, Shantanu Jadhav, Brandeis University . . . . . . . . . . . . . . . 215
III-71. Generalized leaky integrate-and-fire models classify multiple neuron types
Corinne Teeter, Ramakrishnan Iyer, Vilas Menon, Nathan W Gouwens, Nicholas Cain, David Feng, Jim
Berg, Christof Koch, Stefan Mihalas, Allen Institute for Brain Science . . . . . . . . . . . . . . . . . . . . 216
III-72. Environmental adaptation of olfactory receptor distributions
Tiberiu Tesileanu, Simona Cocco, Remi Monasson, Vijay Balasubramanian, CUNY Graduate Center . . . 216
III-73. Choice-supportive asymmetry in learning rates
Heloise Thero, Henri Vandendriessche, Valerian Chambon, Stefano Palminteri, Ecole Normale Superieure 217
III-74. Topological analysis of neural population activity in the auditory system of the European starling
Bradley Theilman, Timothy Gentner, University of California, San Diego . . . . . . . . . . . . . . . . . . 217
III-75. Shared perceptual spaces for high-dimensional natural acoustic signals
Marvin Thielk, Tim Sainburg, Tatyana Sharpee, Timothy Gentner, University of California, San Diego . . . 218
III-76. A data-driven approach to identify ion channel correlations in homeostatic regulation
Kun Tian, Cengiz Gunay, Astrid Prinz, Emory University . . . . . . . . . . . . . . . . . . . . . . . . . . . 219

COSYNE 2017

21

Posters III
III-77. Modeling network oscillations in precise subcallosal cingulate white matter deep brain stimulation
Vineet Tiruvadi, Allison Waters, Liangyu Tao, Rohit Konda, Andrea Crowell, Patricio Riva-Posse, Robert
Butera, Helen Mayberg, Emory University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
III-78. MAP inference in linear models with sparse connectivity using sister mitral cells.
Sina Tootoonian, Peter Latham, Gatsby Computational Neuroscience Unit . . . . . . . . . . . . . . . . . 220
III-79. Decoding arm force from neural population dynamics in PMd and M1 during reaching
Eric Trautmann, Sergey Stavisky, Jonathan Kao, Stephen Ryu, Krishna Shenoy, Stanford University . . . 220
III-80. Selective involvement of mouse prefrontal activity during an auditory discrimination task
Hua-an Tseng, Xue Han, Boston University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
III-81. Neural coding of leg proprioception in Drosophila
John Tuthill, Akira Mamiya, University of Washington . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
III-82. The Fruit Fly Brain Observatory: from structure to function
Nikul H Ukani, Chung-Heng Yeh, Adam Tomkins, Yiyin Zhou, Yu-Chi Huang, Dorian Florescu, Carlos
Luna Ortiz, Cheng-Te Wang, Paul Richmond, Chung-Chuan Lo, Daniel Coca, Ann-Shyn Chiang, Aurel
Lazar, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
III-83. Finding order within chaos in neural representations: A trajectory model of sensorimotor computation
Cevat Ustun, University of California, Los Angeles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
III-84. How dentate gyrus place cells represent distinct place memories
Milenna van Dijk, Andre Fenton, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
III-85. Unbiased log-likelihood estimation with inverse binomial sampling
Bas van Opheusden, Luigi Acerbi, Weiji Ma, New York University . . . . . . . . . . . . . . . . . . . . . . 224
III-86. Inference by reparameterization using population codes
Rajkumar Vasudeva Raju, Xaq Pitkow, Rice University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
III-87. Compressive approaches for neuronal reconstruction using DNA barcodes
Alexander Vaughan, Xiaoyin Chen, Anthony Zador, Cold Spring Harbor Laboratory . . . . . . . . . . . . 225
III-88. Experimental isolation of parallel information pathways embedded in distributed, convergent systems
Nicholas Wall, Peter Neumann, Kevin Beier, Ava Mokhtari, Liqun Luo, Robert Malenka, Stanford University 225
III-89. The speed of neural dynamics as a neural code for motor timing
Jing Wang, Mehrdad Jazayeri, Eghbal Hosseini, Devika Narain, Massachusetts Institute of Technology

. 226

III-90. Early motion processing circuit uses gap junctions to achieve efficient stimuli encoding
Siwei Wang, Noga Zaslavsky, Alexander Borst, Naftali Tishby, Idan Segev, Hebrew University of Jerusalem226
III-91. Graph clustering with coupled oscillators: A retinal model.
Christopher Warner, Friedrich Sommer, University of California, Berkeley . . . . . . . . . . . . . . . . . . 227
III-92. Deep multi-view representation learning of brain responses to natural stimuli
Leila Wehbe, Anwar Nunez-Elizalde, Alexander Huth, Fatma Deniz, Natalia Bilenko, Jack Gallant, University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
III-93. Layer-specific differences between spontaneous and visually evoked spiking correlations in V1
Jacob Westerberg, Michele Cox, Kacie Dougherty, Alexander Maier, Vanderbilt University . . . . . . . . . 228
III-94. Nonlinear latent variable approaches for understanding population activity in sensory cortex
Matthew Whiteway, Karolina Socha, Vincent Bonin, Daniel A Butts, University of Maryland . . . . . . . . 229
III-95. High cellular and columnar variability underlies the absence of early orientation selectivity
David E Whitney, Gordon B Smith, Bettina Hein, Matthias Kaschube, David Fitzpatrick, Max Planck Florida
Institute for Neuroscience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
III-96. Efficient supervised learning of hierarchical cortical networks in the predictive coding framework
James Whittington, Rafal Bogacz, University of Oxford . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230

22

COSYNE 2017

Posters III
III-97. Cortical projection neurons support a parallel and complementary analysis of stimulus features
Ross S Williamson, Daniel Polley, Massachusetts Eye and Ear Infirmary . . . . . . . . . . . . . . . . . . 230
III-98. Rapid and concentration-invariant coding of olfactory identity using early neural activity
Christopher Wilson, Dmitry Rinberg, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . 231
III-99. Cellular and synaptic mechanisms of direction selectivity in visual cortex
Daniel Wilson, Benjamin Scholl, David Fitzpatrick, Max Planck Florida Institute for Neuroscience . . . . . 232
III-100. Reinforcement learning over time: effects of spacing on the mechanisms supporting feedback
learning
G Elliott Wimmer, Russ Poldrack, Stanford University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
III-101. Visual cortical circuits in a state of flux
Fred Wolf, David E Whitney, Justin C Crowley, Juan Daniel Florez Weidinger, Max Planck Institute for
Dynamics and Self-Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
III-102. Essential nonlinear properties in neural decoding
Qianli Yang, Xaq Pitkow, Rice University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
III-103. A neural circuit for efficient analysis-by-synthesis in the primate face processing system
Ilker Yildirim, Winrich Freiwald, Josh Tenenbaum, Massachusetts Institute of Technology . . . . . . . . . 234
III-104. Decoding of replay events from unsorted spike data using a particle filter applied to a marked
point
Ali Yousefi, Loren Frank, Uri Eden, Harvard University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
III-105. Three-dimensional spatiotemporal receptive field structure in macaque area MT
Andrew Zaharia, Robbe Goris, J. Anthony Movshon, Eero P Simoncelli, New York University . . . . . . . 235
III-106. A model-based analysis of the interaction between pre-stimulation neural state and stimulation
Syed Zaidi, Mark Connolly, Babak Mahmoudi, Robert Gross, Emory University . . . . . . . . . . . . . . . 236
III-107. Structure and dynamics of robust associative networks operating at maximum capacity
Danke Zhang, Armen Stepanyants, Northeastern University . . . . . . . . . . . . . . . . . . . . . . . . . 236
III-108. Optimizing information transmission in an array of diverse sensory neurons
Yilun Zhang, David Kastner, Stephen A Baccus, Tatyana Sharpee, Salk Institute for Biological Studies . . 237
III-109. A two-dimensional seperable random field model of within and cross-trial neural spiking dynamics
Yingzhuo Zhang, Noa Shinitski, Stephen Allsop, Kay Tye, Demba Ba, Harvard University . . . . . . . . . 238
III-110. Neural network models of the tactile system develop first-order units with complex receptive fields
Charlie Zhao, Mark Daley, Andrew Pruszynski, Yale University . . . . . . . . . . . . . . . . . . . . . . . . 238
III-111. Gotta infer’em all: dynamical features from neural trajectories
Yuan Zhao, Il Memming Park, Stony Brook University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
III-112. Non-equilibrium driven dynamics of continuous attractors in place cell networks
Weishun Zhong, Hyun Jin Kim, David Schwab, Arvind Murugan, University of Chicago . . . . . . . . . . 239
III-113. Interdigitating subnetworks of intracortical projection neurons in mouse V1
Petr Znamenskiy, Meanhwan Kim, Maria Florencia Iacaruso, Thomas Mrsic-Flogel, University of Basel . . 240

COSYNE 2017

23

Posters III

24

COSYNE 2017

Posters III

The online version of this document includes abstracts for each presentation.
This PDF can be downloaded at: http://cosyne.org/cosyne17/Cosyne2017_program_book.pdf

COSYNE 2017

25

Posters III

26

COSYNE 2017

T-1 – T-2

Abstracts
Abstracts for talks appear first, in order of presentation; those for posters next, in order of poster session
and board number. An index of all authors appears at the back.

T-1. “Deep“ neuroscience in high dimensions
Surya Ganguli
Stanford University
Remarkable advances in experimental neuroscience now enable us to simultaneously observe the activity of
many neurons, thereby providing an opportunity to understand how the moment by moment collective dynamics
of the brain instantiates cognition and behavior. However, efficiently extracting such a conceptual understanding
from large, high dimensional neural datasets requires concomitant advances in theoretically driven experimental
design, data analysis, and neural circuit modeling. We will discuss how the development of modern theories
in high dimensional statistics and deep learning can advance conceptual understanding in neuroscience. In
particular we will discuss: (1) optimal ways to discover patterns in high dimensional data, (2) how to tradeoff very
different experimental resources, like numbers of recorded neurons and trials to discover latent cognitive states,
(3) lessons from the dynamics of deep learning over high dimensional weight spaces, and (4) deep learning
models that accurately capture the retina’s response to natural scenes as well as its internal structure and function.
Overall, these examples illustrate how a synthesis of ideas from disparate fields, including physics, mathematics,
computer science and statistics can catalyze conceptual advances in neuroscience.

T-2. Neuroscience for the 99%: How low tech versions of high tech laboratories enable exploratory research experiments and hands-on instruction
Greg Gage
Backyard Brains
At the dawn of neuroscience investigations, researchers themselves built tools to investigate the brain. Hodgkin
and Huxley’s famous discoveries were based on their invention of the voltage clamp applied to neuroscience. Lord
Adrian and Matthews built their own amplifiers and oscilloscopes using the recently invented vacuum tube in order
to record the first action potential in 1928. David Hubel invented the microelectrode and the hydraulic microdrive
that he would later use with Torsten Wiesel to greatly expanded our knowledge of the sensory processing. To
advance any field of science, new tools are invented.
The recent “maker movement“ has seen the birth of new consumer technologies (e.g. 3D-printers, CNC laser cutters, low volume PCB) which enable labs to quickly build tools to further scientific investigation. Our organization
(Backyard Brains) develops open-source DIY neuroscience tools which are appropriate for the benchtop of both
research and instructional teaching labs. Our focus is on hands-on experiments and electrophysiology. This lecture will provide an overview of our mission to re-engineer research-grade lab equipment using first principles and
will highlight basic principles of neuroscience in a “DIY“ fashion: neurophysiology, functional electrical stimulation,
micro-stimulation effect on animal behavior, neuropharmacology, even neuroprosthesis and optogenetics!
Finally, with faculty academic position becoming harder to obtain, I will discuss an alternative academic career
path: entrepreneurship. It is possible to be an academic and do research, publish papers, write grants, and train
students all outside the traditional university setting.

COSYNE 2017

27

T-3 – T-4

T-3. Cortical circuits supporting context-dependent flexibility in auditory processing
Maria Geffen
University of Pennsylvania
Hearing perception relies on our ability to tell apart the spectral content of different sounds, and to learn to use this
difference to distinguish behaviorally relevant (such as dangerous and safe) sounds. Recently, we demonstrated
that the auditory cortex regulates frequency discrimination acuity following emotional learning. However, the neuronal circuits that underlie this modulation remain unknown. In the auditory cortex, the excitatory neurons serve
the dominant function in transmitting information about the sensory world within and across brain areas, whereas
inhibitory interneurons carry a range of modulatory functions, shaping the way information is represented and
processed. I will discuss the results of two of our recent studies that elucidate the function of specific inhibitory
neuronal population in sound encoding and perception. First, we found that the most common class of interneurons, parvalbumin-positive (PVs), modulate frequency selectivity of excitatory neurons in the auditory cortex, and
regulate frequency discrimination acuity and specificity of discriminative auditory emotional. Our results demonstrate that cortical inhibition can improve or impair acuity of innate and learned auditory behaviors that rely on
frequency discrimination. Second, we found that another class of interneurons, somatostatin- positive interneurons (SOMs), regulate adaptation in responses of cortical neurons to frequent sounds, in a stimulus-specific
fashion. By selectively reducing responses to frequently, but not rarely, occurring sounds, auditory cortical neurons are enhance the brain’s ability to detect unexpected events through stimulus-specific adaptation (SSA). We
found that SOMs selectively reduced excitatory responses to frequent tones, whereas PVs amplify SSA by providing non-specific inhibition. More recent experiments demonstrate that the role of SOMs extends to other forms
of adaptation beyond SSA. These results expand our understanding of how specific cortical circuits contribute to
auditory perception in everyday acoustic environments.

T-4. Breaking balance: the space and time of shared cortical variability
Brent Doiron
University of Pittsburgh
A characteristic feature of cortex is the large temporal and trial-to-trial variability of its spiking activity. Seminal
theoretical and modeling studies have explored how such variability can be an emergent property of networks
that have a dynamic and balanced tension between large excitation and inhibition. While balanced networks
readily capture the statistics of single unit cortical spiking activity the population activity in these models is asynchronous. This is at odds with the abundant experimental evidence from population recordings showing a weakly
correlated cortical state. Further, these recordings show that a large fraction of population correlations are low
dimensional, and cortical state (attention, arousal, anesthetics etc.) modulates this low dimensional component.
These discrepancies between the variability exhibited in real and model networks limit the contributions of mechanism based theoretical neuroscience to systems neuroscience. We explore how spatially structured connectivity
in balanced network models decouples balanced firing rate solutions and asynchronous network dynamics. We
establish the circuit requirements for an internally generated low dimensional correlated network state, and one
that is easily modulated through top-down recruitment of inhibitory activity. Our theoretical work captures the
population-wide variability recorded during attention modulated tasks in the primate visual system (joint work with
Marlene Cohen). As population recording and targeted cell specific manipulation techniques advance we will
require mechanistically derived theories such as we propose to give interpretation to emerging data sets.

28

COSYNE 2017

T-5 – T-7

T-5. Mechanisms of fear control in humans
Elizabeth A Phelps
New York University
Animal models of associative threat learning provide a basis for understanding human fears and anxiety. Building on research from animal models, we explore a range of means maladaptive defensive responses can be
diminished in humans. First, I will outline how extinction and emotion regulation, techniques adapted in cognitive
behavioral therapy, can be used to control learned defensive responses via inhibitory signals from the ventromedial prefrontal cortex to the amygdala. One drawback of these techniques is that these responses are only
inhibited and can return, with one factor being stress. I will then review research examining them lasting control
of maladaptive defensive responses by targeting memory reconsolidation and present evidence suggesting that
the behavioral interference of reconsolidation in humans diminishes involvement of the prefrontal cortex inhibitory
circuitry, although there are limitations to its efficacy. Finally, I will describe two novel behavioral techniques
that might result in a more lasting fear reduction, the first by providing control over stressor and the second by
substituting a novel, neutral cue for the aversive unconditioned stimulus.

T-6. Homogeneity and heterogeneity of dopamine signals
Naoshige Uchida
Harvard University
Dopamine is a neuromodulator that plays important roles in reward-based learning, motivation and movement.
However, the nature of dopamine signals remains debated. It has been postulated that dopamine neurons constitute a homogeneous population signaling reward prediction errors or the discrepancy between actual and predicted reward. However, increasing evidence suggests that dopamine neurons projecting to different targets
encode different information. For instance, it has been shown that different parts of the striatum receive distinct
dopamine signals. In a recent study, we have compared dopamine axon signals in the ventral striatum (“VS
dopamine“) and the “tail“ of the striatum (“TS dopamine“) using fiber fluorometry (photometry). TS dopamine
showed strong excitation to novel odor cues, whereas VS dopamine showed no responses to novel odor cues.
Additionally, TS dopamine showed excitation to several types of stimuli including rewarding, aversive, and neutral
stimuli whereas VS dopamine showed excitation only to reward or reward-predicting cues. Together, these results
demonstrated that there are distinct populations of dopamine neurons, one signaling reward prediction error (VS
dopamine) and another signaling salience (TS dopamine). These results suggest that we need a better theory to
account for the nature of dopamine signals as well as the function of dopamine.

T-7. Neural circuits underlying positive and negative valence
Kay Tye
Massachusetts Institute of Technology
The Tye Lab is interested in understanding how neural circuits important for driving positive and negative motivational valence (seeking pleasure or avoiding punishment) are anatomically, genetically and functionally arranged.
We study the neural mechanisms that underlie a wide range of behaviors ranging from learned to innate, including social, feeding, reward-seeking and anxiety-related behaviors. How are these circuits interconnected with one
another, and how are competing mechanisms orchestrated on a neural population level? We employ optogenetic,
electrophysiological, electrochemical, pharmacological and imaging approaches to probe these circuits during
behavior.

COSYNE 2017

29

T-8 – T-10

T-8. Extracting low-dimensional structure from high-dimensional neural activity
Jonathan Pillow
Princeton University
A growing body of evidence indicates that the inherent dimensionality of neural population activity is far smaller
than the number of neurons that can be recorded with current technology. However, it is also clear that single
neurons often represent multiple kinds of information simultaneously, a phenomenon known as “mixed selectivity“, indicating that neural population activity represents complex mixtures of sensory, cognitive, and behavioral
variables. In this talk, I will describe new statistical techniques for uncovering low-dimensional latent structure
from high-dimensional neural datasets. Our work represents an extension of “Targeted Dimensionality Reduction“
(Mante et al, 2013), which seeks to identify subspaces that carry information about distinct task variables. We
have applied our method to neural population data from macaque prefrontal cortex during a context-dependent
perceptual discrimination task. It reveals the existence of independent multi-dimensional subspaces of neural activity space devoted to the coding of sensory, context, and decision-related variables on multiple timescales. I will
discuss implications of this and related approaches for understanding the dimensions of neural activity underlying
complex behaviors.

T-9. Neuronal representation of social information in the mouse medial amygdala
Catherine Dulac
Harvard University
Howard Hughes Medical Institutew
The medial amygdala (MeA) is a key hub of the sensory-motor transformation from chemosensory cues to instinctive social behaviors. However, the principles by which this deep brain structure encodes social information have
remained elusive. By using a miniature microscope to image the dynamics of large neural ensembles over several
months in awake behaving mice, we were able to address two fundamental questions on how neural circuitries
underlie behavior control: what is the nature of the neural codes that provide the brain with an internal representation of social information, and to what extent does individual experience change these codes in the short- or
long-term? The identification of basic organization principles by which social features are represented in the MeA
provides a general scheme to understand how behaviorally relevant information is represented in the brain, which
neural strategies are used to shape coding specificity, and how neural codes are regulated by previous experience
and internal status in awake behaving animals.

T-10. Brain mechanisms for memory guided decision-making in humans
Daphna Shohamy
Columbia University
Learning is central to adaptive behavior. From robots to humans, the ability to learn from experience turns a rigid
response system into a flexible, adaptive one. How are decisions shaped by past experience? What are the
neurobiological mechanisms that allow past experience to change the way we perceive the world and act in it?
To address these questions, our research takes as a starting point a longstanding idea in cognitive and systems
neuroscience: that the brain learns in different ways by using multiple specialized learning systems. Implicit
learning of habits is thought to depend on the striatum and its dopaminergic inputs, while explicit memory for

30

COSYNE 2017

T-11 – T-13
specific episodes is known to depend on the hippocampus. However, despite progress in mapping these different
forms of learning to different neural structures, the separation of learning into distinct systems has left open crucial
questions about the nature of the interactions between them, the kinds of representations they build, and their
role in computing values for decisions. Here, I will present evidence for a critical role for memory mechanisms
in the hippocampus in biasing value-based decisions, focusing on two distinct mechanisms. The first concerns
the integration of information across discrete past events to support generalization of past experience towards
novel decisions. The second concerns the retrieval and use of memories for rare, “one shot” events when making
decisions about reward. Finally, I will discuss how results emerging from this work challenge the traditional view of
learning systems and advance understanding of how memory biases decisions in both adaptive and maladaptive
ways.

T-11. Light sleep
Gero Miesenbock
University of Oxford
Sleep disconnects animals from the external world, at considerable risks and costs that must be offset by a vital
benefit. Insight into this mysterious benefit will come from understanding sleep homeostasis: to monitor sleep
need, an internal bookkeeper must track physiological changes that are linked to the core function of sleep. In
Drosophila, a crucial component of the machinery for sleep homeostasis is a cluster of neurons innervating the
dorsal fan-shaped body of the central complex. Artificial activation of these cells induces sleep, whereas reductions in excitability cause insomnia. I will present evidence that homeostatic sleep control works by switching
these sleep-promoting neurons between active and quiescent states, and that state switching regulates the accumulation and dissipation of sleep pressure.

T-12. Circuit mechanisms for flexible sensory processing in Drosophila
Vanessa Ruta
Rockefeller University
In a complex and dynamic environment, animals must constantly vary their behavior to accommodate changing
circumstances and contingencies. Neuromodulation plays an important role in mediating this behavioral flexibility
by rapidly reconfiguring neural circuits to allow the same sensory signals to activate alternative output pathways.
My lab uses the simple Drosophila nervous system to gain insight into how neuromodulatory pathways represent
the ongoing experience of an animal to shape synaptic transmission, circuit processing, and behavior. We have
been exploring the role of dopaminergic modulation in the Drosophila mushroom body, a higher brain center
essential for flexible and learned olfactory behaviors. Functional imaging suggests that the mushroom body
dopaminergic neurons work in concert to encode both salient signals in the environment as well as the animal’s
behavioral state, providing a moment-by-moment representation of the experience of a fly. We are currently
exploring how these dynamic patterns of dopaminergic neuron activity influence downstream circuits to modulate
olfactory preferences and behavior.

T-13. Bridging the gap between deep learning and neuroscience
Yoshua Bengio
University of Montreal

COSYNE 2017

31

T-14 – T-15
Connectionist ideas from three decades ago have fuelled a revolution in artificial intelligence with the rise of deep
learning methods. Both the older connectionist ideas and the newer ones owe a lot to inspiration from the brain,
but the gap between deep learning and neuroscience remains wide. We lay down some of these old ideas, based
on learning distributed representations in order to jointly optimize by a gradient-based method all the modules of
the system with respect to an objective function linked to a task or to capturing many aspects of the unknown
distribution of observed data. We also discuss the new ideas from deep learning, including a discussion of the
newly acquired theoretical understanding of the advantages brought by jointly optimizing a deep architecture.
Finally, we summarize some of the recent work aimed at bridging the remaining gap between deep learning and
neuroscience, including approaches to implement functional equivalents to backpropagation in a more biologically
plausible way, as well as ongoing work to extend this towards the modelling of high-dimensional joint distributions,
which is at the heart of deep unsupervised learning.

T-14. Olfactory processing channels organize into functional clusters in higher
brain regions
James Jeanne
Mehmet Fisek
Rachel Wilson

JAMES JEANNE @ HMS . HARVARD. EDU
FISEKM @ GMAIL . COM
RACHEL WILSON @ HMS . HARVARD. EDU

Harvard University
Different odors can evoke strikingly different behavioral responses, often without any prior experience. How does
the brain organize the broad diversity of odors to ultimately support innate behavior? In the olfactory system, odors
are encoded by the combined activity of multiple glomeruli, but how downstream circuitry reads out this glomerular
population code is incompletely understood. In the fruit fly, Drosophila melanogaster, glomerular projections
target the mushroom body and lateral horn, two regions of higher-order olfactory processing. The lateral horn is
implicated in mediating innate odor-driven behavior and, consistently, glomerular projections to the lateral horn are
stereotyped across animals. However, a broad-scale understanding of connectivity rules between glomeruli and
lateral horn neurons (LHNs) is lacking. To uncover connectivity with single glomerulus and single LHN resolution,
we have combined two-photon optogenetics with patch-clamp electrophysiology. This methodology allows the
systematic mapping of the glomerular inputs to individual LHNs in individual experiments. Our data reveal that the
connectivity of different glomeruli onto LHNs is not specified independently. Rather, clusters of glomeruli converge
onto many of the same LHNs. In most cases, glomeruli that wire together have similar odor tuning, but in some
cases glomeruli that wire together can have very different odor tuning. In addition, clusters of LHNs receive
divergent input from many of the same glomeruli. However, glomerular clusters and LHN clusters map onto each
other only loosely, suggesting a combinatorial connectivity pattern. Clustered glomeruli define odor patterns with
ethological importance. Clustered LHNs suggest odor associations and behaviors linked to specific higher brain
regions. These results constitute an important step towards understanding the computations underlying innate
behavioral responses to odors.

T-15. The significance of nominally non-responsive activity in auditory perception and behavior
Michele Insanally1
Ioana Carcea1
Badr Albanna2
Robert Froemke1
1 New

MNI 1@ NYU. EDU
IOANA . CARCEA @ MED. NYU. EDU
BADR . ALBANNA @ GMAIL . COM
ROBERT. FROEMKE @ MED. NYU. EDU

York University
University

2 Fordham

32

COSYNE 2017

T-16
Spike trains recorded from the cortex of behaving animals can be complex, highly variable from trial-to-trial and
therefore challenging to interpret. A fraction of cells exhibit trial-averaged responses with obvious task-related
features such as orientation tuning in the visual cortex or pure tone frequency tuning in the auditory cortex. However, a substantial number do not appear to fire in a task-related manner1. These cells – including cells in primary
sensory cortex – typically comprise >50% of datasets yet are often neglected from analysis1. Even cells with classical response profiles lose their stimulus representation during task-engagement without impairing behavior2,3.
These results suggest that cells with no discernible trial-averaged responses may play an underappreciated role
in sensory processing and cognition. To understand their role, we devised a novel single-trial, spike-timing-based
analysis to evaluate whether the activity of single-units recorded from auditory and frontal cortex encode task variables in behaving rats. Using this analysis, we have made four major discoveries: 1. Nominally non-responsive
cells reveal hidden task information. The activity of cells that seem unresponsive when trial-averaged often encode task-relevant information at levels comparable to responsive cells. 2. Frontal cortex is more informative about
task-relevant auditory stimuli than auditory cortex. Auditory cortex reliably responds to pure tones in untrained
animals. However, when tones take on behavioral significance, this information is encoded more accurately in
frontal cortex suggesting that this region is critical for extracting task-relevant stimulus information. 3. Excluding
nominally non-responsive cells impairs decoding ability. Removing nominally non-responsive cells from small ensembles impairs decoding performance indicating the activity of nominally non-responsive cells is necessary for
encoding behavioral variables. 4. Frontal cortex benefits most from decoding using small ensembles. Decoding
performance in frontal cortex dramatically improves when using small ensembles whereas decoding from auditory
cortex increases only marginally suggesting a highly redundant coding scheme in frontal cortex

T-16. Domain-specialised CNNs of realistic depth best explain FFA and PPA
representations
Katherine Storrs1
Johannes Mehrer1
Alexander Walther2
Nikolaus Kriegeskorte1
1 University
2 University

KATHERINE . STORRS @ MRC - CBU. CAM . AC. UK
JOHANNES . MEHRER @ MRC - CBU. CAM . AC. UK
AWALTHERMAIL @ GMAIL . COM
NIKOLAUS . KRIEGESKORTE @ MRC - CBU. CAM . AC. UK

of Cambridge
College London

Humans are able to classify complex visual objects with extremely high accuracy. Recently, deep convolutional
neural network (DCNN) models have reached and even surpassed human performance at this task. Among recent
networks, the deeper the architecture, the better the performance. Although loosely inspired by biological brains,
it remains unclear whether models reaching human-level accuracy also perform computations similar to those in
the human brain. Here we investigated the effect of depth and task training on the degree to which DCNNs can
explain representations in human inferior temporal cortex (hIT) and its category-selective subregions, the fusiform
face area (FFA) and parahippocampal place area (PPA). We found that the deepest, best-performing models are
not best at explaining hIT. Instead, models of moderate depth, comparable to the number of areas along the
primate ventral stream, best explain hIT representations. In earlier studies using shallower architectures with
poorer object classification accuracy, greater depth and higher task performance were associated with improved
explanation of IT. Our results here show that this is not the case for current state-of-the-art deep architectures that
near or surpass human performance. We further find that training task and stimuli have a large impact on how
well a model can explain the representations in FFA and PPA. A 16-layer DCNN architecture (VGG-16) trained
exclusively to identify faces best explained face-selective FFA among the tested models. The same architecture
trained exclusively to categorise scenes best explained scene-selective PPA. For both subregions, it was layer
14 (of 16) whose representation was most similar to the brain representation in question, consistent with earlier
findings showing that the representations in these subregions are strongly, but not purely categorical. To fully
explain hIT representations, networks may need to perform well on multiple tasks and/or explicitly model the
functional organisation of human ventral visual cortex.

COSYNE 2017

33

T-17 – T-18

T-17. High dimensional geometry of cortical population activity
Marius Pachitariu
Carsen Stringer
Nicholas Steinmetz
Mario Dipoppa
Sylvia Schroeder
Matteo Carandini
Kenneth Harris

MARIUS 10 P @ GMAIL . COM
CARSEN . STRINGER @ GMAIL . COM
NICK . STEINMETZ @ GMAIL . COM
MARIO. DIPOPPA @ GMAIL . COM
NICHOLAS . BURT @ UCL . AC. UK
MATTEO @ CORTEXLAB . NET
KENNETH . HARRIS @ UCL . AC. UK

University College London
The cortex operates by the combined activity of large neuronal populations, but the rules by which their firing
is coordinated are not understood. We studied this question by recording ≈ 10, 000 neurons for several hours
of spontaneous activity, and in response to thousands of stimuli, in the visual cortex of awake head-fixed mice,
while identifying cell classes using transgenic mouse lines. We found that the stimulus response space was
full-dimensional over our set of 3,000 stimuli, and the spontaneous activity contained approximately two hundred
components of shared population activity. Neurons with high spontaneous correlations did not have preferentially
high stimulus correlations, in contradiction to previous studies, which we suggest used biased statistical methods.
We next investigated the properties of the spontaneous components and found that they were active on a range of
timescales from hundreds of milliseconds to tens of seconds, and were distributed randomly over the ≈ 0.3mmˆ3
imaged volume. At least tens of dimensions of GAD+ interneuron activity were aligned to the GAD- excitatory
components, and each interneuron subtype studied (VIP, SST and PVALB) contained several dimensions shared
with the rest of the population. The spontaneous activity components could be predicted from 10–20 dimensions
of behavioral activity, extracted by dimensionality reduction from movies of the face, pupil and whiskers. We
also investigated the spontaneous components at a larger spatial scale, by recording electrophysiologically up
to 1,000 total neurons simultaneously from 4–8 brain areas with two 384-channel “neuropixels” probes. Slow
timescale components (>100ms) were distributed widely across the brain, but fast timescale components were
limited to anatomically defined brain areas. Our results call for a reevaluation of the structure, function and origins
of ongoing neural activity in awake animals, and point to behavioral states as a key contributor to high-dimensional
brain-wide patterns of activity.

T-18. From dynamics to computations: a theory of recurrent random networks with low-dimensional structure
Francesca Mastrogiuseppe
Srdjan Ostojic

FRANCESCA . MASTROGIUSEPPE @ ENS . FR
SRDJAN . OSTOJIC @ ENS . FR

Ecole Normale Superieure
Recurrent networks with random connectivity can self-sustain irregular activity which resembles the cortical patterns observed in-vivo. As their dynamics are mathematically tractable, such networks have become a powerful theoretical paradigm [Sompolinsky 1988, van Vreeswijk 1996, Brunel 2000]. Cortical networks however act
as computational units, where specific input-output transformations are thought to emerge from specific wiring
structures in the synaptic connectivity between neurons. Several distinct frameworks have been proposed for
implementing specific computations within re- current neural networks [Hopfield 1982, Eliasmith 2005, Sussillo
2009, Boerlin 2013]. The common point between these approaches is that computations rely on a minimal,
low-dimensional connectivity structure that is possibly added on top of random connectivity. How these two components interact to shape network dynamics and computations is not fully understood. To address this question,
we develop a theory of random networks perturbed by weak low-dimensional connectivity, and examine the computational capacity of such a setup. Using dynamical mean field theory, we show that weak low-dimensional
connectivity structure results in finite perturbations at the level of the mean network activity. As a consequence,
rank one and rank two perturbations can push the network into states where bistable attractors or oscillatory

34

COSYNE 2017

T-19 – T-20
patterns co-exist with the irregularly fluctuating activity. The mean field framework allows to exactly compute the
phase diagrams of such systems, determine when the desired computations are dynamically stable, and estimate
the corresponding error. Our theoretical framework sets the ground for designing ad-hoc perturbations which set
random networks into the appropriate state to generate the desired output.

T-19. Posterior Parietal Cortex conveys sensory history that acts as a prior
during a parametric WM task
Athena Akrami1
Charles Kopec1
Carlos Brody1,2

AAKRAMI @ PRINCETON . EDU
CKOPEC @ PRINCETON . EDU
BRODY @ PRINCETON . EDU

1 Princeton
2 Howard

University
Hughes Medical Institute

What neural mechanisms allow prior information about the statistics of stimuli to affect perception? A behavioral
task which displays effects consistent with the use of priors in Bayesian inference is the sequential comparison
of two graded stimuli separated by a delay period of a few seconds. This task requires subjects to maintain an
analog value in memory, allowing quantitative assessments of how that memory is stored. We refer to this task
as a “parametric working memory” (PWM) task. It has long been known that primate PWM subjects show “contraction bias”, in which the estimated magnitude of memorized stimuli is systematically biased by prior stimulus
statistics (Hollingworth 1910, Ashourian et al 2011). We have developed an auditory version of this task for rats,
adapted from a previous tactile task (Fassihi and Akrami et al 2014). Using a regression model, we quantify how
performance on each trial depends on the stimulus and trial history, and we find that rats show a contraction bias
effect, due to the recent history of sensory stimuli, remarkably similar to humans. We carried out the first local
brain inactivations during PWM, using optogenetics to bilaterally silence posterior parietal cortex (PPC), and found
that, surprisingly, this led to improved performance in the task. Quantitative analysis suggested that performance
improvement could be due to a reduction in the biases caused by the history of stimulus statistics. Furthermore,
recordings from PPC show that neurons carry significant amount of information about the previous trials’ sensory
stimuli, even while carrying no information during the delay period about the current trial’s first stimulus. These
results suggest a sensory history-dependent firing rates in PPC during PWM may underlie the century-long psychophysics phenomenon known as contraction bias. Our data identifies the PPC as a critical node in conveying
information about priors in the Bayesian sense.

T-20. A cerebellar model for learning Bayesian statistics of time intervals
Devika Narain
Mehrdad Jazayeri

DNARAIN @ MIT. EDU
MJAZ @ MIT. EDU

Massachusetts Institute of Technology
Humans estimate time intervals by combining noisy measurements with the prior distribution of intervals, as predicted by a Bayesian integration strategy. While normative Bayesian models capture human estimates remarkably
well, the neural mechanisms underlying Bayesian behavior remain unknown. Decades of research implicates the
cerebellum in the acquisition of fixed time intervals in tasks like trace conditioning. Here, we exploit the known
learning mechanisms for acquiring interval time in the cerebellum to develop a model for encoding prior distributions. In the model, the start of the interval triggers rich and reproducible firing patterns in granule cells (GCs)
that serve as a basis for learning temporal intervals. The cue signaling the end of the interval activates climbing
fibers (CFs). The conjunctive activation of GCs and CFs induces long-term synaptic depression (LTD) of the
GCs driving Purkinje cells (PCs). Repeated exposure to LTD across trials precipitates the encoding of the prior
distribution in the firing rate dynamics of PCs. The model incorporates a weak and slow restoring potentiation

COSYNE 2017

35

T-21 – T-22
that allows synapses to recover to baseline in the absence of sensory input. Integration of the cerebellar output
over time furnishes a temporal estimate which exhibits the interval-dependent biases that characterize Bayesian
time-interval estimation. The model output captures the physiological signatures of prior learning in downstream
cortical areas, and additionally, replicates behavioral characteristics of Bayesian integration during a time interval
reproduction task. The model can learn a variety of prior distributions and remains robust to large parametric
variations. Further, it provides a mechanistic explanation for previously reported dynamics of Bayesian learning
during interval estimation. Our findings thus underscore the intriguing possibility that the cerebellum serves as a
substrate for Bayesian time estimation.

T-21. Discrete oscillatory bursts for flexible read out from working memory
Mikael Lundqvist1
Pawel Herman
Melissa Warden2
Scott L Brincat1
Earl K Miller1

LUNDQVIS @ MIT. EDU
PAHERMAN @ KTH . SE
MRWARDEN @ GMAIL . COM
SBRINCAT @ MIT. EDU
EKMILLER @ MIT. EDU

1 Massachusetts
2 Cornell

Institute of Technology
University

Averaging neurophysiological data across multiple trials removes noise and thus can allow a better understanding
of the underlying brain function. However, if the underlying dynamics are inherently non-stationary such averaging
will also remove key aspects of the to-be studied phenomenon. Fueled by predictions from a computational
model we recently performed single-trial analysis of working memory data. As the model predicted, we found
that primate prefrontal cortex dynamics are indeed highly non-stationary with substantial inter-trial variability.
Brief bursts of activity in beta and gamma frequency bands in the recorded local-field potentials (LFPs) occurred
during encoding, maintenance and decoding of working memory information. Gamma bursts carried information
about working memory content while beta bursts suppressed this information. Here we expand upon this work
by performing single trial analysis of spikes and LFPs recorded prefrontally while monkeys serially encoded and
then tested two objects. In particular, we studied the interval between the first and second test probe, during
which the animals had to decide whether the first presented item matched the first test item, and yet had to
avoid any behavioral response. We found that both gamma and beta bursting evolved differently depending on
if the first test object matched the first encoded item or not. These patterns also predicted if the animal was
going to make an error or not. Overall, beta and gamma followed the behavior predicted by the model, where
gamma bursts reflected activity in cell assemblies coding for items held in working memory, and beta bursting
reflected the suppression of these assemblies at specific times and sites. Our results provide novel insight into
the process of accessing working memory content and how information about a specific object can be read out
when multiple objects are stored. Understanding this central ability may have important implications for cognitive
and computational neuroscience.

T-22. Synaptically imprinted memories reignite bump-attractor dynamics prior
to stimulus in a vsWM task
Joao Barbosa1
Christos Constantinidis2
Albert Compte1

PALERMA @ GMAIL . COM
CCONSTAN @ WAKEHEALTH . EDU
ACOMPTE @ CLINIC. CAT

1 IDIBAPS
2 Wake

Forest University

Persistent activity of prefrontal neurons is thought to maintain locations in memory during the delay of spatial work-

36

COSYNE 2017

T-23
ing memory tasks. The bump-attractor model offers an elegant explanation for the physiology and the behavioral
precision via diffusing bumps. Such model can naturally explain serial biases in this task, whereby previous memoranda interfere attractively with newly stored locations. However, this is only consistent with a bump-attractor
perspective if instead of being reset after the subject’s report, the circuit keeps old memory representations as
activity bumps that interfere with future trials. To address the neural basis of this interference, we analyzed behavioral and prefrontal neural data from monkeys performing an oculomotor delayed response task. We found
that monkeys showed a bias towards previous reported locations, which was attractive for previous reports similar
to the currently memorized location, and repulsive for more distant previous reports. This could be explained by
interacting bump-attractors. However, single neuron dynamics was not consistent with this hypothesis, since neuronal firing rates typically returned to spontaneous levels in inter-trial intervals (ITI). Nevertheless, ≈ 500ms before
the cue presentation, single neuron activity became consistent with a re-emergent activity bump at the population
level: we found significant tuning to previous cue, negative noise correlations between pairs of neurons on the
opposite flank of the bump and significant decoding accuracies for previous stimuli. Since the previous stimulus
code was temporarily absent and later reemerged, this is suggestive of a second mechanism holding latent information possibly at the synaptic level. This was supported by stimulus selectivity in the peak cross-correlation of
pairs of neurons with similar memory fields during the ITI. This showed that during fixation the prefrontal network
is still imprinted with previous memories, revealing two distinct but interacting memory substrates in the prefrontal
circuit based on persistent activity and short-term synaptic plasticity.

T-23. A midbrain-basal ganglia circuit is critical to externally and internally
reinforced vocal learning.
Erin Hisey
Matthew Kearney
Richard Mooney

EEHISEY @ GMAIL . COM
MATTHEW. GENE . KEARNEY @ GMAIL . COM
MOONEY @ NEURO. DUKE . EDU

Duke University
Humans rely on both external and internal reinforcement to learn a variety of motor skills. Midbrain dopaminereleasing neurons that innervate the basal ganglia (BG) are known to play a key role in externally reinforced forms
of learning in vertebrates. In contrast, the role of dopamine in internally reinforced forms of learning remains
relatively poorly understood. Songbirds provide a powerful model in which to explore the roles that dopamine
signaling plays in internally and externally reinforced learning: as juveniles, they learn to copy a tutor song in a
process that is internally reinforced; as adults, they can learn to shift the fundamental frequency (i.e., pitch) of
a syllable that is targeted in a contingent manner with noise, a form of externally reinforced learning; and the
pitch of the targeted syllable will return to baseline values when noise is discontinued, a process that is internally
reinforced. Here we used an intersectional genetic method to ablate midbrain dopaminergic cells (VTAX cells) that
project to Area X, a region of the BG specialized for vocal learning. Though both juvenile copying and adult pitch
learning were severely impaired following VTAX cell ablation, adult recovery of baseline pitch was unaffected.
In another set of experiments, we found that infusing dopamine receptor type 1 (D1R) antagonists into Area X
also impaired juvenile copying and adult pitch learning, but did not affect adult recovery. Finally, we found that
pitch-contingent optogenetic stimulation of VTAX terminals in Area X was sufficient to reinforce lasting shifts in
syllable pitch. These findings indicate that VTAX cells and the D1Rs they activate in the BG are necessary to
both internally and externally reinforced forms of learning, and that performance-contingent activation of VTAX
terminals is sufficient to reinforce learned vocal behavior.

COSYNE 2017

37

T-24 – T-25

T-24. Choice-selective sequential activity in cortical neurons that project to
the nucleus accumbens
Nathan Parker
Malavika Murugan
Ilana Witten

NPARKER 327@ GMAIL . COM
MMURUGAN @ PRINCETON . EDU
IWITTEN @ PRINCETON . EDU

Princeton University
How is the brain able to attribute reinforcement to a prior action appropriately, given that an action typically occurs before reinforcement is signaled? Although this problem, often referred to as the temporal credit assignment
problem, has inspired a myriad of theoretical solutions, the neural underpinnings remain a mystery. Previous research has implicated the nucleus accumbens (NAc), a structure in the ventral portion of the striatum, in learning
the relationship between a stimulus or action and its outcome. The medial prefrontal cortex (mPFC) provides a
major source of glutamatergic innervation to this region, and the synaptic connection between mPFC neurons
and the NAc output neurons are thought to be an important site of plasticity that underlies reinforcement learning.
However, given the great heterogeneity of cell-types within both the mPFC and the NAc, it is not known what is
encoded in the mPFC neurons that project to NAc (mPFC-NAc), and whether they may convey information to
bridge the gap in time between actions and reinforcement signals. To address this question, we performed the
first measurements of cellular-resolution activity in mPFC- NAc neurons during a reinforcement learning task, and
discovered that (i) mPFC-NAc neurons display population-level sequential activations in relation to task events, (ii)
mPFC-NAc neurons are highly selective for actions as opposed to sensory stimuli, and (iii) mPFC-NAc sequences
are choice-specific and persist for several seconds after the action. The encoding of prior actions through sequential activity in this neural population suggests a mechanism to bridge prior actions and subsequent reinforcement,
thus providing a potential neural solution to the temporal credit assignment problem.

T-25. A pathway for hunger modulation of learned food cue responses in insular cortex
Mark L Andermann1
Yoav Livneh2
Rohan Ramesh1
Christian Burgess2
Kirsten Levandowski2
Joseph Madara2
Henning Fenselau2
Glenn Goldey3
Nick Jikomes2
Jon Resch2
Bradford Lowell2

MANDERMA @ BIDMC. HARVARD. EDU
YLIVNEH @ BIDMC. HARVARD. EDU
RAMESH @ FAS . HARVARD. EDU
CBURGESS @ BIDMC. HARVARD. EDU
KLEVANDO @ GMAIL . COM
JMADARA @ BIDMC. HARVARD. EDU
HFENSELA @ BIDMC. HARVARD. EDU
GOLDEY. GLENN @ GMAIL . COM
NJIKOMES @ GMAIL . COM
JRESCH @ BIDMC. HARVARD. EDU
BLOWELL @ BIDMC. HARVARD. EDU

1 Harvard

University
Israel Deaconess Medical Center
3 University of California, Los Angeles
2 Beth

Hunger is typically elicited by negative energy balance, and causes a state of increased motivation to seek out,
work for, and eat food. Hunger biases attention toward food-associated cues (e.g. candy bar wrappers) so
that calorie-dense foods can be found and consumed in order to restore energy balance. Enhanced behavioral
sensitivity to food cues remains a major obstacle to weight-loss programs involving food restriction, and can even
persist in satiated individuals suffering from obesity or eating disorders. Despite the clinical importance of this
phenomenon, the cellular and circuit mechanisms by which hunger biases cognitive processing towards foodpredicting cues remain largely unknown. A key brain area known to integrate information about internal bodily
states such as hunger with external sensory cues to drive goal-directed behavior is the insular cortex (InsCtx).

38

COSYNE 2017

T-26 – T-27
Neuroimaging studies in humans have consistently found that hunger-dependent increases in the incentive value
of visual food cues correlates with increased food-cue-evoked responses in InsCtx. In rodents, an intact InsCtx is
critical for learned food-predicting cues to induce food-seeking behavior, potentially due to its role in the retrieval
of the incentive value of these cues. We imaged visual food cue responses from the same InsCtx neurons
while mice were either hungry or sated. We find that InsCtx neurons have a biased overrepresentation of food
cues in hungry mice, which is abolished following satiation. When activated by caloric deficiency, Agouti-related
peptide (AgRP) neurons in the arcuate nucleus of the hypothalamus drive food-seeking and feeding. We find that
artificial activation of AgRP neurons in sated mice mimics hunger in restoring the food cue bias in behavior and
in InsCtx neuronal responses. Using circuit-mapping techniques, we uncover a pathway that links AgRP neurons
to the InsCtx. These results provide insights into how a bodily need state selectively enhances the salience of
motivationally-relevant sensory cues.

T-26. The stabilized supralinear network replicates neural and performance
correlates of attention
Grace Lindsay1
Dan B Rubin2
Kenneth D Miller1
1 Columbia

GRACEWLINDSAY @ GMAIL . COM
RUBIN . DANB @ GMAIL . COM
KENDMILLER @ GMAIL . COM

University
of Neurology, Massachusetts Gen.

2 Department

Covert visual attention can increase accuracy and decrease reaction times in challenging visual tasks. Neural
recordings performed while animals execute these tasks have captured neural correlates of attention. In this
study, we demonstrate how a biologically realistic model of visual cortical circuits (the stabilized supralinear network, SSN) can replicate both neural correlates and performance effects of attention. The SSN was previously
shown to implement balanced amplification, normalization and surround suppression, all known to occur in visual
cortex. Here we show that these same mechanisms also replicate many of the neural effects associated with attention including multiplicative scaling of tuning curves, effect of distractors on attentional strength, and decreases
in correlations across neurons. We then explored whether these changes are causally related to attention’s ability
to enhance performance. Frequently, assumptions about downstream decoders are required to assess the impact
changes in neural activity have on performance. We bypass this problem by training a multi-layer neural network
(deemed the “SSN-CNN”) to perform a visual classification task, allowing a direct readout of performance. Unlike traditional feedforward-only networks for image classification, the first two layers of this network contain the
recurrently-connected SSN, which implements cross-feature normalization dynamically. Applying attention in this
network replicated two behavioral findings: (1) feature-based attention aides detection in challenging visual tasks
and (2) spatial attention increases performance in a change detection task. Taken together, this work both provides a circuit mechanism to explain the neural correlates of attention and ties those neural correlates directly to
behavioral enhancements.

T-27. Low-dimensional state-space trajectory of choice at the population level
in area MT
Yuan Zhao1
Jacob Yates2
Il Memming Park1
1 Stony

YUAN . ZHAO @ STONYBROOK . EDU
JYATES 7@ UR . ROCHESTER . EDU
MEMMING . PARK @ STONYBROOK . EDU

Brook University
of Rochester

2 University

For stimuli near perceptual threshold, the trial-by-trial activity of single neurons in many sensory areas are cor-

COSYNE 2017

39

T-28
related with an animal’s perceptual report [1]. The interpretation of this phenomenon (often summarized as
choice probability (CP)) is complicated by the presence of correlations between neurons [2], which are often lowdimensional and shared across many neurons [3, 4, 5]. Here we extracted single-trial low-dimensional neural
trajectory using variational latent Gaussian process (vLGP) algorithm [6] from population recordings in area MT
[7] to understand how stimulus and choice are encoded across the population. The recovered neural trajectory
captures the slow covariation in the population spike trains. The generative model of vLGP assumes multiplicative
interaction among the latent processes in addition to an autoregressive point process observation model for each
spike train. vLGP decomposes the population response as the shared variability (represented as neural trajectory) and individual neuron’s private noise. We found that the neural trajectories strongly encoded the direction
of the stimulus with similar temporal signature as single MT neurons. We then computed a time-weighted CP
for the neural trajectories; this “population CP” was much higher than any single MT unit CP and increased over
time unlike single MT unit CP. Most of the explanatory power of neural trajectories came from 2 latent dimensions,
which were spatially localized along the recording electrode, suggesting anatomical organization correlated with
functional information representation. Taken together, these observations offer new interpretive frameworks for
population level activity in relation to perception.

T-28. Dimension reduction of multi-trial neural data by tensor decomposition
Alex Williams1
Hyun Kim1
Forea Wang
Saurabh Vyas1
Krishna Shenoy1
Mark Schnitzer1
Tamara Kolda2
Surya Ganguli1

ALEX . H . WILLIA @ GMAIL . COM
KIMTH @ STANFORD. EDU
FOREA @ STANFORD. EDU
THEVYAS 19@ GMAIL . COM
SHENOY @ STANFORD. EDU
MSCHNITZER @ GMAIL . COM
TGKOLDA @ SANDIA . GOV
SGANGULI @ STANFORD. EDU

1 Stanford
2 Sandia

University
National Laboratories

Decision-making, sensation, and motor behaviors occur within fractions of seconds, while learned memories can
require many hours, days, or years to mature. Technologies for long-term monitoring of neural activity enable
examination of all these processes in a single experiment. However, exploratory analysis of these data is challenging due their size (thousands of neurons and behavioral trials) and diversity of timescales. Commonly used
methods for dimensionality reduction identify low-dimensional features of within trial neural dynamics, but do not
model changes in neural activity across trials. We propose to represent multi-trial data as a three-dimensional
data array (a third-order tensor) with each entry indicating the activity of a particular neuron at a particular time
on a particular trial (Fig 1). Approximating these data with the canonical polyadic (CP) tensor decomposition
(reviewed in [3]) produces low-dimensional factors that summarize neural correlations, within-trial dynamics, and
across-trial changes in dynamics. Applying CP decomposition to simulated multi-trial data precisely identified lowdimensional network inputs that varied across trials, whereas classical methods (PCA and ICA) failed to recover
these signals. We then examined two experimental datasets: (a) prefrontal cortical neurons monitored by fluorescence microendoscopy in mice performing a spatial navigation task, and (b) multi-unit extracellular recordings
in the premotor and motor cortices of a Rhesus monkey moving a virtual cursor through a brain-machine interface. In both cases, CP decomposition uncovered specific neural sub-populations with interpretable within-trial
as well as across trial dynamics, reflecting task structure, strategies, rewards, and BMI perturbations. The CP
decomposition is broadly applicable to common experimental designs in systems neuroscience, is simpler to fit
and interpret than complex nonlinear models, and is more informative than classical techniques that represent
neural data as a matrix.

40

COSYNE 2017

T-29 – T-30

T-29. Low-rank non-stationary population dynamics can account for robustness to optogenetic stimulation
Lea Duncker1,2
Daniel O’Shea3
Werapong Goo3
Krishna Shenoy3
Maneesh Sahani1,2

DUNCKER @ GATSBY. UCL . AC. UK
DJOSHEA @ STANFORD. EDU
WERAPONG . GOO @ GMAIL . COM
SHENOY @ STANFORD. EDU
MANEESH @ GATSBY. UCL . AC. UK

1 Gatsby

Computational Neuroscience Unit
College London
3 Stanford University
2 University

Genetically or anatomically targeted perturbations can provide invaluable insight into the functional role of subpopulations within neural circuits, but may be difficult to interpret. We have shown previously [O’Shea et al.
COSYNE14] that focal optogenetic stimulation in primate motor cortex drives large concurrent changes in firing
rates across a local population of neurons — perturbing, at least in part, patterns of activity correlated with the
behavioural task — and yet decays rapidly with only subtle effects on reaction time and the kinematics of upcoming
or ongoing movements. Does such robustness in the circuit require corrective influences from other populations
[Li et al. Nature 2016] or could it arise within the local population itself? If the dynamics of population activity are
low-rank, then robustness to stimulation could arise because the perturbation projects into the nullspace of the
dynamical operator. However, a non-normal dynamical system may produce systematic, task-relevant output in
this nullspace, and so it cannot be characterised by methods that depend purely on variance or regression to task
parameters. We developed an efficient algorithm to learn a non-stationary, low-rank, population dynamical model
from data. We applied this model to population recordings from macaque dorsal premotor cortex (PMd) during an
instructed-delay center-out reaching task with optogenetically perturbed trials. The model captured the evolution
of population activity throughout reach initiation and execution, successfully recovering the robust evolution of
the neural population on stimulated trials. Examination of the model parameters revealed that indeed evolution
was robust because the optogenetic stimulation projected into a nullspace of the identified low-rank dynamics.
Ultimately, a clear understanding of the functional and behavioural role of neural populations will depend on a
better understanding of both the effects of perturbation, and the dynamics that govern the evolution of the neural
population.

T-30. Hippocampal coding arises from probabilistic self-localization across
many ambiguous environments
Ingmar Kanitscheider
Ila R Fiete

IKANITSCHEIDER @ MAIL . CLM . UTEXAS . EDU
FIETE @ MAIL . CLM . UTEXAS . EDU

University of Texas at Austin
How do animals self-localize accurately enough that their neurons exhibit spatial tuning over ≈ 10 − minute trajectories while their motion estimates are noisy enough that path integration would lead to disorientation over this
time? Landmarks help, but can be spatially extended (walls) or resemble other landmarks, providing ambiguous
information and leading to complex, multi-peaked posterior distributions over position. Robotics algorithms solve
the problem by sequential probabilistic inference, but despite the rich literature on the brain’s spatial representations it is unclear how neural circuits execute similar computations. We study the problem of localization with
noisy motion cues in environments of different geometry, where walls are sensed only upon contact. We take a
“model-free” approach—constrained by task but not neural tuning—and train recurrent neural networks to localize. We scrutinize the resulting performance and representations. The trained network exploits motion cue and
environmental geometry statistics to vastly outperform path integration and methods that update a best single
estimate of position using motion and landmark cues, even in novel environments of unknown specific shape.
In an environment of known geometry, performance matches the Bayes-optimal particle filter. After training in

COSYNE 2017

41

T-31 – T-32
100 environments, the network can start at a random location, make a few contacts with the boundary, and correctly classify the environment by simultaneously refining its noisy location and classification estimates. Many
nontrivial hippocampal cell properties emerge in the network’s hidden units: 1. Place fields that are stable (generalize) across novel trajectories in an environment but change relative positions between environments (global
remapping), 2. A sparse (long-tailed) distribution of spatial selectivity, 3. Weak head direction tuning, 4. Context(classification) signaling cells that tend to be less spatially selective. Our approach illustrates a way to begin piecing together disparate phenomenological results in the hippocampus into a coherent functional framework related
to sequential probabilistic computation.

T-31. Conserved circuits for the parallel neuromodulation of brain state
Matthew Lovett-Barron1
William E Allen1
Aaron S Andalman1
Isaac Kauvar1
Karl Deisseroth1,2

MATTLB @ STANFORD. EDU
WALLEN @ STANFORD. EDU
AARON . ANDALMAN @ GMAIL . COM
IKAUVAR @ STANFORD. EDU
DEISSERO @ STANFORD. EDU

1 Stanford
2 Howard

University
Hughes Medical Institute

Internal states fluctuate during behavior, impacting perception, cognition, and action. Internal states are governed
in part by neuromodulation, but the set of neuromodulatory cell types involved are not fully known. Here we
screened for neuromodulatory neuron activity in larval zebrafish during a behavioral task that reports fluctuations
in the internal state of alertness. We developed a method to classify cell types from brain-wide neural activity
imaging through post hoc immunohistochemistry and volume registration, allowing us to record from neurons in
22 distinct neuromodulatory nuclei during behavior. We identified multiple noradrenergic, cholinergic, dopaminergic, serotonergic, and peptidergic cell types that encode alertness. We used these results to guide recordings
from homologous neuromodulatory nuclei in behaving mice, revealing common cell type-specific dynamics across
species. Optogenetic activation demonstrated that any alertness-encoding cell type is sufficient to change alertness. These results reveal an evolutionarily conserved network of multiple neuromodulatory systems that regulate
internal state in parallel.

T-32. Paradoxical effects of activating and inactivating cortical interneurons:
a cautionary tale
Andrea Hasenstaub
Elizabeth Phillips

ANDREA . HASENSTAUB @ UCSF. EDU
ELIZABETH . PHILLIPS @ UCSF. EDU

University of California, San Francisco
Bidirectional manipulations—for instance, optogenetic activation and inactivation—are widely used to identify the
functions supported by specific neuron types. Implicit in much of this work is the notion that tonic activation and
inactivation will both produce valid, internally consistent insights into neurons’ computational roles. Here, using
single-unit recordings in auditory cortex of awake mice, we show that this may not generally hold true. Optogenetically manipulating somatostatin-positive (Sst+) or parvalbumin-positive (Pvalb+) interneurons while recording
tone-responses showed that Sst+ inactivation increased response gain, while Pvalb+ inactivation weakened tuning and decreased information transfer, implying that these neurons support delineable computational functions.
But activating Sst+ and Pvalb+ interneurons revealed no such differences. We used a simple network model to
understand this asymmetry, and showed how relatively small changes in key parameters, such as spontaneous
activity level or strength of the light manipulation, determined whether activation and inactivation would produce
consistent or paradoxical conclusions regarding interneurons’ computational functions. We conclude that the pre-

42

COSYNE 2017

T-33 – T-34
cise way in which we use causal tools, and the network context in which the manipulations are performed, will
govern the conclusions drawn about neural function, and that the ease with which manipulation experiments can
be performed belies the difficulty of their interpretation.

T-33. Distinct eye movement strategies differentially reshape visual space
Daniel Wood1
Pavan Ramkumar1,2
Joshua Glaser1
Patrick Lawlor2
Konrad Kording2
Mark A Segraves1
1 Northwestern
2 Rehabilitation

DANIELKENTWOOD @ GMAIL . COM
PAVAN . RAMKUMAR @ GMAIL . COM
JOSHGLASER 88@ GMAIL . COM
P. N . LAWLOR @ GMAIL . COM
KK @ NORTHWESTERN . EDU
M - SEGRAVES @ NORTHWESTERN . EDU

University
Institute of Chicago

In primates, the Frontal Eye Fields (FEF) control fixation choice and attention. The receptive fields (RFs) of FEF
neurons are not always statically fixed to a retinotopic location. Rather, they predictively “jump” or remap to their
postsaccadic locations just before the saccade begins. Two modes of perisaccadic remapping have been observed. In forward remapping, the RF jump follows a vector parallel to the upcoming saccade. In convergent
remapping, the RF is pulled toward the endpoint of the upcoming saccade. Here, we ask what adaptive purpose remapping may serve for visual search behavior in natural scenes, characterized by exploratory saccades
to gather peripheral visual information, and exploitative saccades to foveate the search target. We hypothesize
that convergent remapping is adaptive for exploitative saccades because they momentarily enhance visual representation around the saccade target. Likewise, we hypothesize that forward remapping is adaptive for exploratory
saccades because it preserves visual space around the post-saccadic location. To test these hypotheses, we
recorded from FEF neurons while monkeys searched for a target in a noise background. To facilitate RF mapping, salient visual probes were rapidly flashed at random locations during the search behavior. We used reverse
correlation to estimate how RFs interacted with the probes. Critically, our approach enabled us to infer RF location continuously throughout natural search behavior. As predicted, we observed forward remapping around
exploratory saccades and convergent remapping around exploitative saccades as quantified by the difference
between the distance of the estimated RF centers and the expected RF centers for the forward vs. convergent
hypotheses (see Details). Thus, the reshaping of visual space in FEF is adaptive to saccadic strategy.

T-34. Optimal degrees of synaptic connectivity
Ashok Litwin-Kumar1
Kameron Decker Harris2
Richard Axel1
Haim Sompolinsky3
LF Abbott1

AK 3625@ COLUMBIA . EDU
KAMDH @ U. WASHINGTON . EDU
RA 27@ COLUMBIA . EDU
HAIM @ FIZ . HUJI . AC. IL
LFA 2103@ COLUMBIA . EDU

1 Columbia

University
of Washington
3 Hebrew University of Jerusalem
2 University

Synaptic connectivity varies widely across neuronal cell types. In the cerebellar cortex, for example, granule cells
receive five orders of magnitude fewer inputs than the Purkinje cells they innervate. A large divergence in synaptic
connectivity is also seen in other circuits with cerebellum-like architectures, including the insect mushroom body.
In the cerebral cortex, on the other hand, the number of inputs per neuron is more uniform and large. To determine
the consequences of these different degrees of connectivity, we investigate how the dimension of a representation

COSYNE 2017

43

T-35 – T-36
formed by a population of neurons depends on the number of inputs they each receive. We then formally relate
our measure of dimensionality to the performance of a Hebbian classifier of population responses. Our theory
predicts optimal values for the number of inputs to cerebellar granule cells and Kenyon cells of the Drosophila
mushroom body that match the observed values, demonstrating that sparse connectivity (fewer than ten inputs
per neuron) can be superior to dense connectivity (thousands) in some cases. These conclusions hold for circuits
with locally random wiring (as shown for mushroom body Kenyon cells and hypothesized for cerebellar granule
cells) whose input synapses are not subject to supervised synaptic plasticity. If these synapses are subject to
supervised learning, however, the optimal connectivity shifts toward dense wiring. We propose that the presence
of supervised plasticity distinguishes processing in the input layers of cerebral and cerebellar cortices.

T-35. Meta-reinforcement learning: A bridge between prefrontal and dopaminergic function
Jane Wang1
Zeb Kurth-Nelson1
Dhruva Tirumala1
Joel Leibo1
Hubert Soyer1
Dharshan Kumaran1
Matthew Botvinick1,2

WANGJANE @ GOOGLE . COM
ZEBK @ GOOGLE . COM
DHRUVAT @ GOOGLE . COM
JZL @ GOOGLE . COM
SOYER @ GOOGLE . COM
DKUMARAN @ GOOGLE . COM
BOTVINICK @ GOOGLE . COM

1 DeepMind
2 University

College London

There currently exist two largely independent literatures addressing the neural mechanisms underlying rewardbased learning, one focusing on dopaminergic function and the other focusing on neocortex, and in particular the
prefrontal cortex. We advance a novel computational framework aimed at bridging the gap between these two
literatures. Building on recent advances in machine learning, we introduce deep meta-reinforcement learning,
a computational paradigm in which model-free reinforcement learning (RL) optimizes the connection weights in
a recurrent neural network, during training on a series of related problems. As we demonstrate through proofof-concept simulations, this setup gives rise emergently to a second, free-standing RL algorithm which is implemented in the dynamics of the recurrent network, and which is adapted to the structure of the training domain.
Deep meta-RL has a natural neuroscientific interpretation, which identifies the primal model-free RL algorithm
with dopaminergic function and the recurrent network with prefrontal cortex. Through a series of five simulation
studies we demonstrate how deep meta-RL can provide a parsimonious account for a range of neuroscientific
findings pertaining both to dopaminergic and prefrontal function. Among other points, the framework provides
an explanation for previously puzzling aspects of dopamine dynamics, and offers a novel perspective on the
distinction between model-free and model-based RL.

T-36. Thalamic contributions to neocortico-hippocampal interactions during
sleep
Carmen Varela
Matthew Wilson

CARMENV @ MIT. EDU
MWILSON @ MIT. EDU

Massachusetts Institute of Technology
Understanding the dynamics of brain region interactions at the cellular level is a fundamental challenge in systems
neuroscience. The long-term storage of memory traces requires precisely timed interactions between cells in the
hippocampus (which acquires and stores recent episodic memory) and the neocortex (involved in long-term memory storage) during sleep. The largely unexplored nuclei of the midline thalamus (e.g., the reuniens, ventromedial

44

COSYNE 2017

T-37 – T-38
and mediodorsal nuclei) are uniquely positioned to regulate the memory network formed by CA1 in the hippocampus and the medial prefrontal cortex (mPFC). All midline nuclei have strong projections to mPFC, and the nucleus
reuniens in particular sends collateral projections to both mPFC and CA1 (Varela et al., 2014; Varela, 2014). I will
present results from the first simultaneous extracellular recordings of single units and local field potentials (LFPs)
in the midline thalamus, mPFC and CA1 in freely behaving rats. The results of these pioneering experiments
suggest that the midline thalamus may contribute to sleep interactions between neocortex and hippocampus in
two ways: 1. by setting up the neocortical stage for the reception of hippocampal information, and 2. by gating
the transfer of information from hippocampus to neocortex.

T-37. Humans flexibly incorporate attention-dependent uncertainty into perceptual decisions and confidence
Rachel Denison
William Adler
Marisa Carrasco
Weiji Ma

RACHEL . DENISON @ NYU. EDU
WILL . ADLER @ NYU. EDU
MARISA . CARRASCO @ NYU. EDU
WEIJIMA @ NYU. EDU

New York University
Humans are known to take uncertainty induced by stimulus factors such as contrast into account to improve
perceptual decision-making. However, uncertainty depends not only on stimulus properties, but also on internal
factors such as attention. Here we asked whether humans adjust their perceptual decisions and confidence
reports to account for uncertainty, when uncertainty is manipulated from trial to trial using an attentional cue. In
an orientation categorization task, we found that decision and confidence boundaries shifted as an approximately
Bayesian function of attention-dependent uncertainty. The results are the first to show that perceptual decisionmaking responds flexibly to attention-dependent uncertainty. Because attention is typically distributed unevenly
across a visual scene, this responsiveness likely improves perceptual decisions in natural vision.

T-38. Layer-specific reorganization of neocortical dendritic inhibition during
active wakefulness.
Robin Tremblay1,2
William Munoz1
Daniel Levenstein1
Bernardo Rudy1
1 The
2 New

TREMBLAY. ROBIN 83@ GMAIL . COM
WMM 247@ NYUMC. ORG
DANIEL . LEVENSTEIN @ NYU. EDU
BERNARDO. RUDY @ NYUMC. ORG

Neuroscience Institute
York University Medical Center

Dendritic inhibition is a powerful circuit mechanism gating synaptic integration and regenerative events along the
dendritic arbor of neocortical pyramidal cells. Anatomical and imaging methods have provided major advance into
our understanding of the laminar organization of excitatory axons coming from different sources, their activity patterns during behavior and their impact on dendritic integration and pyramidal cells firing. Inhibition strategically positioned onto dendrites could tightly regulate the differential impact of incoming signals during specific behavioral
contexts. However, little is known about the spatiotemporal pattern of dendritic inhibition in neocortex during behavior. In this study, we show layer-specific dynamics of the inhibitory action of somatostatin-expressing interneurons, a major class of GABAergic neuron specialized for dendritic inhibition. Using an optic fiber-coupled patch
electrode in awake mice expressing channel-rhodopsin in somatostatin cells, we were able to record the activity
and recover the complete dendritic and axonal morphology of a large number of light-responsive, somatostatinexpressing interneurons from all cortical layers. Somatostatin interneuron morphological subtypes, residing in
different cortical layers and innervating distinct laminar domains, exhibited strikingly different activity patterns dur-

COSYNE 2017

45

T-39 – T-40
ing active wakefulness. This revealed the spatiotemporal pattern of dendritic inhibition during different states of
wakefulness, where specific layers, hosting specific cell types and dendritic compartments such as NMDA or
Ca2+ spikes initiation zones are subject to increases or decreases in dendritic inhibition during quiet or active behaviors. Phamacological, pharmacogenetics and genetic manipulations in vivo revealed the causal involvement
of Vip-expressing interneurons and cholinergic modulation acting as a push-pull mechanism determining somatostatin interneuron’s in vivo activity patterns during those states. Our results reveal a context-dependent laminar
organization of somatostatin interneuron-mediated inhibition, with implications for compartmentalized regulation
of dendritic signaling in the mammalian neocortex during behavior.

T-39. How could neuroscientists understand a microprocessor?
Eric Jonas1
Konrad Kording2
1 University

JONAS @ EECS . BERKELEY. EDU
KOERDING @ GMAIL . COM

of California, Berkeley
University

2 Northwestern

The development of high-throughput data acquisition technologies in neuroscience has lead to a renewed interest in neurostatistical analysis of complex datasets. However, there has been debate among the computational
neuroscience community, especially at COSYNE, as to the interpretability and scientific utility of dimensionalityreduction techniques. As we lack an actual understanding of the inner-workings of cortical areas and even subcortical areas like basal ganglia, and hippocampus, it can be incredibly difficult to evaluate the quality of the recovered
insight. We have recently highlighted problems in neuroscience in converting datasets into meaningful insights,
showing how common neuroscience approaches, when applied to data from a known system, a microprocessor,
often fail to give meaningful answers. Such technical systems should be particularly easy to understand, after
all, in a simulation, it is possible to do any imaginable experiment with perfect precision. This approach led us
to review a range of problems in the way with which neuroscience approaches make inferences about function.
There are three obvious ways in which neuroscience can overcome these problems. (1) Experiments that truly
isolate a single computation can lead to more meaningful insights. (2) Data analysis methods that do justice to
nonlinear, heterogeneous, recurrent systems can improve the resulting conclusions. (3) Theories that are more
constrained can make the analysis far simpler.

T-40. Quantitatively-tuned internal predictions in a Drosophila visuomotor
network
Anmo Kim
Lisa Fenk
Cheng Lyu
Gaby Maimon

ANMO. KIM @ GMAIL . COM
LFENK @ ROCKEFELLER . EDU
CLYU @ ROCKEFELLER . EDU
MAIMON @ ROCKEFELLER . EDU

Rockefeller University
Vision influences behavior, but ongoing behavior also modulates vision across the animal kingdom. The function
and mechanisms of most such modulations remain unresolved. We used patch-clamp electrophysiology to comprehensively measure motor-related inputs to a network of eighteen identified visual neurons in tethered, flying
Drosophila. During flight turns, we found that motor-related inputs arrive to all visual neurons, but the strength
of the input varied systematically across cell types. Why are some visual neurons modulated more strongly than
others? Insight into this question arose from examining each cell’s visual responses to rotational optic-flow. We
found that the stronger a neuron responded to optic flow about the yaw axis, the larger was the measured motor
related modulation in that cell. Via high-speed videography, we next measured the head movements that flies
make during flight turns and found that Drosophila attempt to maintain their heads stable along roll and pitch axes

46

COSYNE 2017

T-41
during turns, but they make an active, anti-stability, head rotation about the yaw axis, in the direction of the turn.
Finally, we used genetic methods to silence the eighteen optic-flow sensitive neurons of interest and found that
flies could no longer perform normal visually driven stability movements of their heads. Together, these data support the hypothesis that these optic-flow sensitive neurons participate in the control of stabilizing head movements
and that motor-related inputs aim to mute the predicted visual responses to yaw visual motion in this population
during turns, thus facilitating an anti-stability yaw head movement toward the new gaze direction. Motor-related
inputs to a stability reflex system that aim to suppress that system’s output during active, non-stabilizing, intended
actions are called efference copies, and these modulations in the fly visual lobe are thus consistent with them
serving an efference-copy function. This work describes a sophisticated, network-level computation in a small
brain.

T-41. Learning nonlinear dynamics in spiking networks with local plasticity
rules
Alireza Alemi1
Jean-Jacques Slotine2
Christian Machens3
Sophie Deneve1

ALIREZA . ALEMI @ ENS . FR
JJS @ MIT. EDU
CHRISTIAN . MACHENS @ NEURO. FCHAMPALIMAUD. ORG
SOPHIE . DENEVE @ GMAIL . COM

1 Ecole

Normale Superieure
Institute of Technology
3 Champalimaud Centre for the Unknown
2 Massachusetts

Understanding how neural networks can learn to predict and represent time-varying variables is a fundamental
challenge in neuroscience. A key complication is the error credit assignment problem: how to determine the
local contribution of each synapse to the network’s global output error. Previous work on solving this problem
in spiking networks has either been restricted to linear systems (Boerlin, Machens, Deneve 2013; Bourdoukan,
Deneve 2015), or to non-local learning rules (FORCE learning; Sussillo, Abbott 2009; Thalmeier et al 2016). Here
we show how to learn arbitrary non-linear dynamical systems with local learning rules. Our approach uses tools
from adaptive control theory, and applies them to a spiking network with nonlinear dendrites. The spiking network
receives its own tracking error through feedback and learns to approximate a nonlinear dynamical systems using a
purely local learning rule. The local credit assignment problem is solved because each neuron effectively contains
partial information of the error made by the entire network. This error is captured by the tightly balanced voltage
of each neuron. Here, a balanced network effectively acts as a predictive auto-encoder that learns to cancel its
own error and feedback. The resulting network is extremely efficient in terms of the number of spikes fired, and
it is highly robust to noise and neural elimination. It produces asynchronous, irregular spiking activity matching
the Poisson-like neural variability observed experimentally. Our framework has several important implications. It
suggests that a global learning problem, like learning to implement complex nonlinear dynamics from examples,
can be solved with local rules, as long as output errors are fed back as driving input signals. Our approach
inherits the analytical tools of control theory such as convergence and stability theorems that can now be applied
to learning in spiking networks.

COSYNE 2017

47

T-42 – T-43

T-42. The emergence of distributed functional networks in the early developing cortex
Bettina Hein1
Gordon B Smith2
David E Whitney2
Philipp Huelsdunk1
David Fitzpatrick2
Matthias Kaschube1

HEIN @ FIAS . UNI - FRANKFURT. DE
GORDON . SMITH @ MPFI . ORG
DAVID. WHITNEY @ MPFI . ORG
HUELSDUNK @ FIAS . UNI - FRANKFURT. DE
DAVID. FITZPATRICK @ MPFI . ORG
KASCHUBE @ FIAS . UNI - FRANKFURT. DE

1 FIAS
2 Max

Planck Florida Institute for Neuroscience

The cortical networks that underlie behaviour exhibit an orderly functional organisation at local and global scales,
an organisation that is especially evident in the visual cortex of carnivores and primates. Here, the full range
of stimulus orientations is represented in the activity of neighbouring columns of neurons contained within a
millimeter of cortical surface area, and these responses are shaped by a distributed network that shares similar
functional properties and is arranged in an iterated fashion across several millimeters of the cortex. In this study,
we use endogenous (spontaneous) cortical activity patterns to explore the fine scale functional architecture of this
distributed network and how the coordinated local and global structure of the network arises during development.
First, using in vivo imaging of calcium signals in the ferret, we show that in the mature visual cortex, the local
structure of orientation columns is accurately predicted by correlations of spontaneous activity with neurons that
lie at distances several millimeters away. Next, we show that large-scale modular correlation patterns, predictive
of the mature organisation, are evident at early states of cortical development, prior to the formation of long-range
horizontal network connections. The early emergence of long-range modular correlations without long-range
connections can be explained by local connections using a neural field model with connectivity consistent with
observed local correlation structure. Our results suggest that local connections in early cortical circuits can
generate structured long-range network correlations that underlie the formation of distributed functional networks.

T-43. Modular structures in recurrent neural networks trained for many cognitive tasks
Guangyu Robert Yang1
H Francis Song1
William Newsome2
Xiao-Jing Wang1
1 New

GYYANG . NEURO @ GMAIL . COM
SONG . FRANCIS @ GMAIL . COM
BNEWSOME @ STANFORD. EDU
XJWANG @ NYU. EDU

York University
University

2 Stanford

Recurrent neural networks trained for cognitive tasks have re-emerged as tools to understand computational
mechanisms of the brain. Previous studies mostly focused on networks trained to perform one or two tasks.
However, daily behaviors of animals involve a wide range of cognitive tasks, and it remains unclear how are
many tasks represented in the same neural circuit. To understand this question, here we analyze single networks
successfully trained to perform up to twenty widely-studied cognitive tasks. With clustering analysis, we show that
after training the networks developed task-specific modules: groups of units that are activated primarily in subsets
of tasks. Most units belong to one of these task-specific modules. In particular, we analyzed in detail a module
of units engaged only in a set of sensori-motor remapping tasks. By inspecting input and output connectivities
of these units, and by lesioning them, we delineated the neural circuit mechanism in these trained networks
underlying the remapping tasks. Using state-space analysis, we studied the neural representations across three
context-dependent decision-making tasks. Besides confirming previous experimental results, our model makes
predictions on the relationship of neural representations across these tasks. We also found a distinct group of
units that are task-generic instead of task-specific. They are insensitive to tasks or stimuli, but are selective to

48

COSYNE 2017

I-1 – I-2
task epochs. Our work demonstrates the efficiency and versatility of the approach of trained network analysis.
Model networks trained for many cognitive tasks allow us to address fundamental questions regarding neural
representations of multiple tasks and mechanisms of task switching that could otherwise be challenging to study.
We can further make experimentally-testable predictions on what kinds of cognitive tasks would be supported by
either shared or distinct neural substrates.

I-1. Exponential increase of storage capacity in an expansive autoencoder:
an analytical lower bound
Alia Abbara
Alireza Alemi

ALIA . ABBARA @ ENS . FR
ALIREZA . ALEMI @ ENS . FR

Ecole Normale Superieure
Neural circuits need to store information in order to perform a variety of computations, such as making inference,
prediction, and generalization. A fundamental problem for information storage, where memories are stored as
fixed-points of network dynamics in recurrent connections, is finding the maximal storage capacity. This problem
has been extensively studied in recurrent neural networks, making strong predictions about neural microcircuits
based on optimality of storage (Brunel 2016). However, optimal storage in networks with hidden units is not
well-understood. Here, we take a step toward characterizing storage properties of such networks by studying
the storage capacity of a simplified model with hidden units: an expansive (or overcomplete) autoencoder in
which the expansion ratio, defined as the number of hidden units over the number of visible units, is larger than
one. The encoding weights are set at random and kept fixed, while the decoding weights are learned using
a local learning rule. The information is stored as fixed-points of the network dynamics. We perform a naive
mean-field approximation (MFA) of the model, which becomes amenable to calculating an analytical lower bound
for the capacity using Gardner’s replica theory. The analytical results of the MFA confirm previous simulation
results (Alemi 2016), showing the lower bound for the storage capacity in the expansive autoencoder grows
exponentially with the expansion ratio. Additionally, the degree of symmetry between the encoding and decoding
weights increases with the expansion ratio when the network operates at maximal capacity. The network can also
become robust to noise by introducing a robustness parameter in the learning process of decoding connections.
These novel analytical results show how neural architectures with expansive hidden layers can benefit information
storage, paving the way towards characterizing storage and generalization properties of deep networks.

I-2. Continuous-time point-process GPFA using sparse variational methods
Vincent Adam1,2
Lea Duncker1,2
Maneesh Sahani1,2
1 Gatsby

VINCENTA @ GATSBY. UCL . AC. UK
DUNCKER @ GATSBY. UCL . AC. UK
MANEESH @ GATSBY. UCL . AC. UK

Computational Neuroscience Unit
College London

2 University

The latent structure underlying high-dimensional neural spike trains on single trials may reflect the encoding
of both external signals and internal computational variables, and thus methods to characterise such structure
have become an important tool in the analysis of motor behavior, decision making, and sensory processing on a
single-trial basis. While the general analysis of time-series is well-understood, adapting such methods to neural
spike data is challenging. Tractable models assume conjugate Gaussian likelihoods, which often provide poor
descriptions of neural data on a millisecond time-scale, and more appropriate Poisson count models require
computationally burdensome approximations that do not scale well to higher temporal resolutions. Thus, most
current approaches describe spike counts using bins of widths on the order of tens of milliseconds or more,
and so cannot identify signals that may be carried by the more precise alignment of spikes. To overcome these

COSYNE 2017

49

I-3 – I-4
challenges, we propose a novel method for inferring neural trajectories from high-dimensional spike trains via a
continuous-time point-process extension of Gaussian Process Factor Analysis (GPFA). Our method models spike
times directly, without the need to fix a bin resolution. The cost of inference scales linearly with the number of
spiking events, rather than cubically in the number of time-bins (as does standard GPFA, which typically prohibits
its application at very fine timescales). Furthermore, the variational structure which makes this transition to
continuous time possible also facilitates the use of stochastic gradient methods and application to large data sets.
We compare our continuous time point-process approach with a discretised-time Poisson count analogue. Using
simulated data we show that our model better recovers latent structure in spike trains and provides improved firing
rate estimates.

I-3. A planning game reveals individual differences in strategy that are highly
constrained
Gautam Agarwal1
Tiago Quendera1
Marta Ferreira1
Zachary Mainen2
1 Champalimaud
2 Champalimaud

AGARWALLED @ GMAIL . COM
TIAGOQUENDERA @ GMAIL . COM
MARTA . RAMALHO. FERREIRA @ GMAIL . COM
ZMAINEN @ NEURO. FCHAMPALIMAUD. ORG

Neuroscience Program
Centre for the Unknown

Decision-making has been modeled in great detail based on 2-alternative choice (2AC) tasks; however it remains
unclear how these models apply to more naturalistic settings, where choices can have long-term and diverse
consequences. In turn, quantitatively modeling more complex decisions poses a challenge, requiring adequate
sampling of behavior over a larger state space. To address this problem, we have developed a video game in
which subjects can flexibly solve multi-step planning problems that have identifiable optimal solutions. In this
game, subjects must plan a path that allows them to collect multiple growing objects at appropriate times in the
future. By parametrically varying the number and spatio-temporal arrangement of targets, we densely sample
the stimulus space. We examine three influences on planning behavior: 1. inter-subject variability due to the
different strategies used by different individuals; 2. inter-stimulus variability arising from how subjects represent
and act on different target configurations; 3. trial-by-trial changes that reflect learning, as well as cognitive and
motor sources of noise. We find that over time, subjects settle onto one of two strategies to solve the task: the
first is to collect targets in the order that they appear, regardless of their location; the other is to collect nearby
targets before proceeding to more distant ones. Surprisingly, subjects using a particular strategy show similarities
in other, seemingly unrelated, behavioral attributes, such as how they update their plan during navigation, and the
cushion they leave for reaching the final target on time. We propose that these co-occurring behavioral attributes
represent cognitive “synergies” that accommodate the unique challenges arising while using specific strategies.

I-4. Codependent synaptic plasticity for stable dynamics and efficient learning
Everton J Agnes
Tim P Vogels

EVERTON . AGNES @ CNCB . OX . AC. UK
TIM . VOGELS @ CNCB . OX . AC. UK

University of Oxford
Growing experimental evidence shows that excitatory and inhibitory synapses interact during long-term synaptic
plasticity. Such a direct interaction has been ignored in all previous plasticity models, which focus on isolated
mechanisms for each synapse type. When multiple plasticity rules are combined in the same network, each
rule must be precisely tuned to avoid disparate homeostatic firing rate set points for excitatory and inhibitory
synaptic plasticity (ESP and ISP, respectively). Such unrealistic fine-scale control of thousands, or even millions of

50

COSYNE 2017

I-5
synapses is necessary for stable receptive fields formation and synaptic weights that neither explode nor vanish.
Here we present a phenomenological model of synaptic plasticity which includes a direct interaction between
synapses to autonomously stabilize dynamics and function. We show that when ISP depends on spike times
AND currents from neighbouring excitatory and inhibitory synapses, such “codependent“ ISP balances excitation
and inhibition without imposing a firing rate fixed point, and thus preventing a destructive competition with ESP.
Similarly, codependent ESP is also controlled by both inhibitory and excitatory inputs. Inhibitory currents create
an efficient gate for learning based on disinhibition, allowing quick development of stable receptive fields, while
excitatory currents ensures that the sum of excitatory weights is constant. As a result, a neuron may be tuned to
respond to a combination of correlated inputs, similar to performing principal component analysis. Finally, using a
simplified dendritic tree model, we show that our codependent learning rules provide a clustering mechanism for
inputs with correlated activity at the dendritic level too. In agreement with recent experiments, our results highlight
the functional implications of codependent plasticity mechanisms for memory formation and network stability.

I-5. C. elegans locomotion is governed by a 6D quasi-Hamiltonian chaotic
attractor
Tosif Ahamed
Greg Stephens

TOSIF. AHAMED @ OIST. JP
GREG . STEPHENS @ OIST. JP

Okinawa Institute of Science and Technology
Animal behavior is often thought to be composed of discrete stereotyped motifs and transitions between them.
This view has been strengthened by recent studies that utilized advances in Machine Learning to map the behavioral motifs for several different model animals. However, this discrete description is only an approximation,
and ignores the variability within each motif. Here, we propose an alternative description of behavior based on
the fact that it is fundamentally a spatiotemporal dynamical system. We combine video tracking of the 2D postures of C. elegans with concepts from the theory of nonlinear dynamical systems to reconstruct a 6 dimensional
phase space that provides a continuous description of worm’s locomotion. Dynamics in the reconstructed phase
space lie on a low-dimensional attractor. Globally, the attractor is composed of three sets of cyclic trajectories
that form the animal’s most stereotyped behaviors: forward, backward and turning locomotion. Thus, discrete
behavioral motifs naturally emerge out of our continuous description. In contrast to this global stereotypy, we
also observe substantial variability at small scales, which is reflected in the estimated Lyapunov spectrum. We
find two positive and a zero Lyapunov exponent, along with three negative exponents indicating the presence of
a hyperchaotic attractor. Surprisingly, the Lyapunov spectrum is symmetric about a small negative value, suggesting a quasi-Hamiltonian structure to the dynamics where the system differs from Hamiltonian systems only
by a uniform damping term. Finally, we estimate the Kolmogorov-Sinai entropy and show that it decreases as
the animals adapt from a more complex diffusive search strategy to a simple ballistic search strategy. Combined
with genetic perturbations and modern tools to record and control neural activity, our analysis will enable detailed
inquiry into the nature of animal behavior and how it is controlled at a genetic and neural level in this important
model organism and beyond.

COSYNE 2017

51

I-6 – I-7

I-6. Observing the observer observing: forgetful world modelling in a selfstimulation task
Sanjeevan Ahilan1,2
Rebecca B Solomon3
Kent Conover3
Ritwik Niyogi4,5
Peter Shizgal3
Peter Dayan1,2

AHILAN @ GATSBY. UCL . AC. UK
RB SOLOMON @ HOTMAIL . COM
KENT. CONOVER @ GMAIL . COM
RITWIK 7@ GMAIL . COM
PETER . SHIZGAL @ CONCORDIA . CA
DAYAN @ GATSBY. UCL . AC. UK

1 Gatsby

Computational Neuroscience Unit
College London
3 Concordia University
4 Johns Hopkins University
5 University of Oxford
2 University

Along with the direct requirements which humans and other animals must meet to solve the explicit tasks that
experimenters pose, there are often implicit rules, contingencies and constraints that can be exploited to achieve
efficient and effective performance. These often involve complex structural and long-range temporal dependencies which are hard to learn. Here, we examined a series of experiments in which rats had the opportunity over the
course of many trials to build and use models of higher order transition structures. We show characteristic signs
that they had indeed learnt these contingencies, albeit suffering at times from immediate inferential imperfections
as to their current state within the structure. To explain these instances, we built a hidden Markov model (HMM) of
the subjects’ models of the experiment, describing overall behaviour as integrating current sensory evidence with
an imperfect memory. We found that over the course of training, subjects tracked more accurately their progress
through the task, being better able to infer the true state. Parameter fits using maximum likelihood estimation
attributed this improvement to decreased forgetting of the previous state. This ‘learning to remember’ decreases
reliance on the current sensory evidence, which can be misleading, in favour of a more dependable memory.

I-7. Ultra-fast whole brain imaging during behavior in adult Drosophila
Sophie Aimon1
Takeo Katsuki1
Logan Grosenick2
Michael Broxton2
Karl Deisseroth2,3
Terrence Sejnowski4
Ralph Greenspan1

AIMON . SOPHIE @ GMAIL . COM
TKATSUKI @ UCSD. EDU
LOGANG @ GMAIL . COM
BROXTON @ GMAIL . COM
DEISSERO @ STANFORD. EDU
TERRY @ SALK . EDU
RGREENSPAN @ UCSD. EDU

1 University

of California, San Diego
University
3 Howard Hughes Medical Institute
4 Salk Institute for Biological Studies
2 Stanford

Our goal is to characterize brain activity at the scale of the whole network and close to the speed of the fastest
neuronal computations, while a fly is behaving spontaneously head-fixed under the microscope. Fast calcium or
voltage sensors are expressed, either pan-neuronally, or in subsets of neurons broadly distributed in the brain (e.
g. acetylcholine or dopamine neurons). We image the fly brain fluorescence using light field microscopy, which
captures large volumes in a single camera exposure, albeit at reduced lateral resolution compared to the normal
2D diffraction limit of the microscope. 3D stacks are then reconstructed offline from the light field images using
wave optics to model point spread functions, before applying 3D deconvolution. This technique makes it possible
to image the whole brain at a frame rate of 200 Hz, which is at least an order of magnitude faster than other existing
techniques to our knowledge. We first make comparative maps of activity during the different behaviors. We find

52

COSYNE 2017

I-8
that the activity increases on a global scale when the fly walks but not when it grooms. We also use statistical
techniques (PCA and spatial ICA) to extract maps of spatially distinct sources of activity as well as their time series.
Even though PCA and ICA are mathematical algorithms that make minimal assumptions about the brain (ICA
assumes spatial sparsity), most component’s maps correspond to well-characterized anatomical structures. The
associated time series permit the identification of sub-neuropile areas and sometimes specific neurons involved in
walking, turning, or grooming. Patterns of spontaneous activity specific to some brain structures are also detected,
including a flip-flop activity in the noduli and protocerebral bridge.

I-8. All-optical mapping of local circuit connectivity in vivo
Laurence Aitchison1
Philippe Castonguay2,3
Jinyao Yan2,3
Adam Packer4
Lloyd Russell4
Michael Hausser4
Srini Turaga2

LAURENCE . AITCHISON @ GMAIL . COM
PH . CASTONGUAY @ GMAIL . COM
YANJ 11@ JANELIA . HHMI . ORG
ADAMPACKER @ GMAIL . COM
LLERUSSELL @ GMAIL . COM
M . HAUSSER @ UCL . AC. UK
TURAGAS @ JANELIA . HHMI . ORG

1 University

of Cambridge
Research Campus
3 Howard Hughes Medical Institute
4 University College London
2 Janelia

Mapping synaptic connectivity in vivo is a causal inference problem which cannot be solved with observational
data alone: correlations in neural activity can have many explanations ranging from unobserved common input
to a direct connection in either (or both) directions. Directly stimulating neurons is thus essential to the unambiguous inference of neural connectivity and its direction. Recent advances in optics and optogenetics enable
simultaneous cellular-resolution stimulation and recording of neural activity. Such techniques open the possibility
of mapping neural connectivity in vivo by photo-stimulating a single neuron and observing the response. However, inferring connectivity from such data remains a challenging problem as calcium provides an indirect, highly
variable measure of only supra-threshold activity. We developed a model-free connectivity inference scheme
by testing whether activity in a post-synaptic neuron depends on pre-synaptic photo-stimulation. While the precise change in post-synaptic activity evoked by photo-stimulation can be difficult to determine accurately, we
asked a simpler question: does post-synaptic activity allow us to determine whether a given pre-synaptic neuron
was photo-stimulated? If so, then the two variables (photo-stimulation and the post-synaptic calcium signal) are
dependent. As we have independent control over the photo-stimulation time, dependency strongly suggests a
mono-synaptic or poly-synaptic pathway from the pre-synaptic neuron to the post-synaptic neuron. We performed
independence testing by training a deep neural network for each pair of neurons, resulting in many tens of thousands of independence tests. We generated a null distribution for this hypothesis test using shuffled data. The
final hypothesis test yielded predictions of neural connectivity for neurons in layer 2/3 of V1 which correlated with
similarity in neural tuning. As a control, we considered a dataset recorded with and without synaptic blockers. Our
method found significantly fewer connections in the data with blockers.

COSYNE 2017

53

I-9 – I-10

I-9. STDP under physiological calcium concentration and implications for
memory maintenance
Johnatan Aljadeff1
Yanis Inglebert2
Dominique Debanne2
Nicolas Brunel1
1 University

ALJADEFF @ UCHICAGO. EDU
Y. INGLEBERT @ GMAIL . COM
DOMINIQUE . DEBANNE @ UNIV- AMU. FR
NBRUNEL @ GALTON . UCHICAGO. EDU

of Chicago
Universite

2 Aix-Marseille

In classical spike-timing dependent plasticity (STDP), a presynaptic spike followed by a postsynaptic spike leads
to long-term potentiation (LTP), while the reverse timing induces long-term depression (LTD). Beyond timing,
important factors that control synaptic plasticity are the firing-rates and postsynaptic Ca2+ entry. Previous measurements of STDP used non-physiological extracellular calcium concentrations (2–3mM). The lower calcium
concentration in young rats (1.3–1.8mM) is thought to correspond to smaller calcium transients, which in turn are
expected to modify STDP. We measured STDP at the CA1-CA3 synapse at three different extracellular calcium
concentrations, [Ca2+] = 1.3, 1.8 and 3.0mM, and found that the sign, shape and magnitude of plasticity strongly
depend on [Ca2+]. A protocol with a single pre and postsynaptic spike yielded only LTD at 1.8mM and no plasticity at 1.3mM. We observed LTD at 1.3mM, and both LTD and LTP at 1.8mM when the postsynaptic neuron fired
more than once. Repeating the protocol at higher pairing frequencies resulted in some cases in a change of sign,
from LTD to LTP. We constructed a calcium based plasticity model, where the synaptic efficacy is controlled by
intrinsic bistability and where depression and potentiation depend on the transient postsynaptic calcium. A crucial
component of the model that allows it to quantitatively account for the entire dataset is the contribution of the
NMDA nonlinearity to the transient calcium concentration. In line with recent experimental work, this contribution
depends on coincident firing of pre and postsynaptic neurons, and the resulting calcium transient is endowed with
a long decay timescale. Finally we studied the memory retention properties of this synapse model in the face of
irregular background activity, and found that the strong nonlinearity implies a resiliency to noise, especially when
firing-rate correlations are weak.

I-10. Global representations of goal-directed behavior in distinct cell types of
mouse neocortex
William E Allen*
Isaac Kauvar*
Michael Z Chen
Ethan Richman
Liqun Luo
Karl Deisseroth

WALLEN @ STANFORD. EDU
IKAUVAR @ STANFORD. EDU
MCHEN 96@ STANFORD. EDU
RICHMAN @ STANFORD. EDU
LLUO @ STANFORD. EDU
DEISSERO @ STANFORD. DU

Stanford University
The successful planning and execution of even simple adaptive behaviors may require the long-range coordination
of neural circuitry. In complex mammalian nervous systems capable of diverse actions and cognitive operations,
cooperation among networks of neurons throughout the cerebral cortex is thought to be necessary for the generation of discrete and consistent goal-directed behaviors. ‘Top-down’ theories of cortical function suggest that
representations of information about an animal’s current state and past experience could be used to orchestrate cortical activity toward a single goal; however the neuronal implementation of such cortex-wide coordination
signals remains unclear. Here, we develop and apply complementary methods of cortex-wide Ca2+ imaging in
awake, behaving mice performing a goal-directed decision-making behavior, and identify a global cortical representation of task engagement that is decoupled from motor output. Surprisingly, this information is reliably
encoded in the activity dynamics of both single cells and superficial neuropil distributed across the majority of
dorsal cortex, spanning many anatomically defined cortical regions. Using multiple modeling approaches, we find

54

COSYNE 2017

I-11 – I-12
cortex-wide clusters of single cells with similar reliable activity, and show that cellular activity is more significantly
predicted by task state than by motor output. The activity of multiple molecularly-defined cell types was found
to reflect this global representation in distinct ways with class-specific dynamics. Local inhibition of a premotor
cortical region blocks both voluntary behavior and the cortex-wide response to task-initiating cues. Optogenetic
stimulation of this premotor region is likewise capable of evoking a cortex-wide response. These findings show
that task-related information is broadcast globally to multiple cell types which respond differently through the cortex, and that a major cortical node gates this broadcast, revealing cell type-specific processes in cortex for globally
communicating neural activity representing behavioral goals.

I-11. Medial frontal circuits encode reward context and control consummatory
behavior
Linda Amarante1
Marrcelo Caetano2
Nicole Horst3
Mark Laubach1
1 American

LINDA . AMARANTE @ AMERICAN . EDU
MARCELOSCAETANO @ GMAIL . COM
NKHORST @ GMAIL . COM
MARK . LAUBACH @ AMERICAN . EDU

University

2 UFABC
3 Cambridge

University

Previous studies from our lab have reported top-down control signals from the mPFC that enable adaptive control
over actions (e.g. Narayanan and Laubach, 2006). Key findings were that persistent neuronal activity encodes
behavioral outcomes from previous trials and is terminated by rewards (e.g. Narayanan et al., 2013). We later
found that the act of consuming a reward itself activates neurons in the rostral part of the mPFC (Horst and
Laubach, 2013). Here, we used a new task to examine reward-related signals in the medial, orbital, and motor
cortices (Parent et al., 2015a,b). Time-frequency analysis of field potentials and spike activity found learningrelated changes in the delta, theta, and beta frequencies. These signals encoded relative reward value. Pairwise
mutual information and multivariate dimensionality reduction methods found evidence for a clustering of neuronal
activity that was strictly partitioned across the three cortical areas. Directional coherence analysis found evidence
for forward (driving) influences at 8–12 and 20 Hz from the motor, orbital, and caudal mPFC on licking-related
signals in the rostral mPFC. The strongest influences on mPFC activity was from the oral motor cortex. We used
pharmacological and chemogenetic methods to perturb the oral motor cortex while recording neuronal activity in
the mPFC. These perturbations increased delta band power, reduced theta band entrainment, and reduced crosselectrode coherence in the mPFC. To determine if these signals encode reward per se or the context/expectation
of a given reward magnitude, the task was modified so that rats made instrumental licks in two reward contexts
(blocked presentations of higher and lower magnitude rewards). This experiment found that the mPFC is driven
by reward context, not reinforcement per se. Together, our findings suggest a functional circuit in which the rodent
mPFC is influenced by other frontal regions to encode the relative value of reward and control consummatory
behavior.

I-12. Somatosensory cortex plays an essential role in forelimb motor adaptation in mice
Mackenzie Amoroso
Alexander Mathis
Naoshige Uchida

AMOROSO @ G . HARVARD. EDU
AMATHIS @ FAS . HARVARD. EDU
UCHIDA @ MCB . HARVARD. EDU

Harvard University
Our motor outputs are constantly re-calibrated to adapt to systematic perturbations. This motor adaptation is

COSYNE 2017

55

I-13 – I-14
thought to depend on the ability to form a memory of a systematic perturbation, often called an internal model.
However, the mechanisms underlying the formation, storage, and expression of such models remain unknown.
Here, we developed a mouse model to study forelimb adaptation to force field perturbations. We found that mice
can adapt to the perturbation: forelimb displacement was reduced over tens of trials, and this adaptation was
best fit by a model that learned from sensory prediction errors, but not reward prediction errors. Using this task,
we next sought to examine neural circuits underlying motor adaptation. We found that optogenetic inactivation of
somatosensory cortex (S1) applied concurrently with the force field essentially abolished the motor adaptation.
This S1 inactivation did not impair basic motor control abilities or the mice’s performance in a reward-based
learning task. While this result indicates that S1 is required to adapt, it was not clear whether S1 is required for
updating or holding the internal model of the force-field. We found that S1 inactivation after partial adaptation
blocked further adaptation but did not affect the expression of already-adapted motor commands. These results
demonstrate that S1 is critically involved in updating the estimate of a force-field.

I-13. A model of high-acuity vision in the presence of fixational eye movements
Alexander Anderson
Kavitha Ratnam
Austin Roorda
Bruno Olshausen

AGA @ BERKELEY. EDU
KAVITHA @ BERKELEY. EDU
AROORDA @ BERKELEY. EDU
BAOLSHAUSEN @ BERKELEY. EDU

University of California, Berkeley
Experiments by Ratnam et al. (2015) demonstrate the benefit of eye movements for the discrimination of a
diffraction-limited tumbling E near the sampling limit of the cone photoreceptor array. Compared to a stabilized
E, participants perform better when the projection of the E moves on the retina with the same motion statistics
as drift eye movements, but not necessarily correlated to the true eye motion. In order to better understand
the neural circuitry that underlies these psychophysical results, we propose a computational model based on a
Bayesian ideal observer that attempts to estimate the spatial pattern on the retina given simulated RGC spikes.
Our Bayesian model both corroborates the psychophysical measurements and suggests a neural mechanism.
We extend previous work by Burak et al. (2010) by creating a novel, online approximation to the expectationmaximization algorithm that generalizes to the case of continuous eye movements and non-trivial image priors.
From this emerges a neural model containing two populations of cells which we hypothesize to exist in primary
visual cortex: one which encodes the spatial pattern using a sparse code and another which tracks eye position
and is used to dynamically route information coming from LGN afferents feeding into the pattern cells. Finally,
we demonstrate that our algorithm points the way to a novel retina-like camera that uses spatially sharp receptor
acceptance angles (compared to the lattice spacing) to achieve super-resolution.

I-14. Cortical gain adaptation to extract signals from background noise
Chris Angeloni
Mark Aizenberg
Maria Geffen

CHRISAN @ SAS . UPENN . EDU
EISENBERG @ GMAIL . COM
MGEFFEN @ MED. UPENN . EDU

University of Pennsylvania
Natural acoustic environments are noisy, and in order to function in them it is necessary to reliably detect and
differentiate important sounds from background noise. Recent work demonstrated that neurons in auditory cortex
(AC) modulate their response gain in an adaptive manner to account for changes in spectro-temporal statistics
of acoustic scenes (Rabinowitz et al., Neuron, 2011; Natan et al., Cereb. Cortex, 2016). These results provide
a potential mechanism by which neurons can develop stimulus representations that are invariant to a variety of

56

COSYNE 2017

I-15 – I-16
background noise profiles. However, the relationship between gain modulation and behavioral detection of signals
in noise is currently unknown. Here, we test whether and how gain adaptation shapes neural and behavioral
responses to tones embedded in noise and examine the temporal dynamics of this process. To assess the
impact of gain adaptation on behavioral performance, we trained mice to perform a go-nogo task in which they
detected a tone embedded in white noise and presented tones at threshold volume embedded in variable lengths
of preceding noise. We recorded single-unit responses in AC to the same stimuli, both before training and during
detection behavior. Behaviorally, detection performance at threshold SNR levels exhibited a pattern consistent
with increased adaptation to the background noise. Mice were less likely to detect tones preceded by a short
durations of noise, but were more likely to detect tones preceded by long durations of noise. These results agree
quantitatively with electrophysiological recordings in that most of the recorded neurons in auditory cortex exhibited
similar modulation by noise duration. Namely, tone-evoked responses AC neurons increased as the duration of the
preceding noise increased. Taken together, these results are consistent with our hypothesis that gain adaptation
in auditory cortex improves behavioral detection of signals embedded in noise.

I-15. Marked point process filter for clusterless, adaptive encoding for realtime decoding
Kensuke Arai1
Daniel Liu2
Loren Frank2
Uri Eden1
1 Boston

KENSUKE . Y. ARAI @ GMAIL . COM
DANIEL . LIU @ UCSF. EDU
LOREN @ PHY. UCSF. EDU
TZVI @ MATH . BU. EDU

University
of California, San Francisco

2 University

Causal relationships between specific neural activity and behavior can be investigated with real-time, closedloop experiments. An important advance in realizing this is the marked point process filtering framework which
utilizes the “mark“ or the waveform features of unsorted spikes, to construct a relationship between these features
and behavior, described by the mark intensity function, which we call the encoding model, which is necessary
to compute likelihoods of behavioral variables in the decoding model. This relationship changes throughout the
duration of the experiment, because learning changes coding properties of individual neurons, and electrodes
can physically move during the experiment, changing waveform characteristics. At the previous meeting, we
introduced a sequential, approximate Bayesian encoding framework where new information can be incorporated
on the fly to adapt the parametric encoding model, which keeps the size and calculation cost of the decoding model
constant, even with the arrival of new information. In this contribution, we demonstrate that the encoding model
can be fit fast enough for real-time implementation in a closed-loop experiment, and using data from hippocampus
of rat performing a spatial alteration task in a W maze, we demonstrate that it also captures changes in the multiunit activity, and improves decode performance relative to a static encoding model.

I-16. Specific activity patterns are reinforced via closed-loop pairing with phasic VTA activation
Vivek Athalye1,2
Fernando Santos1
Jose M. Carmena2
Rui Costa1

VIVEKA @ EECS . BERKELEY. EDU
FERNANDO. SANTOS @ NEURO. FCHAMPALIMAUD. ORG
JCARMENA @ BERKELEY. EDU
RUI . COSTA @ NEURO. FCHAMPALIMAUD. ORG

1 Champalimaud
2 University

Neuroscience Programme
of California, Berkeley

Learning novel skills is accompanied by the gradual consolidation of action-specific neural activity patterns in mo-

COSYNE 2017

57

I-17
tor control neural circuits. However, the neural circuit mechanisms enabling reproduction of neural spatiotemporal
patterns leading to rewarding outcomes are not well understood. Phasic dopaminergic neural activity encodes a
reward prediction error which guides learning (Steinberg et al., 2013), and its activation can substitute for natural
reinforcements in promoting behavioral selection (Tsai et al., 2009; Kim et al., 2012). Thus, we hypothesized that
specific neural spatiotemporal patterns can be reinforced via pairing with phasic activation of dopaminergic cells
in the Ventral Tegmental Area (VTA).
To test our hypothesis, we established a causal contingency between motor cortical activity and optogenetic
stimulation of dopaminergic VTA neurons in freely-behaving Th-cre mice expressing either channelrhodopsin2 (N=10) or YFP (N=6). A Brain-Machine Interface algorithm transformed the activity of two arbitrarily-chosen
motor cortical ensembles (2-4 units) into an auditory feedback tone in real-time (Koralek et al. 2012). When mice
produced the rare motor cortical activity pattern mapped to the target audio tone, they received phasic optogenetic
VTA stimulation.
Over training, we found that ChR2 mice significantly increased their production of the reinforced audio tone as
well as nearby tones while YFP animals showed no change. We hypothesized that VTA stimulation reinforced
shared inputs to the task-relevant ensembles in order to reproduce rewarded patterns. Thus, we analyzed neural
co-variability using Factor Analysis in a window preceding target hit. We found that changes in neural co-variability
over ChR2 group training significantly correlated with changes in rewarded tone preference. Further, we found
that neural co-variability increased over sessions in the ChR2 animals which learned best, while it did not increase
in ChR2 non-learners and YFP animals.
Thus, our results suggest a neural circuit mechanism enabling reproduction of specific neural spatiotemporal patterns whereby contingent phasic dopaminergic VTA activation strengthens shared inputs to task-relevant neural
ensembles.

I-17. Functional organization of the rat whisker pad
Rony Azouz
Erez Gugig
Praveen Kuruppath

RAZOUZ @ BGU. AC. IL
EREZGUGIG @ GMAIL . COM
PRAAVEENK @ GMAIL . COM

Ben-Gurion University of the Negev
The array of vibrissae on a rat’s face is the first stage in a high-resolution tactile sensing system. Moving from
rostral to caudal in any vibrissal row results in an increase in whisker length and thickness. This in turn may provide
a systematic map of separate tactile channels governed by the mechanical properties of the whiskers. To examine
the functional role of this organization, we monitored the whisking activity of awake rats. We found an anteriorposterior gradient of whisking angle movements in which the longer caudal vibrissae spanned a larger space,
whereas the rostral shorter vibrissae hardly moved. To determine whether this gradient was also expressed in a
location-dependent transformation of surface coarseness into whisker vibrations, we replayed head movements
in anaesthetized rats, and monitored the whisker movements across various surfaces. We found that whiskers,
which are the first stage in sensory information translation, form location-dependent gradients in which texture
information is transmitted more robustly through the rostral whiskers. Recordings from first-order sensory neurons
revealed an anterior-posterior gradient of tactile information transmission in which longer caudal vibrissae mainly
transmitted object touch information, whereas rostral shorter vibrissae transmitted both object touch and surface
coarseness information. Differential elimination of caudal or rostral whiskers in rats foraging for food led to a
change in behavioral strategies during food searching and handling. Together, these results suggest that the
whisker array in rodents forms a sensory structure in which different facets of tactile information are transmitted
through the location-dependent gradient of vibrissae on the rat’s face.

58

COSYNE 2017

I-18 – I-19

I-18. Subspace alignment by inhibitory plasticity recovers excitation-inhibition
balance
Itamar Landau
Haim Sompolinsky

ITAMAR . LANDAU @ MAIL . HUJI . AC. IL
HAIM @ FIZ . HUJI . AC. IL

Hebrew University of Jerusalem
Neurons in the neocortex are bombarded by thousands of excitatory and inhibitory synaptic inputs. The proper
functioning of cortical circuits therefore requires balancing between the resulting excitatory and inhibitory currents
in order to keep each neuron’s membrane potential near firing threshold and allow for rapid response to changes in
their inputs. The theory of balanced state networks, describing how balance can arise dynamically with no need for
fine tuning, has been primarily developed on networks with homogeneous random structure. A number of recent
experimental studies have found significant heterogeneity and deviations from random connectivity within local
circuits. We have previously shown that non-randomness in the connectivity structure can prevent the emergence
of excitation-inhibition balance and yield non-realistic firing patterns. Here we develop a theoretical framework for
studying the constraints on connectivity structure that enable balance. We show how a highly connected network
can be characterized by a small number of dominant subspaces and we derive the required relationship between
these subspaces in order for balance to emerge. We apply this theory to study the low-dimensional structure of
an anatomically constrained connectivity model of the rat barrel cortex showing that the failure of balance can be
understood in this framework as resulting from a misalignment of the dominant subspaces. Finally we show that
inhibitory plasticity is capable of aligning these subspaces and facilitating the emergence of excitation-inhibition
balance and realistic stimulus-evoked firing properties.

I-19. Beyond reward prediction errors: human striatum updates rule values
during learning
Ian Ballard1
Eric Miller1
Steven Piantadosi2
Noah Goodman1
Samuel McClure3

IBALLARD @ STANFORD. EDU
ERMILLER @ GMAIL . COM
SPIANTAD @ UR . ROCHESTER . EDU
NDGOODMAN @ STANFORD. EDU
SAMUEL . MCCLURE @ ASU. EDU

1 Stanford

University
of Rochester
3 Arizona State University
2 University

Humans naturally group the world into categories defined by membership rules. Rules can be learned implicitly
by building associations using reinforcement learning (RL) or by using explicit reasoning. Little is known about
the neural systems underlying learning of explicit rules. Using fMRI in humans learning a categorization task,
we find that abstract rules are represented in the posterior inferior frontal sulcus, and the values of these cortical
representations are held in the executive portion of the striatum. This circuit is a novel target for future research
on the etiology of behavioral disorders and maladaptive decision making. We were additionally interested in
an apparent paradox between the animal and human research on the functional role of the striatum. In animal
studies, striatal neurons are involved in representing and gating actions, and their firing rates correlate with the
values of stimuli or actions. Conversely, in human fMRI, activation in the striatum correlates with a prediction
error signal. This signal is often treated as a read-out of learning and is widely used to interpret experimental
manipulations in both health and disease. We argue that apparent striatal BOLD prediction error response are in
fact value update responses. Further, these value updates do not necessarily arise from a reinforcement learning
system that conveys prediction errors. Previous studies are unable to address this hypothesis because, in simple
reinforcement learning, prediction error is perfectly correlated with value updating. Using a novel task-design
and computational methods, we found clear support for the hypothesis that the striatal feedback response arises
from value updates derived from a Bayesian rule-learning model, rather than the representation of error. We

COSYNE 2017

59

I-20 – I-21
propose that the mesolimbic dopamine system is not specialized for conveying reward prediction errors; rather, it
implements multiple learning algorithms with different update signals appropriate to the learning context.

I-20. Computation of concentration-invariant odor identity in the early olfactory system
Arkarup Banerjee1,2
Honggoo Chae2
Dinu F Albeanu2

ARKARUP @ GMAIL . COM
HGCHAE @ CSHL . EDU
ALBEANU @ CSHL . EDU

1 Watson
2 Cold

School of Biological Sciences
Spring Harbor Laboratory

Neural mechanisms that enable concentration-invariant odor identification in the olfactory system remain largely
unknown. In this study, we analyze the neuronal population activity of the inputs and outputs of the olfactory bulb
(OB)—the first olfactory processing area, in awake mice, using two-photon imaging. We asked two questions –
first, how information about odorant identity and concentrations are encoded in the neuronal activity of OB input
and output populations and second, whether such encoding supports linear readout of concentration invariant
odor identity in cortical projection targets of the OB. We find that unlike the OB inputs, concentration-tuning
curves (CTCs) of individual OB output neurons are complex and often non-monotonic. Modeling suggests that
such increased diversity of CTCs in OB outputs may arise from a divisive normalization computation on the OB
inputs. This computation leads to a non-linear mixing of odorant identity and concentration information, which
makes individual OB output cells poor decoders of concentration-invariant odor identity. However, we find that a
biologically plausible linear readout of the ensemble activity can be demixed to yield both concentration-invariant
odorant identity as well as stimulus concentration. Next, to probe whether the two OB output channels (mitral
and tufted cells) encode odor identity and concentration differentially, we analyzed their responses separately and
found significant differences in their CTC shapes, odor correlation patterns and dimensionality of odor responses.
Additionally, the cross-validated linear decoding performance for concentration-invariant odor identity was found
to be significantly better from tufted cell population than from the mitral cell population. Taken together with known
differences in the anatomical projection targets of these two OB output channels, our result suggests that thirdorder neurons in the projection target areas of tufted cells such as the anterior olfactory nucleus and olfactory
tubercle may represent odor category signals with concentration invariance.

I-21. Geometry of V1’s response to non-Cartesian versus Cartesian stimuli
Faisal Baqai1
Janne Kauttonen1
Pati Stan2
Tai Sing Lee1
Sandra Kuhlman1
1 Carnegie

FBAQAI @ ANDREW. CMU. EDU
JANNE . KAUTTONEN @ GMAIL . COM
PLS 23@ PITT. EDU
TAI @ CNBC. CMU. EDU
SKUHLMAN . UCLA @ GMAIL . COM

Mellon University
of Pittsburgh

2 University

There are many possible representational bases that the visual system could use to cover the space of natural
images. Different representational bases might be work together in distinct functional networks. Historically, the
primary visual cortex was tested with Cartesian gratings, which typically revealed the orientation tunings of the
neurons and the functional networks of orientation-selective neurons (e.g. Hubel and Wiesel’s characterization of
simple and complex cells). However, computational analysis and psychological studies have suggested that the
brain might also be tuned to non-Cartesian stimuli such as hyperbolic gratings and polar gratings (including radial,
concentric and spiral gratings), as these operations are important for maintaining perceptual constancy in face of

60

COSYNE 2017

I-22 – I-23
the rotation, expansion and contraction of the visual stimuli in our natural dynamic experience of vision. Earlier
studies have established extrastriate cortical neurons in monkeys are sensitive to non-Cartesian stimuli in V2,
V4 and MST. We ask whether the neurons in mice V1 have the capacity to represent the non-Cartesian bases,
and whether neurons form distinct functional networks for representing Cartesian and non-Cartesian stimuli. We
performed large-scale 2-photon Ca imaging on mice V1 in response to a set of Cartesian and non-Cartesian
stimuli, and demonstrated that many mouse V1 neurons are indeed sensitive to and often prefer non-Cartesian
grating stimuli. To investigate whether the representations of these two classes of stimuli utilize the same or
distinct functional networks, we evaluated the difference between the low-dimensional manifold spanned by the
Cartesian stimuli and that spanned by the non-Cartesian stimuli, in terms of the informative dimensionality of the
neural response to these two stimuli sets separately and jointly, as well as the principal angles between the two
manifolds. We concluded that the two manifolds are relatively disjoint, suggesting that the two classes of stimuli
are represented by distinct functional networks.

I-22. Intuitive planning: global navigation of cognitive maps with grid-like
global representations
Alon Baram
Timothy H Muller
Timothy EJ Behrens

ALON . BARAM @ MERTON . OX . AC. UK
TIMOTHY. MULLER @ MAGD. OX . AC. UK
BEHRENS @ FMRIB . OX . AC. UK

University of Oxford
It is proposed that a cognitive map encoding the relationships between objects supports the ability to flexibly
navigate the world. Place cells and grid cells provide evidence for such a map in a spatial context. Emerging
evidence suggests analogous cells code for non-spatial information. Further, it has been shown that grid cells
resemble the eigenvectors of the relationship between place cells and can be learnt from local inputs. Here we
show that these locally-learnt eigenvectors contain not only local information but also global knowledge that can
provide both distributions over future states as well as a global distance measure encoding approximate distances
between every object in the world. We highlight the power of such a global representation, and present a simple
algorithm that can use these measures to globally navigate arbitrary topologies without searching more than one
step ahead. We refer to this as intuitive planning.

I-23. Cerebellar learning using perturbations
Boris Barbour1,2
Guy Bouvier1,3
Claudia Clopath4
Celian Bimbard1
Jean-Pierre Nadal1
Nicolas Brunel5
Vincent Hakim1,2
1 Ecole

BORIS . BARBOUR @ ENS . FR
GUY. BOUVIER @ UCSF. EDU
CLAUDIA . CLOPATH @ GMAIL . COM
CELIAN . BIMBARD @ ENS . FR
JEAN - PIERRE . NADAL @ ENS . FR
NBRUNEL @ GALTON . UCHICAGO. EDU
VINCENT. HAKIM @ ENS . FR

Normale Superieure

2 CNRS
3 University

of California, San Francisco
College London
5 University of Chicago
4 Imperial

Many learning tasks require solving the credit assignment problem of translating limited information from the evaluation of behaviour into distributed synaptic modifications optimising future actions. Stochastic gradient methods
offer a possible solution, but no plausible implementation has been proposed for any brain region. Here, we first

COSYNE 2017

61

I-24
consider the problem of adjusting the discharge rates of several neurones under the guidance of a global error.
We propose that at each trial execution a subset of neurones is perturbed and modified according to the comparison of the trial error with the (stored) error estimate. We analyze the convergence and capacity of this algorithm,
stochastic gradient descent with estimated global errors (SGDEGE). Second, we identify in the olivo-cerebellar
system a complete cellular implementation of SGDEGE. The cerebellum aids the learning and execution of fast coordinated movements, with acquired information being stored by plasticity of parallel fibre-Purkinje cell synapses.
In our proposed mechanism, spontaneous complex spikes perturb ongoing movements, create an eligibility trace
for plasticity and signal resulting error changes to guide plasticity. These error changes are extracted by adaptively cancelling the average error transmitted to the olivary neurons. This implementation of SGDGE differs from
the classic Marr-Albus theory (which does not address the credit assignment problem) and generates specific
predictions for synaptic plasticity rules at the parallel fibre-Purkinje cell synapse. Third, we report data from in
vitro plasticity experiments under physiological conditions that verify our predictions. The results highlight the
sensitivity of plasticity studies to experimental conditions. Finally, we suggest that SGDEGE may operate during
optimisation of action sequences by the basal ganglia, where dopamine could both initiate movements and signal
rewards, analogously to the dual perturbation and correction role of the climbing fibre outlined here.

I-24. Noise-limited inference predicts facilitatory normalization in the visual
cortex
Gregory Barello
Yashar Ahmadian

GBARELLO @ UOREGON . EDU
YASHAR @ UOREGON . EDU

University of Oregon
Visual cortical neurons integrate information across the visual field and local features to support global perception.
Global integration manifests in the modulation of neurons’ responses by the surrounding context of their receptive
fields, as in surround suppression. A related effect is normalization: the sublinear addition of responses to superpositions of local features. Normalization and contextual modulation are canonical brain computations iterated
across cortical areas; they are typically suppressive and sub-additive, with suppression weakening with diminishing stimulus strength. We have formerly shown that a parsimonious model of cortical circuitry mechanistically
explains this weakening, and predicts a transition to facilitative or super-additive summation for weak stimuli. Here
we show that a normative, top-down analysis, based on principles of efficient coding of natural scenes, robustly
leads to a similar prediction in the primary visual cortex (V1). We follow previous studies that have successfully
linked suppressive normalization and natural scene statistics, as stylized by the Gaussian Scale Mixture generative model. We model V1 responses as optimal estimators of underlying features of natural scenes modulo a
shared scaling factor treated as a nuisance parameter. In contrast to previous studies, however, here we take
into account the trial-to-trial noise in the thalamic inputs to V1 neurons. We find that, unlike in the noise-free
model, the optimal responses exhibit weakening of suppressive normalization and a transition to facilitative stimulus summation as contrast is lowered. In particular, we find a transition from suppressive to facilitative modulation
by surrounding stimuli for small center stimulus contrasts, as observed experimentally. We also find that optimal
responses to superpositions of low contrast gratings sum super-additively. These results provide a normative
explanation, based on optimal Bayesian inference, for why the cortex should switch from suppressive to facilitative summation as stimulus strength is lowered, and corroborate the previous mechanistic prediction of such a
transition.

62

COSYNE 2017

I-25 – I-26

I-25. Information scaling in large neural ensembles from the macaque prefrontal cortex
Ramon Bartolo1
Richard Saunders1
Philip Browning1
Andrew Mitz1
Bruno Averbeck2

RAMON . BARTOLOOROZCO @ NIH . GOV
RICHARDSAUNDERS @ MAIL . NIH . GOV
PHILIP. BROWNING @ NIH . GOV
MITZA @ MAIL . NIH . GOV
AVERBECKBB @ MAIL . NIH . GOV

1 NIMH/NIH
2 NIH

Studying information processing in large populations has been difficult because it has not been possible to record
large numbers of neurons simultaneously from awake, behaving animals. Multi-electrode recording techniques
are making it possible to perform large scale recordings. In the present study, we recorded single unit activity using
8 microelectrode arrays (96 channels per array) implanted bilaterally in the prefrontal cortex of macaques. In a
given recording session, we were able to isolate hundreds of single and multi-units (>500) during the execution
of a saccade task. We were also able to maintain the isolations for a large number of trials (approx. 1500
trials). The animal made saccades to targets presented randomly to the left or right of a fixation point. We were
interested in estimating information scaling with population size, as many theoretical models have suggested that
information saturates in large neuronal populations. Therefore, we selected multiple random subsets of neurons at
a range of population sizes within a recording session (25–500) and estimated information in each subset. Spike
counts (0–500ms after target presentation) and saccade direction were used to train a generalized linear model
regularized with early stopping. We further projected population activity on the linear decision boundary, and used
the distribution of projected data to estimate d-squared. We found that decoding accuracy rapidly increased as
a function of the sample size, and was nearly perfect for samples with more than a few hundred neurons. On
the other hand, d-squared continued to increase with ensemble size. We have also compared information coding
in the original data with information coding in data in which the correlations between neurons were destroyed by
shuffling trials between neurons, within condition. For ensembles greater than about 200 neurons, information
was larger in the shuffled ensembles than it was in the original ensembles.

I-26. Pitch recognition with sparse-coding recurrent neural network
Oded Barzelay1,2
Omri Barak1
Miriam Furst2

ODEDBARZ @ GMAIL . COM
OMRIB @ TX . TECHNION . AC. IL
MIRA @ ENG . TAU. AC. IL

1 Technion
2 Tel-Aviv

- Israel Institute of Technology
University

Pitch is a percept of the mind that correlates to, but not solely with, acoustical periodicities. Simply stated, it relates
different acoustic signals that share the same repetition rate into one percept. In contrast, various stimuli sharing
the same pitch are expressed differently in the auditory nerves (AN) population response. Broadly, two leading approaches exist for modeling pitch perception: temporal models and spatial models. Both classes of models suffer
from mechanistic and phenomenological shortcomings. On the mechanistic side, it is unclear where the delays of
temporal models or the templates of spatial models arise. On the phenomenological side, temporal models predict a pitch percept in transposed tones, contrary to humans, whereas spatial models have difficulty in processing
stimuli with non-resolved harmonics and high amplitudes. We propose a novel and pricncipled approach to bridge
the gap between these two extremes, based on optimality constraints that assume parsimonious representation
of the sensory auditory input. Based on sparse coding principles (Olshausen& Field 1996), a recurrent neural
network (RNN) is trained to extract spatiotemporal patterns in the AN population activities (auditory periphery).
A hierarchical representation of groups is suggested, to deal with phase sensitivities arising from working with
spatiotemporal templates. The resulting representation can be linearly related to the pitch of incoming stimuli.

COSYNE 2017

63

I-27 – I-28
On the mechanistic side, the model is comprised of an RNN, with local learning rules and without long delays.
On the phenomenological side, the model is consistent with a wide array of psychoacoustic experiments such
as: resolved and unresolved pitch, Transposed Tones, iterated rippled noise, different amplitudes of the stimuli
(nonlinear responses of the cochlea), harmonic shift, and musical notes. We thus conclude that the mathematical
framework (Tibshirani 1996) of sparse coding that was suggested to be shared across modalities (Olshausen&
Field 2004) can explain many properties of pitch perception.

I-27. Using bayesian inference to estimate receptive fields from a small number of spikes
Giacomo Bassetto
Jakob Macke

GIACOMO. BASSETTO @ CAESAR . DE
JAKOB . MACKE @ CAESAR . DE

research center caesar
A crucial step towards understanding how the external world is represented by sensory neurons is the characterization of neural receptive fields. Advances in experimental methods give increasing opportunity to study
sensory processing in behaving animals, but also necessitate the ability to estimate receptive fields from very
small spike-counts. For visual neurons, the stimulus space can be very high dimensional, raising challenges
for data-analysis: How can one accurately estimate neural receptive fields using only a few spikes, and obtain
quantitative uncertainty-estimates about tuning properties (such as location and preferred orientation)? For many
sensory areas, there are canonical parametric models of receptive field shapes (e.g., Gabor functions for primary
visual cortex) which can be used to constrain receptive fields – we will use such parametric models for receptive
field estimation in low-data regimes using full Bayesian inference. We will focus on modelling simple cells in primary visual cortex, but our approach will be applicable more generally. We model the spike generation process
using a generalized linear model (GLM), with a receptive field parameterized as a time-modulated Gabor. Use
of the parametric model dramatically reduces the number of parameters, and allows us to directly estimate the
posterior distribution over interpretable model parameters. We develop an efficient Markov Chain Monte Carlo
procedure which is adapted to receptive field estimation from movie-data, by exploiting spatio-temporal separability of receptive fields. We show that the method successfully detects the presence or absence of a receptive field
in simulated data even when the total number of spikes is low, and can correctly recover ground-truth parameters.
When applied to electrophysiological recordings, it returns estimates of model parameters which are consistent
across different subsets of the data. In comparison with non-parametric methods based on Gaussian Processes,
we find that it leads to better spike-prediction performance.

I-28. The carrot or the stick: opposite effects of rewards and punishments on
human vigor
Ulrik Beierholm1
Benjamin Griffiths2
1 Durham

ULRIK . BEIERHOLM @ DURHAM . AC. UK
B . J. GRIFFITHS .1@ PGR . BHAM . AC. UK

University
of Birmingham

2 University

The vigor with which humans and animals engage in a task is often a determinant of the likelihood of the task’s
success. An influential theoretical model suggests that the speed and rate at which responses are made should
depend on the availability of rewards and punishments (Niv et al. 2005). While vigor facilitates the gathering of
rewards in a bountiful environment, there is an incentive to slow down when punishments are forthcoming so as
to decrease the rate of punishments (Cools et al. 2011). In some tasks this is in direct competition with the urge to
perform fast to escape punishment. Previous experiments (Guitart-Masip et al. 2011, Beierholm et al. 2013) have
confirmed the effect of reward, leaving the importance of the effect of punishment unanswered. We extended the

64

COSYNE 2017

I-29 – I-30
previous model by incorporating response time uncertainty in order to make predictions for subject’s optimal vigor
(inverse of response time). We tested the influence of punishment in an experiment involving economic incentives
and contrasted this with reward related behavior on the same task. We found that behavior corresponded with
the theoretical model; while the instantaneous threat of punishment caused subjects to increase the vigor of their
response, subjects’ response times would slow as the overall rate of punishment increased. We quantitatively
show that this is in direct contrast to increases in vigor in the face of increased overall reward rates. These results
highlight the opposed effects of rewards and punishments and provide further evidence for their roles in the variety
of types of human decisions.

I-29. Stabilizing patterns in time: neural network approach
Nadav Ben-Shushan
Misha Tsodyks

NADAV. BEN - SHUSHAN @ MAIL . HUJI . AC. IL
MISHA @ WEIZMANN . AC. IL

Weizmann Institute of Science
Recurrent and feedback networks are capable of holding dynamic memories. Nonetheless, training a network
for that task is challenging. In order to do so, one should face non-linear propagation of errors in the system.
Small deviations from the desired dynamics due to error or inherent noise might have a dramatic effect in the
future. A method to cope with these difficulties is thus needed. In this work we focus on recurrent networks with
linear activation functions and binary output unit. We characterize the network’s ability to reproduce a temporal
sequence of actions over its output unit. We suggest casting the temporal learning problem as a perceptron
problem for both discrete and continuous time cases. In the discrete case a finite margin appears, providing the
network, to some extent, robustness to noise, for which it performs perfectly (i.e. producing a desired sequence
for an arbitrary number of cycles flawlessly). In the continuous case the margin approaches zero when the output
unit changes its state, hence the network is only able to reproduce the sequence with slight jitters. Numerical
simulations suggest that in the discrete time case, for a large enough network, the longest sequence that can be
learned linearly scales with the network size. We also found that there exists an optimal number of sequences
one can learn in parallel, which maximizes the total length of all of them. The total length also substantially
exceeds the length of the longest single sequence the network can learn. In the continuous time case we quantify
the capacity of the network and provide a statistical estimate for it in one regime. This work suggests a way to
overcome stability problems for training recurrent networks and further quantifies the performance of a network
under the specific learning scheme.

I-30. Better spike encoding with neural nets and boosted trees
Ari Benjamin
Hugo Fernades
Konrad Kording

ARISBENJAMIN @ GMAIL . COM
HUGOGUH @ GMAIL . COM
KOERDING @ GMAIL . COM

Northwestern University
An immediate goal in the quest to understand the neural origins of perception and behavior is the development
of predictive models that accurately relate neural activity to stimuli or tasks. Such models are called encoding
models. Researchers have long chosen parsimonious encoding models that provide concrete and relatively simple answers to questions of the form “what causes a neuron to spike?” One such common model is the linear
non-linear Poisson model, also known as a Generalized Linear Model (GLM) with Poisson spike output. Here we
show that standard machine learning algorithms are far more successful than a GLM at actually predicting neural
responses. In particular, neural networks and gradient boosted trees (XGBoost) achieve pseudo-R2 prediction
scores consistently two to three times higher across several neural populations. These models furthermore perform equally as well with and without feature engineering. The models are trained on published data from three

COSYNE 2017

65

I-31 – I-32
separate brain areas: M1/Pmd neurons recorded during a macaque reaching experiment, S1 neurons in a similar
reaching experiment, and rat hippocampal cells during a navigation experiment. These results point to shortcomings in current theories of neural representation, and may serve as benchmarks by which new theories could be
evaluated. Furthermore, these techniques are well-established and easy to implement, and could thus be quickly
used to answer questions of feature importance and representation.

I-31. Visibility of eigen-distortions of hierarchical models
Alexander Berardino1
Valero Laparra2
Johannes Balle1
Eero P Simoncelli1,3

AGB 313@ NYU. EDU
VALERO. LAPARRA @ UV. ES
JOHANNES . BALLE @ NYU. EDU
EPS 2@ NYU. EDU

1 New

York University
de Valencia
3 Howard Hughes Medical Institute
2 Universitat

We compare several models of visual representation in terms of their ability to predict human judgments of visual
distortion. Each model is defined by a differentiable mapping from image inputs to a response vector, which is then
corrupted by additive Gaussian noise. We use the Fisher Information matrix to predict discrimination thresholds
for the visibility of arbitrary distortions, up to an unknown scale factor. We test this by generating, for each model,
a pair of specific distortions corresponding to the largest and smallest eigenvectors of the Fisher Information
matrix, which represent the model-predicted most and least noticeable changes to the image, respectively. We
distort the image by adding multiples of each vector, and measure detection thresholds for human subjects in a
two-alternative forced-choice task. Results are quantified using the difference, D, of the log amplitude thresholds
for detection of the two vectors. Two random perturbation vectors would yield of value of approximately D=0, and
larger values of D indicate that the sensitivity of the model is better aligned with that of humans. We used this
methodology to test three models: a simple model of LGN neurons that includes local luminance and contrast
normalization and two different 4-stage convolutional neural networks. Each model was trained on a database
of human perceptual judgments. We found that, despite performing slightly better in terms of cross-validated
correlation with the database, both artificial neural networks performed much worse than the LGN model on this
test. We conclude that in this situation, where data are somewhat scarce, cross validation is not powerful enough
to expose failures of a particular model class. Our method provides a more general form of cross-validation, based
on the synthesis of model-optimized stimuli and comparison with human judgments on these targeted stimuli, and
ensures that the model generalizes beyond curated data.

I-32. Standardizing and benchmarking data analysis for calcium imaging
Philipp Berens1
Lucas Theis2
Jasmine T Stone3
Nicholas Sofroniew4
Andreas Tolias5
Matthias Bethge1
Jeremy Freeman4

PHILIPP. BERENS @ UNI - TUEBINGEN . DE
LUCAS @ THEIS . IO
JASMINETSTONE @ GMAIL . COM
SOFRONIEWN @ JANELIA . HHMI . ORG
ASTOLIAS @ BCM . EDU
MATTHIAS @ BETHGELAB . ORG
FREEMAN . JEREMY @ GMAIL . COM

1 University

of Tuebingen
Cortex
3 Yale University
4 Janelia Research Campus
5 Baylor College of Medicine
2 Twitter

66

COSYNE 2017

I-33
Two-photon laser scanning microscopy with fluorescent calcium indicators is used widely to measure the activity
of large populations of neurons. Extracting biologically relevant signals of interest without manual intervention
remains a challenge. Two key problems are identifying image regions corresponding to individual neurons, and
then detecting the timing of individual spikes from their derived fluorescence traces. The neuroscience community still lacks automated and agreed-upon solutions to these problems. Motivated by algorithm benchmarking
efforts in computer vision and machine learning, we built two web-based benchmarking systems, Neurofinder
(http://neurofinder.codeneuro.org) and Spikefinder (http://spikefinder.codeneuro.org), to compare algorithm performance on standardized datasets. Both were built with modular and modern open-source tools, allowing easy
reuse for other data analysis problems. Neurofinder considers the problem of identifying neuron somata in fluorescence movies. We assembled a collection of training datasets from multiple labs in a standardized format, each
with labeled regions defined manually, in some cases guided by activity-independent anatomical markers. Algorithm results are submitted through a web application and evaluated on independent test data, for which labels
have not been made public. Evaluation metrics separately assess accuracy of neuronal locations and shapes.
Submitted results are stored in a database and metrics are presented in a leaderboard. Spikefinder considers
the problem of detecting spike times from fluorescence traces, building on a recent quantitative comparison of
existing spike inference algorithms (Theis et al. 2016). Here, we assembled training data with simultaneously
measured calcium traces and electrophysiologically-recorded action potentials. Performance of submitted algorithms is evaluated on a test dataset using several metrics including correlation, information gain, and standard
measures from signal detection. Both challenges are currently running with publicly contributed algorithms. We
hope this approach will both improve our understanding of how current algorithms perform, and generate new
crowd-sourced solutions to current and future analysis problems.

I-33. Dale’s principle preserves sequentiality in neural circuits
Alberto Bernacchia1
Jozsef Fiser2
Guillaume Hennequin1
Mate Lengyel1

AB 2349@ CAM . AC. UK
FISERJ @ CEU. EDU
G . HENNEQUIN @ ENG . CAM . AC. UK
M . LENGYEL @ ENG . CAM . AC. UK

1 University
2 Central

of Cambridge
European University

Cortical circuits obey Dale’s principle: each neuron either excites or inhibits all its postsynaptic targets. There
is no known principled justification for why this must be so; in fact, Dale’s principle is considered – if at all – a
mere constraint in neural network models. Here we provide a novel rationale for Dale’s principle: networks with
separate excitatory (E) and inhibitory (I) populations preserve the temporal relationships between their inputs, thus
preventing spurious temporal correlations that could mislead spike timing-dependent plasticity (STDP). To show
this, we study a recurrent firing rate network model with arbitrary nonlinear response functions. We assume that,
in line with known Hebbian mechanisms at both excitatory and inhibitory synapses, the magnitudes of recurrent
synaptic weights are proportional to the covariance of pre- and postsynaptic rates, while their sign is determined
by the E/I identity of the presynaptic cell. We show that this connectivity pattern is both necessary and sufficient
to ensure that neural circuit output will be non-sequential, if the input has no specific temporal ordering of its
elements. Conversely, if there is some specific temporal ordering of inputs to different neurons, then the neural
circuit output will also have sequences that reproduce those of the input. Our theory predicts the relative degree
of sequentiality of V1 responses to visual stimuli with different statistics, which we confirmed in cortical recordings:
stimuli that are similar in lacking temporal ordering evoke responses that differ in their sequentiality, depending
on whether V1 has been adapted to them. Our results suggest a novel and unexpected connection between the
ubiquitous Dale’s principle and STDP, namely that Dale’s principle acts as a control mechanism to guarantee that
STDP will act only on input-driven temporal sequences, rather than on internally generated ones.

COSYNE 2017

67

I-34 – I-35

I-34. Modeling the perceptual experience of retinal prosthesis patients
Michael Beyeler
Ariel Rokem
Geoffrey M Boynton
Ione Fine

MBEYELER @ UW. EDU
AROKEM @ UW. EDU
GBOYNTON @ UW. EDU
IONEFINE @ UW. EDU

University of Washington
By 2020 roughly 200 million people worldwide will suffer from retinal diseases such as retinitis pigmentosa and
age-related macular degeneration. Consequently, a variety of retinal sight restoration technologies are being developed to target these diseases. Currently three varieties of electronic retinal prostheses have been approved
for commercial use in patients. Analogous to cochlear implants, the goal of electronic retinal prostheses is to
produce meaningful visual information by electrically stimulating remaining retinal cells. However, these devices
do not restore anything resembling natural vision: Interactions between the electronics and the underlying neurophysiology result in significant distortions of the perceptual experience (Fine and Boynton 2015). Here we
present a linear-nonlinear cascade model (http://github.com/uwescience/pulse2percept) that can predict the perceptual experience of epiretinal prosthesis patients during single- and paired-pulse electrode stimulation. The
model was developed using a wide variety of data describing the brightness and shape of phosphenes elicited
by stimulating a single electrode (e.g., Greenwald, Horsager et al. (2009), Horsager, Greenwald et al. (2009),
Nanduri, Fine et al. (2012)). Here we show that this model can also capture spatiotemporal interactions between
multiple electrodes. The output of the model was compared to psychophysical data from two Argus I (Second
Sight Medical Products Inc.) epiretinal prosthesis patients performing a brightness matching task on 15 different
electrode pairs. The model closely reproduced patient psychophysical data: Specifically, the model captured the
phenomenon whereby spatiotemporal interactions varied between suppression, independence, and summation
depending on whether current fields overlapped and/or fell on the same ganglion axon pathway. Simulations such
as these, which provide an insight into the perceptual outcomes of prosthetic vision, are likely to be critical for
providing realistic estimates of prosthetic vision, providing regulatory bodies with guidance into what sort of visual
tests are appropriate for evaluating prosthetic performance, and improving current and future technology.

I-35. Extracting stable representations of neural population state from unstable neural recordings
William Bishop1
Alan Degenhart2
Emily Oby2
Aaron Batista
Steven Chase
Byron Yu
1 Carnegie

WBISHOP @ CS . CMU. EDU
ALAN . DEGENHART @ PITT. EDU
EMO 22@ PITT. EDU
APB 10@ PITT. EDU
SCHASE @ CMU. EDU
BYRONYU @ CMU. EDU

Mellon University
of Pittsburgh

2 University

Neural population recordings have yielded key insights into the neural computations underlying many sensory,
motor and cognitive processes and enabled impressive advances in brain-computer interfaces (BCI). However,
many recording techniques are limited in their ability to stably record from the same neurons over long durations,
hindering the study of neural populations across time and the clinical utility of BCI systems. To address this
limitation, we develop theory and methods for extracting a consistent low-dimensional representation of neural
population state from population recordings across many days, where the neurons being recorded may change
from day to day. We build on recent work establishing that a small number of latent variables can summarize the
state of hundreds of neurons. The key intuition behind our approach is that latent variable models can be fit to
data from each day, and we provide a way to align the latent representations across days. This permits the activity
of an entire population of neurons to be related across time, even when it is only possible to record from small

68

COSYNE 2017

I-36 – I-37
portions of the population at once. We validate this approach using a self-recalibrating BCI, which leverages a
conserved relationship between user intent and brain state by extracting a stable, low-dimensional representation
of neural state across time, removing the need for manual recalibration required by most current BCI systems. We
tested the self-recalibrating BCI in online, closed-loop experiments with two rhesus macaques. The algorithm was
able to significantly improve performance after the introduction of instabilities to the recorded neural signals in 41
of 42 experiments. These results demonstrate the ability to identify a stable representation of neural population
state, thereby facilitating the study of neural population activity across days.

I-36. Activating distinct neuronal subtypes in auditory cortex differentially affects collicular responses
Jennifer Blackwell
Mark Aizenberg
Winnie Rao
Ryan Natan
Maria Geffen

BLAJE @ MAIL . MED. UPENN . EDU
MARK . EISENBERG @ GMAIL . COM
W. RAO 1993@ GMAIL . COM
RNATAN @ GMAIL . COM
MGEFFEN @ MAIL . MED. UPENN . EDU

University of Pennsylvania
Within the auditory cortex, excitatory and inhibitory dynamics determine frequency selectivity and frequencymodulated (FM) sweep tuning in excitatory cells (Aizenberg et al., 2015; Seybold et al., 2015). Auditory cortex
sends an extensive descending pathway to the inferior colliculus, but the function of this pathway remains unclear.
Previous studies demonstrated that responses of neurons in the inferior colliculus are altered by focal electrical
stimulation and pharmacological inactivation of auditory cortex. However, these methods lack the ability to manipulate specific cell-types. In this study we use optogenetic techniques to modulate activity of excitatory cells and
parvalbumin-positive (PV) interneurons in the auditory cortex to test whether and how this pathway modulates
frequency selectivity and FM sweep tuning in the inferior colliculus. We found that activation of excitatory cells in
auditory cortex decreased frequency selectivity, while activation of PV interneurons increased frequency selectivity and FM sweep selectivity in putative excitatory cells. However, whereas activation of excitatory cells decreased
frequency selectivity in the inferior colliculus, activation of PV interneurons had only weak effects on tone-evoked
responses in the inferior colliculus. These findings suggest that modulation of frequency selectivity and FM sweep
tuning in auditory cortex by inhibitory neurons does not necessarily propagate to the inferior colliculus.

I-37. Olfactory navigation: information theoretic scene analysis motivating a
history-based algorithm
Sebastian Boie1
John Crimaldi2
Bard Ermentrout3
Margaret McHugh2
Katherine Nagel4
Jonathan Victor5

SDB 2008@ MED. CORNELL . EDU
CRIMALDI @ COLORADO. EDU
BARD @ PITT. EDU
MARGARET. MCHUGH @ COLORADO. EDU
KATHERINE . NAGEL @ NYUMC. ORG
JDVICTO @ MED. CORNELL . EDU

1 Cornell

University
of Colorado Boulder
3 University of Pittsburgh
4 New York University
5 Weill Cornell Medical College
2 University

Diverse species rely on their sense of smell to find food sources or mates. In environments where the odor concentration decreases smoothly with increasing distance from the source, it suffices to follow a gradient. However,

COSYNE 2017

69

I-38
odor environments are typically turbulent and intermittent; in such environments, this well-known chemotactic
strategy will fail. To understand how animals navigate in turbulent environments, we took a two-phase approach:
first, we used information theory to identify the statistical features of the odor plume that might be useful for navigation; second, we framed a model with biologically-plausible components that used these features and simulated
its behavior. To characterize the statistical features of an odor field that may be informative for navigation, we began with measurements of spatiotemporal odor profiles for a neutrally-buoyant odorant in realistic wind conditions,
obtained via planar laser-induced fluorescence. We then computed the mutual information that different kinds of
sampling strategies reveal about location. Specifically, we examined the effects of fine vs. coarse binning of odor
concentration, one vs. multiple temporal samples, and one vs. two nearby spatial samples. Results showed
that a high resolution for odor concentration yields little benefit, but repeated temporal sampling of the odor field
is very valuable. Motivated by these findings, we constructed a simple navigation algorithm, based on coarse
discretization and temporal integration of two nearby spatial samples. At the front end, the odor concentration is
binarized into “low” or “high” sensory signals, based on comparison with an adapting threshold. Turning decisions
are based on side-to-side comparison of multiple samples in time, with older values discounted. This is a purely
local strategy which neither computes information nor creates a cognitive map. Nevertheless, this navigational
strategy, which is based on biophysically plausible components, is effective in finding the odor source.

I-38. Recurrent circuitry in piriform cortex implements concentration-invariant
odor identification
Kevin Bolding
Kevin Franks

BOLDING @ NEURO. DUKE . EDU
FRANKS @ NEURO. DUKE . EDU

Duke University
Recurrent collateral connectivity is a prominent feature of cortical circuits but its role in cortical processing remains
elusive. The piriform cortex (PCx) presents a highly tractable model circuit to address this problem directly. We
recorded spiking activity in large populations of olfactory bulb (OB) and PCx neurons to determine how different
features of an odor stimulus are represented. Despite sustained input from OB, PCx odor responses were characterized by a brief increase in population spiking rapidly followed by sustained suppression. Total spiking output
remained constant over a range of odorant concentrations. Recordings in PCx in response to direct, optogenetic
activation of OB output showed that both suppression and normalization occur within PCx itself. We then blocked
output from PCx pyramidal cells to examine how recurrent collateral connections shape these responses. Under
these conditions responses no longer showed strong suppression and spiking output scaled steeply with odor
concentration. These responses in a disrupted cortex indicate how the interplay of diffuse recurrent excitation
and consequent feedback inhibition normally shape PCx odor responses. A linear decoder could normally classify responses by odorant stimulus, odor concentration, and could generalize for odor identity across concentrations. Moreover, we found distinct phases of the odor response, with a fast concentration-invariant representation
shortly after inhalation followed by a subsequent concentration-specific representation. After blocking recurrent
excitation, classifiers could still distinguish responses to different odor stimuli or different odorant concentrations.
However, the decoder failed dramatically when asked to generalize for odor identity across concentrations. Thus,
recurrent collateral circuitry shapes PCx odor responses by truncating and normalizing output, and thereby helps
implement a concentration-invariant representation of odor identity.

70

COSYNE 2017

I-39 – I-40

I-39. Encoding and decoding in neural populations with non-Gaussian tuning: the example of 3D motion
Kathryn Bonnen1
Thaddeus B Czuba1
Adam Kohn2
Lawrence K Cormack1
Alex Huk1

KATHRYN . BONNEN @ UTEXAS . EDU
CZUBA @ UTEXAS . EDU
ADAM . KOHN @ EINSTEIN . YU. EDU
CORMACK @ UTEXAS . EDU
HUK @ UTEXAS . EDU

1 University
2 Albert

of Texas at Austin
Einstein College of Medicine

Classical neuronal tuning almost always follows a bell-shaped tuning function. Although this is a comforting empirical regularity, little is known about how such simple sensory encoding might represent more complex features
and/or be used to infer the real-world elements that gave rise to these responses. Here we study how selectivity
for 3D motion in MT neurons might be constructed from tuning for basic motion cues. We show that canonical
forms of frontoparallel velocity tuning interact with the geometry of 3D space and binocularity to yield encoding of
3D direction that is distinctly non-Gaussian. We then demonstrate the implications for decoding of 3D direction
in the environment. The encoding model uses MT’s canonical tuning to frontoparallel velocities (log-Gaussian) to
represent monocular responses. The binocular response to 3D motion direction is predicted well (r>.5 for 75%
of neurons) by simply adding the expected monocular responses for the corresponding left and right eye retinal
motion projections. Applying a poisson independent decoder to this encoding model, we can then estimate and
discriminate 3D motion directions, resulting in the following insights: a) Neurons with strong ocular dominance
may drive coarse direction discrimination, rather than differential velocity tuning across eyes; b) Estimation of
3D motion direction is more precise for motions roughly towards/away than motions closer to frontoparallel; c)
If 3D motion perception relies on the representation found in MT neurons, performance on 3D motion direction
discrimination tasks should change dramatically as a function of viewing distance. This closer examination of
3D motion direction selectivity in MT is a useful exploration of more complex feature representations in cortex.
This framework should generalize to encoding and decoding of other environmentally-realistic features, e.g. how
retinal orientation relates to 3D slant and tilt.

I-40. Mapping the latent space of human auditory functional special- ization
with binary sparse coding
Moritz Boos1
Joerg Luecke2
Jochem W. Rieger
1 Applied

MORITZ . BOOS @ GMAIL . COM
JOERG . LUECKE @ UNI - OLDENBURG . DE
JOCHEM . RIEGER @ UNI - OLDENBURG . DE

Neurocognitive Psychology Lab
of Oldenburg

2 University

In humans, brain activity can be predicted from a given stimulus representation combining fMRI with voxel- wise
encoding models (Naselaris et al., 2011). Yet, the choice of stimulus representation can limit the interpretability of
the encoding model. In the visual system (Gucu and van Gerven, 2014) showed that features learned with sparse
coding better predict BOLD fMRI than hand-crafted Gabor wavelets. In the auditory domain, an efficient coding
of natural sounds accounts for tuning properties of auditory nerve fibers (Lewicki, 2002). Our aim here, was to
go beyond prediction, by testing if encoding models based on auditory features learned with an unsupervised
approach from natural auditory stimuli can reveal more about the principles of cortical functional organization
than hand-engineered features. For 15 participants from an open 7-Tesla fMRI dataset (Hanke et al., 2014), we
predict BOLD activity elicited by an auditory movie. We compare the performance of Mel-frequency spectrogram
features with a representation learned from the data using sparse coding with binary latents (BSC) (Lucke and
Eggert, 2010; Henniges et al., 2010). We decompose each encoding model into latent dimensions, using principal
component analysis. This reveals one spatially unique dimension in the spectrogram-based encoding model and

COSYNE 2017

71

I-41 – I-42
two in the BSC-based one, all highly similar across participants. Both encoding models’ first dimensions predict
activity in primary auditory cortex and are highly correlated with stimulus energy. The BSC-based encoding
model’s second dimension is related to speech and predicts activity in posterior/anterior superior temporal gyrus,
as well as in the superior temporal sulcus. In conclusion, basis functions learned with BSC improve voxel-wise
predictions in areas of the temporal lobe known to be involved in language processing, and the latent space of the
sparse encoding model reveals general principles of the functional anatomical organization of human temporal
cortices.

I-41. The rates of visual information encoding in parallel retinal bipolar cell
pathways
Bart G Borghuis
Charles Ratliff

BART. BORGHUIS @ LOUISVILLE . EDU
DUTCHRATLIFF @ GMAIL . COM

University of Louisville
Bipolar cells in the mammalian retina comprise about fourteen types. With few exceptions, these types all sample
from the same cone photoreceptors, yet their visual responses differ. These differences contribute critically to
selective encoding of stimulus features, e.g., light increments, decrements, color, and stimulus onset or persistence, in the postsynaptic ganglion cells. While bipolar cell pathways exemplify parallel neural circuit architecture,
and known differences in synaptic connectivity, glutamate receptor expression, response contrast polarity, and
temporal bandwidth support this, a quantitative understanding of the differences and similarities in the encoding
of visual information across types is lacking. Here, we used two-photon fluorescence-guided whole-cell electrophysiology, spatio-temporal white noise stimulation, and LN model analysis to measure visual receptive fields, and
information theoretic methods to compute the rates of visual information encoding in identified bipolar cell types in
the intact (whole-mount) mouse retina. We compared information rates at the level of the excitatory synaptic input
(voltage clamp) and membrane voltage (current clamp) across types, and related information rates of bipolar cells
to those of identified ganglion cells. Visual receptive fields of bipolar cell types 1, 4, and 6 showed minor spatial
differences but substantial temporal differences. Information rates at the level of the membrane voltage response
differed between types, ranging from ≈ 25bits/s in type 7 to 45–50 bits/s in type 3a and 6 bipolar cells. For all
cells, information rates of the excitatory input exceeded those of the membrane voltage response by up to 45%,
indicating that inhibition removes information, potentially reducing redundancy across types. Alpha-type ganglion
cells showed similar information rates as the bipolar cells, 33–43 bits/s, whereas in delta-type ganglion cells the
rate was lower, ≈ 16bits/s. These data show for the first time that parallel bipolar cell pathways encode visual
information at different rates.

I-42. Examining weight perturbations in plastic neural networks
Colin Bredenberg
Brent Doiron

CJB 107@ PITT. EDU
BDOIRON @ PITT. EDU

University of Pittsburgh
Large-scale cortical networks employing homeostatic mechanisms and synaptic plasticity rules have been shown
to differentiate into neural ensembles when common stimuli are applied in tandem to selected subsets of neurons.
These ensembles were found to be stable in response to small perturbations to synaptic strengths–such ensemble
stability is a critical feature for network-based memory. Previous studies applied relatively simple perturbations to
probe the stability of the network–all synapses within a given population were lowered by a uniform percentage.
The goal of this work has been to analyze whether more complex perturbations can reveal more information about
network stability. Towards this aim, we constructed a reduced stochastic Wilson-Cowan model, which captures
the same perturbation phenomenon observed in spiking simulations, but which is analytically much simpler. We

72

COSYNE 2017

I-43 – I-44
found that when the mean self-excitatory synaptic weight for a population was preserved, perturbations that were
distributed more evenly among synapses would lead to a more stable response than focused perturbations, and
that this was caused by quantization of neural activity levels within a population.

I-43. Explicit calculation of effective interactions in strongly-coupled networks
Braden Brinkman1
Fred Rieke1
Eric Shea-Brown1
Michae Buice2

BRADENB @ UW. EDU
RIEKE @ U. WASHINGTON . EDU
ETSB @ UW. EDU
MICHAELBU @ ALLENINSTITUTE . ORG

1 University
2 Allen

of Washington
Institute for Brain Science

Neuroscientists and neurons in downstream brain circuitry must solve a common problem: given observations
of the activity of only a small fraction of neurons in a network, we do not measure the true synaptic interactions
between these neurons, only “effective“ interactions that describe how one neuron modulates the activity of another through the rest of the unobserved network. To properly understand how information about network-wide
computations can be faithfully read out from the activity of a subset of neurons, we must determine how network
architecture and activity shape the effective interactions between observed neurons. To this end, we use a generalized linear point process model as our generative network model and calculate explicitly how hidden neurons
contribute to the (time-dependent) effective interactions between observed neurons. We have shown that the
effective interaction from a pre-synaptic neuron to a post-synaptic neuron can be decomposed into the true interaction between these neurons plus weighted contributions from all paths through hidden neurons that a signal
can travel from the pre- to post-synaptic neuron. The importance of the contributions through hidden neurons depends crucially on how the weight of the true synaptic interactions scale with network size N. If weights between
neurons scale as N ˆ−1 (“classical scaling“), direct interactions are the dominant contribution to the effective interactions between neurons. However, if interaction weights scale as N ˆ−1/2 (“balanced scaling“), contributions
from paths through hidden neurons are just as important as direct interactions. This suggests that neural circuits
with balanced scaling may leverage interactions through long pathways to perform network-wide computations.

I-44. The Allen Brain Observatory
Michael Buice
Saskia DeVries
Jerome Lecoq
David Feng

MABUICE @ GMAIL . COM
SASKIAD @ ALLENINSTITUTE . ORG
JEROMEL @ ALLENINSTITUTE . ORG
DAVIDF @ ALLENINSTITUTE . ORG

Allen Institute for Brain Science
One of the fundamental questions in neuroscience is how sensory stimuli and behavior are represented in the
neural activity of the cortex. Part of the mission of The Allen Institute for Brain Science is to provide resources to
the scientific community to aid in answering such fundamental questions. We describe our most recent product,
the Allen Brain Observatory, a public scientific resource that provides neural responses of cortical neurons to a
diverse set of visual stimuli, recorded using high-throughput two-photon calcium imaging in the awake mouse.
Currently containing data from over 27,000 neurons col- lected from 112 experiments, this dataset provides the
scientific community unprecedented access to single cell and population responses and provides a unique opportunity to explore the collective characteristics of neural dynamics. Using transgenically expressed GCaMP6f under
the control of specific Cre drivers, we systematically imaged the responses from specific excitatory neuron populations to a range of visual stimuli designed to efficiently capture the visual response properties of cortical neurons
including gratings, sparse noise, natural images, and natural movies. Data was collected from cells found in four

COSYNE 2017

73

I-45
cortical visual areas, three cortical layers, and using six different excitatory Cre lines. The Brain Observatory data
can be accessed via our website (http://observatory.brain-map.org/visualcoding) that uses novel visualizations to
allow users to browse and filter the visual responses of the neurons. In addition, data can be downloaded and
accessed using the Allen SDK, a Python-based software package. Here we present the experimental design,
quality control measures, and contents of this data set as well as an introduction to the Allen SDK. We feel this
product is of particular value to the computational neuroscience community as a tool for exploring many questions
in neuroscience such as population coding, neural variability, and correlated activity.

I-45. Characterizing nonlinear neuronal computation within a single stage of
processing
Daniel A Butts1
Gregory E Perrin
Yuwei Cui2
Matthew Whiteway1
Jonathan Demb
Josh Singer
1 University
2 Numenta,

DANIEL . BUTTS @ NIH . GOV
GREGORY. E . PERRIN @ GMAIL . COM
YCUI @ NUMENTA . COM
THEMATTINTHEHATT @ GMAIL . COM
JONATHAN . DEMB @ YALE . EDU
JHSINGER @ UMD. EDU

of Maryland
Inc

The function of sensory neurons is intimately tied to how their neural activity is representing the stimulus. The
stimulus-response relationship can be quite complex, but such complexity likely builds from simpler computations
accumulated over successive stages of processing. We hypothesize an interpretable description of sensory
neuron function this relies on isolating and characterizing the computations performed at each stage preceding
that neuron. Here, we study a single stage of nonlinear processing using what is likely the first nonlinear stage of
processing in the mammalian visual system: the bipolar->ganglion cell synapse in the retina. Two experiments
were performed to examine excitatory currents generated at this synapse in response to a temporally varying
light stimulus (a spot centered in the ganglion cell’s receptive field): one, photoreceptors were stimulated directly,
and two, bipolar cells expressing channel-rhodopsin were stimulated when photoreceptor outputs were blocked.
While simple linear-nonlinear (LN) cascade models failed to predict temporal transients and contrast adaptation
in the current response, a form of spike-triggered covariance adapted to continuous current data identified a
two-dimensional “feature” space. When coupled to a two-dimensional nonlinearity, the resulting model predicted
the response with greater than 90% cross-validated accuracy across contrast. But, this model lacked readilyinterpretable circuit elements. Thus, we searched within this identified stimulus-feature subspace using models of
simpler mathematical form, and discovered that a simple multiplicative interaction between two pathways made
the same predictions. This both validated the use of standard dimensionality reduction approaches (such as
STC) to identify relevant feature spaces, and demonstrated what can be gained by going further in characterizing
nonlinear interactions using models of more restricted mathematical forms. This approach provides a general
means to characterize neural computation in any context where the inputs to an area are known.

74

COSYNE 2017

I-46 – I-47

I-46. A goal-driven deep learning approach for V1 system identification
Santiago Cadena1
Alexander Ecker1
George Denfield2
Edgar Walker2
Andreas Tolias2
Matthias Bethge1

SA . CADENA 721@ GMAIL . COM
ALEXANDER . ECKER @ UNI - TUEBINGEN . DE
GHDENFIE @ BCM . EDU
EYWALKER @ BCM . EDU
ASTOLIAS @ BCM . EDU
MATTHIAS . BETHGE @ UNI - TUEBINGEN . DE

1 University
2 Baylor

of Tuebingen
College of Medicine

Understanding sensory processing in the visual system results from accurate predictions of its neural responses
to arbitrary stimuli. Despite great efforts over the last decades, we still lack a full characterization of the computations in primary visual cortex (V1) and their role in higher cognitive functional tasks (e.g. object recognition).
Recent goal-driven deep learning models have provided unprecedented predictive performance on the visual
ventral stream and revealed a hierarchical correspondence. However, we still have to assess if their learned
representations can also be used to predict single cell responses in V1. Here, we leverage these learned representations to build a model that predicts responses to natural images across layers of monkey V1. We use the
internal representations of a high-performing convolutional neural network (CNN) trained on object recognition as
a non-linear feature space for a Generalized Linear Model. We found that intermediate early layers in the CNN
provided the best predictive performance on held out data. Our model significantly outperformed classical and
current state-of-the-art methods on V1 identification. When exploring the properties of the best predictive layers
in the CNN, we found striking similarities with known V1 computation. Our model is not only interpretable, but also
interpolates between recent subunit-based hierarchical models and goal-driven deep learning models, leading to
results that argue in favor of shared representations in the brain.

I-47. A principled model of robust neural dynamics in premotor cortex.
Nuno Calaim*1
Pierre-Etienne Fiquet*2
Sophie Deneve3
Christian Machens1

NUNO. CALAIM @ NEURO. FCHAMPALIMAUD. ORG
PEF 246@ NYU. EDU
SOPHIE . DENEVE @ ENS . FR
CHRISTIAN . MACHENS @ NEURO. FCHAMPALIMAUD. ORG

1 Champalimaud

Centre for the Unknown
York University
3 Ecole Normale Superieure
2 New

How do neural systems respond to perturbations? A recent study in mouse anterior lateral motor cortex (ALM)
demonstrated a specific type of robustness of neural systems to (optogenetic) perturbations (Li et al, 2016, Nature). Neurons that were transiently silenced on one side of the brain, while an animal was waiting to perform one
of two actions, recovered their original, action-dependent activity levels. This robustness to perturbations resulted
from the interplay of left and right ALM. Here we show that these neuronal responses to perturbations and the
corresponding interplay between sub-populations are natural consequences of the spike coding framework developed in (Boerlin et al, 2013, PLOS CB). We apply the framework to the object discrimination task from Li et al. and
show how to obtain neurons with similar activity patterns as in the data, including persistent and ramping activity,
as well as mixed selectivities. Neurons in the model are tightly balanced, display Poisson-like variability from
trial to trial, yet represent all relevant variables with maximal efficiency. After construction of the network model,
neurons are automatically able to recover their original activity levels after a simulated optogenetic perturbation,
just as in the ALM data. We explain the system’s robustness to perturbations by separately analyzing two key
concepts of the model, the iso-coding manifold (ICM), which consists of all points in firing rate state space that
code for the same variables, and the functional manifold (FM), which consists of all points along which the system’s function evolves in a given trial. We show that the system is robust because the net effect of a perturbation

COSYNE 2017

75

I-48 – I-49
is always amplified along the ICM, but contracted along the FM. In turn, the perturbation-induced changes in firing
rates do not affect the function of the system, but only the way it represents the relevant variables.

I-48. Neural population dynamics of short-term and long-term learning in the
primate prefrontal cortex
Ioana Calangiu1,2
Valerio Mante3,2

CALANGIU @ INI . UZH . CH
VALERIO @ INI . UZH . CH

1 Institute

for Neuroinformatics
of Zurich
3 ETH Zurich
2 University

Prefrontal cortex (PFC) plays a crucial role in goal directed behavior, giving us the ability to learn rules and flexibly
adapt to new situations. Here we study neural population responses in prefrontal cortex during a reversal-learning
task, requiring monkeys to continuously adapt to changing reward contingencies. On each trial, monkeys had to
choose with a saccade between two visual targets associated with different and changing reward probabilities.
The recordings (Utah-arrays) covered a period of many weeks, starting from the monkeys’ first experience with
this task, and thus allow us to study the behavioral and neural correlates of learning on both long (days and
weeks; rule learning) and short time-scales (seconds and minutes; changes in reward contingencies). We find
that a logistic regression model captures many aspects of the observed behavior. The model indicates that
the choice on a given trial depends almost exclusively on the choice and outcome of the preceding trial, and
that the nature of this dependency gradually becomes optimal (i.e. results in maximal reward) over the course
of training. We then used Targeted Dimensionality Reduction to identify the low-dimensional sub-space of the
dynamics capturing variance due to the inferred behavioral variables. In addition to signals representing relevant
behavioral parameters like trial-outcome, this analysis revealed several distinct, large, choice-related signals. For
one, classical, “ramping” choice-predictive activity that is modulated by previous outcome in manner predicted by
the dependencies estimated from the behavior with logistic regression. For another, choice-“postdictive” activity,
which peaks and persists after a saccade and reflects the direction of the just-made choice. The concurrent
representation of predictive and postdictive signals is consistent with a central role of prefrontal cortex in this task,
and provides a platform to study how rule-learning is implemented by changing population level-dynamics in these
areas.

I-49. Estimating behavioral state for sensorimotor transformations
Adam Calhoun1
Pip Coen2
Jonathan Pillow1
Mala Murthy1
1 Princeton
2 University

ADAMJC @ PRINCETON . EDU
P. COEN @ UCL . AC. UK
PILLOW @ PRINCETON . EDU
MMURTHY @ PRINCETON . EDU

University
College London

Behavior can partly be explained as a series of sensorimotor transformations, turning sensory input into actions.
However, behavioral decisions also rely on internal states of the animal. We aim to use unsupervised learning
to automatically cluster behavior into ‘states’ that exhibit distinct sensorimotor transformations. To do so, we
combine hidden Markov models (HMMs) and generalized linear models (GLMs) to create a novel method that
is able to estimate behavioral states in an unsupervised fashion. We use this model to dissect song patterning
from Drosophila melanogaster. During courtship, male Drosophila will follow a female and vibrate his wings to
emit three distinct types of song, each of which varies in terms of its intensity (amplitude), carrier frequency, and
inter-event interval, and the choice of which is dependent on sensory input. If the song is good enough, the

76

COSYNE 2017

I-50 – I-51
female allows the male to copulate. In order to understand how the male decides which type of song to sing, we
utilize a novel behavioral chamber that allows us to collect visual and auditory data at high temporal frequency.
With this chamber, we have already collected a data set containing >400 animals and >100,000 bouts of song
and associated fly tracks. We model this data set as a sequence of multinomial (song type) and continuous
(eg amplitude) emissions whose probability are dependent on sensory features and the hidden state. Using
this method, we identify two previously-unknown courtship states, in which the animal produces song that relies
on distinct combinations of sensory cues integrated over different timescales. By using these states and their
associated sensorimotor transformations, we can correctly predict song patterning choices >94% of the time.

I-50. Dynamics of cortical activity during behavioral engagement and auditory perception
Ioana Carcea
Michele Insanally
Robert Froemke

IOANA . CARCEA @ MED. NYU. EDU
MNI 1@ NYU. EDU
ROBERT. FROEMKE @ MED. NYU. EDU

New York University
Behavioral engagement can enhance sensory perception. However, the neuronal mechanisms by which behavioral states affect stimulus perception remain poorly understood. We recorded single units from auditory cortex
of rats performing a self-initiated go/no-go auditory task. Self-initiation transformed cortical tuning curves and
activity patterns. In some cells, self-initiation modulated responses to target and non-target tones in opposite directions, leading to sharpening or dramatic transformation of frequency tuning profiles. In other cells, self-initiation
changed responses to target and non-target tones in the same direction. Trial self-initiation decreased the rate of
spontaneous activity in the majority of recorded cells. Optogenetic disruption of cortical activity before and during tone presentation showed that these changes in evoked and spontaneous activity were important for sound
perception. Thus, behavioral engagement can prepare cortical circuits for sensory processing by dynamically
changing receptive fields and controlling the pattern of spontaneous activity.

I-51. Sparse synaptic connectivity is required for pattern separation in divergent feedforward networks
Alex Cayco Gajic1
Claudia Clopath2
R Angus Silver1

NATASHA . GAJIC @ UCL . AC. UK
C. CLOPATH @ IMPERIAL . AC. UK
A . SILVER @ UCL . AC. UK

1 University
2 Imperial

College London
College London

The ability to separate and distinguish similar patterns is a core feature of sensory processing. Early theoretical
work showed that divergent feedforward networks (i.e., those that have more output neurons than input neurons)
can separate activity patterns by expanding the dimensionality of the coding space and reducing neural activity
to a sparse code (Marr 1969, Albus 1971). Such feedforward networks are widespread in the nervous system of
both vertebrates and invertebrates, including olfactory bulb, mushroom body, dorsal cochlear nucleus, and dentate
gyrus. Perhaps the most striking example is the input layer of the cerebellar cortex, which contains the majority of
all neurons in the vertebrate central nervous system. In cerebellar cortex, granule cells receive input from only 2-7
mossy fibres; sparse connectivity has also been observed in dorsal cochlear nucleus and the fly olfactory system.
However, the relationship between synaptic connectivity, pattern separation, and learning is not yet understood.
We examined the relationship between network structure and pattern separation in the cerebellar input layer using
a combination of simplified and biologically detailed modeling. We analyzed how feedforward networks separate
spatially-clustered, highly overlapping mossy fibre input patterns. Using a novel partial shuffling method, we were

COSYNE 2017

77

I-52 – I-53
able to disentangle the effects of expansion, sparsening, and correlation on pattern separation and learning speed
of a downstream perceptron decoder. We found that network connectivity strongly influences pattern separation
and learning. Sparse connectivity combines an expansion of coding space, sparsening, and pattern decorrelation
for rapid learning, whereas increasing synaptic connectivity counteracts the beneficial effect of expanding coding
space through shared input correlations. Our results suggest that the evolutionarily conserved sparse synaptic
connectivity found in divergent feedforward networks is essential for separating spatially correlated input.

I-52. Learning modifies the collective dynamics of cortical interneuron networks
Angus Chadwick1,2
Adil Khan3
Jasper Poort2
Antonin Blot3
Thomas Mrsic-Flogel3
Sonja B Hofer3
Maneesh Sahani1,2

ANGUS @ GATSBY. UCL . AC. UK
ADILGHANI . KHAN @ UNIBAS . CH
J. POORT @ UCL . AC. UK
ANTONIN . BLOT @ UNIBAS . CH
THOMAS . MRSIC - FLOGEL @ UNIBAS . CH
SONJA . HOFER @ UNIBAS . CH
MANEESH @ GATSBY. UCL . AC. UK

1 Gatsby

Computational Neuroscience Unit
College London
3 University of Basel
2 University

During learning, cortical networks reorganize to enhance processing of task-relevant information. However, although the network response is known to depend on interactions between different types of cells, little is understood about how these interactions change with learning. To investigate the role of different cell types in
learning, we simultaneously imaged the activity of pyramidal (PYR) and parvalbumin (PV), somatostatin (SOM)
and vasoactive intestinal-peptide (VIP) expressing interneurons in primary visual cortex of mice as they learned
a visual discrimination task. We observed complex and widespread changes in single-neuron responses as well
as pairwise noise correlations over the course of learning, including a striking increase in the selectivity of PV
responses to task-relevant stimuli and a decorrelation of SOM neuron responses from those of other cell types
within the network. To assess how these diverse response changes involving multiple cell classes arose, we
fit a linear dynamical system (LDS) model to the experimental data. This allowed us to infer functional interactions amongst simultaneously imaged neurons together with their stimulus-driven and locomotion-related inputs.
Using the LDS, we found that the collective changes in population responses were explained by a small number of changes in interactions between identified cell types, largely mediated by PV interneurons. Changes in
PV selectivity were linked to the emergence of selective interactions within the PYR-PV subnetwork with learning.
Moreover, increases in PV selectivity contributed to PYR selectivity increases, highlighting an important functional
role for learning-dependent changes in inhibitory interactions. SOM decorrelation was contingent on changes in
PV to SOM interactions, with PV cells actively decorrelating SOM neurons after but not before learning. Thus, we
find that learning induces specific changes in local inhibitory circuit interactions that drive widespread changes in
cortical network dynamics, leading to enhanced representations of behaviorally-relevant stimuli.

I-53. Variational information bottleneck: a general framework for efficient,
task-relevant coding
Matthew Chalk1
Olivier Marre2
Gasper Tkacik1

MATTHEWJCHALK @ GMAIL . COM
OLIVIER . MARRE @ GMAIL . COM
GTKACIK @ IST. AC. AT

1 Institute
2 Institut

78

of Science and Technology Austria
de la Vision

COSYNE 2017

I-54

A major goal in theoretical neuroscience is to predict response properties of sensory neurons from first principles.
An influential attempt has been the ‘efficient-coding’ hypothesis, which posits that sensory systems maximise
information encoded about natural stimuli. While successful, efficient coding ascribes equal importance to all
sensory information, despite empirical evidence that neural systems prioritise behaviourally relevant (and not
just statistically likely) stimuli. Further, efficient coding is well-posed only with additional ad hoc assumptions
about biological constraints (e.g. metabolic or wiring costs), reducing its generality. ‘Information bottleneck‘ (IB)
can, in principle, overcome these limitations. Here, one maximises information about a ‘task-relevant‘ variable,
constrained on the total information encoded about the stimulus. IB naturally captures the dependence of the
neural code on the ‘task-relevance‘ of incoming sensory signals, and its objective function does not require extra
biological assumptions for the coding cost. Unfortunately, the IB problem is extremely difficult to solve in all but
the most trivial cases (i.e. gaussian or discrete data), which has greatly impeded its use in neuroscience. We
recently proposed a tractable variational IB algorithm (varIB) that generalises to non-gaussian stimuli and nonlinear input-output relations. Our framework is a powerful tool for constructing top-down models of task-relevant
efficient coding. In various limiting cases, the varIB framework replicates either traditional efficient or sparse
coding results. In general however, the new theory goes far beyond classical results, predicting how encoded
features vary with external noise, internal coding constraints, and task-demands. We demonstrate that non-linear
(‘kernelized‘) extensions of the framework capture high-level phenomena such as the ‘filling-in‘ of an occluded
visual region. Finally, we use the framework convolutionally, to resolve the conflict between efficient temporal
coding (which favours stimulus decorrelation) and predictive coding (which favours extraction of slow features).

I-54. Continuous-time partitioning of binned spike counts
Adam S Charles
Jonathan Pillow

ADAMSC @ PRINCETON . EDU
PILLOW @ PRINCETON . EDU

Princeton University
Accurate models of neural firing patterns are critical for understanding information processing in neural circuits.
Recent work has focused on extending the traditional Poisson model to account for the fact that spike counts
tend to have greater-than-Poisson variability in large time bins. While recent models can explain general overdispersion statistics, these approaches typically describe more complex count models within fixed bin-sizes. Interestingly, some such models make testable predictions for the behavior of spike counts as the bin-size varies. For
example, the doubly-stochastic process in (Goris et al., 2014) predicts quadratic growth in over-dispersion with
increasing bin-size—a prediction not yet evaluated on real data. Bin-size decisions can greatly effect the statistics
of count data and lead to differing possible scientific conclusions, making it important to carefully validate models
of spike count variability. To address this problem, we propose a continuous-time extension the model in (Goris
et al., 2014) where the modulatory variable is a continuous-time stochastic process instead of a constant during
each trial. We take an approach similar to one described recently, in which the gain is described with a log-normal
process, that is, an exponentiated Gaussian process. This process is parametrized by a covariance kernel that
describes how quickly the gain variable fluctuates over time, and we set the mean as a function of the covariance
kernel so that the gain maintains unit mean. We apply this model to V1 data previously analyzed in (Goris et al.,
2014) and show that by fitting the covariance we can accurately account for the statistical relationship between
over-dispersion and bin-size. The model accuracy indicates that neural modulations are temporally correlated. A
number of mechanisms might underlie this variability (e.g. competing computations, representational uncertainty,
etc.), and understanding noise dynamics can guide bin-size selection and inform analysis into what drives these
additional modulations.

COSYNE 2017

79

I-55 – I-56

I-55. Articulatory gesture encoding in human sensorimotor cortex during continuous speech production
Josh Chartier1,2
Gopala Krishna Anumanchipalli1
Edward Chang1
1 University
2 University

JOSH . CHARTIER @ UCSF. EDU
ANUMANCHIPALLIG @ NEUROSURG . UCSF. EDU
EDWARD. CHANG @ UCSF. EDU

of California, San Francisco
of California, Berkeley

To speak, we must coordinate over 100 muscles to precisely actuate our lips, jaw, tongue, larynx, and other vocal tract articulators. It is an extraordinary motor control feat, yet nearly all of us produce fluent speech. Our
previous work focused on short consonant-vowel syllable production, and demonstrated that the human ventral
sensorimotor cortex (vSMC) is functionally activated along somatotopic representations of articulators. However,
natural continuous speech is far more complex and dynamic than single syllables because of co-articulation between adjacent segments and execution of motor plans over longer duration. To address this, we studied the
encoding of kinematic properties in the human vSMC during natural sentence production. We recorded highdensity intracranial electrocorticography signals, while speakers produced a set of sentences designed to cover
all phonetic contexts in American English (MOCHA-TIMIT). We first developed a method to estimate vocal tract
kinematic parameters from phonetic transcriptions and produced acoustics (acoustic-to-articulatory inversion).
We then fit linear kinematic-trajectory models to each electrode using kinematic parameters to predict neural
activity. We found single electrode encoding of autonomous, dynamical representations of highly specific, coordinated out-and-back trajectories of articulators (e.g. tongue protrusion, lip closure, etc). Kinematic trajectories
of electrodes clustered into four main categories differentiated by place of vocal tract constriction. Each trajectory category appeared to be spatially localized in the sensorimotor cortex. These trajectories were correlated to
speaker generic vocal tract gestures derived using only the behavioral data of 4 speakers. Lastly, speaker generic
gestures, better explained electrode activity when compared against phoneme and single articulator representations. We have used natural continuous speech to demonstrate the neural representation of articulatory gestures
in speech production.

I-56. Unsupervised latent variable extraction from neural data to characterize
processing across states
Rishidev Chaudhuri
Berk Gercek
Biraj Pandey
Ila Fiete

RCHAUDHURI @ AUSTIN . UTEXAS . EDU
BDGERCEK @ UTEXAS . EDU
BIRAJ. PANDEY @ UTEXAS . EDU
ILAFIETE @ MAIL . CLM . UTEXAS . EDU

University of Texas at Austin
In circuits at the sensory and motor periphery and in rare instances in cognitive areas, the identity of the primary
independent variable represented by the neural responses is known. This is the exception: In many brain regions
or in different states (e.g. sleep), the independent encoded variable is unknown. To what extent is it possible to
discover the identity of the independent variable and decode its value from neural data, without direct knowledge
of what it is? We present efforts to discover structure in multiple single-unit data, extract the unknown independent variable, and perform fully unsupervised decoding of it. Our method involves parameterizing low-dimensional
manifold structure in high-dimensional data. We apply it to recordings from the mouse thalamus and postsubiculum (Peyrache et al., 2015) to examine neural circuit dynamics during awake exploration, REM, and slow-wave
sleep (SWS). The method discovers a ring structure, which it parameterizes to extract a circular variable, consistent with head direction coding in these circuits. The accuracy of unsupervised decoding of the latent variable
on many sessions is on par with decoders constructed using head direction. Per-neuron tuning curves for the
latent variable match (up to rotation) the actual head direction tuning curves. REM and SWS states fall on the
waking manifold, showing–without the assumptions involved in applying awake-state encoding models to sleep

80

COSYNE 2017

I-57
decoding–that sleep and awake states share the same structure. Unlike waking, REM states perform a random
walk on the circular variable, while SWS states sample a subset of angle values and make large jumps between
them. Thus, we show it is possible to discover the structure of a variable that drives neural responses and decode it without knowing its identity, simply from time-series neural data. Such approaches can help understand
encoding and dynamics in neural circuits, across behavioral states.

I-57. Target masking by natural backgrounds in primate V1 predicts behavioral detection sensitivity
Spencer Chen
Yoon Bai
Stephen Sebastian
Yuzhi Chen
Wilson Geisler
Eyal Seidemann

SPENCER . CHEN @ UTEXAS . EDU
YOONBAI @ UTEXAS . EDU
SEBASTIAN @ UTEXAS . EDU
YUZHICHEN @ UTEXAS . EDU
GEISLER @ MAIL . CPS . UTEXAS . EDU
EYAL @ AUSTIN . UTEXAS . EDU

University of Texas at Austin
A central goal of systems neuroscience is to understand the neural mechanisms that mediate behavioral performance in natural, day-to-day, perceptual tasks. One fundamental natural task is detection of targets in natural
backgrounds. The goals of the current study are to examine how local properties of natural visual backgrounds
affect neural representations in the primate primary visual cortex (V1), and to determine the possible contributions of these neural effects to behavioral detection sensitivity. To study the masking effects of local background
features, we binned natural background patches by their local luminance (L), contrast (C) and similarity (S) to
the detection target (0.84ˆ◦, 4 cpd Gabor). Behavioral detection sensitivity of the additive target on 3ˆ◦ natural
background patches at 2 − −3ˆ◦ eccentricity was assessed. While a rhesus monkey performed the task, voltagesensitive-dye (VSD) optical imaging captured the neural population response from the corresponding location in
V1. 37 VSD experiments were conducted to assess detection from 7 selected LCS bins. Matched templates
for the Gabor target at the retinotopic and orientation columnar scales were obtained in the same experiments.
From these, we constructed decoders and estimated their neural response thresholds. We found that the masking
effects of LCS natural backgrounds properties on V1 neural responses are highly correlated with their behavioral
masking effects. Behavioral and neural detection thresholds measured on natural backgrounds were significantly
higher than those obtained on uniform backgrounds (background contrast = 0). The monkey’s behavioral thresholds increased with background luminance, contrast and target-background similarity. These trends are consistent
with foveal and parafoveal human psychophysical thresholds. At both the retinotopic and columnar scales, the
behavioral and neural thresholds were highly correlated across the LCS bins. The strong correlation between
neural and behavioral thresholds suggests that behavioral detectability of a visual target is largely determined by
neural masking at, or prior to, V1.

COSYNE 2017

81

I-58 – I-59

I-58. Repeated membrane potential patterns in the hippocampus of awake
mice are related to spiking
Ilya Kolb1
Giovanni Talei Franzesi2
Michael Wang1
Suhasa Kodandaramaiah3
Craig Forest1
Edward Boyden2
Annabelle C Singer1

ILYA . KOLB @ GATECH . EDU
GEGGIO @ MIT. EDU
MWANG 370@ GATECH . EDU
SUHASABK @ UMN . EDU
CFOREST @ GATECH . EDU
ESB @ MEDIA . MIT. EDU
ANNABELLE . SINGER @ BME . GATECH . EDU

1 Georgia

Institute of Technology
Institute of Technology
3 University of Minnesota
2 Massachusetts

Long, recurrent membrane potential fluctuations have been observed in cultured neuronal networks, cortical
slices and in-vivo anesthetized preparations; however, their existence and significance in the awake brain have
not been examined. Here we report the discovery of long (≥ 900ms) recurring membrane potential patterns in
CA1 neurons of awake, behaving mice. Using established statistical controls and new analyses, we find that these
repeated patterns (or “repeats”) occur more often than expected by chance. These repeats are non-oscillatory,
millisecond-precision sequences that are closely tied to the cell’s spiking activity. Our results reveal a surprising
degree of recurring activity in the awake brain, which is traditionally thought to be more stochastic than in-vitro
and anesthetized preparations. The relationship between spiking and repeated subthreshold patterns suggests a
physiological relevance of repeated activity in neuronal computation.

I-59. Perturbation of visual salience representation in FEF following inactivation of parietal cortex
Xiaomo Chen1,2
Marc Zirnsak1
Stephen Lomber3
Tirin Moore1

XIAOMO @ STANFORD. EDU
MZIRNSAK @ STANFORD. EDU
STEVE . LOMBER @ UWO. CA
TIRIN @ STANFORD. EDU

1 Stanford

University
Hughes Medical Institute
3 University of Western Ontario
2 Howard

The detection of salient stimuli is a key capacity of the primate visual system. It is widely believed that frontoparietal areas contribute to the computation of salience. For example, representations of salience have been
demonstrated for neurons within parietal cortex and the frontal eye field (FEF). However, clear, causal links between those representations and the emergence of salience are lacking. Here we investigate the effect of reversible inactivation of parietal cortex on the processing of salience by FEF neurons. We implanted two cryoloops
in the intraparietal sulcus of a macaque to inactivate neuronal activity within nearby structures. Simultaneously,
we recorded from the FEF and measured spiking activity together with local field potentials (LFPs) elicited by single stimuli, homogenous stimulus arrays, and pop-out arrays (i.e., a single, unique stimulus among homogenous
flankers). Confirming the effectiveness of our parietal inactivation, we observed behavioral biases consistent with
deficits in humans exhibiting neglect and deficits observed in prior inactivation and lesion studies in monkeys.
Furthermore, spiking-derived selectivity to pop-out stimuli was significantly reduced (-33% compared to pop-out
stimuli presented outside the receptive field and -44% compared to homogenous stimulus arrays). In contrast, the
spiking responses of FEF neurons to single stimuli remained unchanged. Moreover, our preliminary results reveal
an effect of parietal inactivation on the LFP spectra that is band dependent. Following parietal inactivation, we
observed increased beta and low-gamma band power (20-50 Hz), as well as reduced spatial visual information
represented in the alpha (8-12 Hz) and high-gamma bands (80-150 Hz). This reduction is more pronounced for

82

COSYNE 2017

I-60 – I-61
pop-out stimuli than for single stimuli, and more frequent in alpha band than high-gamma band. These results
suggest a dependence of visual salience signals in the FEF on input from parietal cortex and a necessity of this
input to band dependent LFP components in FEF.

I-60. Emergence of foveal image sampling from learning to attend in visual
scenes
Brian Cheung
Eric Weiss
Bruno Olshausen

BCHEUNG @ BERKELEY. EDU
EAWEISS @ BERKELEY. EDU
BAOLSHAUSEN @ BERKELEY. EDU

University of California, Berkeley
We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual
search task requiring the classification of an object embedded in a visual scene amidst background distractors
using the smallest number of fixations. We explore the tiling properties that emerge in the model’s retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity dependent sampling
lattice of the primate retina, with a high resolution region in the fovea surrounded by a low resolution periphery.
Furthermore, we find conditions where these emergent properties are amplified or eliminated providing clues to
their function.

I-61. Effect of mnemonic strategies on tuning of prefrontal neurons
Feng-Kuei Chiang
Joni Wallis

FKCHIANG @ BERKELEY. EDU
WALLIS @ BERKELEY. EDU

University of California, Berkeley
The capacity of WM is limited to no more than four items but we rarely notice this constraint as we are able to
implement mnemonic strategies, such as chunking, that can overcome this limitation. Prefrontal neurons encode
information in working memory, but it is unclear how this tuning is affected by mnemonic strategies. To access this,
we trained two monkeys to perform a spatial self-ordered search task with six targets. The subject was required
to saccade to each target one at a time, returning to fixation after each target, thereby requiring him to use WM
to keep track of which targets he has already visited. We identified and quantified two strategies that subjects
employed to lower the WM demands and perform the task with fewer errors. The first strategy was to search each
target in a routine sequence. The second strategy was to chunk the targets into two groups and search within one
group before moving to the other group. We recorded the activity of single neurons that were widely distributed
across dorsolateral prefrontal cortex (DLPFC). Around 35% of DLPFC neurons showed changes in firing rate
that depended on the type of strategy being followed. First, stereotyped search patterns were associated with
reduced spatial information being encoded in DLPFC neurons suggesting that a stereotyped strategy can indeed
reduce the load on WM. Second, DLPFC neurons adaptively changed their degree of spatial tuning in relation
to the chunk boundary, increasing the degree of spatial tuning across the boundary and decreasing their tuning
within a chunk. In sum, our findings support a role for DLPFC in implementing strategies that can improve WM
performance.

COSYNE 2017

83

I-62 – I-63

I-62. Blind nonnegative source separation using biological neural networks
Dmitri Chklovskii1,2
Cengiz Pehlevan3,1
Sreyas Mohan1,4

MITYA @ SIMONSFOUNDATION . ORG
CPEHLEVAN @ SIMONSFOUNDATION . ORG
EE 13 B 124@ EE . IITM . AC. IN

1 Simons

Foundation
York University
3 Flatiron Institute
4 IIT Madras
2 New

Extraction of latent causes, or sources, from complex stimuli is essential for making sense of the world. Such
stimuli could be mixtures of sounds, mixtures of odors, or natural images. If supervision, or ground truth, about
the causes is lacking the problem is known as blind source separation. Here, we address a special and biologically
relevant case of this problem when sources (but not the mixing matrix) are known to be nonnegative, for example,
due to the physical nature of the sources. We search for the solution to this problem that can be implemented using
biologically plausible neural networks. Specifically, we consider the online setting where the dataset is streamed
to a neural network. The novelty of our approach is that we formulate blind nonnegative source separation as
a similarity matching problem and derive neural networks from the similarity matching objective. Importantly,
synaptic weights in our networks are updated per biologically plausible local learning rules. The resulting network
architecture is reminiscent of the early stages of sensory processing in the brain.

I-63. Predictive coding in area V4 as a mechanism for recognition of partially
occluded shapes
Hannah Choi
Anitha Pasupathy
Eric Shea-Brown

MYHANNAHCHOI @ GMAIL . COM
PASUPAT @ UW. EDU
ETSB @ UW. EDU

University of Washington
The primate visual system recognizes objects in natural scenes extremely well, even though most objects appear partially occluded. Despite a recent surge of development in computer vision, the mechanisms underlying
the robust shape discrimination in presence of occlusion are still poorly understood. Recent electrophysiological
recordings suggest that the response dynamics in intermediate visual cortical area V4, shaped by feedback from
prefrontal cortex (PFC), may underlie the robust shape discriminability under partial occlusion. Motivated by these
experimental results, we investigate the algorithmic role of feedback in shape recognition based on a hierarchical
encoding of stimuli known as predictive coding. We constructed a network model that implements predictive coding via distinct V4 and PFC populations. The network structure features convergence of signals so that neurons
in higher cortical areas in the hierarchy have more mixed representations of sensory inputs. Feedback signals are
interpreted as predictions made by PFC about V4 activity. We reformulate the conventional framework of predictive coding, linking V4 neuronal responses to the optimal representation of the internal states rather than to the
residual errors between the feedback predictions and the cortical activity, to overcome limitations of previous predictive coding models in describing visual responses. The model predicts two main features of experimental data:
an initial decrease in shape-selective V4 responses with increasing occlusion when the responses are estimated
solely by the feedforward sensory inputs, and a delayed increase in robustness of shape-selective V4 responses
to occlusion, as a result of inclusion of feedback prediction signals. In conclusion, our novel implementation of
predictive coding in area V4 and PFC, motivated by physiological data, addresses a possible algorithmic role of
feedback in recognition of occluded shapes, and further provides key insights into relationships between prior
expectation and recognition of complex images in V4 and higher cortical areas.

84

COSYNE 2017

I-64 – I-65

I-64. Mapping sensory inputs to behavior using patterned optogenetics
Edmund Chong
Anthony Oganov
Johannes Kappel
Dmitry Rinberg

EDMUND. W. CHONG @ GMAIL . COM
ACO 280@ NYU. EDU
JOHANNESMAXKAPPEL @ GMAIL . COM
RINBERG @ NYU. EDU

New York University
A general approach to understanding sensory codes in animal models involves manipulating simple, well-controlled
input representations and measuring subsequent neural activity or behavioral consequences. In olfaction, odorants evoke distributed patterns of activity in the primary input layer, the olfactory bulb (OB). However, achieving
fine manipulation of OB representations is challenging. Conventional odor manipulations (e.g. varying odor concentrations) cannot achieve full parametric variation of activity patterns in the OB. To overcome such technical
limitations, we developed a novel experimental framework to directly generate and manipulate OB patterns using
optogenetics, while measuring perceptual decisions in behaving mice. OB patterns encode odors in a combinatorial fashion, where odor identity is encoded in combinations of OB components. Additionally, recent work
suggests that temporal dynamics of activity patterns may carry odor information. However, how spatio-temporal
features of OB patterns give rise to olfactory recognition remains untested. We addressed two important aspects of this question: (1) how invariant are olfactory percepts to small spatial shifts in OB patterns, and (2)
do temporal relationships within OB patterns dictate odor recognition? We trained mice to recognize “artificial
odors”—optogenetically-driven patterns of OB activity. We then measured whether and how recognition degrades
with precise spatial or temporal perturbations of the original patterns. Our findings uncovered several trends:
first, recognition degrades gradually as spatially-defined OB components are removed from the original pattern;
second, temporal shifts of OB components impair recognition. Additionally, recognition impairment is only observed when removing OB components that are activated within ≈ 100ms of inhalation onset, consistent with
a previously-proposed “primacy” window for olfactory processing. Taken together, our results suggest that perceptual changes are proportional to variations in OB patterns, defined by their spatio-temporal components. Our
experimental framework opens up new possibilities for the fine dissection of olfactory representations and establishing how they guide olfactory recognition.

I-65. Topology of stimulus space via directed network persistent homology
Samir Chowdhury
Facundo Memoli
Bowen Dai

CHOWDHURY.57@ OSU. EDU
MEMOLI @ MATH . OSU. EDU
DAI .171@ OSU. EDU

Ohio State University
Computational methods such as transfer entropy and Granger causality have been widely applied in studying
causal interactions between brain regions. The outputs of such methods are asymmetric causality matrices which
can then be further processed as desired. A natural consideration in applying such a method is how it can be
further analyzed to reveal the structure and organization of the input data. One tool that has recently gained
popularity for its ability to detect the organization of data is persistent homology. For example, prior results
in the literature have established a “learning time model“ that uses persistent homology to recover topological
information (i.e. connectivity of locations) about the stimulus space of rodent place cells from their spike trains. In
an attempt to incorporate causal interactions into such a model, we ask: Given the output of a causality analysis
method applied to spike trains of rodent hippocampal place cells, can one still recover the topology of the physical
stimulus space? Standard persistent homology requires input data to be symmetric, and cannot be applied directly
to asymmetric causal interaction data. We explore a recently-developed tool called Dowker Persistent Homology
(DPH) that accepts asymmetric, non-metric data as input, and use it to provide a positive answer to the preceding
question. We simulate spike train data using parameters established for the learning time model, and preprocess
this data into Markov chains that capture causal interactions between place cells. We then apply DPH to these

COSYNE 2017

85

I-66 – I-67
Markov chains, and are able to recover topological information about the physical stimulus space. Our results
suggest that DPH can be effectively combined with causality inference methods for computational modeling.

I-66. Model-based analysis of state-dependent modulation in rodent ventral
stream
Amelia Christensen1
Jonathan Pillow2
1 Stanford

AMYJC @ STANFORD. EDU
PILLOW @ PRINCETON . EDU

University
University

2 Princeton

Locomotion profoundly alters stimulus response properties in mouse V1; during bouts of running, stimulus response variability decreases, noise correlations decrease, and SNR increases. These observations are nominally
consistent with selective attention to vision, although the complex tuning to running some neurons exhibit challenges this interpretation. To investigate this, we developed a novel generalized linear model (GLM) designed
for Ca2+ imaging data, and then applied it to the Allen Institute Brain Observatory data. Our model employs
gamma-distributed noise, resulting in a linear-nonlinear-gamma (LNG) cascade model. The gamma distribution
is better suited to describing the calcium fluorescence measurements than either the traditional Poisson or Gaussian noise because it is non-negative, positively skewed, and defined for non-integer quantities. Moreover, we
show that the variance-scaling of the gamma distribution, which has standard deviation proportional to mean, is
well-matched to the fluorescence proxy of neural activities. We prove a novel result about the convexity of our
estimator; the log-likelihood is concave whenever the nonlinearity is monotonic and log-convex. We apply this
model to simulated data and demonstrate that even in non-convex cases, we can recover the ground truth model
parameters. We apply the model to investigate running speed modulation in posterior medial (VISpm) cortex.
Unlike in V1, or other higher-order visual areas, we find that in VISpm the majority of neurons that are tuned to
running speed display a monotonic decrease of stimulus evoked visual responses with increased running speed.
Models separately fit to VISpm data recorded while mice were stationary or running show the neural responses
are more predictable during stationary periods. Our results suggest that running speed modulation may act as a
state-dependent selection mechanism for particular visual features, as opposed to triggering a broad increase in
visual sensitivity.

I-67. Representation and readout of object manifolds
SueYeon Chung1
Daniel Lee2
Haim Sompolinsky3

SCHUNG @ FAS . HARVARD. EDU
DDLEE @ SEAS . UPENN . EDU
HAIM @ FIZ . HUJI . AC. IL

1 Harvard

University
of Pennsylvania
3 Hebrew University of Jerusalem
2 University

Objects are represented in each stage of the sensory hierarchy as manifolds due to variabilities in stimulus features such as location, orientation, and intensity. What makes manifold representations at higher stages of the
hierarchy better suited for invariant decoding of object information by downstream circuits than earlier stages?
It has been suggested that the sensory hierarchy becomes increasingly untangled, but the notion of “tangled
manifolds” remains vague. In this work, we consider linear readout of objects as a model of biologically plausible
computation of sensory signals to determine which statistical features of the neuronal representation of object
manifolds are more amenable to this computation. Extending the statistical mechanics of linear classification of
random points, we establish a theory of linear classification of generic manifolds synthesizing statistical and geometric properties of high dimensional signals. The exact capacity of manifold classification generally depends

86

COSYNE 2017

I-68 – I-69
on the full geometrical details of their convex hulls; however, we show that in a broad parameter regime, manifold classification depends primarily on two quantities: their effective dimension, and their effective radius. We
have developed a novel efficient algorithm that can learn the synaptic weights of the neuronal readout guaranteed
to converge to the solution with good generalization and robustness properties. We demonstrate results from
applying our method to both neuronal networks and deep networks from machine learning.

I-68. Mean and variance adaptation in auditory receptor neurons of Drosophila
Jan Clemens
Nofar Ozeri
Mala Murthy

CLEMENSJAN @ GOOGLEMAIL . COM
NOFAROZ 01@ GMAIL . COM
MMURTHY @ PRINCETON . EDU

Princeton University
Adaptation is ubiquitous in sensory systems and improves stimulus representations. Adaptation to different properties of the stimulus distribution, e.g. its mean or variance, coexists within the same sensory pathway, yet the
interaction between different types of adaptation is rarely examined. We address this issue in the context of
courtship song recognition in the fruit fly. During courtship, the male produces a song, the features of which
inform the female’s mating decision. Song is perceived via movement of the arista, a feathery extension on the
antenna. Both the mean as well as the variance of arista movement – corresponding to an additive offset and to
sound intensity – change constantly. To support an efficient representation of song, the auditory system should
thus perform mean and variance adaptation. Using electrophysiology and modeling, we examine how these two
forms of adaptation are organized in the auditory receptors neurons – Johnston’s organ neurons (JON). Previous
studies have demonstrated that mean adaptation is subtractive and arises in the subthreshold currents. We find
that JON also adapt to sound intensity – the variance of arista movement. The intensity adaptation is divisive,
produces a near intensity-invariant code and arises in the subthreshold responses just like the mean adaptation.
Thus, two arithmetically distinct forms of adaptation are implemented in a single neuron. Despite this compact organization of adaptation, there exists only limited crosstalk between both forms: By independently controlling the
mean and variance of antennal position during recordings, we find that mean adaptation does not affect sound
sensitivity. Using modeling we examine the implementation of adaptation in JON. Only a serial arrangement
where mean adaptation precedes variance adaptation reproduces our data. Rectification is essential for separating mean and variance adaptation. Similar arrangements are found in other sensory systems, suggesting that
this implementation is a general organizational principle for adaptation in the brain.

I-69. Rate-distortion analysis of R1-R6 fly photoreceptors
Daniel Coca
Carlos Luna Ortiz
Inaki Esnaola

D. COCA @ SHEFFIELD. AC. UK
CARLOS . LUNA @ SHEFFIELD. AC. UK
ESNAOLA @ SHEFFIELD. AC. UK

University of Sheffield
A recent study has shown that R1-R6 photoreceptors in the fruit fly exploit nonlinearity to selectively enhance
and encode phase congruency1. The analysis involved computing spectral and temporal decompositions of
the photoreceptor response to a synthetic stimulus into linear and nonlinear response components that could be
characterized separately. Here we use this response decomposition approach to characterize, from an information
theoretic point of view, the linear and nonlinear transductions at photoreceptor level. We perform rate-distortion
analysis for the linear, nonlinear and total photoreceptor responses to sequences of i.i.d. Bernoulli distributed
pulses with different levels of additive white Gaussian noise, using the Hamming distance between the encoded
and original pulse sequence as the distortion measure. We demonstrate that the nonlinear encoding of pulses
is more efficient and robust than the linear encoding. Specifically, we show that for all input noise levels the

COSYNE 2017

87

I-70 – I-71
nonlinear encoding of the pulse sequence by photoreceptors achieves lower distortion whilst requiring much lower
bit rates compared with the linear encoding, leading to a significant improvement in the overall efficiency of the
photoreceptor encoding i.e. the combined linear and nonlinear encoding outperforms linear encoding. In all three
cases analyzed the gap to the optimal performance theoretically attainable decreases monotonically as the input
noise level increases. This shows that fly photoreceptors are tuned to perform efficiently in noisy environments.
Remarkably, as the input noise level increases, this gap reduces much faster for the nonlinear encoding than
for the linear encoding. Even, for high levels of noise, the distortion introduced by nonlinear coding is below the
distortion achievable by the trivial strategy of encoding to the mean of the input sequence distribution, i.e. a zero
rate encoding. This highlights the key role played by nonlinear transductions in encoding efficiently and robustly
the behaviourally relevant features.

I-70. Variability of V1 population responses to natural images reflects probabilistic inference
Ruben Coen-Cagli
Adam Kohn

RUBEN . COEN - CAGLI @ EINSTEIN . YU. EDU
ADAM . KOHN @ EINSTEIN . YU. EDU

Albert Einstein College of Medicine
The visual cortex processes visual inputs through the coordinated activity of large neuronal populations. Recent
work has characterized population activity patterns and variability in detail, particularly pairwise interactions and
low-dimensional latent structure. However, this progress has mostly focused on responses to simple, artificial
stimuli rather than stimuli encountered in the natural environment. Knowledge derived from artificial stimuli generalizes poorly to naturalistic inputs, even when focusing on the mean responses of single neurons. Therefore, a
better understanding of cortical population activity and its role in processing natural scenes is critical for understanding natural vision. To address this issue, we recorded simultaneously from tens of neurons in anesthetized
macaque V1 while presenting natural images of different sizes. Consistent with previous studies that used gratings, we found a broad distribution of noise correlations (median 0.11; n=274,985) that was substantially reduced
when stimulating the receptive field surround (median 0.06). Similar results held for Fano factors. However, the
surround modulation of Fano factors and response co-variability was strong for some images, but negligible for
others. To understand these observations, we extended a model based on efficient coding principles that accounts
for single-neuron mean responses to natural images [1], and we assumed neurons collectively represent a posterior distribution over image features. The resulting model’s population responses are sensitive to segmentation
cues, which are computed via Bayesian inference about the statistical dependence between neighboring image
regions. As predicted by the model, weaker evidence for statistical dependence between regions led to larger
Fano factors and reduced the effect of surround stimulation on co-variability. Together our results suggest that the
structure of population activity in V1 is modulated by the statistical properties of natural images, consistent with
efficient coding of those images. Our results also support the view that V1 populations represent the posterior
distribution over image features.

I-71. Circuit mechanisms for flexible sensorimotor processing in Drosophila
Raphael Cohn
Aryeh Zolin
Adam Samulak
Ianessa Morantte
Vanessa Ruta

RCOHN @ ROCKEFELLER . EDU
AZOLIN @ ROCKEFELLER . EDU
ASAMULAK @ ROCKEFELLER . EDU
IMORANTTE @ ROCKEFELLER . EDU
RUTA @ ROCKEFELLER . EDU

Rockefeller University
To survive in a complex and dynamic environment, animals must adapt their behavior based on current needs and

88

COSYNE 2017

I-72
prior experiences. This flexibility is often mediated by neuromodulation within neural circuits that flexibly link sensory representations to alternative behavioral responses depending on contextual cues and learned associations.
In Drosophila, the mushroom body is a prominent neural structure essential for olfactory learning. Dopaminergic
Neurons (DANs) convey salient information about reward and punishment to the mushroom body in order to adjust
synaptic connectivity between Kenyon cells, the neurons representing olfactory stimuli, and the mushroom body
output neurons (MBONs) that ultimately influence behavior. However, we still lack a mechanistic understanding of
how the DANs represent the moment-to-moment experience of a fly and drive changes in this sensory-to-motor
transformation. We took advantage of the mushroom body’s modular circuit organization to investigate how the
DAN population encodes different contextual cues. In vivo functional imaging of the DANs reveals that they represent both external reinforcement stimuli, like sugar rewards or punitive electric shock, as well as the fly’s motor
state, through coordinated and partially antagonistic activity patterns across the population. This dopaminergic
signal modifies neurotransmission with synaptic specificity and temporal precision to coordinately regulate sensorimotor transduction. Together, our work reveals that the Drosophila dopaminergic system modulates the activity
of the MBON population at both acute and enduring time scales to guide immediate behaviors and learned responses. To explore how these output pathways ultimately influence olfactory navigation, we have developed a
closed loop olfactory paradigm in which we can monitor and manipulate the MBON circuitry as a fly navigates in a
virtual olfactory environment. We believe that probing the mushroom body circuit in the context of this motivated
olfactory navigation will significantly further our insight into the principles of sensorimotor transduction.

I-72. Macaque orbitofrontal cortex shows context-dependent adaptation to
changes in minimum value
Katherine Conen
Camillo Padoa-Schioppa

KCONEN @ WUSTL . EDU
CAMILLO @ WUSTL . EDU

Washington University in St Louis
Neuronal adaptation is a recurring feature in sensory circuits. Recently it has also been observed among neurons
in the orbitofrontal cortex that encode subjective value. Several questions remain about the properties of valuebased adaptation. In particular, it is not clear what features of the value distribution drive adaptation. In this
experiment, we test the possibility that the maximum and minimum values in a given context provide reference
points for neuronal encoding. We recorded single units from the orbitofrontal cortex of macaque monkeys during
a juice-choice task. In this task, the subject was given a series of choices of between two juices. Each session
consisted of two to three blocks with 200-250 trials each. Within a block, the range of values remained constant
and the quantities varied pseudorandomly across trials. Between blocks, the range of values for each juice
changed in one of the following ways: the maximum increased/decreased while the minimum remained constant,
the minimum increased/decreased while the maximum remained constant, or the maximum and minimum shifted
concurrently while the difference between them was unchanged. Overall, firing rates changed more steeply with
value in conditions with a narrow range. Notably, there was a fundamental difference between adaptation to
the maximum and minimum values. Changes in the maximum value consistently induced adaptation in valueencoding neurons. In contrast, adaptation to changes in minimum value depended on the way the way the range
changed between blocks. In cases where the maximum and minimum changed concurrently, responses adapted
completely to the minimum value as well as the maximum. In contrast, when only the minimum value changed,
little to no adaptation occurred. This difference raises the possibility that value adaptation must be triggered by a
salient change in the distribution of potential options, which the minimum value alone may not provide.

COSYNE 2017

89

I-73 – I-74

I-73. Closed-loop optimization of neural dynamics: Towards linking neural
state and behavior
Mark Connolly1,2
Robert Gross1
Babak Mahmoudi1
1 Emory

MARK . CONNOLLY @ EMORY. EDU
RGROSS @ EMORY. EDU
B . MAHMOUDI @ EMORY. EDU

University
Institute of Technology

2 Georgia

Closed-loop neuromodulation provides a unique possibility for the functional dissection of neural processes and
understanding their link to behavior. Studying the behavioral manifestation of controlling certain aspects of neural
dynamics has important implications for basic neuroscience research and clinical applications. This is a complex
problem due to the high-dimensional parameter space and identifying measurable and controllable biomarkers
that can serve as measures of the neural state. We approached this problem by developing a platform for realtime optimization of neural modulation in the rat hippocampus. The first step was to characterize the relationship between the electrical stimulation parameters (amplitude and frequency), the pre-stimulation state, and the
change in the neural state. Based on this data, it was possible to identify which measures of neural state could
be modulated with electrical stimulation, and the theoretical optimum stimulation policy. Among the measures we
considered, the 1-50Hz power of the CA1 region of the hippocampus was most controllable by electrical stimulation. We used this data to generate an in silico model of the objective function to analyze the behavior of different
optimization approaches. Next we utilized cross-entropy optimization to search in real-time for the optimal stimulation parameters for open- and closed-loop stimulation policy. We found that the optimization algorithm converged
at our hypothesized optimum, and that the closed-loop policy better optimized the objective function. Real-time
optimization provides a promising approach study the brain and further explore the mechanisms of how neural
circuits respond to stimulation.

I-74. Economic choices reveal risk aversion and probability distortion in rats
Christine Constantinople1
Carlos Brody1,2

CMC 9@ PRINCETON . EDU
BRODY @ PRINCETON . EDU

1 Princeton
2 Howard

University
Hughes Medical Institute

Humans evaluate economic options based on their utility, or the subjective satisfaction those options will provide
(Von Neumann & Morgenstern, 1944). Additionally, humans exhibit probability distortion: we tend to overestimate
the probability of unlikely events and underestimate the probability of likely events (Kahneman & Tversky, Econometrica, 1979; Stauffer et al., J Neuroscience, 2015). Utility and probability distortion account for many aspects
of human choice behavior, but their neural basis is not understood. We have developed a high-throughput behavioral training approach for studying economic decision-making in rats, which reveals the utility and probability
weighting functions of individual rats. Rats are presented with light flashes from the left and right side; these
flashes convey the probability of receiving water reward at each port. Simultaneously presented auditory click
rates convey the volume of water reward baited at each port. Rats thus make a choice between explicitly cued
probabilistic gambles. Using a range of both reward probabilities and volumes allows evaluating both utility and
probability weighting functions. High-throughput training has yielded tens of thousands of choices per rat in eight
well-trained rats so far, enabling exploration of several parameters and conditions per animal. Subjects consistently maximize rewards and exhibit risk attitudes similar to humans and monkeys. They also exhibit probability
distortion, overweighting low probabilities. We found that pharmacological inactivation of the orbitofrontal cortex
does not affect probability distortion, but does make rats more risk-seeking, as revealed by shifts in their utility
functions. These experiments represent, to our knowledge, the first time that utility and probability distortion have
been evaluated in rodents, enabling use of powerful tools to manipulate and monitor neural circuits underlying
these phenomena.

90

COSYNE 2017

I-75 – I-76

I-75. Statistical long-term synaptic plasticity predicts optimal excitation-inhibition
balance
Rui Ponte Costa1
Zahid Padamsey1
James D’Amour2
Nigel Emptage1
Robert Froemke2
Tim P Vogels3,1

RUI . PONTE . COSTA @ GMAIL . COM
ZAHID. PADAMSEY @ PHARM . OX . AC. UK
JAMESDMR @ GMAIL . COM
NIGEL . EMPTAGE @ PHARM . OX . AC. UK
ROBERT. FROEMKE @ MED. NYU. EDU
TIM . VOGELS @ CNCB . OX . AC. UK

1 University

of Oxford
York University
3 CNCB
2 New

Long-term modifications in neuronal connections are critical for reliable memory storage in the brain. However,
pre- and postsynaptic components can make synapses highly unreliable. How synaptic plasticity modifies this
variability is poorly understood. Here we introduce a theoretical framework in which long-term plasticity performs
an optimisation of the postsynaptic response statistics constrained by physiological bounds. In this framework of
statistical long-term synaptic plasticity the state of the synapse at the time of plasticity induction determines the
ratio of pre- and postsynaptic changes. When applied to plasticity of excitatory synapses, our theory explains
the observed diversity in expression loci of individual hippocampal and neocortical potentiation and depression
experiments. Moreover, our theory predicts changes at inhibitory synapses that are bounded by the mean excitation, which suggests an efficient excitation-inhibition balance in the brain. To test this prediction, we analysed
experimental data from excitation-inhibition receptive field development experiments, which shows that inhibitory
plasticity in vivo aims at the mean excitation, consistent with our theoretical predictions. Our results propose a
principled view of the diversity in expression loci of long-term synaptic plasticity observed in a wide range of slice
experiments and reveal a statistically optimal, excitation-inhibition balance in the intact brain.

I-76. Distance covariance analysis
Benjamin Cowley1
Joao Semedo1
Amin Zandvakili2
Matthew Smith3
Adam Kohn4
Byron Yu

BCOWLEY @ CS . CMU. EDU
JOAO. SEMEDO @ NEURO. FCHAMPALIMAUD. ORG
ZANDVAKILI @ GMAIL . COM
SMITHMA @ PITT. EDU
ADAM . KOHN @ EINSTEIN . YU. EDU
BYRONYU @ CMU. EDU

1 Carnegie

Mellon University
University
3 University of Pittsburgh
4 Albert Einstein College of Medicine
2 Brown

A key goal in neuroscience is to understand interactions between populations of neurons across different brain
areas. These interactions are likely nonlinear, which poses a problem for linear approaches. We propose distance
covariance analysis (DCA), a method that identifies weighted combinations of neurons that are related to one another across different brain areas. Importantly, by maximizing a correlational statistic called distance covariance,
DCA can detect linear and nonlinear interactions. Intuitively, DCA detects an interaction between brain areas
whereby, if the population responses in one area are similar for two given trials, then another area’s responses for
those two trials should also be similar. DCA is fast, scales to many variables and samples, and can be applied to
continuous and categorical variables. To showcase DCA’s ability to detect nonlinear interactions, we applied it to
simulated population activity from three brain areas, as well as to simultaneously-recorded V1 and V2 population
activity from a macaque monkey. DCA is a versatile method for neuroscientists interested in detecting linear and
nonlinear relationships, and has many potential applications for relating neural activity to other quantities, such as

COSYNE 2017

91

I-77 – I-78
stimulus or behavioral parameters, outputs from computational models, and neural activity from other subjects.

I-77. Two distinct motion detection algorithms regulate turning and walking
speed in Drosophila
Matthew S Creamer
Omer Mano
Damon Clark

MATTHEW. CREAMER @ YALE . EDU
OMER . MANO @ YALE . EDU
DAMON . CLARK @ YALE . EDU

Yale University
As animals move through the world, they generate optic flow on their retina that is used to regulate their rotational
and translational speed. For the last 60 years, insects, and Drosophila in particular, have served as an excellent
model system to understand how rotational motion is detected and used to stabilize body orientation in a sensorimotor transformation known as the optomotor turning response. The optomotor turning response is well described
by the Hassenstein-Reichardt Correlator (HRC) mathematical model, which accurately predicts that the rotational
motion percept is temporal frequency tuned – that is, it depends on both the velocity and the spatial structure of
the stimulus. However, in addition to regulating orientation, animals must regulate their translation through the
environment. In this study, we investigated how flies use visual inputs to regulate their walking speed. We show
that unlike the optomotor turning response, flies regulate their walking speed based on the speed of the stimulus,
independent of the spatial structure. This different tuning indicates that the walking response is not consistent
with an HRC, and suggests that it is regulated by circuits distinct from those mediating the optomotor turning response. To identify the neural implementation of the walking optomotor response, we genetically silenced various
neuron types known to be involved in the optomotor turning response. Silencing specific cells types produced a
variety of outcomes: abolishing walking speed regulation entirely, reducing responses to one direction of motion
but not the other, and reverting the characteristic speed tuning to temporal frequency tuning. Finally, calcium activity measurements of the tuning properties of cells required for the optomotor walking response show that they
are temporal frequency tuned. Interestingly, cortical models of direction-selectivity show how temporal-frequency
tuned signals may be combined to create velocity-tuned ones, suggesting there may be a shared computational
mechanism between insects and mammals.

I-78. Adaptive neural control counters unexpected reaching dynamics
Frederic Crevecoeur
Philippe Lefevre

FREDERIC. CREVECOEUR @ UCLOUVAIN . BE
PHILIPPE . LEFEVRE @ UCLOUVAIN . BE

Universite Catholique de Louvain
Learning to use a new tool or to move in a novel environment requires updating the internal representation of
movement dynamics. A wealth of studies has highlighted that humans are very good at this task: following
exposure to a novel force field during reaching, movement errors gradually decrease and participants rapidly
recover straight reach paths with bell-shape velocity profiles. Although the literature clearly shows that internal
models of reach dynamics improve with practice, the question of how the first movements are controlled, when
the model error is maximal, has remained unexplored. This is striking because under natural conditions, humans
and animals can rarely practice movements several times to improve performance. Thus we hypothesized that the
nervous system uses adaptive control, enabling rapid online adjustment of the internal models during movement.
We investigated this hypothesis by exposing participants (N=14) to force field trials (FF) interleaved with a large
number of trials corresponding to the null field (baseline). We found that the maximum reach deviations were
constant across all FF trials, indicating that participants could not anticipate the effect of the force field. In contrast,
online control at the end of the trials improved with practice, indicating that control was adjusted during movement
to reduce the impact of the force field and stabilize to the goal target. We further show that these results are

92

COSYNE 2017

I-79 – I-80
expected if the nervous system uses adaptive control. In this framework, movement errors together with the
motor commands are used to update the internal model online. Altogether, our results show that control policies
are updated as we move, and suggest that adaptive neural control counters unexpected reach dynamics during
movement.

I-79. Multisensory control of orientation in tethered flying Drosophila
Timothy Currier
Katherine Nagel

CURRIER @ NYU. EDU
KATHERINE . NAGEL @ NYUMC. ORG

New York University
A basic task that all animals must perform is selecting one stimulus to orient towards in a complex multisensory
environment. Circuits mediating the control of orientation have been identified in vertebrates, and appear to also
be involved in the control of gaze and attention. However, a cellular-level understanding of these circuits has been
hampered by the difficulty of monitoring and manipulating the neurons that comprise them. Here we have developed a behavioral paradigm to study multisensory control of orientation in the fruitfly, Drosophila melanogaster.
We have adapted a classical flight-simulator paradigm to allow tethered flies to steer with respect to both a wind
direction cue and a visual cue. In response to wind alone, we find that flies orient down- or cross-wind. In response
to a visual cue located on the upwind side of the apparatus, flies orient “upwind.” Orienting responses to combined
wind and visual stimuli appear to depend on the relative cross-modal intensity of the stimuli. Flies with normal visual capability orient according to the visual stimulus almost exclusively, while those with weak vision and normal
flies in low light adopt a mixture of crosswind and upwind orientations. We are currently developing a computational model of this intensity-dependent cue selection behavior based on summation of visual and wind-derived
positional attractors. Moreover, we are using genetic silencing experiments to test the hypothesis that recently
discovered circuits encoding the orientation of a visual stimulus in flies are also involved in selecting between
competing multisensory orientation cues. In the long term, we hope to use the formidable Drosophila genetic
toolkit to dissect the cellular mechanisms underlying stimulus selection in a complex multisensory environment.

I-80. Spontaneous cortical waves in Area MT of the awake marmoset modulate neural and perceptual sensitivi
Zachary Davis1
Lyle Muller
Terrence Sejnowski1
Julio Martinez-Trujillo2
John Reynolds
1 Salk

ZDAVIS @ SALK . EDU
LMULLER @ SALK . EDU
TERRY @ SALK . EDU
JULIO. MARTINEZ @ ROBARTS . CA
REYNOLDS @ SALK . EDU

Institute for Biological Studies
Research Institute

2 Robarts

Visual cortical neuronal activity is highly variable, with each presentation of an identical stimulus evoking different
spiking patterns. This variability is typically treated as a source of noise that impairs sensory perception. Here,
we find evidence that variability is due, at least in part, to spontaneous fluctuations in network states. We used
a Utah array to record neuronal responses in Area MT of the common marmoset, which rests on the cortical
surface. The animal performed a threshold detection task in which a low contrast drifting Gabor appeared at an
unpredictable time during visual fixation. The animal was rewarded for making a saccade to the target when it
appeared. We applied a recently-developed phase-based statistical method to analyze spatiotemporal patterns
of local field potentials (LFP) activity recorded across the array. We find that MT exhibits robust waves of 8-13
Hz LFP activity, which propagate across the cortical surface with speeds consistent with the velocities of action
potentials moving along long range excitatory horizontal projections. These waves are present throughout the

COSYNE 2017

93

I-81 – I-82
trial, including the period of visual fixation prior to appearance of the target. We hypothesize that the trough of the
wave corresponds to a period of net depolarization in the local population, which renders neurons more sensitive
to incoming sensory evoked spiking activity. Consistent with this, we find that spiking activity is coupled to wave
phase, with spiking probability elevated at the trough of the wave. The likelihood of the monkey correctly detecting
the target is also modulated by wave phase: target detection is higher when the onset of the target-evoked
response is aligned with the trough of the wave. Our results suggest that cortical waves represent a source of
variability in spiking activity that modulates neuronal and perceptual sensitivity to incoming stimuli.

I-81. Semantic representation in the human brain during reading and listening
to stories and passages
Fatma Deniz1,2
Alexander Huth1
Jack Gallant1
1 University

FATMA @ BERKELEY. EDU
ALEX . HUTH @ BERKELEY. EDU
GALLANT @ BERKELEY. EDU

of California, Berkeley
Computer Science Institute

2 International

An integral part of human language is the capacity to extract the meaning of words through different sensory
modalities. For example, humans can easily comprehend the meaning of language presented through auditory
speech or through written text. A recent study from our laboratory showed that semantic information from spoken
language is represented in a broad network of semantically-selective areas distributed across the human cerebral
cortex. However, it is unclear whether these representations are specific to the modality of speech or whether they
are amodal. Here, we studied how the human brain represents the semantic content of narratives received through
two different modalities, listening and reading. We used functional magnetic resonance imaging to record brain
activity in four separate scanning sessions while four participants both listened to and read the same narrative
stories and factual passages. We then constructed a semantic feature space based on word co-occurrence
statistics calculated over a large text corpus. This feature space was used in voxel-wise modeling to characterize
semantic selectivity of voxels located across the entire cerebral cortex of each individual subject. We find that, in a
variety of regions across temporal, parietal and prefrontal cortices, voxel-wise models estimated from one modality
accurately predict responses in the other modality. However, cross-modal predictions fail in sensory regions such
as early auditory and visual cortices. To recover semantic selectivity for each voxel in these two modalities we
performed principal component analysis on the estimated model weights. We find strong correlations between
the cortical PC maps of semantic content produced from listening and reading. Furthermore, estimated model
weights are correlated between narrative stories and factual passages. These results suggest that semantic
representation of language outside of early sensory areas is independent of the specific modality through which
the semantic information is received and can be generalized across stimuli.

I-82. Grid cells store local positional information
Dori Derdikman1
Omri Barak1
Kate Jeffery2
Revekka Ismakov1
1 Technion

DERDIK @ TECHNION . AC. IL
OMRI . BARAK @ GMAIL . COM
K . JEFFERY @ UCL . AC. UK
RISMAKOV @ GMAIL . COM

- Israel Institute of Technology
College London

2 University

Grid cells are spatially modulated neurons located within the medial entorhinal cortex whose firing activity forms a
regular hexagonal pattern. The nodes of this hexagonal array, called “firing fields“, are positioned by a combination
of environmental and self-motion information. How this interaction occurs is unknown. We analyzed the firing rate

94

COSYNE 2017

I-83 – I-84
variability between different fields in a single grid cell and found that, first, grid cell firing fields exhibit large
variability; second, that individual fields express a characteristic firing rate that is preserved across trials and is
robust to arena deformation, and third, that contextual remapping in grid cells produces only a subtle translation
of the whole grid, but a complete scrambling of the relative grid field strengths. These findings shed light on the
hitherto unanswered question of the homogeneity of the global firing pattern of grid cells. They differ from previous
results suggesting homogeneous grids. Thus grid fields are not simply ‘rulers‘ for the place cell chart, but also
express fine-grained localizing information. All in all, our results indicate that grid cells encode local information
within their oscillating global grid pattern, and suggest that non-uniform recurrent connections or projections from
other regions into grid cells could create these effects. This may link between grid cell encoding and local spatial
information during formation and retrieval of episodic memories.

I-83. Task-relevant motor variability is dynamically regulated by reward history
Ashesh Dhawale
Yohsuke Miyamoto
Maurice Smith
Bence Olveczky

DHAWALE @ FAS . HARVARD. EDU
YRM @ SEAS . HARVARD. EDU
MAS @ SEAS . HARVARD. EDU
OLVECZKY @ FAS . HARVARD. EDU

Harvard University
Variability in motor output underlies trial-and-error motor learning. It allows the brain to explore in motor space
and can help discover efficient solutions to the task at hand. To maximize its utility for learning, levels of variability
should be tightly regulated to allow exploitation of successful actions when reward rates are high, while favoring
exploration of motor space when reward rates are low. To determine if the reward context has changed, the
motor system needs to contrast past and present performance but the computations underlying this comparison
and the timescales over which reward information is integrated are unknown. Furthermore, since exhaustive
exploration of high-dimensional motor space is impractical, it may be more efficient for variability to be principally
regulated along dimensions of motor space that are meaningful for task performance. However, little is known
about whether the motor system indeed dynamically regulates variability in a ‘task-relevant’ manner. To address
these outstanding questions, we developed a novel behavioral paradigm in which rats are trained to displace a
2-d joystick to match specific target angles. Over the course of their training rats performed approx. 200,000
trials, enabling high-powered statistical analyses. Surprisingly, we found that the presence/absence of reward on
even single trials modulated motor variability over the next 20 trials, with reward delivery decreasing variability
and absence of reward increasing variability. We also observed an identical result when we probabilistically
administered or withheld reward on single ‘catch’ trials, thus demonstrating that single trial reinforcement causally
regulated subsequent motor variability. We found that such modulation of motor variability was restricted to the
task-relevant feature - the press angle – and not observed in task-irrelevant features such as press distance and
velocity. Our results demonstrate that the motor system can use single trial reinforcement information to regulate
movement variability in a sophisticated and temporally precise manner.

I-84. Signal transmission between monkey areas V2 and V4 is causally dependent on phase synchronization
Eric Drebitz
Lukas-Paul Rausch
Heiko Stemmann
Andreas Kreiter

DREBITZ @ NEURO. UNI - BREMEN . DE
LRAUSCH @ UNI - BREMEN . DE
STEMMANN @ BRAIN . UNI - BREMEN . DE
KREITER @ BRAIN . UNI - BREMEN . DE

University of Bremen

COSYNE 2017

95

I-85
Selective routing and processing of neuronal signals is a fundamental prerequisite of cognitive processing. This
need for dynamic selection of changing subsets of signals is particularly evident in extra-striate visual cortex,
where neurons receive competing input simultaneously from different stimuli within their large receptive fields.
Selective attention is well known to resolve this conflict in favor of responses to the attended stimulus, but possible neuronal mechanisms for selecting the attended stimulus are strongly debated. One possibility is routing
by synchronization, which suggests enhanced signal transmission for afferent input arriving at a specific phase
of the receiving neurons’ gamma oscillation. Theoretical studies support such a mechanism and our previous
experimental work showed indeed strong attention-dependent changes of the gamma-band coherence between
V4 neurons and subsets of their afferent populations encoding for target and distractor stimuli in V1. However,
whether this highly selective phase locking is causally responsible for enhanced transmission or rather a side
effect of another mechanism is an open question. In order to resolve this issue, we investigated in monkeys
trained to a demanding shape-tracking task, whether the gamma phase of the receiving population in area V4 is
crucial for the effectiveness of electrically evoked spikes generated in an afferent V2 population. We found that
the impact of these spikes strongly depended on the phase of the ongoing gamma cycle in the V4 population.
The effectivity by which they evoked spikes and increased gamma-power in V4 was significantly dependent on the
gamma phase of the V4 oscillation. Furthermore, we found a significant retardation of reaction times, when the
electrically evoked spikes arrived during the phase of the V4 gamma cycle that promotes effective signal transfer.
Thus, our results suggest a causal role of phase-synchronization between afferent input and gamma oscillations
of the receiving neurons for modulating effective connectivity.

I-85. The temporal tuning of Drosophila motion detectors is given by the dynamics of their input elements
Michael Drews
Alexander Arenz
Florian Richter
Georg Ammer
Alexander Borst

DREWS @ NEURO. MPG . DE
ARENZ @ NEURO. MPG . DE
FRICHTER @ NEURO. MPG . DE
GAMMER @ NEURO. MPG . DE
ABORST @ NEURO. MPG . DE

Max Planck Institute of Neurobiology
Identifying the direction of motion is important for any animal that relies on vision to perform behaviours such
as course control, object tracking, escape reactions and many more. Motion detection can be understood as a
process extracting spatiotemporal correlations in the activity of neighbouring photoreceptors by the downstream
neuronal circuitry. Classical models of elementary motion detection consist of two input lines that first become
delayed relative to each other by an asymmetric temporal filtering stage. Two alternative models then propose a
nonlinear interaction between these two inputs signals such that responses are either enhanced if motion occurs
along the preferred direction of the neuron or suppressed if it occurs along the opposite, null direction. Recent
findings in elementary motion detection units in Drosophila, T4 and T5 neurons, however have shown that in fact
both mechanisms are implemented at the same time (Haag et al., 2016), thus explaining the strong direction
selectivity of these neurons and leading to the requirement for at least three input lines to the model. In this study,
we perform 2-photon calcium imaging of all columnar input neurons to direction selective T4 and T5 neurons
in Drosophila and map their spatiotemporal filtering properties. We find markedly differential temporal tuning
characteristics between these and are able to assign only a small number of possible anatomical arrangements
in a meaningful way onto a computational model of the recently proposed 3-arm elementary motion detector.
Additionally, we show that the temporal frequency optimum of T4 and T5 neurons can be shifted towards higher
frequencies by pharmacologically activating octopamine receptors. A corresponding tuning shift can be observed
in the input elements. Our suggested model architecture accounts for both the high degree of direction selectivity
of elementary motion detectors in Drosophila as well as their temporal tuning characteristics under two conditions.

96

COSYNE 2017

I-86 – I-87

I-86. Scalable variational inference for low-rank receptive fields with nonstationary smoothness
Lea Duncker1,2
Sneha Ravi3
Greg Field3
Jonathan Pillow4

DUNCKER @ GATSBY. UCL . AC. UK
SNEHA . RAVI @ DUKE . EDU
GREGORY. FIELD @ DUKE . EDU
JPILLOW @ GMAIL . COM

1 Gatsby

Computational Neuroscience Unit
College London
3 Duke University
4 Princeton University
2 University

An important problem in the analysis of neurophysiology data is to estimate the neuron’s linear spatiotemporal
receptive field (STRF), which characterizes how the cell integrates stimuli over space and time. STRF estimates
derived from natural stimuli are often high dimensional, with tens to hundreds of thousands of coefficients, posing
major statistical and computational challenges for STRF estimation. Maximum likelihood and MAP estimators
have a computational cost that scales cubically in the number of STRF coefficients, severely restricting their use
in high-dimensional settings. Alternative estimators like the spike-triggered average are simpler to compute, but
take many samples to converge and may be severely biased when applied to correlated stimuli. We propose
to overcome these difficulties by introducing a hierarchical STRF model designed to flexibly parameterize lowrank receptive fields with non-stationary smoothness in both Gaussian and Poisson generalized linear encoding
models. We introduce a novel Gaussian Process covariance function, which we refer to as a recency kernel,
which can impose high levels of smoothness at long time-lags while allowing sharp transitions at short timelags; this provides a natural model for receptive fields that have both smoothness and roughness on different
timescales. Combining these methods with priors for spatial smoothness and locality, we develop a scalable
variational inference algorithm for inferring low-rank STRFs that automatically learns their recency, smoothness,
and locality hyperparameters. The resulting estimator offers major statistical and computational advances via
analytic sparse representations of prior covariances, allowing for more accurate estimates of high-dimensional
STRFs from limited datasets, while avoiding the cubic cost of classic MAP estimation. We demonstrate the
effectiveness of our estimator using electrophysiological recordings from rat retinal ganglion cells with a total of
36K STRF coefficients. Our estimator exhibits a substantial improvement over low-rank decomposition of the
spike-triggered average.

I-87. Control of recollection by slow gamma dominating medium gamma in
hippocampus CA1
Dino Dvorak1
Basma Radwan1
Fraser T Sparks2
Zoe Talbot1
Andre Fenton1
1 New

DD 1348@ NYU. EDU
BASMA . RADWAN @ GMAIL . COM
NEUROSPARKS @ GMAIL . COM
ZNT 201@ NYU. EDU
AFENTON @ NYU. EDU

York University
University

2 Columbia

Behavior is used to assess memory, but behavior is a proxy for the internal neural events that define cognitive variables like recollection. We identified an electrophysiological signature of recollection in mouse dorsal
CA1 hippocampus. During a shocked-place avoidance task, rates of intra-hippocampal CA3-driven slow gamma
(SG: 30-60 Hz) oscillations dominated extra-hippocampal entrorhinal cortex layer III-driven medium gamma (MG:
60-90 Hz) oscillations 2-3 seconds before initiating successful avoidance movements. In contrast, the SG/MG
dominance was absent before unsuccessful entrances into the invisible shock zone where mice received shock.
Based on these observations, we formulated the hypothesis that SG/MG dominance is a hippocampal network

COSYNE 2017

97

I-88 – I-89
mechanism of recollection. As predicted, when mice freely forage in an open field environment, in the absence
of an explicit memory task, SG/MG dominance appeared preferentially during alert stillness and was associated
with non-local discharge of CA1 place cells, possibly signaling recollected locations. Furthermore, decoding the
mouse’s location from the ensemble discharge of place cells, revealed that the Bayesian location decoding error transiently increased during SG/MG dominance, decreased for recently visited locations and increased for
locations that will be visited in the near future, as expected if SG/MG dominance is a general mechanism of
recollection even in the absence of an explicit memory task. As such, in the avoidance memory task, Bayesian
decoded location faithfully indicated the mouse’s current location but during SG/MG dominance the posterior transiently pointed towards the locations that the mouse will avoid. We conclude that the SG/MG ratio provides an
electrophysiological index of recollection, measuring the status of the ongoing competition in hippocampus CA1
between intra-hippocampal information processing associated with SG and the processing of neocortical inputs
associated with MG.

I-88. Bottom-up salience drives choice during exploration
Becket Ebitz1
Tirin Moore2
Tim Buschman1

REBITZ @ GMAIL . COM
TIRIN @ STANFORD. EDU
TBUSCHMA @ PRINCETON . EDU

1 Princeton
2 Stanford

University
University

The brain is capable of cognitive control; flexibly responding in identical circumstances. Yet, some low level biases—such as our preference for high contrast visual stimuli—are hardwired into the nervous system. Little is
known about how flexible top-down processes and reflexive bottom-up processes interact, much less about how
we regulate the relative influence of these processes to achieve different goals. In order to begin to understand
these questions, we examined how the balance between bottom-up drive and top-down control varies as rhesus
macaques transition between two different goal states: exploitation and exploration. In volatile or uncertain environments, periods of pursuing options known to be rewarding (“exploit” states) are interspersed with periods
of sampling alternative options (“explore” states). We probe these states with a restless k-armed bandit task, in
which emerging evidence suggests that exploration is accomplished via the random allocation of choices. While
random choice is an efficient and sufficient strategy in this task, the problem of generating random choices is not
trivial and its neural bases are unclear. Here, we report that exploratory choices are not random in all dimensions. Instead, choice depends more on bottom-up stimulus salience—monkeys choose high contrast targets
more often. We fit a dynamical attractor network model in order to determine whether this is due to increased
bottom-up drive or a decrease in top-down control. Model predictions suggest that the increase in high contrast
choice during exploration is not due to distraction. Instead, contrast is permitted to control behavior because
top-down influences are reduced. These results suggest that the brain could “randomize” behavior via specifically disorganizing behavior with respect to top-down information—without compromising fundamental, hardwired
competencies. In this way, exploration is a powerful behavioral model for understanding how the brain implements
and adjusts control.

I-89. GSM=SSN: recurrent neural circuits optimised for probabilistic inference
Rodrigo Echeveste
Guillaume Hennequin
Mate Lengyel

RODRIGO. ECHEVESTE @ ENG . CAM . AC. UK
G . HENNEQUIN @ ENG . CAM . AC. UK
M . LENGYEL @ ENG . CAM . AC. UK

University of Cambridge
The operation of cortical circuits has traditionally been studied using either bottom-up or top-down approaches.

98

COSYNE 2017

I-90
The former identified dynamical mechanisms responsible for a wealth of empirical data, but without reference
to computational function, while the laer extracted signatures of specific computations from neural activity, but
remained agnostic as to the underlying mechanisms. Here we bridge these two approaches and study the dynamics and function of cortical circuits in a principled unifying framework. In contrast to recent optimisation-based
approaches, which use highly simplified architectures and only address trial-average responses, here we train
stochastic, recurrent neural circuits with realistic components that allow us to link response dynamics and variability more directly to computational function. We train networks to perform sampling-based probabilistic inference
under a widely-used generative model of natural images, the Gaussian Scale Mixture (GSM) model. We first
show that the GSM posterior mean grows with stimulus contrast z, superlinearly for small z and saturating for
large z, while the posterior variance decreases with z. We then employ a novel, assumed density filtering-based
approach to obtain the moments of activity in stochastic networks as smooth, dierentiable functions of network
parameters, and match them to those of the GSM posterior for a set of training stimuli. We show that the network
appropriately generalizes to novel stimuli, reproducing the scaling of means and variances with contrast. Furthermore, the networks thus obtained operate in the dynamical regime of stabilised supralinear networks (SSN)
that has recently been proposed to underlie response normalization in V1. Thus, our results suggest a generic
function for inhibition stabilised dynamics with a loose excitatory-inhibitory balance: they provide ideal substrates
of recognition models for probabilistic inference. Conversely, our approach could also be used to infer the brain’s
internal models based on observed dynamics.

I-90. Learning local statistics in serial electron microscopy recordings of neural tissue
Felix Effenberger1,2
Christopher Hillar3
Xiaotang Lu4
Richard Schalek4
Jeff Lichtman4

EFFENBERGER @ FIAS . UNI - FRANKFURT. DE
CHILLAR @ MSRI . ORG
XIAOTANG LU @ FAS . HARVARD. EDU
RSCHALEK @ FAS . HARVARD. EDU
JEFF @ MCB . HARVARD. EDU

1 Frankfurt

Institute for Advanced Studies
Strungmann Institute for Neuroscience
3 Redwood Center for Theoretical Neuroscience
4 Harvard University
2 Ernst

Serial electron microscopy (SEM) has become an important approach for probing the largely unknown structure
of neural tissue at nanometer resolution. This technique allows for the complete reconstruction of volumes of
tissue containing all cellular (somata, dendrites, axons, etc.) and many sub-cellular components (synapses and
their vesicles, dendritic spines, etc.). In addition to the storage of vast amounts of data, a major limiting factor
in SEM image acquisition is the speed of the scanning hardware. However, decreasing exposure times per
raw pixel necessarily increases noise levels. Thus, techniques which allow faster scanning while not sacrificing
connectomics reconstruction performance are essential for handling larger volumes of brain tissue. We propose
here a novel method for the unsupervised preprocessing of these SEM images based on estimating a discrete
model of the local statistics of small patches. Specifically, we fit a second-order maximum entropy model to
binarized, whitened image patches and then use its associated Hopfield recurrent network to substantially denoise
and compress the raw images. Our method furthermore facilitates a significant undersampling of the specimen.
By skipping every second pixel in both special dimensions independently and using the network dynamics to fill
in the missing information (up to 75% undersampling), it is possible to achieve a reduction of scanning speeds by
a factor of 4. Moreover, errors due to the fill-in are small and virtually unnoticeable perceptually. How this process
affects the quality of automated and manual reconstructions of neuronal processes remains to be elucidated, as
this work is still in progress. However, we expect our preprocessing to match previous performance benchmarks,
even with a substantial undersampling of the raw data. Altogether, our findings suggest that it is possible for our
methods to facilitate an order of magnitude increase in both compression and acquisition speed.

COSYNE 2017

99

I-91 – I-92

I-91. Cortical neurons regulate dynamics to integrate sensory input into motor plans
Seth Egger
Chia-Jung Chang
Mehrdad Jazayeri

SWEGGER @ MIT. EDU
CHIAJUNG @ MIT. EDU
MJAZ @ MIT. EDU

Massachusetts Institute of Technology
Timing mechanisms in the brain are crucial for generating and updating motor plans in coordination with anticipated sensory events. Recent experimental and theoretical advances suggest timing information is represented
in the dynamic response of neural populations. However, the nature of the neural code within the dynamic representation and the mechanisms by which this representation is leveraged to guide flexible actions remain poorly
understood. To address these questions, we analyzed activity of simultaneously recorded units in the dorsomedial
frontal cortex (dMFC) of macaques performing a sensorimotor timing task. Animals were presented with three
isochronous flashes and were required to initiate a saccade at the anticipated time of a fourth flash, which was
not presented. The sample interval between contiguous flashes was drawn from a uniform prior distribution. This
design allowed us to characterize population responses as the animal incorporated information about the prior
distribution of the sample interval and the two successive measurements into its motor plan. Using principal component analysis (PCA), we inferred the state of the neural system in reduced dimensions as a function of time –
the neural trajectory. We then examined the relationship of the observed trajectories to the sample interval and
the produced interval in each trial. Our results indicate that the motor plan is updated via a control mechanism
that makes use of the state and its rate of change as a function of time (speed) along the neural trajectory. The
state at the time of each flash provides a measure of the sample interval, which is used to update the motor plan
by adjusting the speed of the dynamics after the flash. Our result suggests a novel mechanism whereby the brain
controls both the firing rates and their derivative to implement flexible sensorimotor integration.

I-92. Dynamic integration onset during perceptual decision making in trained
recurrent neural networks
Daniel Ehrlich
Hyojung Seo
Daeyeol Lee
John D Murray

DANIEL . EHRLICH @ YALE . EDU
HYOJUNG . SEO @ YALE . EDU
DAEYEOL . LEE @ YALE . EDU
JOHN . MURRAY @ YALE . EDU

Yale University
Real world environments are often highly dynamic, so while we rely on perceptual information to guide our behavior, useful signals cannot be assumed to persist. It is therefore vital that we time our perceptual processing to
integrate information while available and halt perceptual processing when signals begin to shift. In this study we
use data collected from rhesus macaques during a temporal sensory integration task in conjunction with trained
recurrent neural networks (RNN) to understand the process of temporally modulated information accumulation.
Our task required animals to accurately identify a stimuli as predominantly composed of green pixels or blue pixels.
Before stimuli onset, however, animals were shown a variable length (0, 400, 800 ms) “null” stimuli with half blue
and green pixels. As such, the animals could not be certain whether they were viewing an informative stimulus
or a null delay stimulus. Intriguingly, when analyzing trials where the animal made a premature decision, the RT
histogram was multi-modal and time-locked to the 400 ms periodicity of information onset hinting at a temporally
modulated behavioral strategy. Helping validate our RNN, we found that our model both learned to successfully
perform the task and surprisingly also produced this characteristic time-locking in premature responses. Despite
this, several algorithmic models of integration onset could account for the observed behavior. For this reason we
turned to a state space analyses of a pseudo-population activity of Frontal Eye Field (FEF) neurons recorded
from the macaques as well as our RNN. Through strong inference, we were able to compare candidate algorithm
predictions in both the animals and RNNs. We found that an algorithm where integration onset is withheld to times

100

COSYNE 2017

I-93 – I-94
at which a change in the environment was expected to be the most plausible model to explain our behavioral and
neural data.

I-93. Contribution of cortical On-Off dynamics to spike-count variability and
correlations
Tatiana Engel1
Nicholas Steinmetz2
Tirin Moore1
Kwabena Boahen1
1 Stanford

TATIANA . ENGEL @ STANFORD. EDU
NICK . STEINMETZ @ GMAIL . COM
TIRIN . MOORE @ GMAIL . COM
BOAHEN @ STANFORD. EDU

University
College London

2 University

Cortical responses vary across repeated measurements. This trial-to-trial response variability is typically characterized by counting spikes within a fixed time-window on each trial and then computing statistics of these
spike-counts across trials. Recent studies have examined spike-count variability in many cortical areas under
a variety of behavioral and stimulus conditions, revealing three general features. First, spike-count variability
is correlated among neurons [1]; second, it declines during the onset of a sensory stimulus [2]; and third, it is
modulated during attention [3-5]. However, the origins of this correlated variability observed in spike-counts and
the mechanisms by which it interacts with sensory and cognitive events remain unknown. We demonstrate that
spike-count variability and correlations largely arise from spontaneous fluctuations between episodes of vigorous
(On) and faint (Off) spiking that occur synchronously across cortical layers. Recently, we have shown that these
endogenous On-Off dynamics in the primate visual cortex, reflecting global changes in arousal, also interact with
sensory stimulation and are modulated at a local scale during spatial attention [6]. We analyzed this dataset—
recorded with linear electrode-arrays across cortical layers in the visual area V4—to investigate to what extent
the local changes in the On-Off dynamics account for changes in spike-count variability and correlations during
sensory stimulation and spatial attention. In both cases, changes in spike-count variability and correlations in
the data were accurately matched by analytical predictions computed from just a few parameters of the On-Off
dynamics, which we estimated from the data using a Hidden Markov Model. Our results elucidate how the three
general features of correlated spike-count variability arise from a single unifying mechanism: endogenous On-Off
dynamics interacting with sensory and cognitive events.

I-94. Dimensionality and entropy of spontaneous and evoked rate activity
Rainer Engelken1,2
Fred Wolf1
1 Max

RAINER @ NLD. DS . MPG . DE
FRED @ NLD. DS . MPG . DE

Planck Institute for Dynamics and Self-Organization
Center for Computational Neuroscience

2 Bernstein

Cortical circuits exhibit complex activity patterns both spontaneously and evoked by external stimuli. Finding lowdimensional structure in this high-dimensional population activity is a challenge both for experiments and theory.
What is the diversity of the collective neural activity and how is it affected by an external stimulus? We present a
novel approach to answer these long-standing questions in firing-rate networks. Using concepts from dynamical
systems theory, we for the first time calculate the attractor dimensionality and dynamical entropy production of
these networks. The dimensionality measures the diversity of collective activity states. Dynamical entropy quantifies the uncertainty amplification due to sensitivity to initial conditions. We obtain these two canonical measures of
the collective network dynamics from the full set of Lyapunov exponents that characterize the exponential sensitivity to small perturbations in every direction of the phase space. Our approach is applicable for arbitrary network
topology and firing-rate dynamics. For concreteness, we consider a randomly-wired firing-rate network that ex-

COSYNE 2017

101

I-95 – I-96
hibits chaotic rate fluctuations for sufficiently strong synaptic weights. We show that dynamical entropy scales
logarithmically with synaptic coupling strength, while the attractor dimensionality saturates. Thus, despite the
increasing uncertainty, the diversity of collective activity saturates for strong coupling. We find that a time-varying
external stimulus drastically reduces both entropy and dimensionality. Finally, we analytically approximate the full
Lyapunov spectrum in several limiting cases by random matrix theory. This reveals how coupling strength, autocorrelations and noise affect the chaotic network dynamics. Our study opens a novel avenue to characterize the
complex dynamics of rate networks and the geometric structure of the corresponding high-dimensional chaotic
attractor. This not only gives a deeper understanding of the dynamics but also help to harness its computational
capacities, e.g. for plasticity and learning of stable trajectories.

I-95. Deep learning approach towards generating neuronal morphology
Roozbeh Farhoodi1,2
Pavan Ramkumar2,3
Konrad Kording2

ROOZBEHFARHOUDI @ GMAIL . COM
PAVAN . RAMKUMAR @ GMAIL . COM
KOERDING @ GMAIL . COM

1 Sharif

University of Technology
University
3 Rehabilitation Institute of Chicago
2 Northwestern

The brain has a tremendous diversity of cell types, whose 3d morphology and physiology are actively being
mapped out using cutting edge experimental techniques. A statistical generative model of the morphology of
neurons would help constrain inference of their structure and function from noisy anatomical and physiological
data. Here, we leverage the most recent advances in deep generative models to generate coarse-to-fine neuronal
structure. We use a database of digitized neuronal morphologies (neuromorpho.org), in which each neuron can
be described by a geometric graph. Each node in the graph is a sphere uniquely specified by its centroid, radius,
and parent node. We then train a hierarchical graphical generative adversarial network (HG-GAN) to produce
new graphs. In the first level, we train a GAN to produce a neuron described by k nodes. Each subsequent level
in the hierarchy adds k nodes based on the graph generated by all previous levels. In general, the nˆth level
adds k nodes to the previous levels, described by (n − 1) × k nodes. The architecture of each GAN is designed
to generate geometric aspects of each node and the structure of the graph separately (see Additional Details).
We show that HG-GANs can produce novel and realistic neuronal morphologies so that descriptive statistics on
generated neurons (e.g. density of nodes vs. distance, histogram of angles, . . . ) match the distributions of the
neurons in the real dataset. Our method can be applied not only to neuronal morphologies but to a wide class of
data described by a graph with geometric structure which is found widely in biology.

I-96. Cherchez les auxiliaires: interneurons are key for high-capacity attractor
networks
Dylan Festa
Guillaume Hennequin
Mate Lengyel

DYLAN . FESTA @ GMAIL . COM
G . HENNEQUIN @ ENG . CAM . AC. UK
M . LENGYEL @ ENG . CAM . AC. UK

University of Cambridge
All cortical areas are characterised by a fundamental division of labour between neurons: principal (or projection)
neurons project to other brain areas, whereas the influence of interneurons remains local. Thus, information about
the activity of interneurons is lost in downstream areas, yet, their activity seems crucial for proper functioning.
What principles govern the functional role of interneurons? We explored this question in the context of attractor
dynamics, a paradigmatic model of cortical computation. Using techniques that we have recently developed
to embed multiple robust attractors in realistic neural networks by optimising their synaptic weights [Festa et

102

COSYNE 2017

I-97 – I-98
al., NIPS 2014], we found that interneurons are crucial for achieving high capacity. We studied a progression
of models with increasing biological realism, ranging from neurons with saturating input/output gain functions
and no distinct excitatory/inhibitory (E/I) identity (standard dynamic Hopfield scenario), to distinct E/I neurons
with expanding gain functions that do not saturate over the relevant dynamic range. We distinguished between
principal and interneurons based on whether their activities were pre-specified by the attractors or could freely
vary throughout optimisation. Our optimisation procedure reproduced the well-known high capacity of the Hopfield
network without interneurons when trained with the perceptron rule. However, interneurons became crucial for the
robust stabilisation of attractor states when neurons had non-saturating gain functions. In particular, the strategy
used by capacity-optimised circuits was to use interneurons to “index” memories, such that they exhibited a
number of unique, experimentally testable features in their connectivity and their dynamics.

I-97. Oscillations in local field potentials and spike times link the attention
network to behavior
Ian Fiebelkorn
Mark Pinsk
Sabine Kastner

IANCF @ PRINCETON . EDU
MPINSK @ PRINCETON . EDU
SKASTNER @ PRINCETON . EDU

Princeton University
Spatial selection is often compared to a spotlight, scanning the visual environment and pausing to illuminate
potentially relevant locations. Classic theories of attention posit that this metaphorical spotlight is sustained over
time. Recent evidence, however, suggests that the spotlight of spatial selection would be better characterized
as a blinking spotlight. That is, spatial selection leads to rhythmic fluctuations in environmental sampling and
therefore behavioral performance. The precise neural correlates of these rhythms are unknown. We investigated
links between neural oscillations and behavioral performance under conditions of sustained spatial selection,
simultaneously recording from three hubs of the macaque attention network: the frontal eye fields (FEF), lateral
intraparietal area (LIP), and the medial pulvinar nucleus of the thalamus (mPUL). The phase of oscillations in
local field potentials (LFPs) is linked to behavioral performance (i.e., hit rates and response times) in all three
hubs, suggesting that all three hubs represent neural correlates of rhythmic environmental sampling. The link
between neural oscillations in the attention network and behavioral performance is also revealed through spikeLFP phase coupling: (1) stronger oscillatory patterns in spike times reflect greater network connectivity and (2)
stronger oscillatory patterns in spike times are associated with better behavioral performance. The presence and
frequency of spike-LFP phase coupling varies with neuronal subtype. In comparison, spike rates, regardless of
neuronal subtype, are not linked to behavioral performance. These findings thus demonstrate that changes in
the temporal pattern of spikes, without changes in the spike rate, can influence success or failure in a given task.
Our findings also demonstrate that stronger network connectivity, primarily in the beta band (15–30 Hz), is linked
to better behavioral performance. In sum, local and between-region spike-LFP phase coupling are fundamental
correlates of attention-related rhythms in environmental sampling.

I-98. Estimates and updates of reward probabilities in frontal and parietal cortex
Nicholas C Foley
Jacqueline Gottlieb

NCF 2109@ COLUMBIA . EDU
JG 2141@ COLUMBIA . EDU

Columbia University
Estimating reward probability and uncertainty is vital for behavior. However, while reward based decision-making
has been extensively characterized, less is known about how rewards influence other cognitive functions. Behavioral results suggest that reward probability and uncertainty are important determinants of cognitive control,

COSYNE 2017

103

I-99
including recruitment of selective attention, but the areas involved are largely unknown. Here we examined how
reward probability is encoded in the dorsolateral prefrontal cortex (dlPFC) and parietal area 7a, areas that are
implicated in cognitive control and can influence attention through projections to the frontal and parietal eye fields.
Two monkeys performed a task in which they received reward probability information by means of a visual cue,
but could not make a decision based on that cue. This allowed us to examine the encoding of probabilistic reward
information independent of any instrumental rewards that may be acquired by acting on the information. Anticipatory licking behavior showed that the monkeys formed reward predictions from both the current trial’s cue and
the outcome of the previous trial, suggesting that the monkeys employed a mixed strategy that integrated both
knowledge of past rewards and the meaning of the current cue. Both dlPFC and 7a neurons showed little firing
rate modulation related to reward probability during the delay period prior to reward delivery, although non-linear
methods could read out some reward information during this epoch. However, both areas showed prominent
modulations in the post-reward epoch, which (1) were typically larger after a non-rewarded than a rewarded trial,
(2) were sensitive to both the current trial’s cue and the prior trial reward, and (3) appeared much earlier in the
dlPFC relative to 7a. This suggests that areas dlPFC and 7a participate in the updating of reward probability
estimates.

I-99. Thalamus relays choice and reward-related signals to layer 1 of visual
cortex
Marina Fridman1
Tiago Marques1
Dmitry Kobak
Christian Machens1
Leopoldo Petreanu2
1 Champalimaud
2 Champalimaud

MARINA . FRIDMAN @ NEURO. FCHAMPALIMAUD. ORG
TIAGO. MARQUES @ NEURO. FCHAMPALIMAUD. ORG
DMITRY. KOBAK @ NEURO. FCHAMPALIMAUD. ORG
CHRISTIAN . MACHENS @ NEURO. FCHAMPALIMAUD. ORG
LEOPOLDO. PETREANU @ NEURO. FCHAMPALIMAUD. ORG

Centre for the Unknown
Center for the Unknown

The role of the higher order nuclei of the thalamus, such as the pulvinar complex or lateral posterior nucleus (LP),
is largely unknown. Their dense and widespread projection to layer 1 of visual cortex suggests that they may exert
significant influences on cortical computation. However, the signals relayed by higher-order nuclei to sensory
cortices remain mostly uncharacterized. Here we investigated what information LP sends to primary visual cortex
(V1) in the context of a sensory discrimination task. We used calcium imaging to record activity of individual LP
axons in layer 1 of V1 while mice performed a self-initiated two alternative forced choice motion discrimination
task with stimuli of varying difficulty. LP activity showed diverse and complex tuning and carried information
about visual stimuli, motor activity and reward. Motion direction, motion coherence, running speed, choice and
reward could all be decoded from LP population activity. Furthermore, individual boutons often responded to
a combination of features, such as reward after a specific choice. This activity is more reminiscent of mixed
selectivity found in prefrontal association cortex than primary sensory areas. We then compared the responses of
LP axons with that of neurons in L2/3 of V1 during the same task. V1 neurons showed a clear sensory response,
but not choice or reward modulation as seen in LP. Thus, higher order visual thalamus provides primary visual
cortex with not only information about sensory identity but also with multifaceted contextual information including
choice and reward-related signals.

104

COSYNE 2017

I-100 – I-101

I-100. Fast active set methods for online deconvolution of calcium imaging
data
Johannes Friedrich1
Pengcheng Zhou2
Liam Paninski1
1 Columbia
2 Carnegie

JF 2954@ COLUMBIA . EDU
PENGCHEZ @ ANDREW. CMU. EDU
LIAM @ STAT. COLUMBIA . EDU

University
Mellon University

Fluorescent calcium indicators are a popular means for observing the spiking activity of large neuronal populations, but extracting the activity of each neuron from raw fluorescence calcium imaging data is a nontrivial problem.
We present a fast online active set method to solve this sparse non-negative deconvolution problem. Importantly,
the algorithm progresses through each time series sequentially from beginning to end, thus enabling real-time online estimation of neural activity during the imaging session. Our algorithm is a generalization of the pool adjacent
violators algorithm (PAVA) for isotonic regression and inherits its linear-time computational complexity. We gain
remarkable increases in processing speed: more than one order of magnitude compared to currently employed
state of the art convex solvers relying on interior point methods. Our method can exploit warm starts; therefore
optimizing model hyperparameters only requires a handful of passes through the data. A minor modification can
further improve the quality of activity inference by imposing a constraint on the minimum spike size. The algorithm
enables real-time simultaneous deconvolution of O(10ˆ5) traces of whole-brain larval zebrafish imaging data on
a laptop.

I-101. Modeling experience and time pressure in human gameplay
Gianni Galbiati
Yunqi Li
Bas van Opheusden
Weiji Ma

GVG 218@ NYU. EDU
YL 2120@ NYU. EDU
SVO 213@ NYU. EDU
WEIJIMA @ NYU. EDU

New York University
Thinking ahead is a key feature of human cognition. Previously, we developed generative models of human behavior in a simple strategy game to investigate the cognitive processes underlying sequential planning. Our most
successful model, inspired by heuristic search algorithms from artificial intelligence research, uses a weightedfeature value function in a best-first tree search algorithm with value-based pruning, stochastic feature dropping
to mimic attentional oversights, value noise, and stochastic lapses. We found that this model built larger decision trees and dropped fewer features when representing stronger players. We conducted two experiments to
see whether our model consistently captured behavioral responses to experimental manipulations. First, subjects
played our experimental game for five sessions. We predicted that as players gained experience, their performance would improve and the corresponding fitted model would build larger trees and drop fewer features. Our
predictions were supported by correlations between experience and performance, experience and model tree
size, and experience and the feature drop rate parameter. Second, we imposed three levels of time limit on players. We expected that subject performance would decrease as time limit increased, and that the model would
capture falling performance by building smaller trees and dropping more features. However, performance surprisingly did not correlate with time limit. Instead, both tree size and feature drop rate correlated positively with
the time limit. The increasing drop rate found by the model suggests an explanation for the lack of a correlation
between performance and time limit. More thinking time may allow subjects to construct larger game trees, but
because working memory is limited, game tree representations may degrade as their size grows. Collectively,
these findings demonstrate the quality of this algorithm as a model of human decision-making in sequential tasks.

COSYNE 2017

105

I-102 – I-103

I-102. Population level trial-to-trial variability as a signature of neural dynamics
Aniruddh Galgali1,2
Valerio Mante1,2
1 ETH

GALGALIA @ INI . UZH . CH
VALERIO @ INI . UZH . CH

Zurich
of Zurich

2 University

Neural population responses in associative and motor areas are often considered to be the manifestation of an underlying dynamical system. For instance, population responses during working memory, evidence accumulation,
and movement generation can be explained as reflecting point attractors, line attractors, or rotational dynamics.
However, the evidence for such dynamics remains indirect, and is often limited to analyses of responses averaged across many trials. More direct evidence for attractor or rotational dynamics may be obtained by leveraging
trial-to-trial variability of population responses, as different dynamical systems predict population-level variability
with different structure and time course. Here we propose a novel state-space approach to assess the temporal
dynamics of trial-to-trial variability at the level of a population, and thus to obtain a relatively model-free estimate
of the underlying, latent dynamics. Our approach estimates a quantity we call the “propagating variance”, defined
as the fraction of the total shared variability at any time ‘t‘ explainable by variability at a different time in the trial
along a specific direction in state space. Simulations based on stationary and non-stationary dynamical systems
show that the propagating variance, unlike condition-averaged responses, can distinguish between a variety of
underlying dynamical systems. Applying this approach to Utah-array recordings from monkey prefrontal cortex
during a direction-discrimination task revealed highly non-stationary and distinct dynamics after the onset of a
random-dot stimulus and around the time of the choice-saccade. Variability after stimulus onset is consistent with
progressively slower, point-attractor dynamics along specific state-space directions; these dynamics are abruptly
interrupted shortly before saccade onset, when variability is quenched and stops propagating into the future.
These observations strongly constrain the properties of the underlying dynamics, and provide a novel, broadly
applicable lens to study neural population dynamics.

I-103. The development of spatial acuity in mouse visual cortex
Sunil Gandhi
Dario Figueroa Velez
Melissa Davis

SUNIL . GANDHI @ UCI . EDU
FIGUEROD @ UCI . EDU
MFDAVIS @ UCI . EDU

University of California, Irvine
Disruptions of early childhood visual experience prevent the development of high spatial acuity vision. It is widely
assumed that the critical period for spatial acuity development is defined by the presence of ocular dominance
(OD) plasticity in primary visual cortex. Whereas the critical period for OD plasticity has been extensively mapped
in multiple species, much less is known about the developmental timing of high spatial acuity cortical responses.
First, we assessed the critical period for spatial acuity development in mice using intrinsic signal imaging to determine the spatial frequency cutoff of cortical responses to sinusoidal drifting gratings. We find that brief monocular
deprivation before the onset of ocular dominance plasticity produces an unexpected, long-lasting reduction in
cortical spatial acuity, whereas deprivation during the OD critical period has no persistent effect. Early deprivation disturbs the acuity of both the contralateral and ipsilateral input to visual cortex, suggesting a cortical origin.
Next, using GCaMP6 imaging of cellular spatial frequency tuning, we confirmed that early deprivation prevents
the development of cortical responses to high spatial frequency gratings. We also found that enhancement of
inhibition before the OD critical period, shown to activate precocious OD plasticity, also prevents cortical acuity
development. Lastly, we found that deprivation during the spatial acuity critical period disrupts subsequent OD
plasticity. Together, these results reveal that the critical period for spatial acuity occurs before the OD critical
period and likely involves distinct mechanisms.

106

COSYNE 2017

I-104 – I-105

I-104. Robust estimation of calcium transients by modeling contamination
Jeff Gauthier
Adam S Charles
Jonathan Pillow
David Tank

JLGAUTHI @ PRINCETON . EDU
ADAMSC @ PRINCETON . EDU
PILLOW @ PRINCETON . EDU
DWTANK @ PRINCETON . EDU

Princeton University
Two-photon calcium imaging has enabled recording of large neural populations, but distinguishing the activity
of individual neurons is not always straightforward. In densely stained tissue such as the CA1 region of the
hippocampus, many cell bodies and processes can overlap in a small space, posing the computational challenge
of demixing their signals. While several “cell finding” methods have been developed to identify the shapes and
locations of active sources, they remain susceptible to a fundamental problem: unidentified sources can produce
signals that contaminate the identified sources. For example, if a neurite overlaps a cell body, but only the shape
of the cell body was identified during cell finding, fluorescence activity in the neurite will produce an increase
in the estimated fluorescence of the cell body. In one sample dataset we collected from CA1 in an awake,
behaving mouse, approx. 10% of fluorescence transients were due to contamination. In this work, we develop
an algorithm to remove such false transients. Our method is based on a model in which the noisy observed
movies contain either known sources, or a mixture of known and unknown sources (i.e. neurons or processes
not detected in previous stages of processing). The unknown sources are modeled as a sparse sum of Gaussian
shaped dictionary elements. This allows our method to flexibly construct a new source that describes activity not
accounted for by the known sources. We use this model to develop an approximate maximum likelihood (ML)
estimator. When applied to real movies in which some active neurons were not identified during cell finding,
the estimator accurately distinguishes true transients in identified neurons from contamination by unidentified
neurons, achieving a classification that perfectly agreed with manual human classification. We call this estimator
Sparse Emulation of Unknown Dictionary Objects, or SEUDO.

I-105. A biologically plausible neural network for similarity-based clustering
Alex Genkin1
Cengiz Pehlevan2,3
Dmitri Chklovskii3,1

ALEXANDER . GENKIN @ GMAIL . COM
CPEHLEVAN @ SIMONSFOUNDATION . ORG
MITYA @ SIMONSFOUNDATION . ORG

1 New

York University
Institute
3 Simons Foundation
2 Flatiron

As large labeled datasets are rarely available in real-life biological learning, we expect that biological neural networks must be capable of unsupervised learning such as, for example, clustering of datasets streamed by sensory
organs. Whereas previously proposed winner-take-all neural networks could cluster, they were not derived from a
normative approach and/or lacked biological plausibility. Here, we start with a recent formulation of the clustering
problem using convex optimization. Whereas such formulation in the offline (or batch) setting guarantees that a
global optimum can be found in polynomial time, it is still too slow for large datasets. To formulate clustering in the
online (or streaming) setting we relax the optimization problem reducing it to similarity alignment. Starting with
such objective function with an augmented Lagrangian, we derive an online primal-dual gradient descent/ascent.
Such algorithm clusters streamed data, one sample at a time, and updates cluster centers as it goes along.
We propose that such online algorithm can be implemented by a biologically plausible neural network with local
learning rules. Each principal neuron receives input projected onto a synaptic weight vector representing the
corresponding cluster center and the output activity of that neuron represents the corresponding cluster attribution index. A single inhibitory interneuron reciprocally connected with principal neurons represents the Lagrange
multiplier. Such network can model a stage of the insect olfactory pathway: antennal lobe projection neurons
synapsing onto mushroom body Kenyon cells which are, in turn, reciprocally connected with the giant inhibitory

COSYNE 2017

107

I-106 – I-107
interneuron. Thus, our model identifies operation of a biological network with the steps of an online optimization
algorithm suggesting that interneuron activity may code a Lagrange multiplier. The model accounts for extremely
sparse activity in Kenyon cells and for the existence of only one interneuron. Finally, we suggest that ephaptic
interactions may correspond to the augmented Lagrangian term.

I-106. Behavioral adaptation to variance in sensory input
Marc Gershow
Ruben Gepner
Jason Wolk
Digvijay S Wadekar

MHG 4@ NYU. EDU
RG 1346@ NYU. EDU
JPW 367@ NYU. EDU
DIGVIJAY. WADEKAR @ NYU. EDU

New York University
Summary A rich field of study has examined how neural responses adapt to temporal changes in the variance
of sensory input, but the behavioral correlates of this adaptation have received comparatively little attention.
We used the navigational decisions of larval Drosophila to quantify the dynamics of adaptation to variance in a
model sensory-motor transformation. Drosophila larvae navigate spatially varying environments using a biased
random walk strategy, a key component of which is the decision to initiate a turn (change direction) in response
to declining conditions. We modeled this decision as the output of a Linear-Nonlinear-Poisson cascade and used
reverse correlation with visual and fictive (optogenetically induced) olfactory stimuli to find the parameters of this
model. We changed the variance of the input noise and found that the nonlinear rate function adapted as a result,
allowing larvae to respond more strongly to small changes in low-noise compared to high-noise environments.
We then measured the rate at which the larva adapted its behavior to step changes in variance. We found faster
adaptation to a step increase in variance than to a step decrease, consistent with the behavior of an optimal
estimator. Larvae linearly combine olfactory and visual signals to make the decision to turn; does adaptation to
variance occur before or after this combination? We found that larvae adapt separately to the variance in olfactory
and visual signals. We suggest that variance adaptation can be used to map the flow of information in neural
circuits.

I-107. Modeling dispersion improves decoding of population neural responses
Abed Ghanbari
Ian Stevenson

ABEDXGHANBARI @ GMAIL . COM
IAN . STEVENSON @ UCONN . EDU

University of Connecticut
Neural responses to repeated presentations of an identical stimulus often show substantial trial-to-trial variability.
Although the mean firing rate in response to different stimuli or during different movements (tuning curves) have
been extensively modeled, the variability of neural responses can also have clear tuning independent of the
tuning in the firing rate. This suggests that the variability carries information regarding the stimulus/movement
beyond what is encoded in the mean firing rate. Here we demonstrate how taking variability into account can
improve neural decoding. In a typical neural coding model spike counts are assumed to be Poisson with the mean
response depending on an external variable, such as a stimulus/movement direction. Bayesian decoding methods
then use the probabilities under these Poisson tuning models (the likelihood) to estimate the probability of each
stimulus given the spikes on a given trial (the posterior). However, under the Poisson model, spike count variability
is always exactly equal to the mean (Fano Factor = 1). Here we use the Conway-Maxwell-Poisson (COM-Poisson)
model to more flexibly model how neural variability depends on external stimuli. This model contains the Poisson
distribution as a special case, but has an additional parameter that allows both over- and under-dispersed data,
where the variance is greater than (Fano Factor >1) or less than (Fano Factor <1) the mean, respectively. We
find that neural responses in both primary motor cortex (M1) and primary visual cortex (V1) have diverse tuning

108

COSYNE 2017

I-108 – I-109
in both their mean firing rates and response variability. These tuning patterns can be accurately described by
the COM-Poisson model, and, in both cortical areas, we find that a Bayesian decoder using the COM-Poisson
models improves stimulus/movement estimation by 4-8% compared to the Poisson model. The additional layer of
information in response variability thus appears to be an important part of the neural code.

I-108. Probabilistic inference over switching network dynamics.
Aram Giahi1
Xaq Pitkow2,1

ARAM . GI @ GMAIL . COM
XAQ @ RICE . EDU

1 Baylor
2 Rice

College of Medicine
University

Recent studies of electrocorticographical (ECoG) data show that brain activity is better described by functional
networks that change over time, than by sequentially activated regions. These changing network structures can
be described by distinct network states, with interactions that change over time and with context. It is critical to
understand these dynamic interactions between brain areas, because they provide evidence about information
flow and computation in the brain. Here we evaluate and compare several inference methods to extract these
dynamic networks, each based on probabilistic models of activity with latent states. These probabilistic models
allow stochastic transitions between latent states, and within each latent state the measured neural activity is
described by a linear dynamical system. This autoregressive process can account for slow temporal dynamics
as well as structured covariance in observed neural responses. The two main models we compare are a noiseless autoregressive Hidden Markov Model (aHMM) and a switching state space model (SSM) with observation
noise. We apply state-conditional Kalman smoothing, variational Bayesian (VB) inference, and particle filtering
for inference of dynamics, and use Expectation Maximization (EM) to identify the latent states and learn the network parameters. These inference methods generally deliver good estimates on our multivariate model ECoG
time series data, but the methods have distinct, and complementary, advantages. We show how the structure of
state dynamics influences the success of each of these methods. We also discuss how these methods can be
applied to capture network structures shared across a population, and unique to individual subjects. Compared to
conventional clustering methods based on lagged correlations, our methods automatically learn the uncertainties
associated with the model. These uncertainties can be used to determine whether any differences in brain states
between subjects, groups, or conditions are significant.

I-109. Learning a non-linear dynamical system in a recurrent spiking neural
network
Aditya Gilra
Wulfram Gerstner

ADITYA . GILRA @ EPFL . CH
WULFRAM . GERSTNER @ EPFL . CH

Ecole Polytechnique Federale de Lausanne
The brain needs to construct predictive models of the non-linear dynamics of muscles, limbs and the external world
for motor control and planning. How spiking neural networks can learn such models is an open problem, despite
significant progress via reservoir computing, FORCE learning, and other methods. However, current schemes
require special reservoir characteristics, need non-local learning rules, use continuous-valued neurons, learn only
linear dynamics, and / or have stability issues. Here, we demonstrate a Local, Online and Lyapunov Stable (LOLS)
learning scheme to learn the feedforward and recurrent weights of a spiking neural network to mimic/predict a
low-dimensional non-linear dynamical system. We derive the learning rules showing global uniform (Lyapunov)
stability. The learning rules are synaptically local involving the pre-synaptic firing rate and an error feedback
current injected into the post-synaptic neuron. We provide examples of learning a linear, non-linear and chaotic
system. With this, we propose a more biologically plausible scheme of how the brain may learn forward predictive

COSYNE 2017

109

I-110 – I-111
models (motor output from motor command). Extensions to further biological plausibility, hierarchical coding, and
learning the inverse model (motor command from desired output) are planned for future work.

I-110. CaImAn: An open source toolbox for large scale calcium imaging data
analysis on standalone machines
Andrea Giovannucci1,2
Johannes Friedrich3
Ben Deverett4
Valentina Staneva5
Dmitri Chklovskii1
Eftychios Pnevmatikakis1

ANDREA . GIOVANNUCCI @ GMAIL . COM
JF 2954@ COLUMBIA . EDU
BENDEVERETT @ GMAIL . COM
VMS 16@ UW. EDU
DCHKLOVSKII @ SIMONSFOUNDATION . ORG
EPNEVMATIKAKIS @ SIMONSFOUNDATION . ORG

1 Simons

Foundation
institute
3 Columbia University
4 Princeton University
5 University of Washington
2 Flatiron

Calcium imaging is a popular tool among neuroscientists because of its capability to monitor in-vivo large neural
populations across weeks with single neuron resolution. Advances in microscopy techniques facilitate imaging
larger brain areas with finer time resolution producing data at rates surpassing 50GB/hour. This sheer amount
of data poses significant challenges in terms of handling and processing. While large scale solutions for these
problems are available, their use typically requires dedicated or shared computing clusters, thus hindering their
diffusion to the average user. Here we present CaImAn, a suite of open source tools developed in MATLAB
and Python for large scale Calcium Imaging data Analysis (CaImAn). Our focus has been threefold: First, build
upon state of the art methods to the universal pre-processing problems (motion artifact correction, source separation/segmentation of overlapping fluorescence signals, and neural activity deconvolution) in an optimized and
automated way that requires minimal user intervention. To that end, we introduced a novel framework for fast
non-rigid motion correction and automatic removal of low-quality components produced from the constrained
non-negative matrix factorization (CNMF) source extraction algorithm [1]. Second, enable processing of large
scale datasets on single multi-core machines, as well as high performance computing clusters, by employing parallelization and memory mapping techniques that harness the computational resources of standalone machines.
Third, equip the user with visualization and interactive processing capabilities for reviewing and eventually modifying the results of the analysis. We demonstrate the usage of our suite on a wide variety of 2d and 3d datasets
with sizes that exceed the amount of available memory, and discuss issues on benchmarking our results against
manually annotated datasets.

I-111. Modern machine learning tools for neural decoding
Joshua Glaser
Raeed Chowdhury
Matthew Perich
Lee Miller
Konrad Kording

JOSHGLASER 88@ GMAIL . COM
RAEED. CHOWDHURY @ GMAIL . COM
MPERICH @ GMAIL . COM
LM @ NORTHWESTERN . EDU
KOERDING @ GMAIL . COM

Northwestern University
While machine learning tools have been rapidly advancing, the bulk of neural decoding approaches still use last
century methods. Improving the performance of neural decoding algorithms can help us better understand what
information the brain represents, and can help for engineering applications such as brain machine interfaces.

110

COSYNE 2017

I-112
Here, we apply modern machine learning techniques, including recurrent neural networks (RNNs) and gradient
boosting, to decode spiking activity in 1. motor cortex, 2. somatosensory cortex, and 3. hippocampus. We
compare the predictive ability of these modern methods with traditional decoding methods such as Wiener and
Kalman filters. Modern methods significantly outperform the historical ones. A Gated Recurrent Unit decoder, a
type of RNN, yields the best performance in all three brain areas. This approach typically explains over 30% of
the unexplained variance from a Wiener filter (R2’s of 0.86, 0.59, 0.50 vs. 0.75, 0.42, 0.28). These results suggest
that modern machine learning techniques should be the default for neural decoding.

I-112. Modelling large-scale communities requires effective connectivity
Katharina Glomb1,2
Adrian Ponce-Alvarez1
Matthieu Gilson1
Petra Ritter
Gustavo Deco1

KATHARINA . GLOMB @ UPF. EDU
ADRIAN . PONCE @ UPF. EDU
MATTHIEU. GILSON @ UPF. EDU
PETRA . RITTER @ CHARITE . DE
GUSTAVO. DECO @ UPF. EDU

1 Universitat
2 Center

Pompeu Fabra
for Brain and Cognition

It is well-established that patterns of functional connectivity (FC) - measures of correlated activity between pairs
of voxels or regions observed in the human brain using neuroimaging - are robustly expressed in spontaneous
activity during rest. These patterns are not static, but exhibit complex spatio-temporal dynamics. We combine
fMRI data analysis and modelling of FC dynamics between 66 regions of interest (ROIs) covering the entire cortex. We employ a mean field model that has been shown to explain resting state FC averaged over time and
multiple subjects. This FC summarizes the spatial distribution of pairwise FC while hiding their temporal dynamics. We investigate whether this model, whose dynamics exhibit a single attractor explored via noisy fluctuations,
can also reproduce time-resolved FC patterns. Furthermore, we evaluate how the fit of the simulated patterns to
the empirical ones is affected by the underlying node connectivity because it is known to shape average FC to
a large degree. We compare two types of connectivity: DTI-based structural connectivity (SC) and model-based
effective connectivity (EC). Temporal and spatial information is considered simultaneously by computing FC for
sliding windows. This results in tensors, which are decomposed into a small number of features: weighted sets of
ROIs (“communities”) that share similar time courses. Importantly, communities are allowed to overlap in space
and time. We uncover four common communities which resemble known resting state networks, and show that
they are present in the empirical as well as the simulated data: DMN, visual network, control networks, and sensorimotor network. Simulated communities are more similar to empirical ones when model nodes are connected
with EC than with SC. This means that the EC’s asymmetry and more even degree distribution contribute to symmetric and stable communities. Taken together, time-resolved RSNs are explained by simple attractor dynamics
and the particular structure of the EC.

COSYNE 2017

111

I-113 – II-1

I-113. Neuronal circuit analysis with spike-triggered non-negative matrix factorization
Tim Gollisch1
Jian K. Liu1
Helene M Schreyer1
Arno Onken2
Fernando Rozenblit1
Mohammad H Khani1
Stefano Panzeri2

TIM . GOLLISCH @ MED. UNI - GOETTINGEN . DE
JIAN . LIU @ MED. UNI - GOETTINGEN . DE
HELENE . SCHREYER @ MED. UNI - GOETTINGEN . DE
ARNO. ONKEN @ IIT. IT
FERNANDO. ROZENBLIT @ MED. UNI - GOETTINGEN . DE
MOHAMMAD. KHANI @ MED. UNI - GOETTINGEN . DE
STEFANO. PANZERI @ IIT. IT

1 University
2 Istituto

Medical Center Goettingen
Italiano di Tecnologia

Many neurons throughout different sensory systems integrate their input signals in a nonlinear fashion. These
nonlinearities of signal integration are often critical for how the neurons extract sensory features and encode
sensory information. Computational models typically aim at capturing these nonlinear integration characteristics
by partitioning receptive fields into subunits whose signals are nonlinearly combined. Such subunit models are
commonly used, for example, to describe visual responses of neurons in the retina or primary visual cortex, but
also find application in other sensory systems. Yet, detailed investigations of subunit models and their connections
to the neural circuitry suffer from the difficulty of identifying the concrete set of subunits from the activity of recorded
neurons. To this end, we developed spike-triggered non-negative matrix factorization (STNMF), a novel data
analysis technique, which is based on applying non-negative matrix factorization to the ensemble of spike-eliciting
stimuli, obtained under white-noise stimulation. Using simple neural circuit simulations as well as recordings from
retinal ganglion cells, we demonstrate that the method faithfully identifies the relevant subunits without the need
for prior specification of the number, size, or shape of the subunits or of their nonlinear interactions. Furthermore,
we show 1. that the obtained subunit layouts help in predicting ganglion cell responses under natural stimulation,
2. that the identified ganglion cell subunits correspond to presynaptic bipolar cell receptive fields, as verified by
combined recordings from bipolar and ganglion cells, and 3. that comparison of subunit layouts across populations
of simultaneously recorded ganglion cells reveal which cells and cell types share input from the same bipolar cells.
Thus STNMF provides a promising new tool for connecting neural circuitry and sensory encoding, applicable to a
wide range of sensory systems.

II-1. Learning by neural reassociation
Matthew Golub
Patrick Sadtler
Kristin Quick
Stephen Ryu
Elizabeth Tyler-Kabara
Aaron Batista
Steven Chase
Byron Yu

MGOLUB @ CMU. EDU
PATRICK . T. SADTLER @ GMAIL . COM
KRISTINMQUICK @ GMAIL . COM
SEOULMAN @ STANFORD. EDU
ELIZABETH . TYLER - KABARA @ CHP. EDU
APB 10@ PITT. EDU
SCHASE @ CMU. EDU
BYRONYU @ CMU. EDU

Carnegie Mellon University
Behavior is generated by the coordinated activity of populations of neurons. Learning requires the brain to change
how this population neural activity is generated. However, the neural strategies that drive these population-level
changes are not well understood. We recently developed a BCI learning experiment in primary motor cortex (M1)
of rhesus macaques. Our study was built upon the observation that neural population activity tends to lie in a
low-dimension subspace, which we term the intrinsic manifold. We found that BCI mappings that were consistent
with the intrinsic manifold were more readily learned than those that were not, suggesting that changes within
the manifold play a major role driving behavioral improvements during learning (Sadtler, et al, 2014). Precisely

112

COSYNE 2017

II-2 – II-3
how activity changes within-the-manifold is not yet known and is the key question we address in this study. We
found that a strategy of neural reassociation best described the observed changes in population activity: animals
relied on a largely fixed repertoire of activity patterns, but reassociated patterns within that repertoire with different
movements after learning. These results suggest that the set of activity patterns that a population of neurons can
generate may be even more constrained than previously thought.

II-2. Optimized computation of binocular disparity by populations of simple
and complex cells
Nuno Goncalves
Andrew Welchman

NRG 30@ CAM . AC. UK
AEW 69@ CAM . AC. UK

University of Cambridge
Binocular disparities encapsulate exquisite clues about the depth structure of the environment, providing critical
information for understanding scenes and guiding behavior. How does the visual system construct depth from
binocular disparities? Neurons in primary visual cortex (V1) are thought to play a major role in this process. The
classical disparity energy model states that complex cells in V1 compute the energy for a particular disparity by
summing the activity of a set of simple cells tuned to detect that disparity. While this model has been dominant for
over 25 years, it does not account for well known neurophysiological observations. For instance, it fails to reproduce selective but attenuated responses to random-dot patterns that are binocularly anticorrelated. Additionally,
from an interpretive standpoint, it is unclear why complex cells should compute binocular energy as proposed
by the energy model. To address these issues, we optimized a network of simple and complex units to extract
binocular disparity from naturalistic images, and then compared the properties of model neurons with those of
disparity selective cells in V1. We found that a simple feed-forward network reproduces key properties of binocular neurons in V1: simple units develop binocular receptive fields with hybrid position and phase disparities, and
complex units respond selectively to disparity in artificial random-dot patterns. Most remarkably, complex units
also responded selectively to disparity in anticorrelated random-dot patterns, reproducing the inverted and attenuated response that characterizes many V1 complex cells. We show that the network effectively implements a
polarity constraint, and explores its violations to extract the correct depth percept. Through analyses we capture
the encoding and readout mechanisms in simple analytical form, and show that complex cells approximate the
likelihood function for binocular disparity. Our findings provide a unified account of longstanding puzzles in 3D
vision at the physiological- and perceptual- levels.

II-3. Flexible Bayesian inference for mechanistic models of neural dynamics
Pedro J Goncalves
Jan-Matthis Lueckmann
Giacomo Bassetto
Marcel Nonnenmacher
Jakob Macke

PEDRO. GONCALVES @ CAESAR . DE
JAN - MATTHIS . LUECKMANN @ CAESAR . DE
GIACOMO. BASSETTO @ CAESAR . DE
MARCEL . NONNENMACHER @ CAESAR . DE
JAKOB . MACKE @ CAESAR . DE

research center caesar
One of the central goals of computational neuroscience is to understand the dynamics of single neurons and
neural ensembles. However, linking mechanistic models of neural dynamics to empirical observations of neural
activity has been challenging. Statistical inference is only possible for a few models of neural dynamics (e.g.
GLMs), and no generally applicable, effective statistical inference algorithms are available: As a consequence,
comparisons between models and data are either qualitative or rely on manual parameter tweaking, parameterfitting using heuristics or brute-force search. Furthermore, parameter-fitting approaches typically return a single
best-fitting estimate, but do not characterize the entire space of models that would be consistent with data. We

COSYNE 2017

113

II-4 – II-5
overcome this limitation by presenting a general method for Bayesian inference on mechanistic models of neural
dynamics. Our approach can be applied in a ‘black box’ manner to a wide range of neural models without requiring
model-specific modifications. In particular, it extends to models without explicit likelihoods (e.g. most spiking networks). We achieve this goal by building on recent advances in likelihood-free Bayesian inference (Papamakarios
and Murray 2016, Moreno et al. 2016): the key idea is to simulate multiple data-sets from different parameters,
and then to train a probabilistic neural network which approximates the mapping from data to posterior distribution. We illustrate this approach using Hodgkin-Huxley models of single neurons and models of spiking networks:
On simulated data, estimated posterior distributions recover ground-truth parameters, and reveal the manifold of
parameters for which the model exhibits the same behaviour. On in-vitro recordings of membrane voltages, we
recover multivariate posteriors over biophysical parameters, and voltage traces accurately match empirical data.
Our approach will enable neuroscientists to perform Bayesian inference on complex neural dynamics models without having to design model-specific algorithms, closing the gap between biophysical and statistical approaches to
neural dynamics.

II-4. Slow gain fluctuations limit temporal integration in visual cortex
Robbe Goris1
Corey M Ziemba2
J Anthony Movshon2
Eero P Simoncelli2,3

ROBBE . GORIS @ UTEXAS . EDU
CMZ 250@ NYU. EDU
MOVSHON @ NYU. EDU
EERO. SIMONCELLI @ NYU. EDU

1 University

of Texas at Austin
York University
3 Howard Hughes Medical Institute
2 New

Many visually-driven activities require observers to integrate information over time. Traditional models suggest that
the signal–to–noise ratio (SNR) of integrated sensory information should improve in proportion to the square root
of integration time. However, in experimentally controlled tasks, observer performance grows at a substantially
slower rate, often saturating over relatively short time scales. This failure has been ascribed to multiple causes,
including rapid sensory adaptation, faulty accumulation of available information, and incorporation of task-related
incentives for rapid decision-making. But temporal integration may be limited by a more elementary factor: Temporal correlation of noise (Osborne, Bialek & Lisberger, 2004). We examined responses of orientation-selective
neurons in the primary visual cortex (V1) of two macaque monkeys performing an orientation discrimination task.
The signal–to–noise ratio (SNR) of temporally integrated responses increased for durations up to approximately
250 ms, but saturated for longer durations. This was true even when cells exhibited little or no adaptation in their
response levels. Analysis of another data set revealed similar effects in extrastriate area MT. We show that the
saturation of SNR is consistent with a previously proposed response model in which spikes arise from a Poisson
process whose stimulus-dependent rate is modulated by slow, stimulus-independent fluctuations in gain (Goris,
Movshon & Simoncelli, 2014). Specifically, the response variability arising from the Poisson process is reduced
by temporal integration, but the temporally correlated variability due to gain fluctuations is not. A model with slow
additive fluctuations does not exhibit these behaviors. Our analysis suggests that slow gain fluctuations impose a
fundamental limit on the benefits of temporal integration throughout the visual cortex.

II-5. Striatal and orbitofrontal network dynamics differentially encode time
Vishwa Goudar
Konstantin Bakhurin
Dean Buonomano
Sotiris Masmanidis

VISHWA . GOUDAR @ GMAIL . COM
BAKHURIN @ G . UCLA . EDU
DBUONO @ UCLA . EDU
SMASMANIDIS @ UCLA . EDU

University of California, Los Angeles

114

COSYNE 2017

II-6
Timing on the scale of hundreds of milliseconds to a few seconds is fundamental to many forms of behavior,
including the anticipation of rewarding events. While the mechanisms underlying the brain’s ability to track time
remain unknown, theoretical studies posit that neural populations collectively encode the passage of time in
dynamically changing patterns of activity, giving rise to neural population clocks. Furthermore, a rapidly growing
literature has reported the observation of population clocks in a range of brain areas, including the striatum and the
frontal and prefrontal cortex. But, do different brain areas track time in parallel for a single task, and if so, do some
areas manifest a qualitatively better clock than others? To answer this, we decoded and compared population
clocks in the striatum and the orbitofrontal cortex (OFC), in mice that had learned the temporal relationship
between an odor stimulus and an associated delayed reward, and acknowledged the association with anticipatory
licking during the fixed stimulus-reward interval. In each area, we performed large-scale recordings that yielded
simultaneous firing rate activity for dozens of units which, upon trial-averaging, exhibited a population code for
time elapsed since stimulus onset. Training a Support Vector Machine (SVM) to decode this time on a trial-by-trial
basis, and quantifying the quality of the time code as the correlation between true elapsed time and decoded
time, revealed robust population clocks in both areas. Crucially, the striatum consistently outperformed the OFC
in encoding time. However, the OFC dynamics were significantly higher-dimensional, suggesting that they may
encode other behavioral variables not immediately relevant to the task. Our results support the hypothesis that
many brain areas simultaneously encode time, and that the striatum may play a privileged role in timing relative
to the OFC, integrating temporal codes across multiple cortical areas, thereby producing a more accurate clock.

II-6. Micro-organization of grid cells in layer II of the medial entorhinal cortex
Yi Gu1
Amina Kinkhabwala2
Cristina Domnisoru1
Jeff Gauthier1
David Tank1
1 Princeton

YIG @ PRINCETON . EDU
AMINA . KINKHABWALA @ NORTHWESTERN . EDU
DOMNI @ EXCHANGE . PRINCETON . EDU
JEFF . L . GAUTHIER @ GMAIL . COM
DWTANK @ PRINCETON . EDU

University
University

2 Northwestern

The medial entorhinal cortex (MEC) is important for spatial representation and episodic memory. Grid cells are
abundant in layers II and III of the MEC and exhibit firing fields on the vertices of a triangular lattice as rodents
navigate in open arenas. Layer II of the MEC contains two excitatory cell types: pyramidal and stellate cells.
How grid cells are distributed in the two cell populations has not been definitively determined. In addition, grid
cells form discrete functional modules, in which co-modular grid cells exhibit similar scales and orientations, but
different phases. It is unclear how cells in different modules are anatomically organized and how co-modular grid
cells with different phases are spatially arranged at microstructural level. We addressed these questions by cellular
resolution two-photon imaging of calcium dynamics in GCaMP6f-labled neurons in layer II of the MEC while mice
navigated on virtual linear tracks. We found that grid cells generally form clusters and were mostly stellate cells.
Co-modular grid cells also anatomically clustered and separated from cells in other modules. Within each module,
grid cells with different phases had a geometrical arrangement: at small spatial scale (< 120µm diameter), nearby
cells showed more similar phases than distant cells. At larger spatial scale (> 120µm diameter), cells of similar
phases were arranged in multiple spatially-separated clusters. Spatial autocorrelation analyses suggested that
these phase clusters were arranged in hexagonal patterns. Our study is the first to characterize the microstructure
of grid modules and phases. Their anatomical clustering raises the possibility that cells within the same functional
unit, which is a module or phase cluster, are more strongly connected than cells in different units. This microorganization of grid modules and phases is consistent with predictions of continuous attractor models of the grid
cell network.

COSYNE 2017

115

II-7 – II-8

II-7. Neural processing of color in the larval zebrafish brain
Drago Guggiana-Nilo
Clemens Riegler
Florian Engert

DGUGGIAN @ FAS . HARVARD. EDU
RIEGLER @ FAS . HARVARD. EDU
FLORIAN @ MCB . HARVARD. EDU

Harvard University
Color vision is a highly complex and much understudied visual modality, due in part to the lack of a vertebrate
model organism that combines a color-based visual system with the ability to record from large numbers of
neurons. The zebrafish offers such a compromise, since it possesses a cone-based, tetrachromatic retina containing red, green, blue and UV cones - while at the same time allowing for whole brain imaging. Despite
these advantages, very little is known about the diversity, complexity and nature of the computations taking place
in larval color vision. To start addressing these questions, we used 2-photon calcium imaging to record population
activity from the brain of the animal during visual stimulation. For this, we built a 4-channel projector spanning the
visual range of the animal, and we targeted our recordings to the retinotectal projection, a key step in the visual
pathway of the larva. We used transgenic larvae expressing the calcium indicator GCaMP6s in either of 2 well
defined targets: the axonal terminals of Retinal Ganglion Cells (RGCs) coming from the eye of the animal; and
their targets, the cells of the Optic Tectum (OT) - the main retinorecipient area in the zebrafish. Using full-field
oscillations, we calculated the cone-type contributions to each unit identified. Here we found a UV cone-dominated
population response, with the blue cone mostly contributing inhibitory input, and a large number of color response
types. In separate experiments we also characterized responses to motion, flashes and slow oscillations. We
found color specific responses to all modalities, and an increase in the response diversity at the tectal level, as
expected from processing of the RGC inputs. These results point to a functional color processing system in the
larval zebrafish, and offer the first characterization of a vertebrate color vision system at this density.

II-8. Computational model of spatio-temporal coding in CA3 using speeddependent theta oscillation
Caroline Haimerl1
David Angulo-Garcia2
Alessandro Torcini2
Rosa Cossart2
Arnaud Malvache2
1 New

CH 2880@ NYU. EDU
DANGULOG @ GMAIL . COM
TORCINI @ GMAIL . COM
ROSA . COSSART @ INSERM . FR
ARNAUD. MALVACHE @ INSERM . FR

York University

2 INSERM

Hippocampal sequences associated with theta oscillation have been shown to carry self-referenced spatio-temporal
information. In particular, cells in CA1 become active sequentially in a stable unidirectional order during spontaneous run periods of mice on a treadmill under minimal external cues (Villette et al., 2015). These recurrent
cell sequences seem to integrate either the distance that the animal has run or the time that has elapsed. They
depend on theta oscillation from the medial septum (Wang et al., 2015) and this oscillation was shown to be
modulated in frequency and amplitude by the running speed of the animal. Theta oscillation could thereby carry
the spatio-temporal information required to distinguish distance/time coding. Theta dependent sequences in CA1
could be inherited from similar sequential activity observed in CA3 as CA3’s network structures could initiate sequences due to their recurrent connections and chain motifs (Salz et al., 2016, Guzman et al., 2016). Inspired
by Wang et al (Nature Neuro 2015), we modeled a circular recurrent network of excitatory cells with short-term
synaptic plasticity (Mongillo et al., 2008) and global inhibition. By applying speed-dependent theta oscillation, we
reproduced the dynamics of spatio-temporal coding observed in experimental data and propose a mechanism of
switching between the two coding states through a change in integration of theta input. This suggests that through
the integration of speed-dependent theta oscillation, the same CA3 recurrent network could simultaneously compute different internal information such as elapsed time and run distance.

116

COSYNE 2017

II-9 – II-10

II-9. Selective dopamine signaling underlies bidirectional synaptic plasticity
in a learning center
Annie Handler
Andrew Siliciano
Raphael Cohn
Ianessa Morantte
Vanessa Ruta

AHANDLER @ ROCKEFELLER . EDU
ASILICIANO @ ROCKEFELLER . EDU
RCOHN @ ROCKEFELLER . EDU
IMORANTTE @ ROCKEFELLER . EDU
RUTA @ ROCKEFELLER . EDU

Rockefeller University
Associative learning relies on the relative timing of events. In Drosophila if an odor immediately precedes a
reward, a positive association is formed and flies become attracted to the odor. However, if the reward is subsequently encountered independently of or prior to the odor, flies will learn this association is no longer relevant or
predictive. The ability of an animal to rapidly learn an informative association and forget an incoherent association suggests neural mechanisms for both memory writing and erosion may work in opposition to one another.
Using the simple architecture of the mushroom body, an associative learning center in Drosophila, we are examining how the relative timing of sensory and reinforcement signals is encoded at the molecular, synaptic, and
circuit level. In the mushroom body, sensory input from odor-responsive Kenyon cells (KCs) and instructive input
from dopamine neurons (DANs) converge onto mushroom body output neurons (MBONs), which mediate learned
behavioral responses to odors. Using cell-type specific activation and 2-photon imaging, we show the same
dopaminergic signal bidirectionally modulates KC-MBON synapses, leading to synaptic depression when KC excitation precedes DAN activity, as in associative learning, or synaptic facilitation when DAN activity precedes KC
excitation. We have combined biochemical and functional techniques to explore the mechanistic basis for these
opposing forms of synaptic modulation. We find that two Drosophila dopamine receptors expressed in KCs are
coupled to distinct signaling cascades that are differentially recruited depending on the relative timing of odor
input. Additionally, we provide evidence for functional antagonism between these signaling pathways, assuring
selective deployment of opposing forms of modulation. Together our experiments suggest how highly selective
neuromodulatory signaling pathways fine tune synaptic strength between neurons linking sensory processing to
behavioral output, providing a mechanism to continuously update learned associations in a dynamic and changing
environment.

II-10. Discrete modes of social information processing predict individual behavior of fish in a group
Roy Harpaz1
Gasper Tkacik2
Elad Schneidman1

HARPAZONE @ GMAIL . COM
GTKACIK @ IST. AC. AT
ELAD. SCHNEIDMAN @ WEIZMANN . AC. IL

1 Weizmann
2 Institute

Institute of Science
of Science and Technology Austria

Groups of organisms demonstrate complex coordinated behaviors that may emerge from relatively simple computations at the individual level. The nature of these local rules and how to infer them from animal tracking data,
have been heavily debated, mostly focusing on describing macroscopic behavior of groups and not the individual
level. We present a model predicting individual behavior of adult zebrafish in a group, based on their sensory and
social information. We first show that individuals in a group exhibit two main modes of kinematic behavior: An ‘active’ mode where fish are sensitive to social information encoded in the movement decisions of their neighboring
fish, and a ‘passive’ one where they seem to ignore their conspecifics. We incorporate these modes into a spatiotemporal model of social information processing, reminiscent of ‘receptive field’ models used to describe single
neurons, and extend the model with physical constraints and the effects of non-social stimuli. Our model predicts
individual fish behavior with very high accuracy at a resolution of single time points, and allows us to reconstruct
accurately full trajectories even where fish change their speed and direction considerably. This model outperforms

COSYNE 2017

117

II-11 – II-12
previously suggested models of collective motion that assume a single continuous computation by individuals in
the group, as well as models that rely on simple metric or topological weighing of neighbors’ behavior. We further
explore how the receptive field of a fish changes under different behavioral contexts, suggesting that additional
control modes may exist. At the group level, we find that switching between active and passive modes in one fish
is independent of switching in others, but that the group still shows correlated swimming behavior. Our results
demonstrate collective behavior in a group of animals, relying on distributed and temporally discrete modes of
individual social information processing.

II-11. Dissecting sensorimotor signals in area LIP during oculomotor persistent activity
Eric Hart1
Leor Katz2
Alex Huk1
1 University

EHART 004@ GMAIL . COM
LEOR . KATZ @ GMAIL . COM
HUK @ UTEXAS . EDU

of Texas at Austin

2 NIH

Area LIP has received extensive study as a hub for sensorimotor cognition in the primate brain, but this region has
defied attempts at simple categorization or a unifying explanation. Recent work has shown that neurons in LIP
encode multiplexed signals in heterogeneous ways (Meister et al., 2013), and has also clouded LIP’s relation to
decision formation (Katz et al., 2016). Here we analyze single trial spike trains in individual LIP neurons to dissect
the differential influences of sensory and motor factors during oculomotor working memory. We recorded single
neuron activity in LIP while monkeys performed memory-guided saccades, in which the subject makes an eye
movement to the remembered location of a briefly-flashed target, separated by a variable duration delay. Many
neurons in area LIP exhibit persistent activity during the delay; although such activity is a correlate of memory, it
is unclear whether such persistent activity results from the interplay of decaying visual activity and/or pre-motor
build up, as opposed to cognitive/mnemonic signals per se. Therefore, we used a generalized linear model (GLM)
to characterize how LIP spike trains depended on external task events (including target onset and the impending
saccade), as well as internal variables (such as the cell’s own recent spike history). Surprisingly, we found that
a simple encoding model with a sensory component (visual target response) and a (pre-)motor component could
explain persistent activity in LIP and capture the diversity of responses across the population. Individual neurons
were more or less driven by each of these factors, but it is the superposition of these simple components that
gives rise to the elevated activity across the delay and accounts for idiosyncratic temporal dynamics of individual
neurons. This analysis framework can both guide further experimental manipulations to dissect sensorimotor
sources of persistent activity, and support circuit-level insights from larger-scale recordings.

II-12. An energy-accuracy tradeoff in subneuronal molecular sensing
Sarah Harvey
Subhaneil Lahiri
Surya Ganguli

HARVEYS @ STANFORD. EDU
SULAHIRI @ STANFORD. EDU
SGANGULI @ STANFORD. EDU

Stanford University
Remarkably, the human brain computes using only 20 watts of power, whereas modern supercomputers consume
megawatts. To understand the unrivaled energy efficiency of neural computation, we need better theories governing energy-accuracy tradeoffs in subneuronal molecular dynamics and information processing, as dominant
sources of neural energy dissipation originate in biomolecular processes. Here, we focus on energy-accuracy
tradeoffs in nonequilibrium molecular processes underlying neuronal signaling. We model a subneuronal signaling cascade as an arbitrary, physically realizable, continuous time Markov chain which can sense an external

118

COSYNE 2017

II-13
signal that modulates its transition rates. This framework is widely applicable to many scenarios in neuroscience,
including the sensing of external neurotransmitters by synaptic spines, light by retinal opsins, or sound by cochlear
hair cells. We first derive fundamental limits on sensing accuracy by an ideal observer of the entire temporal trajectory of signaling states, by computing the Fisher information this trajectory has about the external signal. We
find a simple formula for this information in terms of molecular fluxes. However, biologically plausible molecular
observers cannot observe this trajectory and instead observe the time a signaling system spends in an active
state. We next compute a lower bound on the accuracy of such plausible molecular observers in terms of the
power consumed by the signaling system and the observation time. Our lower bounds, confirmed by numerical
simulations, reveal that as power consumption becomes larger, even by a modest amount, a biologically plausible
molecular observer can achieve accuracy comparable to that of the ideal observer. Moreover, as the power consumption is reduced, our bound indicates sensing accuracy will necessarily be lower. At zero power consumption,
our bound matches the classical results of Berg and Purcell. Overall, our work reveals fundamental limits on
sensing accuracy given power consumption and observation time for any biophysical molecular signaling system.

II-13. Robust object learning with cross-cortical column connections
Jeff Hawkins
Yuwei Cui
Subutai Ahmad
Nathanael Romano
Marcus Lewis

JHAWKINS @ NUMENTA . COM
YCUI @ NUMENTA . COM
SAHMAD @ NUMENTA . COM
NAROMANO @ STANFORD. EDU
MLEWIS @ NUMENTA . COM

Numenta, Inc
Many neocortical pyramidal neurons receive a large fraction of their excitatory inputs from long distance connections to neurons in other cortical columns. These cross-columnar inputs innervate basal dendritic segments that
are capable of initiating active dendritic spikes. The function of these lateral connections remains unclear. In
this study, we propose that cross-column connections implement an important neural mechanism that contributes
to robust pattern recognition. We consider object recognition with a set of cortical columns, where each cortical
column processes a different subset of the sensory input space. An object consists of a set of component features
at particular locations on the object. Each cortical column learns an object by forming feedforward connections
from its component features to a set of active neurons in a different cellular layer. After learning, sensation of a
sequence of object features leads to activations of the corresponding neural population representing the object.
Since features can be shared among multiple objects, information received by a single cortical column is often
ambiguous. It may take many sensations before the network converges to the correct representation. We find
that the performance can be dramatically improved by simultaneously considering a set of cortical columns with
lateral connections, where each column learns feedforward connections independently and learns cross-column
lateral connections according to Hebbian rules. The lateral inputs target distal dendritic segments. Although they
are not strong enough to directly activate a neuron, neurons with both lateral input and feedforward input will fire
earlier and prevent other neurons from responding[1]. The cross-columnar connections bias each column to form
a representation that is consistent with the partial knowledge of all the interconnected columns. We show that
objects can be recognized faster and that each cortical column can store more objects by using cross-column
connections.

COSYNE 2017

119

II-14 – II-15

II-14. Predicting neural activity in behaviorally irrelevant dimensions
Jay Hennig
Matthew Golub
Peter Lund
Patrick Sadtler
Kristin Quick
Stephen Ryu
Elizabeth Tyler-Kabara
Aaron Batista
Byron Yu
Steven Chase

JHENNIG @ CMU. EDU
MGOLUB @ CMU. EDU
PETE . LUND @ GMAIL . COM
PATRICK . T. SADTLER @ GMAIL . COM
KRISTINMQUICK @ GMAIL . COM
SEOULMAN @ STANFORD. EDU
ELIZABETH . TYLER - KABARA @ CHP. EDU
APB 10@ PITT. EDU
BYRONYU @ CMU. EDU
SCHASE @ CMU. EDU

Carnegie Mellon University
The activity of millions of neurons in motor cortex drives the activity of hundreds of muscles. Because there are
so many more neurons than muscles, a vast number of population activity patterns generate the same movement.
How does the brain choose from these behaviorally equivalent options? Understanding the selection of activity
in this space can help us understand the principles by which the brain generates movements. Previous work
has considered this problem from the perspective of muscles. There is evidence that muscle activity minimizes
unnecessary energy usage, and is left uncontrolled in “output-null“ dimensions that do not contribute to movement.
Do similar principles hold for the selection of neural activity? To investigate this question, we analyze neural activity
from a brain-computer interface (BCI) center-out task. Rhesus macaques controlled a computer cursor using their
neural activity, recorded in primary motor cortex with a Utah array. In contrast to traditional approaches, in a BCI
the mapping from neural activity to cursor movement is defined by the experimenter. This allows one to define
precisely the set of neural activities that will result in the same cursor movement by defining the output-null space
in which activity has no effect on the cursor. Our aim is to predict the monkey’s selection of output-null neural
activity. We considered two commonly assumed hypotheses from the muscle literature: (1) activity in the outputnull space minimizes spiking (in analogy to minimizing energy for muscles), or (2) activity is left uncontrolled
in dimensions that do not affect the output. We also consider the hypothesis that (3) neural activity has a fixed
structure that persists across tasks, regardless of the output-null space. This last hypothesis best predicted outputnull activity, suggesting that redundancy may be resolved at the level of neural activity in a different manner than
it is resolved with muscles.

II-15. Human noise blindness drives suboptimal cognitive inference
Santiago Herce Castanon1
Dan Bang2
Rani Moran2
Jacqueline Ding1
Tobias Egner
Christopher Summerfield1
1 University
2 University

SANTIAGO. HERCECASTANON @ PSY. OX . AC. UK
DANBANG . DB @ GMAIL . COM
RANI . MORAN @ GMAIL . COM
JACQUELINE . DING @ PSY. OX . AC. UK
TOBIAS . EGNER @ GMAIL . COM
CHRISTOPHER . SUMMERFIELD @ PSY. OX . AC. UK

of Oxford
College London

Humans typically make near-optimal perceptual judgments but humans show systematic biases when making
cognitive decisions. Here, we show that perceptual decisions succeed, whereas cognitive decisions often fail,
because of differential metacognitive sensitivity to two distinct sources of uncertainty that corrupt neural computation. We used three independent experiments, on a novel psychophysical task to independently manipulate the
quality of sensory processing (encoding noise), and the cognitive demand associated with integrating multiple
discordant sources of information (integration noise). Humans adapted their behaviour optimally in the face of
increased encoding noise but were blind to integration noise: when combining more variable sensory signals,

120

COSYNE 2017

II-16 – II-17
they failed to use additional probabilistic cues to compensate for errors, reported their confidence inappropriately,
and overlooked an option to “opt out” of more difficult trials. These findings are captured by an otherwise optimal model that neglects integration noise. Together, our concurrent but orthogonal manipulation of encoding and
integration noise, and the selective neglect of the latter that is captured by our model provide a computationally
grounded account for the role of noise blindness in driving suboptimal choices.

II-16. Statistical inference under resource constraints in dynamic environments
Ann Hermundstad1
Wiktor Mlynarski2
1 Janelia

HERMUNDSTADA @ JANELIA . HHMI . ORG
MLYNAR @ MIT. EDU

Research Campus
Institute of Technology

2 Massachusetts

Biological systems must use limited resources to make reliable inferences about the environment. While many
studies have separately explored resource limitation and optimal inference in the context of neural information
processing, little is known about their interplay. Here, we connect these two areas to ask how an organism
should efficiently allocate limited resources, when the goal is to optimally infer a state of the environment, rather
than reconstruct an incoming signal. We consider a specific implementation of a general scenario in which an
organism (“observer”) must infer a fluctuating distribution parameter (e.g. luminance of a visual signal, or intensity
of a sound) given incoming samples from that distribution (e.g. patterns of light, or waves of air pressure). The
observer maps each sample onto an internal representation, which is then used to compute an estimate of the
environmental state. Such a mapping is, by necessity, compressive, and the resources required to perform this
mapping are set by the number of discrete response levels. Compression therefore reduces resource expenditure
at the cost of estimation accuracy. We consider a compression strategy that simultaneously minimizes resources
and maximizes accuracy and that is modulated by the observer’s current belief about the environment. We find
that this strategy outperforms one that is optimized for signal reconstruction; i.e. estimation accuracy is higher
given a fixed resource constraint. Our results make a number of experimentally-testable predictions. We find
that adaptive compression slows the detection of some environmental fluctuations relative to others. Moreover,
different signals that were at one time encoded as different values will at a later time be mapped to the same
value, if they become equally likely under the observer’s internal model. We predict that such behavior will be
observed in neural systems that perform statistical inference under resource constraints.

II-17. Detailed dendritic excitatory/inhibitory balance through heterosynaptic
STDP
Naoki Hiratani
Tomoki Fukai

N . HIRATANI @ GMAIL . COM
TFUKAI @ RIKEN . JP

RIKEN Brain Science Institute
Spike-timing-dependent plasticity (STDP) is considered as a key mechanism of learning in the brain. Recent
experimental results further revealed that the relative spike timings among neighboring synapses on a dendritic
branch have significant influence on changes in synaptic efficiency of these synapses. Especially, timing of
GABAergic input exerts a great impact on synaptic plasticity at nearby glutamatergic synapses. This heterosynaptic form of STDP (h-STDP) is potentially important for synaptic organization on dendritic tree, and resultant
dendritic computation. However, the functional role of h-STDP remains elusive. Here, we constructed a computational model of h-STDP based on the calcium-based synaptic plasticity model [1], and then considered potential
functional merits of the plasticity. The model reproduces several effects of h-STDP observed in the hippocampal CA1 area [2] and the striatum [3] of rodents, and provides analytical insights for the underlying mechanism.

COSYNE 2017

121

II-18 – II-19
The model further indicates that h-STDP causes the detailed balance between excitatory and inhibitory inputs
on a dendritic branch. This result suggests that not only the number and the total current of excitatory/inhibitory
synapses are balanced at a branch, but temporal input structure is also balanced as observed in the soma. Moreover, by considering dendritic computation, we show that such detailed balance is beneficial for detecting changes
in input activity. The model also reconciles with critical period plasticity of binocular matching observed in V1 of
mice, and provides a candidate explanation on how GABA-maturation modulates the selectivity of excitatory neurons during development. [1] Shouval HZ et al., PNAS 99:10831 (2002); Graupner M & Brunel N, PNAS 109:3991
(2012). [2] Hayama T et al., Nat Neurosci 16:1409 (2013). [3] Paille V et al., J Neurosci 33:9353 (2013).

II-18. Instability of the generalized linear model for spike trains
David Hocker
Il Memming Park

DAVID. HOCKER @ STONYBROOK . EDU
MEMMING . PARK @ STONYBROOK . EDU

Stony Brook University
The generalized linear model (GLM) is a powerful and flexible statistical model widely used in neuroscience,
offering simplicity in its functional form and tractability of its parameter estimation through maximum likelihood
(ML). In particular, autoregressive point process variants of the GLM are a commonly utilized form for modeling
spike trains with a non-Poisson history dependence such as absolute/relative refractory periods, bursting, or long
time-scale self-excitation. Despite these attractive features, we establish here that autoregressive GLMs fit with
ML can result in flawed predictive spiking models subject to runaway self-excitation that manifests in sustained
maximum rate firing, thus degrading the value of the GLM as a predictive model of spike trains. We illustrate
instances in which it exhibits saturated firing rates indicative of this instability, in particular from GLM fits to singleunit recordings from the lateral intraparietal (LIP) area of monkeys during a perceptual decision-making task [Park
et al., Nat. Neuro. 2014], and a GLM fit to piecewise constant firing rate simulations. We numerically analyze the
cause of instability by inspecting the stationary distribution of the GLM, and show that there is a sharp boundary
between stable and unstable parameter regimes caused by excessive self-excitation. In looking for possible
solutions to introduce stability into a flexible statistical model, we pose the generalized quadratic model (GQM)
with ML as a stable alternative [Park et al., NIPS 2013]. We constrain the GQM to have a minimum negative
curvature resulting in an effective bounded firing rate, similar to the stability condition given in [Bremaud and
Massoulie, Annals of Prob. 1996] for continuous-time, nonlinear Hawkes processes.

II-19. Amplitude modulation encoding and decoding in mouse versus primate
auditory cortex
Nerissa Hoglen
Elizabeth AK Phillips
Phillip Larimer
Brian J Malone
Andrea Hasenstaub

NERISSA . HOGLEN @ UCSF. EDU
ELIZABETHAKPHILLIPS @ GMAIL . COM
PHILLIP. LARIMER @ UCSF. EDU
BRIAN . MALONE @ UCSF. EDU
ANDREA . HASENSTAUB @ UCSF. EDU

University of California, San Francisco
Amplitude modulation is a prominent feature of the vocalizations of many species, including primates and rodents.
Sinusoidal amplitude modulation (SAM) has been used to define the nature and limits of temporal encoding in
central auditory neurons in multiple animal models. Increasing use of mouse models for central hearing and optogenetic circuit dissection requires more detailed descriptions of their cortical encoding of the temporal dynamics
of complex stimuli. We address two questions; first, how do the encoding strategies and decoding performance
of the mouse compare to the nonhuman primate (squirrel monkey)? Second, how do specific interneuron circuits
sculpt responses to SAM stimuli? To answer these questions, we presented moderate-amplitude SAM stimuli at

122

COSYNE 2017

II-20 – II-21
several modulation frequencies to passive-listening, head-restrained awake mice expressing an excitatory opsin
in parvalbumin- or somatostatin-positive inhibitory interneurons. Electrophysiological recordings were obtained
using 16-channel probes (NeuroNexus) spanning the laminar depth of the cortex. On 50% of trials, we activated
opsin-expressing cells by illuminating the cortical surface. Responses were assessed with respect to both firing rates and synchronization (vector strength). We used linear decoders (nearest-neighbor Euclidean-distance
classifiers) to identify the modulation frequency associated with individual spike trains to obtain a lower-bound
estimate of the mutual information between stimulus envelope features and spiking responses. Many responses
exhibited robust response synchronization to stimuli at low modulation frequencies (<20 Hz) that dominate animal
vocalizations and human speech sounds. Decoding performance of rate-matched subsampled squirrel monkey
spike trains approximated mouse decoding performance. These results demonstrate that temporal precision of
cortical responses in awake mice is broadly comparable to that observed in other awake model systems and
is sufficiently precise to reward optogenetic experiments. Optogenetically activating populations of interneurons
disrupts this temporal precision: excitatory and inhibitory circuits interact to represent the dynamics of complex
signals including mouse vocalizations and human speech.

II-20. Proto-object based contour detection and figure-ground segmentation
Brian Hu
Rudiger von der Heydt
Ernst Niebur

BHU 6@ JHMI . EDU
VON . DER . HEYDT @ JHU. EDU
NIEBUR @ JHU. EDU

Johns Hopkins University
Segmenting an image into regions corresponding to objects requires contour detection and figure-ground segmentation. Border-ownership selective cells are well-suited for this task, as they encode where an object is
located relative to an edge in their receptive fields. We develop a recurrent neural model using border-ownership
selective (B) cells and grouping (G) cells whose activity represents proto-objects based on the integration of local
feature information. G cells send feedback connections to the B cells that caused their activation, which when
combined with local inhibitory competition, helps to determine figure-ground assignment. We evaluate our model
on the Berkeley Segmentation Dataset (BSDS-300), and achieve performance comparable to recent state-of-theart computer vision approaches. These approaches are computationally expensive due to their requirement for
training and typically provide only limited insight into the underlying functional mechanisms. In comparison, our
model (1) is biologically plausible, (2) is fully image computable, and (3) does not require any training.

II-21. Modeling within and across area neuronal variability in the visual system
Chengcheng Huang
Douglas Ruff
Marlene Cohen
Brent Doiron

CHENGCHENGHUANG 11@ GMAIL . COM
RUFFD @ PITT. EDU
MARLENERCOHEN @ GMAIL . COM
BDOIRON @ PITT. EDU

University of Pittsburgh
Shared variability among neurons (noise correlations) have been commonly observed in multiple cortical areas
(Cohen and Kohn, 2011). Moreover, noise correlations are modulated by cognitive factors, such as overall arousal,
task engagement and attention (Cohen and Maunsell, 2009; Doiron et al. 2016). While there is much discussion
about the consequences of noise correlations on neuronal coding, there is a general lack of understanding of
the circuit mechanisms that generate and modulate shared variability in the brain. Recently, simultaneous microelectrode array recordings from V1 and MT in behaving monkeys (Ruff and Cohen, 2016) suggest that attention
not only decreases correlations within a cortical area (MT), but also increases correlations between cortical ar-

COSYNE 2017

123

II-22 – II-23
eas (V1 and MT). The differential modulation of between-areas and within-area noise correlations impose further
constraints on circuit mechanisms for the generation and propagation of noise correlations. We develop a spiking
neuron network with spatiotemporal dynamics that internally generates shared variability matching the low dimensional structure widely reported across cortex. This variability results from macroscopic chaos in population rates,
which correlates neurons from the same recurrent network while decoupling them from feedforward inputs. Attention is modeled as depolarizing the inhibitory neuron population, which reduces the internally generated shared
variability and allows the network to better track input signal. Moreover, dimensionality analysis of spike trains in
both model and data shows that attention modulates only the first dimension of fluctuations, again consistent with
past phenomenological models of cortical data. Our model provides a much needed mechanism for how shared
variability is both generated and modulated in recurrent cortical networks.

II-22. Serotonergic SK channel modulation promotes adaptive optimized coding of natural stimuli
Chengjie Huang
Diana Martinez
Michael G Metzen
Maurice Chacron

CHENGJIE . HUANG @ MAIL . MCGILL . CA
DIANA . MARTINEZ 2@ MAIL . MCGILL . CA
MICHAEL . METZEN @ MCGILL . CA
MAURICE . CHACRON @ MCGILL . CA

McGill University
Growing evidence suggests that sensory systems optimally encode natural stimuli with given statistics. However,
because natural stimulus statistics change over time, coding mechanisms must be adaptive in order to be efficient.
Such adaptation is seen ubiquitously across systems and species. Indeed, neurons can adapt their response
properties to changes in relatively “simple” stimulus attributes such as mean and variance as well as “complex”
features such as changes in spectral properties. However, the underlying mechanisms are generally poorly
understood. Here we investigated adaptive optimized coding of electrosensory stimuli experienced by weaklyelectric fish in their natural environment. Previous results have shown that sensory pyramidal neurons optimally
encode natural sensory stimuli. We investigated whether such optimized coding was adaptive to changes in
stimulus statistics. To do so, we presented the animal with stimuli whose statistics differed from those typically
found under natural conditions. We found that, over the course of hours, neurons adapted their tuning to optimally
encode the new stimulus. This change in tuning was accompanied by changes in behavioral sensitivity that
shifted to more closely match the new stimulus statistics. Further experiments revealed that adaptive coding
requires the telencephalon as, post-lesions, both neural tuning and behavioral responses did not adapt when a
stimulus with different statistics is presented. Moreover, the descending signal required for adaptation reaches
pyramidal neurons through serotonergic fibers emanating from the raphe nuclei. This is because both neural
tuning and behavioral responses did not adapt after serotonergic antagonist application. Our results thus reveal
a novel functional role for the serotonergic system, which is very well conserved across vertebrates. Based
on previous results by our group, the action of 5-HT on tuning is likely through up and downregulation of SK
channels. Important parallels between the electrosensory and other systems imply that our results will very likely
be generally applicable.

II-23. Mapping brain-wide corticocortical projections at single-cell resolution
by barcoded RNA sequencing
Longwen Huang
Justus Kebschull
Anthony Zador

LHUANG @ CSHL . EDU
JKEBSCHU @ CSHL . EDU
ZADOR @ CSHL . EDU

Cold Spring Harbor Laboratory

124

COSYNE 2017

II-24 – II-25
Neurons transmit information to distant brain regions via long-range axonal connections. Conventional neuroanatomical methods are slow and lack single-neuron resolution. To overcome this, we (Kebschull et al, 2016)
recently introduced MAPseq (Muliplexed Analysis of Projections by sequencing), a method that allowed us to
determine the projections of hundreds of neurons in a single animal. MAPseq uses random sequences of
RNA—“barcodes”—to re-cast neuroanatomy into a form that allows us to exploit the tremendous gains in highthroughput DNA sequencing. Here we scale up MAPseq by two orders of magnitude, and apply it to determine the
all-to-all projections of more than 50,000 cortical neurons in a single mouse, at single neuron resolution. MAPseq
has the potential to usher in the era of high-throughput single neuron mapping.

II-24. Gradient based learning for spiking neural networks
Dongsung Huh
Terrence Sejnowski

DONGSUNGHUH @ GMAIL . COM
TERRY @ SALK . EDU

Salk Institute for Biological Studies
The principle behind spike-based computation is largely unknown even though most neurons in brain communicate via action potentials. This is in part due to the lack of methods for constructing spiking neural network in
top-down manner. Recently, several approaches have been proposed for configuring spiking neuron networks via
optimization. However, most methods exhibit a limited range of applicability: they offer analytic solutions for highly
specific tasks (e.g. auto-encoder), or only allow training the read-out weights while requiring the innate network
dynamics to be dominated by the external input drive. We present a method for approximating the gradient of
spiking neural networks which can be used to update/optimize the full network dynamics, including the feed-in,
the read-out and the recurrent connectivity weights. This allows spiking neural networks to learn general inputoutput tasks, where the input/output can be either spikes or continuous signals. The method can successfully train
spiking networks for tasks which no previous approaches have worked. These results would allow us to further
investigate the computational principles in spiking networks.

II-25. Probabilistic models for neural populations that naturally capture global
coupling and criticality
Jan Humplik
Gasper Tkacik

JHUMPLIK @ IST. AC. AT
GTKACIK @ IST. AC. AT

Institute of Science and Technology Austria
Advances in multi-unit recordings pave the way for statistical modeling of activity patterns in large neural populations. Several recent studies have shown that the summed activity of all neurons strongly shapes the population
response. A separate recent finding has been that the neural populations also exhibit criticality—an anomalously
large dynamic range for the probabilities of different population activity patterns. Motivated by this observation,
here we introduce a general method for constructing probabilistic models which take into account the prior knowledge that the modeled population could be globally coupled and close to critical. The proposed class of models
consists of an energy function which parametrizes interactions between small groups of neurons, and an arbitrary
positive, strictly decreasing, and twice differentiable function which maps the energy of a population pattern to its
probability. Both the parameters of the energy function, and the nonlinear map from energies to probabilities have
to be learned directly from data. We show that augmenting an energy function which models pairwise interactions
between neurons with a nonlinearity yields an accurate description of the activity of retinal ganglion cells which
outperforms previous models based on the summed activity of neurons. Furthermore we show that our models
admit an interpretation in terms of a continuous latent variable globally coupling the system whose distribution we
can infer from data. The presence of such latent variables was recently proposed to underlie criticality in toy models. Criticality has also been observed in many other datasets such as natural scenes or amino acid sequences

COSYNE 2017

125

II-26 – II-27
of proteins suggesting that our method has applications outside of neuroscience as well.

II-26. Deep learning in spiking LIF neurons
Eric Hunsberger
Chris Eliasmith

ERICHUNS @ GMAIL . COM
CELIASMITH @ UWATERLOO. CA

University of Waterloo
The “backprop” algorithm has led to incredible successes for machines on object recognition tasks (among others), but how similar types of supervised learning may occur in the brain remains unclear. We present a fully
spiking, biologically plausible supervised learning algo- rithm that extends the Feedback Alignment (FA) algorithm
to run in spiking LIF neurons. This entirely spiking learning algorithm is a novel hypothesis about how biological
systems may perform deep supervised learning. It addresses a number of the key problems with the biological
plausibility of the backprop algorithm: 1. It does not use the transpose weight matrix to propagate error backwards,
but rather uses a random weight matrix. 2. It does not use the derivative of the hidden unit activation function,
but rather uses a function of the hidden neurons’ filtered spiking outputs. We test this algorithm on a simple inputoutput function learning task with a two-hidden-layer deep network. The algorithm is able to learn at both hidden
layers, and performs much better than shallow learning. Future work includes extending this algorithm to more
challenging datasets, and comparing it with other candidate algorithms for more biologically plausible learning.

II-27. Rats decisions flexibly integrate sensory information and recent history
of outcomes
Alexandre Hyafil1
Ainhoa Hermoso-Mendizabal1
Pavel E Rueda-Orozco2
Santiago Jaramillo3
David Robbe4
Jaime De La Rocha1

ALEXANDRE . HYAFIL @ GMAIL . COM
AINHOAHEM @ GMAIL . COM
RUEDAP @ UNAM . MX
SJARA @ UOREGON . EDU
DAVID. ROBBE @ INSERM . FR
JROCHAV @ CLINIC. UB . ES

1 Idibaps
2 Instituto

de Neurobiologia, UNAM
of Oregon

3 University
4 INMED

Animal decisions not only reflect current sensory information but are also shaped by recent experience. There is
however little understanding about the determinants of these history-dependent decision biases. We used rats in
a novel two-alternative forced choice auditory discrimination task, in which the probability to repeat the previous
stimulus category was varied in blocks of trials. Rats adapted to this environment by developing a strategy that
capitalized on the serial correlations of the stimulus sequence: a bias towards repeating the same response built
up after correct repetitions, and conversely an alternation bias developed after correct alternations. A GLM analysis revealed that rats decisions in each trial relied on: (1) the current sensory stimulus; (2) a lateral bias towards
(away from) the side of recently rewarded (unrewarded) responses, i.e. win-stay-lose-switch strategy; (3) a novel
and strong transition bias that reinforces recent correct transitions (repetitions vs. alternations). Intriguingly and in
contrast to the lateral bias, the transition bias had no impact on choice after error trials. The value of the bias was
not reset but simply ignored after an error, and it was recovered after the first subsequent correct trial. Thus, the
weight of the history-dependent transition bias could be flexibly and transiently put aside after error choices when
possibly the reliability of the internal model was questioned. This nonlinear effect could not be captured by the
GLM so we built a latent generative model of rats decisions, whereby lateral and transitions biases are updated
at each trial and their influence on current decisions is modulated by an outcome-dependent confidence signal.

126

COSYNE 2017

II-28 – II-29
The model accounted quantitatively for all described behavioural effects. Overall, we show that history-dependent
biases in rodent perceptual choices reflect consistent strategic adaptations to behavioural outcomes.

II-28. Suppression-based visual coding and feature extraction of the deep
superior colliculus of the mouse
Shinya Ito
David Feldheim
Alan Litke

SHIXNYA @ GMAIL . COM
DFELDHEI @ UCSC. EDU
ALAN . LITKE @ CERN . CH

University of California, Santa Cruz
The superior colliculus (SC) is an integrative sensorimotor structure used to control multiple vision-dependent
behaviors. It is a laminated structure; the superficial SC layers (sSC) contain cells that exclusively respond to
visual stimuli via direct retinal input, while the deep SC layers (dSC) contain cells that respond to auditory and
somatosensory stimuli in addition to visual stimuli. While it is appreciated that the neurons in the sSC and dSC
have different inputs and outputs, detailed differences of their visual response properties remain unknown. Here
we used a custom-built, large-scale silicon probe recording system to examine the visual response properties
of neurons within the SC of head-fixed, awake, behaving mice. We find that cells in the dSC have three key
differences compared to those in the sSC: (1) The majority of the dSC orientation/direction selective (OS/DS)
cells have their firing rate suppressed by drifting sinusoidal gratings. (2) A smaller fraction of cells encode linear
contrast, and a smaller fraction of OS/DS cells respond to flashing spots in the dSC. (3) The dSC cells lack Y-like
nonlinear spatial summation properties, while a significant fraction of the sSC cells do have the properties. These
results provide the first description of cells in the brain that are suppressed by the orientation or direction of a
stimulus, show that neurons in the dSC have properties similar to complex cells in the visual cortex, and show
that direction/orientation, but not the nonlinear property of Y-cells, is relayed from the sSC to the dSC.

II-29. Extending Levelt’s propositions to perceptual multistability involving
interocular grouping
Alain Jacot-Guillarmod1
Yunjiao Wang2
Claudia Pedroza3
Haluk Ogmen4
Kresimir Josic5
Zachary P Kilpatrick6

ALJACOT @ GMAIL . COM
WANGYX @ TSU. EDU
CLAUDIA . PEDROZA @ UTH . TMC. EDU
OGMEN @ CENTRAL . UH . EDU
JOSIC @ MATH . UH . EDU
ZPKILPAT @ COLORADO. EDU

1 Lausanne

University Hospital
Southern University
3 University of Texas
4 University of Denver
5 University of Houston
6 University of Colorado
2 Texas

We are remarkably adept at interpreting ambiguous visual input. However, competing percepts are not always
disambiguated, and different interpretations of a percept can be perceived in alternation. Such multistable perceptual rivalry has been used extensively to probe visual awareness. Modeling studies have pointed to cortical
mechanisms that underlie binocular rivalry. Such models are frequently constrained by Levelt’s Propositions which
have also been a touchstone for experimental studies. However, it is unclear whether Levelt’s Propositions extend
to perceptual multistability with more than two percepts. And if so, do computational models used to explain binocular rivalry also generalize in a way that is consistent with the extended Levelt’s Propositions? To address this

COSYNE 2017

127

II-30 – II-31
question, we used a combined experimental and modeling approach: We presented human subjects with splitgrating stimuli with complementary halves of the same color (red or green). Subjects reported four percepts in
alternation: the two stimuli presented to each eye, and the two interocularly grouped, single color percepts. Most
subjects responded to increased color saturation by an increase in the predominance of grouped percepts (Levelt’s Proposition I). Increased predominance was due to a decrease in average dominance duration of single-eye
percepts, while that of grouped percepts remained largely unaffected, in accordance with an extension of Levelt’s
Proposition II. In agreement with Levelt’s Proposition III, the alternation rate increased as the difference in the
strength of the percepts decreased. To explain these observations, we introduce a hierarchical model consisting
of low-level neural populations, responding to input at a visual hemifield, and higher-level populations representing the percepts. The model exhibits the changes in dominance duration observed in the data, and conforms to
the extended Levelt’s Propositions. This hierarchical extension of previous models of binocular rivalry therefore
suggests plausible cortical mechanisms that underlie rivalry between interocularly grouped stimuli.

II-30. Thalamo-cortical computations underlying decision making, conflict
resolution, and confidence
Jorge Jaramillo
Xiao-Jing Wang

JJ 99@ NYU. EDU
XJWANG @ NYU. EDU

New York University
The thalamus serves as a bridge between the sensory world and the cortex. Once thought to be a passive relay,
the thalamus is now known to play an active role in many of the cognitive functions usually attributed to the cortex.
Indeed, there are thalamic nuclei that receive most of their input from cortical structures. Two major thalamocortical pathways can be distinguished: (1) a feed-forward pathway that includes cortico-thalamic projections that
are relayed to a second cortical area and (2) a feed-back pathway that includes cortico-thalamic projections that
are reciprocated to the same cortical area. What are the computational roles of these thalamo-cortical pathways?
In this computational study, we focused on the primate pulvinar, one of the most enigmatic thalamic nuclei. Classic
(e.g., Petersen, 1985) and recent (e.g., Komura et al., 2013) studies have shown that the pulvinar is involved in
visual attention and confidence, respectively. It is not known whether these seemingly distinct cognitive functions
emerge from the contributions of the feed-forward and feed-back thalamo-cortical pathways. To address this issue, we designed and implemented a three-module architecture that includes the pulvinar and two interconnected
cortical areas. We found that the pulvinar, by means of the feed-forward pathway, gates the effective connectivity between the two cortical modules. Importantly, this connectivity is subject to flexible control via top-down
modulation onto the pulvinar. We suggest that this feature is used by the brain to resolve conflicting responses
during visual attention tasks. Next, we showed that the feed-back pathway endowed with plastic cortico-thalamic
synapses can account for the representation of confidence in the parietal cortex (Kiani and Shadlen, 2009) and
in the dorsal pulvinar (Komura et al, 2013). Overall, our results suggest that that the engagement of feed-forward
and feed-back pathways enable the pulvinar to participate in computations associated with metacognition and
control.

II-31. A Bayesian learning rule for spiking neurons
Jannes Jegminat1,2
Jean-Pascal Pfister1,2
1 ETH

JANNES @ INI . UZH . CH
JPFISTER @ INI . UZH . CH

Zurich
of Zurich

2 University

Brain computation relies on stochastic hardware. In particular, synaptic transmission is stochastic across a
wide range of species and brain areas. That is, each stochastic synapse represents a distribution over EPSP-

128

COSYNE 2017

II-32 – II-33
amplitudes, henceforth called weights, rather than a fixed amplitude. Despite the omnipresence of stochastic
synaptic transmission, its role in computation is not well understood. Aitchison and Latham propose using Bayes
rule to optimally update the synaptic distribution. However, in deriving their learning rule they make the following
unnecessary assumptions: a fixed relation variance-to-mean ratio of the weight distribution, a linear gaussiannoise neuron model and the presence of an optimal synaptic weight that the synapse tries to adapt to. Further,
their learning rule does not naturally extend to continuous time and spiking networks. Here, starting again from
Bayes rule, we derive an alternative learning rule for weight distributions without these shortcomings. Notably, our
learning rule confirms Aitchison’s two main predictions in the new context of spiking, continuous-time networks:
The learning rate scales with the synaptic variance and the variance increases in the absence of data. Further,
we recover STDP and predict a input-rate dependent variance-to-mean ratio.

II-32. The SOM-PV circuit motif appears designed to scale pyramidal neuron
responses
Lei Jin1
Bardia F Behabadi2
Monika Jadi3
Bartlett W Mel

EDMUND 5719@ HOTMAIL . COM
BEHABADI @ USC. EDU
JADI @ SALK . EDU
MEL @ USC. EDU

1 University

of Southern California
Inc.
3 Salk Institute for Biological Studies
2 Qualcomm

New data relating to the properties of cortical circuits and neurons continues to accumulate rapidly, while our
understanding of the computations performed by those neurons and circuits remains primitive. We noted the
following constellation of findings regarding the effects of long-range horizontal axons (HAs): (1) HAs are biased
to terminate proximally on PN dendrites (conclusion based on EPSP rise times; see Schnepel et al. 2014); (2)
HAs selectively target SOM interneurons; and (3) SOM neurons inhibit distal dendrites of PNs and soma-targeting
PV cells, which should produce a shift from proximal to distal inhibition with increasing SOM activation. What does
this pattern of connections imply about the net modulatory effect of horizontal pathway activation on the responses
PN dendrites? We expect that: (1) proximal excitation will produce a threshold-lowering and gain-boosting of the
cells’ i-o curve; (2) suppression of somatic inhibition will compound this effect by causing additional thresholdlowering and gain-boosting; and (3) dendritic inhibition will counteract the two threshold-lowering effects, pushing
the threshold back higher again. We predict that the net result of these three influences is a nearly pure, highdynamic-range multiplicative boosting of the cell’s unmodulated i-o curve. To test this prediction, we ran detailed
compartmental simulations with excitatory and inhibitory effects as described above, and measured the effect
of increasing activation of the horizontal pathway. Our prediction was validated: compared to the family of i-o
curves without inhibition, which shows a “messy“ compound threshold-lowering/gain-boosting effect, the inclusion
of the SOM-PV inhibitory circuit motif led to nearly pure multiplicative PN response scaling. Our results suggest
that long-range horizontal connections in the cortex can readily produce clean, high-dynamic-range multiplicative
boosting of PN responses.

II-33. Neuronal ensemble dynamics in medial orbitofrontal cortex during cuereward associative learning
Vijay Mohan K Namboodiri
Jose Rodriguez Romaguera
Garret Stuber

VIJAY NAMBOODIRI @ MED. UNC. EDU
RODRIGUEZ . JM @ GMAIL . COM
GARRET STUBER @ MED. UNC. EDU

University of North Carolina at Chapel Hill

COSYNE 2017

129

II-34 – II-35
Orbitofrontal cortex (OFC) is thought to play a fundamental role in the learning and maintenance of cue-reward
associations. While these associations are essential for survival, they can also contribute to maladaptive actions
such as cue induced drug seeking or food craving. Thus, uncovering how ensembles of OFC neurons acquire and
maintain cue-reward associations may instruct our ability to understand and modify adaptive and maladaptive motivated behaviors. Using in vivo 2-photon calcium imaging through implanted GRIN lenses, here we monitored and
tracked the activity of the same ensemble of medial OFC (mOFC) neurons during learning and subsequent variations of a Pavlovian cue-reward trace conditioning task. By recording thousands of cells, we show that mOFC
responses tracked behavioral acquisition of cue reward associations. Interestingly, most neurons showed only
weak tuning of sensory responses even though trace interval responses increased dramatically during conditioning along with reward seeking behavior. After learning was established, we tested whether the same ensemble
representing initial learning was involved in trial-by-trial update of sensory responses based on trial reward history
after the probability of reward was reduced to 50%. Curiously, we found no correlation between initial learning
dynamics and trial-by-trial adaptation of sensory responses, suggesting that these two processes might not recruit shared ensembles. Furthermore, in a separate test, we found that sensory and trace interval responses
are negatively modulated by immediate reward history outside of the cue-reward pairing of interest, suggesting
a rapid adjustment of responses based on general reward history. Both of these findings suggest that individual
cells in mOFC likely do not update their responses in a manner fully consistent with temporal difference learning,
the most common mechanistic model of Pavlovian conditioning. Lastly, we found that ensemble responses during
reinstatement post-extinction are correlated with responses after initial learning, suggesting stable encoding of
cue-reward associations.

II-34. Sensory processing by deep networks with layer-localized learning
Jonathan Kadmon
Haim Sompolinsky

KADMONJ @ ME . COM
HAIM @ FIZ . HUJI . AC. IL

Hebrew University of Jerusalem
Many sensory pathways are organized in an hierarchical fashion, (e.g., vision, audition, olfaction and somatosensation). Current state-of-the-art machine learning architectures – inspired by these structures – preform very well
when trained on real-world data. Synaptic matrices are typically generated through back-propagation learning
algorithm with dubious biological plausibility. Thus, understanding the computational benefits of deep architectures for biological information processing remains an important challenge. We introduce and analyze a class of
Deep Networks, termed Layer-Localized Learning (L^3). Here, learning is unsupervised and furthermore, weights
in each stage depends on their pre- and post activation, independent of the ultimate task or a global objective
function. We apply this paradigm to processing of clustered signals. We study the ability of an afferent neuron
to generalize on a binary classification task. The propagation of the information across the layers is completely
determined by the signal-to-noise ratio (SNR) at each layer, which obeys a nonlinear iterative mapping, and determines the overall performance of the system as a function of load and sparsity. Importantly, we introduce a new
measure of performance of a given network architecture, which allows for the calculation of the optimal architecture given constraints on its total size (total number of neurons or synapses). We show that for a given total size
an optimal depth exists, obeying a power law scaling with the total system size.

II-35. Devaluation modulates representations of expected outcome identity in
orbitofrontal cortex
Thorsten Kahnt
James Howard

THORSTEN . KAHNT @ NORTHWESTERN . EDU
JAMES - HOWARD @ NORTHWESTERN . EDU

Northwestern University

130

COSYNE 2017

II-36 – II-37
Not all of our needs are satisfied at the same time, and thus, the ability to pursue unsatisfied goals when other
needs are met is fundamental for survival. This behavior requires independent representations of specific rewards
that can be updated on the fly (i.e., without learning) based on changes in the internal or external environment.
The orbitofrontal cortex (OFC) has been proposed to host such representations in the form of cognitive maps. In
principle, specific updates could be implemented either by changing representations of specific rewards directly
in OFC, or by changing the assignment of value to these rewards in downstream regions such as ventromedial
prefrontal cortex (vmPFC). To shed light on how representations of expected reward identity are differentially
updated in the human OFC, we utilized sensory-specific devaluation of appetizing food odors in combination
with a decision-making task and pattern-based neuroimaging. We find that after selective satiety, reward identity
representations in lateral OFC were diminished for the sated food odor, but retained for the non-sated counterpart.
In addition, identity general decision signals in the vmPFC were similarly maintained for the non-sated, but not for
the sated reward identity. We find that functional connectivity between the OFC and the vmPFC was modulated by
satiety such that connectivity was stronger for non-sated compared to sated odors after the meal. Moreover, these
connectivity changes were correlated with individual differences in satiety-related choice behavior, suggesting that
functional connectivity between these regions links specific reward representations to identity-general decision
signals. These findings demonstrate how state representations of specific rewards in the OFC are flexibly updated
by devaluation and linked to general decision values in the vmPFC that can guide adaptive behavior.

II-36. Coupling between attractor networks naturally generates a discrete grid
cell hierarchy
Louis Kang
Vijay Balasubramanian

LKANG @ MAIL . MED. UPENN . EDU
VIJAY @ PHYSICS . UPENN . EDU

University of Pennsylvania
The grid system of the mammalian medial entorhinal cortex (MEC) exhibits striking modularity. Rat grid cell
recordings reveal that grid spacings cluster around discrete values that form geometric sequences with scaling
ratios reported in the range 1.3–1.8. Grids with smaller spacings are enriched dorsally along the longitudinal axis
of the MEC, and those with larger spacings are enriched ventrally. The origin of this hierarchical organization
and the discrete clustering of grid spacings is unknown. Conventional attractor mechanisms for producing grid
cell responses use external excitatory inputs and local recurrent inhibition. The resultant grid scales are directly
proportional to the length scale of inhibition, so if the inhibition length varies smoothly along the dorsoventral axis,
the grid period would also vary continuously. Here, we use dynamical systems theory to show that excitatory
coupling between attractors at adjacent dorsoventral depths of the MEC will concentrate this variation of grid
spacing into a few discrete jumps. Moreover, our model explains the spacing ratio between successive modules
√ in
terms of a geometric relationship between commensurate triangular grids whose lattice constants scale by 3 ≈
1.7. The model offers predictions for neural connectivities and grid orientations that can be tested experimentally,
and is the first mechanism proposed for the self-organization of a discrete hierarchy of grid spacings.

II-37. Evidence that feedback is required for object identity inferences computed by the ventral stream
Kohitij Kar1
Jonas Kubilius1,2
Elias B Issa1
Kailyn Schmidt1
James DiCarlo1

KOHITIJ @ MIT. EDU
QBILIUS @ MIT. EDU
ISSA @ MIT. EDU
KAILYN @ MIT. EDU
DICARLO @ MIT. EDU

1 Massachusetts
2 Katholieke

Institute of Technology
Universiteit Leuven

COSYNE 2017

131

II-38

The primate ventral visual stream for object recognition contains prominent cortico-cortical feedback connections.
However, the most accurate models of online, rapid (<200 ms) inference in the ventral stream are largely feedforward (hierarchical convolutional neural networks, HCNN). Might the appropriate inclusion of feedback connections
in those models improve their explanatory power? We reasoned that, the impact of feedback connections would
be most easily revealed in neural population activity at the top of the ventral visual hierarchy (inferior temporal
cortex, IT), because the IT representation benefits from feedback connections along the entire hierarchy. Because prior work shows that linear decoders accurately model IT’s estimate of object identity, we could look for
a neural signature that would imply a computationally-critical role of feedback in online inference. Specifically,
we hypothesized that, for images that require feedback circuits to resolve objects, IT’s estimate of object identity
should emerge later (relative to other images). To discover such images, we behaviorally tested both synthetic
images and photographs to obtain two groups of images—those for which object identity is easily extracted by
the primate brain, but not solved by HCNNs (“challenge” images), and those that primates and models easily
solve (control images). We then recorded IT population activity in two monkeys while they performed core object
identity estimation (100 ms viewing) on each image (1360 images, 10 possible objects, randomly interleaved to
neutralize attention). We found that, in both monkeys, IT’s solution (linear decode) for challenge images took
approx. 20 msec longer to emerge than control images. This difference could not be explained by differences in
neural latency, firing rates, or low-level image properties such as contrast. These results imply the importance of
feedback in ventral stream object inference, and the image-by-image differences constrain the next generation of
ventral stream models.

II-38. Information socialtaxis: efficient collective foraging in groups of information seeking agents
Ehud Karpas
Adi Shklarsh
Elad Schneidman

UDIKARPAS @ GMAIL . COM
ADI . SHKLARSH @ GMAIL . COM
ELAD. SCHNEIDMAN @ WEIZMANN . AC. IL

Weizmann Institute of Science
Individual behavior, from biology to game theory, is often described in terms of balancing exploration and exploitation. Foraging has been a canonical setting for studying reward seeking and information gathering, as it
involves sensory perception, navigation, computation, and decision making. In addition to the obvious ethological
and behavioral aspects, the computational foundations of foraging have been explored theoretically and applied
in learning algorithms and optimization problems. Inspired by chemotaxis, the infotaxis algorithm showed that
locally maximizing expected information gain can lead to efficient and ethological individual foraging. While most
of the experimental and theoretical work on foraging has focused on individual behavior, it is clear that in nature conspecifics can be a valuable source of information about the environment. While the nature and role of
interactions between animals have been studied extensively, the design principles of information processing in
such groups are mostly unknown. We present a novel algorithm for group foraging, we term ‘socialtaxis’, that
unifies infotaxis and social interactions, in which each individual in the group simultaneously maximizes its own
sensory information and a social information term, similar to decorrelation and optimization ideas in theoretical
neuroscience. Surprisingly, we show that when individuals aim to increase their information diversity, efficient
collective behavior emerges in groups of opportunistic agents, which is comparable to the optimal group behavior.
Importantly, socialtaxis does not require parameter tuning, is highly robust, and biologically plausible in terms of
computational complexity and information sharing. We further show a highly efficient version in which agents do
not share any information, but instead just infer it from the behavior of others. Finally, we use the model to predict
distinct optimal couplings in groups of selfish and altruistic agents, reflecting how it can be naturally extended to
study social dynamics and collective computation in general settings.

132

COSYNE 2017

II-39 – II-40

II-39. The branched spline attractor: whole-brain dynamics inform a theory of
behavior sequence generation
Saul Kato

SAUL . KATO @ UCSF. EDU

University of California, San Francisco
A central dogma of systems neuroscience is that the dynamical activity of neurons produces behavior; however,
we lack a self-contained, data-driven theory of robust, flexible behavioral sequence generation. We recently discovered an explicit mapping between single-trial low-dimensional network dynamics and the behavioral command
state sequence in the nematode C. elegans. A close study of the dynamics of individual neurons in relation to
the low-dimensional signal in neural state space has yielded two key observations: neurons rapidly depolarize
and polarize in fixed, simply connected, regions of neural state space, and neurons remain depolarized or hyperpolarized in the exclusion of these regions. These observations have suggested a theory of robust, flexible
sequence generation, comprising 4 statements: (1) the sequence of behaviors is determined by the time evolution
of the low-dimensional dynamics of a broad set of neurons, as defined by a small set of linear projections; (2)
the dynamics are governed by a global attracting set defined as a closed, branched spline – a one-dimensional,
piecewise curved trajectory with branch and merge points; (3) the current location in low-dimensional neural state
space sets the current behavioral command state via joint activation and inhibition of fixed downstream projections; (4) acute selection of alternate branches is determined by recent sensory input near saddle points of the
dynamics. We present here a system of differential equations and construction procedure that implements the
theory. We also present experimental tests of the theory using whole brain imaging under transient perturbation.

II-40. Neuron-level resolution of activity boundaries using calcium imaging
Matthew Kaufman1
Simon Musall1
Daniel Barabasi2
Anne Churchland1
1 Cold

MKAUFMAN @ CSHL . EDU
SMUSALL @ CSHL . EDU
DBARABA 1@ ND. EDU
CHURCHLAND @ CSHL . EDU

Spring Harbor Laboratory
of Notre Dame

2 University

Mammalian cortex is divided into specialized areas. Although cortex is a continuous sheet, different areas are
thought to perform distinct functions, receive specific inputs, make specific outputs, and exhibit diverse molecular
markers. Recent work at the 1mm-scale has suggested that pairwise correlations in the activity of neurons may
reflect area borders (Kiani et al., 2015). Here, we identified area borders at cellular resolution in the spatial
pattern of correlations using two-photon calcium imaging of awake mice. We imaged the spontaneous activity of
≈ 500 neurons at a time across visual and somatosensory cortex in GCaMP6f transgenic animals. We examined
the neural correlation patterns using machine-learning methods, including a novel, iteratively corrected variant of
spectral clustering. This approach revealed two kinds of structure in the physical layout of neural correlations.
First, as expected, there was a smooth falloff of correlation with physical distance. Second, more surprisingly,
there were clearly-identifiable discontinuities in the pattern of correlations. These boundaries were consistent
across several imaging depths and repeated imaging over days. Boundaries were mostly clear-cut, with little
intermingling of cells or gradients at the transitions. The locations of these boundaries were often consistent with
area borders assessed using visual or somatosensory stimuli during wide-field calcium imaging. These results
demonstrate that correlations in spontaneous neural activity can be used to identify sharp boundaries at cellular
resolution in vivo. Any such structure is likely to have an impact on our understanding of the parcellation of
computation in cortex. In addition, our method does not require specific stimuli to assess correlation between
neurons and may thus be particularly useful to identify higher-order brain structures with more complex neuronal
tuning properties.

COSYNE 2017

133

II-41 – II-42

II-41. Boundary-tethered grid shifts reproduce effects of environment deformations on grid and place cells
Alex Keinath
Russell Epstein
Vijay Balasubramanian

ATKEINATH @ GMAIL . COM
EPSTEIN @ PSYCH . UPENN . EDU
VIJAY @ PHYSICS . UPENN . EDU

University of Pennsylvania
Convergent evidence demonstrates that navigational boundaries play a key role in shaping spatial representations
in the hippocampal formation. When a familiar environment’s boundaries are stretched or compressed, largescale grid cell firing patterns distort, while small-scale grid patterns do not. Place fields exhibit a mixture of
stretching, bifurcation, modulation by movement direction, and inhibition after environmental stretching, and a
mixture of duplication, inhibition, and perseverance after boundary insertion. When a familiar linear track is
compressed, place representations update when the bounding endpoint is encountered. Here we propose a single
mechanism to account for these diverse effects, in which boundaries influence grid and place representations
through excitatory projections from border cells. This border input induces discrete shifts in the spatial phase
of grid cells when a displaced boundary is encountered. To test this boundary-tethered grid shift mechanism,
we developed a computational model where connections from border to grid and grid to place cells develop
via Hebbian learning. In this model, large-scale grid fields distort when a familiar environment is stretched or
compressed, while small-scale fields do not. Place fields exhibit a mixture of stretching, bifurcation, modulation
by movement direction, and inhibition when a familiar environment is stretched; a mixture of duplication, inhibition,
and perseverance when a boundary is inserted; and boundary-tethered updating when a familiar linear track is
compressed. These results demonstrate for the first time that a unified mechanism can produce the diverse
changes to grid and place representations observed during environmental deformations. This boundary-tethered
grid shift mechanism challenges the intuition that environmental deformations induce distortion in the grid and
place representations. Rather, this mechanism suggests that distortions observed in time-averaged grid and
place activity may be an artifact of averaging over paths originating from different boundary. Aligned by the most
recent boundary contact, grid and place representations should remain stable.

II-42. Robustness to real-world background noise increases from primary to
non-primary auditory cortex
Alexander Kell
Josh McDermott

ALEXKELL @ MIT. EDU
JHM @ MIT. EDU

Massachusetts Institute of Technology
The sounds we seek to understand are often embedded in auditory scenes, and successful recognition critically
depends on being robust to “background noise”. However, little is known about how auditory cortex achieves
noise robustness, particularly with realistic sources of noise. To assess noise robustness of auditory cortical representations, we measured fMRI responses in human listeners to a diverse set of thirty natural sounds. Sounds
were presented in quiet as well as embedded in thirty everyday background noises, which were selected to have
stable statistics over time (examples: a bustling coffee shop, crickets chirping, rain hitting pavement). To quantify
the extent to which neural responses were robust to the real-world “noise”, we took advantage of the fact that a
voxel’s response typically varies across natural sounds, and we simply correlated each voxel’s response to the
natural sounds in isolation with its response to those same natural sounds when superimposed on background
noise. Responses in primary auditory regions (TE 1.1 and 1.0; defined anatomically) were substantially affected
by background noise (r2 ∼ 0.40). However, noise-robustness increased with distance from these primary areas:
nearby non-primary areas were slightly more robust, while more distal areas were hardly affected by the background noises (r2 ∼ 0.85). Overall response magnitudes in primary and non-primary regions were both slightly
lower in the presence of background noise, indicating that the differences in robustness between regions was
not due to the background noises differentially suppressing responses in primary areas. Our results illustrate

134

COSYNE 2017

II-43 – II-44
the neural basis of a core aspect of real-world listening, and offer evidence for hierarchical processing in human
auditory cortex.

II-43. Olfactory perceptual space and predicting olfactory percepts from molecular structure
Daniel Kepple
Alexei Koulakov

DKEPPLE @ CSHL . EDU
AKULA @ CSHL . EDU

Cold Spring Harbor Laboratory
Given the structure of a novel molecule, there is still no one who can reliably predict what odor percept that
molecule will evoke. The challenge comes both from the difficulty in quantitatively characterizing molecular structure, and the inadequacy of language to fully characterize olfactory perception. Here, we present a novel approach
to both problems. First, we avoid explicit characterization of molecular structure by using a similarity score for each
molecular pair, derived from comparing the molecular structures directly. We show that this method improves on
conventional predictions and need not rely on preexisting knowledge of chemical descriptors. Second, we generate a perceptual space, in which a molecule’s location defines its percept. We show that from a molecule’s
neighbors in this space alone, we are able to reproduce all perceptual descriptors of that molecule. We propose
that predicting olfactory percept from structure can be rethought of as predicting a molecule’s location in this
perceptual space. This provides a framework for understanding and predicting human smell percepts.

II-44. Distinct neural encoding schemes emerge for actions generated by the
same effector
Preeya Khanna1
Vivek Athalye2,1
Rui M Costa2
Jose M Carmena1
1 University

PKHANNA @ BERKELEY. EDU
VIVEKA @ EECS . BERKELEY. EDU
RUIMCOSTA @ GMAIL . COM
JCARMENA @ BERKELEY. EDU

of California, Berkeley
Neuroscience Programme

2 Champalimaud

Evidence from reaching tasks suggests motor cortical cells exhibit different neural activity patterns for the same
action depending on what actions precede or follow1. How does a population differently encode the same set of
sub-actions depending on how they are ordered in a full action? To directly test how single cell tuning and cell
ensemble co-modulations change when asking a subject to make different movements composed of the same
sub-actions, 1 rhesus macaque uses a brain-machine interface to neurally control a cursor in a centerout task
and a centerout obstacle avoidance task. Both tasks require generating neural commands to move a velocitycontrolled 2D cursor using the same decoder. We analyze tuning changes across the two tasks and find that
many significantly tuned neurons exhibit task dependent changes in preferred direction. Further, we analyze
the underlying neural correlations during each task by using Factor Analysis to decompose the neural activity
during neuroprosthetic control into correlated (shared) and uncorrelated (private) sources. In both tasks, the
shared neural activity, which resides in a subspace of high dimensional neural activity space, captures a large
component of neural variance needed to accomplish the neuroprosthetic task, demonstrated by only inputting
shared variance to the decoder during online control, and seeing equal performance. We compare the alignment
of the centerout shared subspace and obstacle shared subspace and find a low alignment compared to expected
alignment bootstrapped from within-task alignment. Further, centerout task performance decreases when the
decoder input is neural activity from the obstacle task shared space. Our findings suggest that the stable tuning
and low-dimensional shared space found in neuroprosthetic control is specific to sequences of neural commands

COSYNE 2017

135

II-45 – II-46
for the task. Performing different sequences involves changes in neural correlations and neural tuning despite
using the same effector and drawing from the same set of velocity commands.

II-45. Neural discrimination of sound category utilizes high-order sound statistics in the central auditory
Fatemeh Khatami
Mina Sadeghi
Heather Read
Ian Stevenson
Monty Escabi

KHATAMI @ ENGR . UCONN . EDU
MINA . SADEGHI NAJAFABADI @ UCONN . EDU
HEATHER . READ @ UCONN . EDU
IAN . STEVENSON @ UCONN . EDU
MONTY. ESCABI @ UCONN . EDU

University of Connecticut
Barlow and others have suggested that organisms and the neural networks that underlie sensory perception are
optimized to capture statistical regularities in sensory scenes. Accordingly, high-order statistical regularities in
natural sounds such as water and fire are critical for perceptually discriminating and categorizing sounds (McDermott and Simoncelli, 2011; Geffen et al., 2011). Though neural responses in the central auditory system vary
with modulation and correlation sound statistics (Escabi and Schreiner, 2002, Rodriguez et al., 2012) it remains
unclear whether sensitivity to high-order statistics could be used to discriminate sound category and how this
might be instantiated. Here we characterize single neuron activity central auditory neurons in the inferior colliculus of awake rabbits in response to an ensemble of sound textures including water, fire and speech babble. We
synthetically manipulate each sound by selectively adding or removing high-order statistics using the synthesis
algorithm developed by McDermott and Simoncelli (2011). First, we find neural response statistics including spike
timing precision and firing reliability change systematically with the sound statistics. Second, we find correlated
spiking across pairs of neurons varies with sound category and its synthetically manipulated statistics. Using neurometric and ideal observer analyses we demonstrate that neural response statistics can be used to discriminate
sounds. Systematic removal of the high-order sound statistics decreases the neural-based sound classification
performance. Conversely, systematic increase in the number of neurons used increases neural-based sound
classification performance. This study indicates that neural response statistics in the inferior colliculus have the
capacity to capture statistical regularities in sounds that are critical for sound categorization. These findings are
significant as they support the concept that statistical regularities are major drivers of sensory systems in general. Moreover, our findings support the concept that mammalian systems have the capacity to optimize sensory
categorization through correlated activity in neuron populations.

II-46. Metaplasticity for a changing world
Peyman Khorsand
Alireza Soltani

PEYMAN @ DARTMOUTH . EDU
ALIREZA . SOLTANI @ DARTMOUTH . EDU

Dartmouth College
Learning from reward feedback in a changing environment requires a high degree of adaptability, but precise
estimation of reward information demands slow updates. This tradeoff, which we refer to as the adaptabilityprecision tradeoff (APT), can be easily demonstrated in the framework of reinforcement learning; larger learning
rates result in higher adaptability but lower precision, and smaller learning rates give rise to lower adaptability but
higher precision. Here, we show that the APT can be substantially overcome via reward-dependent metaplasticity,
or synaptic changes that do not always alter synaptic efficacy. To study the relationship between adaptability and
precision, we considered a general problem of estimating reward probability from a stream of binary outcomes
(reward, no reward). We examined a general class of metaplastic models to find superior metaplastic models
for mitigating the APT. Using these superior models we then identified features that are beneficial for learning

136

COSYNE 2017

II-47 – II-48
under uncertainty and mitigating the APT. We found that metaplastic synapses can achieve better adaptability
and precision by forming two separate sets of meta-states: reservoirs and buffers. Synapses in reservoir metastates do not change their efficacy upon reward feedback whereas those in buffer meta-states can change their
efficacy. In superior models, rapid changes in efficacy are limited to synapses occupying buffers, providing a
bottleneck that reduces noise without significantly decreasing adaptability. In contrast, more-populated reservoirs
can generate a strong signal without manifesting any observable plasticity. We suggest that ubiquitous unreliability
of synaptic changes reflect metaplasticity that can provide a robust mechanism for adaptive learning.

II-47. Visual response similarity predicts the strength of PV-pyramidal neuron
connections in mouse V1
Meanhwan Kim1
Petr Znamenskiy1
Maria Florencia Iacaruso2
Sonja Hofer1
Thomas Mrsic-Flogel1

MEANHWAN . KIM @ UNIBAS . CH
PETER . CSHL @ GMAIL . COM
FLORENCIA . IACARUSO @ GMAIL . COM
SONJA . HOFER @ UNIBAS . CH
THOMAS . MRSIC - FLOGEL @ UNIBAS . CH

1 University
2 Oxford

of Basel
University

A mechanistic understanding of the origins of functional properties of neuronal networks requires knowledge of the
organization of their synaptic connections. In mouse primary visual cortex (V1), excitatory connections between
pyramidal neurons are sparse and exist primarily between cells with similar selectivity for visual stimuli (Ko et al.,
2011, Cossell et al., 2015). On the other hand, connectivity between pyramidal neurons and parvalbumin-positive
(PV) inhibitory interneurons is dense; PV neurons receive input and provide inhibition to the majority of nearby
pyramidal cells independent of their functional properties (Hofer et al., 2011). However, it is not clear how the
strength of these connections relates to the response properties of the pre- and post-synaptic neurons. To address
this question, we used a combination of two-photon calcium imaging in vivo and multiple whole-cell patch-clamp
recordings in vitro in L2/3 of mouse V1 to characterize the connectivity of functionally profiled PV interneurons.
We observed that the correlation of visual responses predicted the strength of PV interneuron/pyramidal neuron
connections, with PV neurons both providing more inhibition to and receiving more excitation from pyramidal cells
with similar responses. These results demonstrate how excitation-inhibition balance in L2/3 is achieved at the
level of individual pairs of neurons.

II-48. Human brain networks that dynamically encode mood
Lowry Kirkby
Francisco Luongo
Mor Nahum
Tom Van Vleet
Morgan Lee
Heather Dawes
Edward Chang
Vikaas Sohal

LOWRY. KIRKBY @ UCSF. EDU
FLUONGO @ GMAIL . COM
MOR . NAHUM @ POSITSCIENCE . COM
TOM . VANVLEET @ POSITSCIENCE . COM
MORGAN . LEE @ UCSF. EDU
HEATHER . DAWES @ UCSF. EDU
EDWARD. CHANG @ UCSF. EDU
VIKAAS . SOHAL @ UCSF. EDU

University of California, San Francisco
Understanding how internally-generated neural network dynamics lead to human cognition, and how these are
disrupted in neuropathologies, are among the most important challenges in neuroscience. Here, we combine
large-scale, chronic electrocorticography (ECoG) recordings from the human limbic system with advanced statistical and machine learning techniques to determine how neural activity patterns dynamically encode mood.

COSYNE 2017

137

II-49
These data are collected as part of the SUBNETS Project (Systems-Based Neurotechnology for Emerging Therapies)—a collaborative project studying circuit function and dysfunction in neuropsychiatric disorders. Recordings
were performed continuously at 1kHz sampling rate over a multi-day hospitalization period, from approximately
100 electrodes distributed across the limbic system, including the amygdala, hippocampus and cingulate cortex.
Patients’ psychological states were monitored at regular intervals during this period. We first used an unsupervised approach to reveal the fundamental, intrinsic subnetworks within the associated brain regions. We used
dimensionality reduction and matrix decomposition methods on time-series of coherence matrices to derive statistically significant, independent components, which we defined as intrinsic coherence networks (ICNs). We then
used a supervised approach to determine how activity patterns within ICNs related to a patient’s psychological
state. For each subject, we found that neural activity within a single ICN in the beta-frequency range consistently
correlated with mood. We demonstrated that across subjects, this mood-dependent ICN was dominated by an
interaction between electrodes located in the amygdala and hippocampus, where transient periods of high coherence between these regions were correlated with worse mood. These findings advance our understanding of
the neural circuits that underlie mood regulation, and have important implications for understanding the pathophysiology of, and possible novel forms of therapeutic intervention for, mood disorders, such as depression and
anxiety.

II-49. Flexible information processing on top of dynamical reference states in
neuronal networks
Christoph Kirst

CKIRST @ ROCKEFELLER . EDU

Rockefeller University
Flexible function is essential for the brain to cope with varying environments, changing quality of sensory information as well as context dependency. This requires organized communication and flexible information processing
among the different sub-circuits in neuronal networks. While network oscillations and in particular coherent dynamics between different sub-networks have been shown to change correlation structures, precise mechanisms
for the self-regulated processing of information in brains are, however, not well understood. Here we propose that
neuronal network activity has two separate components: a collective reference state on top of which information
is encoded and distributed in deviations form this reference, similarly to how radio signals broadcast information
via frequency or amplitude modulation. In networks, switching between dynamical reference states then enables
fast and flexible rerouting of information. In particular, for coupled oscillator networks we analytically show how
the physical network structure and the dynamical reference state co-act in order to generate a specific information
routing pattern, as quantified by transfer entropy. In modular networks, we find that local changes within a subnetwork, e.g. as a result of local processing, are capable of influencing the network’s global reference dynamics
and thereby can actively control the network-wide distribution of information. This in turn influences the local
processing. Thus, in this loop, the network while performing a function is also capable in continuously updating its
function in a dynamic way. We numerically show that this mechanism for self-organized information processing
naturally enables context dependent pattern-recognition in an oscillatory Hopfield network. Finally, our approach
of separating reference states from information carrying dynamics opens novel directions for the analysis of neuronal population recordings. We show that identifying and removing reoccurring dynamical reference motives in
neuronal network activity strongly impacts the inferred communication structure and apply those techniques to
whole brain Calcium recordings of larval Zebrafish.

138

COSYNE 2017

II-50 – II-51

II-50. Dynamic modulation of spike probability and precision in feed-forward
hippocampal circuits
Vitaly Klyachko
Sarah Wahlstrom Helgren

KLYACHKO @ WUSTL . EDU
WAHLSTROMHELGRENS @ WUSTL . EDU

Washington University
Feed-forward inhibitory (FFI) circuits are important for many information-processing functions. FFI circuit operations critically depend on the balance and timing between the excitatory and inhibitory components, which undergo rapid dynamic changes during neural activity due to short-term plasticity (STP) of both components. How
dynamic changes in excitation/inhibition (E/I) balance during spike trains influence FFI circuit operations remains
poorly understood. Here we examined the role of STP in the FFI circuit functions in the mouse hippocampus.
Using a coincidence detection paradigm with simultaneous activation of two Schaffer collateral inputs, we found
that the spiking probability in the target CA1 neuron was increased while spike precision concomitantly decreased
during high-frequency bursts compared to a single spike. Blocking inhibitory synaptic transmission revealed that
dynamics of inhibition predominately modulates the spike precision but not the changes in spiking probability,
while the latter is modulated by the dynamics of excitation. Further analyses combining whole-cell recordings and
simulations of the FFI circuit suggested that dynamics of the inhibitory circuit component may influence spiking
behavior during bursts by broadening the width of excitatory postsynaptic responses and that the strength of this
modulation depends on the basal E/I ratio. We verified these predictions using a mouse model of Fragile X Syndrome, which has an elevated E/I ratio, and found a strongly reduced modulation of postsynaptic response width
during bursts. Our results suggest that changes in the dynamics of excitatory and inhibitory circuit components
due to STP play important yet distinct roles in modulating the properties of FFI circuits.

II-51. Efficient spatiotemporal assessment of naturalistic visual selection behavior
Jonas Knoell1
Jonathan Pillow2
Alex Huk1
1 University
2 Princeton

JONAS . KNOELL @ UTEXAS . EDU
PILLOW @ PRINCETON . EDU
HUK @ UTEXAS . EDU

of Texas at Austin
University

Everyday life requires dynamic matching of behavior to the tasks at hand. Depending on the task, very different
parts of information in the visual field should be used. This is often studied within attentional paradigms with
a single discrete response for each trial. Although much has been learned within this classical paradigm, such
tasks require significant training and do not match natural task-dependent behavior, where both the goal and
the distribution of information can change continuously. We therefore developed a novel paradigm, which allows
quantification of the task-dependent temporal and spatial parameters that drive behavior in a continuous ocular
tracking task, and which is also suitable for characterizing motion-selective neurons. The stimulus consisted of
moving dots creating a large field optic flow. The focus of expansion (FOE) of the flow followed a random walk.
To characterize spatial integration, we divided the field into hexagonal subfields. Each either had a small, random
perturbation from the motion associated with the “true” FOE (or was blank). Perturbations of the subfields were
periodically resampled. Humans, macaques, and marmosets intuitively tracked the FOE with little to no training,
and we used ridge regression to determine the spatiotemporal parameters that best predicted the subject’s gaze.
To study selection, half of the subfields were consistent with the motion of a second, independent FOE, sharing
the same statistics with the other. Subjects could still track the FOE of one flow field with similar integration
profiles. We used the tracking kernels of single FOE conditions to estimate which task was being solved and
split the data based on tracked versus untracked FOE, and extended the single-FOE analyses to characterize
the spatiotemporal selection of one FOE over the other. The paradigm can quantify the spatiotemporal profiles of
motion integration in a variety of behavioral contexts, without extensive training.

COSYNE 2017

139

II-52 – II-53

II-52. Facilitation of pattern and contour selectivity by excitatory intra-cortical
circuits
Erin Koch
Jianzhong Jin
Jose Manuel Alonso
Qasim Zaidi

EKOCH @ SUNYOPT. EDU
JJIN @ SUNYOPT. EDU
JALONSO @ SUNYOPT. EDU
QZ @ SUNYOPT. EDU

State University of New York
We distinguish shapes by extended contours and surfaces by patterns and textures of multiple orientations and
scales. We are able to discriminate between multi-orientation patterns, despite retinal response compression
leading to cross-orientation suppression, suggesting that intra-cortical circuits may facilitate the pattern responses
shown by macaque extra-striate neurons. In striate cortex of primates and carnivores, orientation preference is
arranged as iso-orientation domains that radiate circularly from pinwheel centers. Whether this orientation map
has a function is currently debated, because many mammals, such as rodents, do not have such maps. Using tangential penetrations with multi-electrode arrays, we found that orientation tuning is narrower, and contrast
saturation and cross-orientation suppression stronger, within iso-orientation domains than at pinwheel centers.
We show that these differences develop due to excitation (not normalization) from neighboring oriented neurons.
As a result of these local intra-cortical computations, narrower tuning, greater cross-orientation suppression and
higher contrast gain of iso-orientation cells, lead to extraction of object contours from images; whereas broader
tuning, greater linearity and less suppression of pinwheel cells, generate selectivity for surface patterns and textures. We contrast these model predictions with those from standard divisive normalization based on responses
of neighborhood cells, as opposed to an arbitrary normalization pool that tiles the orientation domain. Based on
the orientation preferences of neighboring cells, local response normalization would incorrectly predict broader
orientation tuning and stronger responses to multi-orientation stimuli in iso-orientation domains than in pinwheels.
Based on the model of local intra-cortical excitation, we do an image processing simulation to show that downstream regions which pool the output of neurons in iso-orientation domains would be well suited to respond to
contours and edges, whereas downstream regions that pool outputs from pinwheel domains would be well suited
to respond to surface patterns and textures consisting of multiple orientations.

II-53. How neuronal and synaptic nonlinearities determine the stimulus susceptibility of cortical networks
Sara Konrad
Tatjana Tchumatchenko

SARA . KONRAD @ BRAIN . MPG . DE
TATJANA . TCHUMATCHENKO @ BRAIN . MPG . DE

Max Planck Institute for Brain Research
Nonlinear responses in sensory cortices that are caused by the integration of multiple stimuli are essential for
complex computations in the brain. Even when the presented stimuli are one dimensional in the sensory feature
space, neuronal responses are nonlinear and show features like small signal amplification, normalization or multistability. How are these operations implemented in recurrent cortical circuits by cellular and synaptic properties?
The inhibition stabilized supralinear network (ISN) with static synapses has been shown to reproduce a variety of
observed features such as sublinear and supralinear response, and surround suppression. Dynamical synapses
have also been shown to evoke nonlinearities as for example bistability, induced by short-term plasticity. We
start by introducing the network susceptibility as a quantitative measure for the degree of linearity of the network
response function. We present a novel analytical framework to quantify how both activity-dependent synaptic
transmission and nonlinear neural transfer functions determine the susceptibilities observed in cortical circuits.
Providing a parametrization for the activity-dependent synaptic efficacy that is applicable to any common activitydependent plasticity rule, we quantify how this model-independent parameter affects the network susceptibility.
For weakly balanced networks the network susceptibility can be separated into two terms where the first arises
only from synaptic nonlinearities whereas the other captures the neuronal nonlinearities. In the limit of perfect

140

COSYNE 2017

II-54 – II-55
excitation-inhibition balance, the network susceptibility is determined completely by the synaptic nonlinearities
and independent of the neural transfer function. Additionally, we demonstrate that the network susceptibility for
the same input is qualitatively different in distinct brain states when bistability occurs. We show that our theoretical findings are in agreement with numerical simulations of spiking neural networks with synaptic plasticity.
Our results give detailed insights to the up to now missing link between synaptic and cellular properties and the
response features of cortical networks.

II-54. The stabilized supralinear network supports bistable, oscillatory and
persistent activity
Nataliya Kraynyukova
Tatjana Tchumatchenko

NATALIYA . KRAYNYUKOVA @ BRAIN . MPG . DE
TATJANA . TCHUMATCHENKO @ BRAIN . MPG . DE

Max Planck Institute for Brain Research
The cortical circuits are able to perform many canonical computations such as normalization, contrast invariance,
non-linear stimulus summation or memory storage and can exhibit a variety of activity states ranging from asynchronous irregular states to oscillations. The computational substrate to perform these functions is thought to
be generated by non-linear interactions in recurrently connected networks of excitatory and inhibitory neurons.
Therefore, it is plausible to assume that a single mathematical model describing these networks could reproduce
many of these canonical computations and activity states. Recently a rate model based on power law activation
of individual neurons (Persi et al. 2011, Ahmadian et al. 2013) has been put forward and has been shown to
be consistent with numerous experimental observations (Rubin et al. 2015). In its simplest form this stabilized
supralinear network model (SSN) is a set of two coupled non-linear equations that describe the activity of excitatory and inhibitory populations. Even though this model is one of the simplest non-linear neuronal network
models, it turns out few methods are available to provide exact solutions for steady states, to study their multiplicity or systematically predict for all connectivity matrices what type of steady state to expect. Here, we present a
method that allows to map the complex, two dimensional steady states of this model to the zero crossings of a
one dimensional characteristic function. This new method allowed us to derive a number of new computational
insights. First, we have proven that the SSN model can undergo a Hopf bifurcation and lead to stable oscillatory
attractors. Second, at most two stable steady states can coexist in this model and provide a substrate for working
memory. Third, we have outlined connectivity and time scale regimes when the SSN leads to the emergence of a
persistent state.

II-55. Dynamics of partially structured recurrent networks with bidirectional
correlations
Alexander Kuczala1,2
Tatyana Sharpee1
1 Salk

AKUCZALA @ UCSD. EDU
SHARPEE @ SNL . SALK . EDU

Institute for Biological Studies
of California, San Diego

2 University

Random matrices serve as a useful tool for analyzing the stability and dynamics of generic neural networks.
In particular, the spectrum of the connectivity matrix determines the stability of the network’s linear dynamics
and onset of chaos of the nonlinear dynamics. Knowledge of the onset of chaos helps determine the network’s
computational capabilities and response to inputs. We study the impact of plasticity, which induces correlations
between forward and reverse connections. In neural circuits, the strength of these correlations may depend on,
for example, the neuronal type or distance between neurons. We develop a technique to efficiently compute the
eigenvalue spectrum of random connectivity matrices with plasticity-induced correlations. We find that, depending

COSYNE 2017

141

II-56 – II-57
on the structure of the correlations, plasticity can either increase or decrease network stability. Our results make
it possible to evaluate the impact of specific local plasticity rules on global network activity.

II-56. Low-dimensional geometry of stimuli shapes the information content of
a neural code
Alex Kunin
Vladimir Itskov

ABK 170@ PSU. EDU
VLADIMIR . ITSKOV @ PSU. EDU

Pennsylvania State University
The seminal paper (Schneidman et. al. 2006, Nature) demonstrated that pairwise correlation captured most
of the information-theoretic content of network states in vertebrate retina. It has been shown that a secondorder maximum-entropy model captured upwards of 95% of the difference between the independent firing model
and the true distribution of the data. This result led to various hypotheses that this observation betrays a general
principle of neural information processing and/or network organization. We suggest that there is an alternative and
more elementary explanation: the observed information-theoretic properties of the neuronal activity are heavily
influenced by the geometry of the underlying receptive fields that induce the pairwise correlations. To test our
hypothesis, we investigated a model neuronal population, where each neuron has a convex receptive field in a
d-dimensional space, and does not have any network interactions with the others. The receptive fields alone
induce a pattern of pairwise correlations that we used to investigate the information content as a function of the
dimension of the stimulus space. We found that the original result of Schneidman et. al., as well as many others,
is well-reproduced in this neuronal population as long as the dimension d of the stimulus space is relatively small.
We also found that for large-enough dimension d of represented stimuli the information content is no longer wellexplained by the pairwise correlations. Furthermore, we found that there is a simple mathematical explanation of
our results: Helly’s theorem restricts the possible intersection patterns of convex receptive fields and that naturally
translates into the result that the higher-order correlations can be mostly explained by the pairwise correlations.
We suggest that information-theoretic analysis of neural codes may shed light on the dimension of the represented
stimuli space of the considered neuronal population.

II-57. A complete map of M1 population representations during posture and
reaching
Hagai Lalazar1
LF Abbott1
Eilon Vaadia2

HAGAI LALAZAR @ YAHOO. COM
LFA 2103@ COLUMBIA . EDU
EILON . VAADIA @ ELSC. HUJI . AC. IL

1 Columbia
2 Hebrew

University
University of Jerusalem

High-dimensional neuronal recordings are often analyzed using dimensionality reduction to extract some salient
structure correlated with task parameters. Inevitably, such structure accounts for only a portion of the total
variance, leaving the nature of the remaining dimensions unknown. We recorded the same M1 neurons while
monkeys made cursor movements in different contexts: arm or Brain-Machine Interface control. Using modern
dimensionality-reduction techniques, we obtained the first complete “map” of all representations in M1 population
activity during posture and reaching movements, accounting for essentially all the data variance. M1 neurons
were recorded while monkeys performed a target-to-target reach and hold task in blocks of arm or BMI control.
In our task, population activity could depend on the task parameters in a complicated, nonlinearly mixed, way.
In contrast, we found that population activity parcellates into an arm-only subspace and an orthogonal subspace
shared between arm and BMI control. Moreover, each subspace further demixed into dimensions that explain
each of the task parameters (context, conditions, time) independently, in addition to their nonlinear interaction.

142

COSYNE 2017

II-58 – II-59
Our approach provides the first full view of the population representations and dynamics in M1 during posture and
reaching. The activity in the shared-conditions subspace correlates with cursor kinematics therefore reflecting a
pure representation of motor intention, independent of effector. During reaching this intention activity is perfectly
correlated with movement direction (100 ms) before movement beginning, which can explain the source of Georgopoulos’ population vector hypothesis. It can also elucidate how neurons involved in arm control can drive the
BMI when the arm is at rest. In the nonlinear-interaction subspaces, representations in one context are simple
rotations of the other context. Taken together, these results suggest that linear demixing and mappings may be a
general principle of how nonlinear cortical networks represent independent task parameters.

II-58. Adaptive optimal training of animal behavior
Ji Hyun Bak1
Jung Yoon Choi2
Athena Akrami2
Ilana Witten2
Jonathan Pillow2
1 Korea

JHBAK @ KIAS . RE . KR
JUNGCHOI @ PRINCETON . EDU
AAKRAMI @ PRINCETON . EDU
IWITTEN @ PRINCETON . EDU
PILLOW @ PRINCETON . EDU

Institute for Advanced Study
University

2 Princeton

Neuroscience experiments often require training animals to perform tasks designed to elicit various sensory,
cognitive, and motor behaviors. Training typically involves a series of gradual adjustments of stimulus conditions
and rewards in order to bring about learning. However, training protocols are usually hand-designed, relying on a
combination of intuition, guesswork, and trial-and-error, and often require weeks or months to achieve a desired
level of task performance. Here we combine ideas from reinforcement learning and adaptive optimal experimental
design to formulate methods for adaptive optimal training of animal behavior. Our work addresses two intriguing
problems at once: first, it seeks to infer the learning rules underlying an animal’s behavioral changes during
training; second, it seeks to exploit these rules to select stimuli that will maximize the rate of learning toward a
desired objective. We develop and test these methods using data collected from rats during training on a twointerval sensory discrimination task. We show that we can accurately infer the parameters of a policy-gradientbased learning algorithm that describes how the animal’s internal model of the task evolves over the course of
training. We then formulate a theory for optimal training, which involves selecting sequences of stimuli that will
drive the animal’s internal policy toward a desired location in the parameter space. Simulations show that our
method can in theory provide a substantial speedup over standard training methods. We feel these results will
hold considerable theoretical and practical implications both for researchers in reinforcement learning and for
experimentalists seeking to train animals.

II-59. Dynamic neural representation of reward predictions in rat ventral striatum during learning
Angela Langdon1
Yuji Takahashi2
Matthew Roesch3
Geoffrey Schoenbaum2
Yael Niv1
1 Princeton

ALANGDON @ PRINCETON . EDU
YUJI . TAKAHASHI @ NIH . GOV
MROESCH @ UMD. EDU
GEOFFREY. SCHOENBAUM @ NIH . GOV
YAEL @ PRINCETON . EDU

University

2 NIDA/NIH
3 University

of Maryland

Adaptive behavior relies on the formation and update of accurate predictions about upcoming rewards during

COSYNE 2017

143

II-60 – II-61
learning. Theoretical and empirical work has suggested the ventral striatum (VS) is an important source of reward
predictions, providing these predictions to dopamine neurons in the midbrain for the computation of reward prediction errors. We recently demonstrated that dopaminergic prediction errors due to changes in the time of reward
delivery depend on the VS, suggesting VS activity encodes expectations about the when upcoming rewards are
expected (Takahashi, Langdon et al., 2016). Here we apply multiclass classification techniques to decode the predicted time of reward delivery from the pattern of firing activity in pseudoensembles of VS neurons. Recordings
were from rats performing an odor-guided choice task in which the timing of reward delivery was manipulated. We
show that after initial learning, the expected time of reward delivery can be reliably decoded from ensemble VS
activity throughout a trial, including during reward expectation, that is, after a choice has been made and before
reward has been delivered. However, we find that the neural representation of the predicted time of reward is
unstable: reward predictions at the end of the reward-expectation window cannot be reliably decoded using classifiers trained on the neural representation isolated at the start of the expectation window. This indicates that the
neural representation of predicted reward in the VS dynamically evolves during reward expectation, suggesting
an implicit code for expectations about reward time. We test how this dynamic representation of predicted reward
changes during learning, by decoding reward predictions after block changes during the task. These findings
provide critical insight into how neural activity in the VS might represent predictions about impending rewards,
and provide an empirical test of computational theories of the role of the VS in reward prediction and learning.

II-60. Perceptual confirmation bias from approximate online inference
Richard Lange
Ankani Chattoraj
Matthew Hochberg
Jacob Yates
Ralf Haefner

RLANGE @ UR . ROCHESTER . EDU
ACHATTOR @ UR . ROCHESTER . EDU
MHOCHBE 2@ U. ROCHESTER . EDU
JACOBY 8 S @ GMAIL . COM
RALF. HAEFNER @ GMAIL . COM

University of Rochester
The confirmation bias (CB) is ubiquitous in psychological studies, but its computational and neural basis is unclear.
An analogous effect is seen in some psychophysics studies: using reverse correlation, it has been shown that
subjects overweight information presented early in a trial to make a choice [1,2] (a CB). Other studies, however,
have found that subjects equally weight all information in a trial [3,4]. We introduce an intuitive probabilistic
framework that distinguishes between these studies as having different sources of uncertainty in the mapping
between the stimulus at any moment and the correct choice at the end of the trial. Those studies in which a
CB is seen use stimuli with strong temporal correlations and weak sensory information, while those with no CB
have the reverse. While exact inference in either case entails no CB, approximate inference methods may. We
simulate decision-making in a sampling-based model [5,6,7], in which we make explicit that the brain relies on
intermediate representations between sensory and decision-making areas. Our model qualitatively shows the
same trends in evidence weighting between the two types of task. Finally, we report preliminary data from a
visual discrimination task that enables us to directly explore these different sources of uncertainty and their effect
on measured CBs. These results offer a concise and unified perspective on existing evidence integration studies,
and provide a valuable framework for future task design.

II-61. Deep learning captures V2 selectivity for natural textures
Md Nasir Uddin Laskar
Luis G Sanchez-Giraldo
Odelia Schwartz

NASIR @ CS . MIAMI . EDU
LGSANCHEZ @ CS . MIAMI . EDU
ODELIA @ CS . MIAMI . EDU

University of Miami

144

COSYNE 2017

II-62
There has been substantial recent progress in building supervised deep convolutional neural networks (CNNs) of
visual scenes. The highest layers of CNNs have been intriguingly linked to high level object recognition areas in
visual cortex, and there has been some indication that middle layers are more predictive of mid-level visual areas.
Here we focus on studying the second layer of CNNs and the potential compatibility with visual area V2. Recent
experimental work and analyses provide a compelling case for V2 selectivity to visual textures [1], [2]. Following
these experiments, we probed CNNs with synthesized textures and spectrally matched noise stimuli to see if there
is any correspondence between CNNs and the biological vision system. We found a good qualitative correspondence between the deep convolutional network layers 1 and 2, and some experimental results in areas V1 and
V2. We evaluated this across a number of metrics: modulation index for texture selectivity; between versus within
texture family variance; t-SNE visualization of texture clustering; representational similarity analysis; and texture
recognition scores. Our findings will lead to better understanding of mid-level visual cortical representations and
how they may develop hierarchically.

II-62. The influence of trait anxiety on decision making under ambiguity: a
computational fMRI study
Emma Lawrance1
Jill O’Reilly1
Janine Bijsterbosch1
Christopher Gagne2
Timothy Behrens1
Sonia Bishop2
1 University
2 University

EMMA . LAWRANCE @ UNIV. OX . AC. UK
JILL . OREILLY @ NDCN . OX . AC. UK
JANINE . BIJSTERBOSCH @ NDCN . OX . AC. UK
CGAGNE @ BERKELEY. EDU
TIMOTHY. BEHRENS @ NDCN . OX . AC. UK
SONIABISHOP @ GMAIL . COM

of Oxford
of California, Berkeley

Anxious individuals report difficulties in decision-making and elevated ambiguity-aversion. However there has
been relatively little attempt to characterize the underlying mechanisms. Using a computational framework, we
examined the neural basis for trait anxiety (TA) related individual differences in decision-making under ambiguity.
Participants performed an Ellsberg style ‘urn’ task while fMRI data were acquired. On each trial, participants
chose between two urns with varying proportions of ‘Os’ and ‘Xs’, with each urn linked to a unique magnitude
of electrical stimulation. These parameters were varied independently across trials. On half the trials, one urn
was ‘ambiguous’, with a varying number of tokens obscured. Hence we manipulated both the presence of ambiguity (categorical) and its level (parametric). Modelling of participants’ behaviour revealed consistent ambiguityaversion across participants, with sub-optimal avoidance of the ambiguous urn. This increased parametrically
with extent of missing information. This parametric index of ambiguity aversion was in turn positively linked to trait
anxiety. fMRI analyses revealed that during the decision-making period, categorical ambiguity was associated
with increased activation in the inferior frontal sulcus (IFS) and dorsomedial prefrontal cortex (dmPFC), and with
amygdala deactivation. Further, activity in both IFS and dmPFC tracked the extent of missing information (parametric ambiguity). Individual difference analyses revealed regions where activity linked to the extent of missing
information correlated with either trait anxiety (amygdala) or behavioural parametric ambiguity aversion (dmPFC).
In rostrolateral prefrontal cortex (RLPFC), activity to missing information was correlated with both trait anxiety and
parametric ambiguity aversion (pAA), with RLPFC activation showing a trend level mediation of the relationship
between TA and pAA. These findings point to a number of regions that might contribute to individual differences
in decision-making under ambiguity. In future work, we hope to further specify the precise role played by each of
these areas.

COSYNE 2017

145

II-63 – II-64

II-63. Flexibility to contingency changes distinguishes habitual and goal-directed
strategies in humans
Julie Lee
Mehdi Keramati

JULIE . LEE .15@ UCL . AC. UK
MEHDI @ GATSBY. UCL . AC. UK

University College London
Model-based and model-free strategies have been suggested to cooperate and compete to influence control in
reinforcement learning. Theory predicts that the degree of behavioral flexibility to manipulations of either reward
values or transition contingencies distinguishes the two strategies. While previous studies have mostly focused on
the former, we developed a novel two-step reinforcement learning task that parametrizes the contribution of modelbased and model-free strategies using a contingency-change paradigm. In our task, transition contingencies
between states at the second step flip between two causal structures every few trials, while reward values attached
to states are stable. Trials can start from either the first or second step but actions are only possible at the first step.
Thus, if a trial starts at the first step, contingency changes are directly attributable to actions, but if it starts from the
second step, changes are only indirectly attributable. Since a habitual strategy would simply retrieve experienced
state values, only a goal-directed system tracking the causal structure of the task could deduce the correct next
action. Behavioral choices after contingency changes revealed that participants followed a hybrid goal-directed
and habitual strategy, confirmed by hierarchical model-fitting. The best-fit model from model comparison was a
hybrid model that increased the relative influence of a model-based strategy over three trial blocks. This increase
in goal-directedness was accompanied by a small increase in reward gained, although differences were only
significant between the first and third blocks. In summary, participants were found to use a hybrid strategy when
reacting to contingency changes, but became more model-based over a session.

II-64. The impact of synaptic unreliability on spiking correlations in recurrent,
balanced networks
Michael Leone1,2
Chengcheng Huang2
Brent Doiron2
1 Carnegie

MLEONE @ ANDREW. CMU. EDU
CHENGCHENGHUANG 11@ GMAIL . COM
BDOIRON @ PITT. EDU

Mellon University
of Pittsburgh

2 University

Neuroscientists have known for decades that synaptic physiology is highly stochastic, with synapses releasing
vesicles unreliably following a pre-synaptic action potential. However, theoretical models of recurrent neuronal
networks often neglect this important feature of biology. In this work, we use a recurrent theory of non-leaky
integrate-and-fire neurons to show that in balanced network models, synaptic unreliability indeed contributes
non-negligible variability to spiking activity. However, we find that this added noise is overshadowed by robust
increases in spike count correlations when recurrent synapses become unreliable, in stark contrast to the effects of
unreliability of synapses in feedforward pathways. Using a theoretical calculation of the input current components
to cells, we demonstrate that the increase in spiking correlations that accompany unreliable recurrent synaptic
activity occurs due to a breakdown in the network’s ability to track and cancel correlated input. Finally, we study the
functional consequences of neuromodulation of excitatory synapse reliability: we find that a simultaneous increase
in feedforward and recurrent excitatory reliability results in an increase in linear Fisher information by increasing
neural response gain while decreasing trial-to-trial correlations. Together, our results suggest the importance
of modeling stochastic synapses in the study of the mechanics of trial-to-trial variability and co-variability in the
cortex.

146

COSYNE 2017

II-65 – II-66

II-65. Neurogenetic manipulation of visuomotor circuitry in freely flying Drosophila
Aljoscha Leonhardt
Alex Mauss
Alexander Borst

LEONHARDT @ NEURO. MPG . DE
AMAUSS @ NEURO. MPG . DE
BORST @ NEURO. MPG . DE

Max Planck Institute for Neurobiology
Locomotion in complex environments poses a severe challenge to sensory systems controlling an animal’s course.
For flies, visual estimation of self-motion is thought to play a critical role in stabilizing trajectories during walking or
flight. Robust sensory feedback seems particularly important when considering the rapid velocities at which flies
maneuver through varied and adverse surroundings. The extent, however, to which flies make use of optic flow
remains unclear, especially in comparison with proprioceptive feedback mediated by halteres. Recently, a circuitlevel account of how Drosophila detects motion has come within reach. The vast majority of these discoveries
was made in either resting or head-fixed tethered animals. Here, we probe the function of optomotor circuitry
under real-world conditions: in freely behaving Drosophila. Our set-up allows us to track multiple flies online while
presenting high-resolution visual stimuli. We express opto- and thermogenetic tools in precisely circumscribed
groups of neurons within the optic lobe, silencing and activating cells during episodes of untethered locomotion in
two or three spatial dimensions. Emphasis rests on the role of local motion detector arrays T4 and T5 for active
navigation. The two cell types process moving bright and dark edges, respectively, and provide the sole directionselective input to the intricate course control networks of the lobula plate. Our previous work on tethered walking
flies has demonstrated that these elements are required for correcting artificially imposed course deviations. Here,
we show path stability to be strongly diminished in freely walking flies rendered motion-blind through inactivation
of T4 and T5, even without external perturbation. We extend these findings to three-dimensional trajectories,
focusing on differential effects between walking and flying. Our approach thus represents a critical step toward
quantifying and understanding the contribution of optomotor circuitry to natural navigation in the fly.

II-66. Inferring distributions of neural data from incomplete observations
Anna Levina1
Viola Priesemann2

ALEVINA @ IST. AC. AT
VIOLA @ NLD. DS . MPG . DE

1 Institute
2 Max

of Science and Technology Austria
Planck Institute for Dynamics and Self-Organization

Inferring global properties of a system from observations is a challenge, even if one can observe the whole system.
The same task becomes even more challenging if one can only sample a small number of units at a time (spatial
subsampling). To still infer global properties, it is necessary to extrapolate from this small sampled fraction to the
full system. Arguably the most prominent system that can be assessed only in very small parts or very coarsely
is the brain. Sampling the spiking activity of neurons, for example with state of the art multi-electrode arrays,
provides sufficient temporal resolution, but spatially at best a few hundred of all neurons can be sampled at a
time. Despite these limitations, the highly parallel recording techniques for the first time allowed some access to
neural network dynamics. Prominently, they have been used to test the hypothesis that the brain shows signatures
of “critical dynamics”. Criticality is a dynamical state that maximizes information processing capacity in models,
and therefore is a favorable candidate state for brain function. Experimental approaches to test for criticality
extract spatio-temporal clusters of spiking activity, called avalanches, and test whether their size and duration
distributions followed power laws, as expected for criticality. However, as these avalanches can propagate over
the entire system, their observation is strongly limited by subsampling. In this study, we present a first approach
to a theory of subsampling. We derive analytically a subsampling scaling framework and demonstrate how to
infer the correct distributions of the underlying full system. We apply subsampling scaling to neuronal avalanche
models and to recordings from developing neural networks. We show that only mature, but not young networks
follow power-law scaling, indicating self-organization to criticality during development

COSYNE 2017

147

II-67 – II-68

II-67. A unified neural network model for reconsolidation and extinction of
fear memory
Ho Ling Li
Wulfram Gerstner

HOLING . LI @ EPFL . CH
WULFRAM . GERSTNER @ EPFL . CH

Ecole Polytechnique Federale de Lausanne
Understanding the neural processes involved in extinguishing fear memory is critical since it sheds light on potential treatments for post-traumatic stress disorder (PTSD). In rodents, extinguishment of fear response depends on
the duration of re-exposure to the fear-conditioned context. A single long retrieval, or multiple short re-exposures
lead to the loss of fear response by forming a new extinction memory. Contrastingly, a short re-exposure causes
labilization of the fear memory, followed by a protein synthesis dependent process called reconsolidation, resulting
in the persistence of fear response. Extinction and reconsolidation are mutually exclusive processes, as proven
by their different requirements on biochemicals (Merlo et al., 2014, and Suzuki et al, 2004). Here we present a
novel mathematical model that describes the two processes by using a single competitive molecular mechanism.
This mechanism regulates the plasticity between the brain structures relevant to the formation of fear memory and
fear extinction. Our neural network model reproduces the temporal signatures of reconsolidation and extinction,
as well as the impacts of pharmacological blocks on molecules crucial to the regulations of the two processes.
Importantly, it predicts that extinction is triggered more efficiently by one long re-exposure than repeated brief
re-exposures within a short time period. To our knowledge, this is the first attempt to study the extinguishment of
fear response using a neural network model. Such a model could help for a better understanding of the underlying
mechanisms of PTSD, and facilitate the development of its treatments.

II-68. Uncertainty-dependent extinction of fear memory in an amygdala-mPFC
neural circuit model
Yuzhe Li
Ken Nakae
Shin Ishii
Honda Naoki

LIYUZHE @ ME . COM
NAKAE - K @ SYS . I . KYOTO - U. AC. JP
ISHII @ I . KYOTO - U. AC. JP
N - HONDA @ SYS . I . KYOTO - U. AC. JP

Kyoto University
Uncertainty of fear conditioning is crucial for the acquisition and extinction of fear memory. Fear memory acquired
through partial pairings of a conditioned stimulus (CS) and an unconditioned stimulus (US) is more resistant
to extinction than that acquired through full pairings; this effect is known as the partial reinforcement extinction
effect (PREE). The neural mechanisms underlying the PREE remain largely unclear. Here, we developed a neural
circuit model based on three distinct types of neurons (fear, persistent and extinction neurons) in the amygdala and
medial prefrontal cortex (mPFC). In the model, the fear, persistent and extinction neurons encode predictions of
net severity, of unconditioned stimulus (US) intensity, and of net safety, respectively. Our simulation successfully
reproduces the PREE. In addition, we extended the model to include amygdala subregions and the mPFC to
address a recent finding that the ventral mPFC (vmPFC) is required for consolidating extinction memory but not
for memory retrieval. Furthermore, model simulations led us to propose a novel procedure to enhance extinction
learning through re-conditioning with a stronger US; strengthened fear memory up-regulates the extinction neuron,
which, in turn, further inhibits the fear neuron during re-extinction.

148

COSYNE 2017

II-69 – II-70

II-69. Human learning in complex environments: episodic memory challenges
the model-free–model-based realm
Vasiliki Liakoni1
Marco Lehmann1
Wulfram Gerstner1
Kerstin Preuschoff2
1 Ecole

VASILIKI . LIAKONI @ EPFL . CH
MARCO. LEHMANN @ EPFL . CH
WULFRAM . GERSTNER @ EPFL . CH
KERSTIN . PREUSCHOFF @ UNIGE . CH

Polytechnique Federale de Lausanne
of Geneva

2 University

Making decisions and learning from their outcomes are among the brain’s most extraordinary capabilities. Recent
advances have indicated multiple learning strategies in the human brain; simple model-free (MF), flexible modelbased (MB), and—oftentimes neglected—episodic memory-based. Human fMRI studies have employed simple
two-stage or spatial navigation tasks (Daw2011, Simon2011) that, despite the wealth of insights they provide,
entail limitations: MB computational expenses - its hindering feature - become trivial, and different strategies can
not be dissociated as they tend to give correlated predictions. Here we employ a novel multi-step sequential
decision making task combined with a strategic use of surprising events that enable us to de-correlate signals
of different strategies. Twenty-three human subjects performed the task in an fMRI scanner. We considered the
following algorithms to explain behavioral data: SARSA (λ) (Sutton1998), Forward Learner and Hybrid Learner
(Glaescher2010), and a memory-based model of episodic policy update with an eligibility trace. We find correlates
of MF signals in the ventral striatum and other areas and MB correlates in the inferior frontal gyrus and insula.
Our results support the existence of two systems in the brain performing MF and MB computations, in agreement
with previous studies (Glaescher2010). Importantly, our behavioral data suggest a memory-based contribution;
subjects seem to update and reinforce chunks of actions leading to reward. We find activity in hippocampal
and temporal lobe regions correlating with reward receipt as a putative signature of an episodic memory-based
strategy. Our study introduces a new task with an innovative manipulation that successfully disentangles different
learning signals. It corroborates previous fMRI findings in a more complex and realistic scenario, while crucially
bringing forward the role of episodic memory contributions in the sequential decision making MF-MB long-standing
debate (Lengyel 2007).

II-70. Emergence of V1 recurrent connectivity pattern and Hebbian rule by
supervised learning
Fangzhou Liao
Xiaolin Hu
Sen Song

LIAOFANGZHOU @ GMAIL . COM
XLHU @ MAIL . TSINGHUA . EDU. CN
SONGSEN @ TSINGHUA . EDU. CN

Tsinghua university
Recurrent connection was discovered more than a decade ago in primary visual cortex. Its rule of connectivity and
function in vision has been studied for a long time. But its cognitive role still remains unclear. We hypothesize that
the connectivity pattern in brain is optimized to achieve high recognition performance. To test this idea, we inserted
monosynaptic recurrent connections in the first convolutional layer (acting as V1) in a convolutional neural network (CNN), and trained the network in supervised learning task by back-propagation (BP) algorithm. According
to biological findings, the recurrent connections in this model were factorized to inter-column (long range) connections and intra-column (short range) connections. After training, the proposed model obtained higher recognition
accuracy than the model without recurrent connections. As to the connectivity, we found that the trained connectivity pattern was similar to those discovered in biological experiments. For inter-column recurrent connections,
neurons with collinear receptive fields (RF) tended to connect stronger. In intra-column connection, neurons with
similar orientation tuning tended to form stronger connection. We also found a general rule governing both the
inter-column and intra-column connections: the synaptic weight between two neurons could be approximated by
the product of their amplitudes and correlation of RF, which is consistent with Hebbian rule and experimental

COSYNE 2017

149

II-71 – II-72
findings. The results suggest that: first, the recurrent connections in the brain may serve the purpose of improving
recognition performance; second, under certain circumstance the BP method and Hebbian rule would lead to the
same result. And for the first time we discovered quantitative connectivity pattern of a deep neural network and
showed its similarity with biological findings, which suggested that our method is a possible general way to further
our understanding of connectome.

II-71. Sensory history affects perception through online updating of prior expectations
Itay Lieder1
Vincent Adam2,3
Maneesh Sahani2,3
Merav Ahissar1

ITAY. LIEDER @ MAIL . HUJI . AC. IL
VINCENT. ADAM 87@ GMAIL . COM
MANEESH @ GATSBY. UCL . AC. UK
MSMERAVA @ GMAIL . COM

1 Hebrew

University of Jerusalem
Computational Neuroscience Unit
3 University College London
2 Gatsby

Perception may be influenced by past stimuli over timescales that range from milliseconds to hours, but it has
been unclear whether these different effects reflect a common computational principle. Here, we studied the
impact of recent and long-term stimulus history on the “contraction bias”. When asked to report which of two
tones separated by brief silence is higher, subjects behave as though they hear the earlier tone “contracted”
in frequency towards a combination of recently presented stimulus frequencies, and the mean of the overall
distribution of frequencies used in the experiment [1]. It has been proposed that the long-term contraction bias
arises normatively, through the combination of a noisy memory trace of the first tone with a prior belief based on
the experimental stimulus distribution; a suggestion consistent with increased bias with delay and cognitive load
[2]. How detailed is this prior belief, how is it formed, and how does it interact with the effects of recent stimuli? We
measured two-tone frequency discriminations made against different background distributions of stimuli. Using
a novel nonlinear regression framework, we found that while one component of the bias reflected the overall
stimulus distribution in detail, a second component revealed a nonlinear influence of recent stimuli that depended
on subjects’ sensitivity in the task. We reconciled these findings within a single coherent model, wherein subjects’
noisy percepts of tone frequency combine with a single prior that they construct online based on their own variable
and uncertain experience, eventually arriving at an approximation to the experimental distribution. This suggests
that both short- and long-term biases arise through a single mechanism, which is tuned to optimize perception in
uncertain conditions. References: [1] Raviv et. al, 2012 [2] Ashourian and Loewenstein, 2011

II-72. Recurrent switching linear dynamical systems
Scott Linderman1
Andrew Miller2
Ryan Adams2
David Blei1
Matthew Johnson2
Liam Paninski1

SWL 2133@ COLUMBIA . EDU
ACM @ SEAS . HARVARD. EDU
RPA @ SEAS . HARVARD. EDU
DAVID. BLEI @ COLUMBIA . EDU
MATTJJ @ CSAIL . MIT. EDU
LIAM @ STAT. COLUMBIA . EDU

1 Columbia
2 Harvard

University
University

Modern recording technologies like silicon multielectrode arrays, microendoscopes, and 3D depth cameras provide unprecedented opportunities to study the neural underpinnings of complex behavior and sensory processes.
However, to realize the potential of these technologies, we must overcome serious statistical and computational

150

COSYNE 2017

II-73
challenges in analyzing the data they produce. Beyond sheer scale, these data often exhibit a number of features
that make analysis difficult: non-Gaussian observations (e.g. spike counts); missing or obscured data; correlation
across trials and dimensions; and, most importantly, complex nonlinear dynamics. These motivate an approach
that balances multiple conflicting concerns: our models should be flexible enough to capture highly nonlinear
data; structured enough to admit efficient Bayesian inference algorithms that handle correlated, missing, and
non-Gaussian data; and interpretable enough to provide insight into the underlying neural systems. We present
a new model class that pragmatically balances these concerns. A natural approach to understanding complex
systems is to decompose them into simpler dynamic units. For example, it has been hypothesized that natural behavior is a composition of reusable units, or “syllables,” each lasting hundreds of milliseconds, and characterized
by relatively simple dynamics. The composition of syllables gives rise to complex, nonlinear behavior. Building on switching linear dynamical systems (SLDS), we present a new model class that not only discovers these
dynamical units, but also explains how the the transitions between units depend on observations or continuous
latent states. These “recurrent” switching linear dynamical systems provide further insight into neural systems by
discovering the conditions under which each unit is deployed, something that traditional SLDS models fail to do.
We leverage recent algorithmic advances to make Bayesian inference in these models fast, easy, and scalable.

II-73. Modeling the spatiotemporal dynamics of human focal seizures
Jyun-you Liou
Catherine Schevon
Laurence Abbott

JL 4115@ CUMC. COLUMBIA . EDU
CAS 2044@ CUMC. COLUMBIA . EDU
LFA 2103@ CUMC. COLUMBIA . EDU

Columbia University
Neocortical focal seizures, despite their huge heterogeneity in etiology, share common clinical manifestations –
gradual recruitment of new territory (e.g. the Jacksonian march), tonic-clonic transitions, and terminal slowing –
that tend to occur consecutively. Recently, we have published direct observations of neuronal activities during
human focal seizures (Smith et al 2016). These observations raise new questions about mechanisms supporting
seizure onset, propagation and termination. We have constructed a model that matches the spatiotemporal activity patterns seen in the seizure data. In the model, seizure activity starts with a localized group of neurons firing
asynchronously at very high rates (the activity bump). Surround inhibition (modeled by Mexican-hat connectivity)
initially limits the spatial spreading of this activity, but in the area immediately outside the bump, chloride starts to
accumulate intracellularly due to a strong feedforward inhibitory effects. Dissipation of transmembrane chloride
gradient compromises surround inhibition and leads to slow expansion of seizure territory. Meanwhile, at the bump
center, intense neuronal firing activates slow adaptation currents. Within seconds, the bump center collapses and
transitions to the “clonic phase” characterized by recurring periodic neuronal bursts. Traveling waves emerge and
propagate inwardly along the gradient of adaptation current strength, a new phenomenon not previously modeled.
Further strengthening of adaptation currents increases interburst intervals and decreases traveling wave speed,
both of which we observed during seizure termination in the human recordings. After a critical transition, the traveling wave solution is no longer supported and the resting state is restored. If we include spike-timing dependent
plasticity in the model, the spatiotemporal evolution of the seizure creates centripetal connectivity that results in
a decreased threshold for the next seizure. Overall, our model explains the complex spatiotemporal dynamics of
focal seizures with minimal assumptions, and it may provide a guide for future developments of close-loop devices
designed to control epilepsy.

COSYNE 2017

151

II-74 – II-75

II-74. Gain adaptation with and without rate adaptation in cortical area MT
Bing Liu
Matthew Macellaio
Leslie Osborne

LIUBING @ UCHICAGO. EDU
MACELLAIO @ UCHICAGO. EDU
OSBORNE @ UCHICAGO. EDU

University of Chicago
In a rapidly changing world, the statistics of sensory stimuli can fluctuate across a wide range. Theoretically, in
order to maximize the information transmission, sensory neurons respond to changes in stimuli with a contextdependent modulation of their response. This adaptive modulation can result in an efficient representation of
the incoming signal within the neuron’s limited response bandwidth. In previous studies of cortical adaptation,
changes in sensitivity to stimulus fluctuations (gain adaptation) were coupled to changes in firing rate, typically
a fast transient rate change that relaxes more slowly over time with dynamics on many time scales (rate adaptation). Here we show that gain adaptation and rate adaptation are independent processes in the intact brain.
We recorded extracellular activity of single units in area MT in alert monkeys using stochastic motion stimuli that
varied in direction on short (20ms) timescales, and stepped between different mean or variance levels at longer
(250ms-4s) intervals. We find step changes in stimulus variance produced gain adaptation without rate change,
while changes in stimulus mean produced rate adaptation without gain change. Rate and gain adaptations encode
different temporal components of stimulus, indicating a temporal multiplexing coding of visual motion information.
An information theoretic analysis in both rate and gain changing cases indicates that the rapid gain rescaling
alone is responsible for efficient coding. We proposed a mechanism model to describe the independent gain and
rate adaptation.

II-75. Interrupting behaviour
Kevin Lloyd1,2
Peter Dayan1,2
1 Gatsby

KLLOYD @ GATSBY. UCL . AC. UK
DAYAN @ GATSBY. UCL . AC. UK

Computational Neuroscience Unit
College London

2 University

Ideal decision-makers should constantly monitor all sources of external information about opportunities and
threats and be able to redetermine their choices promptly in the face of change. However, perpetual monitoring and reassessment impose inordinate computational costs, making them impractical for animals or machines.
The obvious alternative of committing for extended periods of time to particular courses of action can be dangerous and wasteful. Here, we explore the intermediate option of making provisional temporal commitments, but
engaging in limited broader observation with the possibility of interruption. Illustrating the potential benefits of such
a scheme requires an environment with both costs and benefits for action, as well as uncertainty and change. To
this end, we consider a simple example of foraging under predation risk, where a decision-maker needs to trade
off energetic gain against the danger of predation. In agreement with previous suggestions, we show, firstly, that
an agent equipped with the capacity for self-interruption outperforms an agent without this capacity. Secondly, we
find that the optimal interruption policy in our example has an uncomplicated form so that performance is essentially identical when using an approximation based on placing simple thresholds in belief space. This is consistent
with the idea that a relatively simple, ‘low-level’ mechanism can prompt behavioural interruption, analogous to the
operation of peripherally-induced interrupts in digital computers. We interpret our results in the context of putative
neural mechanisms, such as noradrenergic neuromodulation, and diseases of distractability and roving attention.

152

COSYNE 2017

II-76 – II-77

II-76. Optimal inference of hidden states by mice in a probabilistic foraging
task
Eran Lottem
Pietro Vertechi
Matthijs Oude Lohuis
Dario Sarra
Zachary Mainen

ERAN . LOTTEM @ NEURO. FCHAMPALIMAUD. ORG
PIETRO. VERTECHI @ NEURO. FCHAMPALIMAUD. ORG
MATTHIJS . OUDELOHUIS @ NEURO. FCHAMPALIMAUD. ORG
DARIO. SARRA @ NEURO. FCHAMPALIMAUD. ORG
ZMAINEN @ NEURO. FCHAMPALIMAUD. ORG

Champalimaud Centre for the Unknown
Deciding when to abandon a resource to explore elsewhere is a key component of foraging. To study this behavior,
we designed a probabilistic task in which mice exploit resource sites by nose poking for probabilistic rewards. The
probability of each poke delivering reward is initially constant but switches stochastically to a depleted state, with
a second port becoming armed. Although this is a complex problem, which required them to infer a hidden state
of the world from noisy observations, mice learned the task surprisingly quickly (typically 1-2 weeks). Theoretical
analysis suggests that mice should decide when to leave a given port by accumulating evidence that it is depleted
by counting non-rewarded actions. To maximize reward intake, they should do so at a rate set by the statistics of
the environment, including the probabilities of reward delivery and depletion. Agreement with these predictions
was evident in the animals’ behavior. To better understand this computational processes underlying these foraging
decisions, we derived the strategy that maximizes reward over time in the framework of partially observable
Markov decision processes. We show that this can be modeled as a noisy accumulation-to-threshold process
in which a particle representing the inferred probability of site depletion is driven by unrewarded actions until
reaching a fixed bound that signals the decision to leave. By comparing different variants of the diffusion model
we found that a 2-parameter Poisson spike-counting model could most parsimoniously fit the pattern of the mice’s
behavior. Importantly, this model captured an important feature - scalar invariance of the distribution of leave
times. These results suggest that mice naturally optimize probabilistic foraging decisions in an efficient manner
and that therefore this class of tasks can provide a rich and efficient paradigm to study evidence accumulation
processes and the neural circuits sub-serving them.

II-77. Can serial dependencies in choices and neural activity explain choice
probabilities?
Jan-Matthis Lueckmann1
Jakob Macke1
Hendrikje Nienborg2

JAN - MATTHIS . LUECKMANN @ CAESAR . DE
JAKOB . MACKE @ CAESAR . DE
HENDRIKJE . NIENBORG @ CIN . UNI - TUEBINGEN . DE

1 research
2 Centre

center caesar
for Integrative Neuroscience Tuebingen

The activity of sensory neurons co-varies with choice during perceptual decisions, commonly quantified as “choice
probability”. Moreover, choices are influenced by a subject’s previous choice (serial dependencies) and neuronal
activity often shows temporal correlations on long (seconds) timescales. Here, we ask whether these findings
are linked, specifically: How are choice probabilities in sensory neurons influenced by serial dependencies in
choices and neuronal activity? Do serial dependencies in choices and neural activity reflect the same underlying
process? Using generalized linear models (GLMs) we analyze simultaneous measurements of behavior and
V2 neural activity in macaques performing a visual discrimination task. We observe that past decisions are
substantially more predictive of the current choice than the current spike count. Moreover, spiking activity exhibits
strong correlations from trial to trial. We dissect temporal correlations by systematically varying the order of
predictors in the GLM, and find that these correlations reflect two largely separate processes: There is neither a
direct effect of the previous-trial spike count on choice, nor a direct effect of preceding choices on the spike count.
Additionally, variability in spike counts can largely be explained by slow fluctuations across multiple trials (using a
Gaussian Process latent modulator within the GLM). Is choice-probability explained by history effects, i.e. how big

COSYNE 2017

153

II-78
is the residual choice probability after correcting for temporal correlations? We compute semi-partial correlations
between choices and neural activity, which constitute a lower bound on the residual choice probability. We find
that removing history effects by using semi-partial correlations does not systematically change the magnitude
of choice probabilities. We therefore conclude that despite the substantial serial dependencies in choices and
neural activity these do not explain the observed choice probability. Rather, the serial dependencies in choices
and spiking activity reflect two parallel processes which are correlated by instantaneous co-variations between
choices and activity.

II-78. Common sound-frequency dependence for linear and nonlinear responses
in the auditory midbrain
Dominika Lyzwa1
Chen Chen
Monty Escabi
Heather Read2
1 Max

DOMINIKA @ NLD. DS . MPG . DE
CLAIRECLE @ GMAIL . COM
ESCABI @ ENGR . UCONN . EDU
HEATHER . READ @ UCONN . EDU

Planck Institute for Dynamics and Self-Organization
of Connecticut

2 University

Natural sounds contain correlations within and across their spectral and temporal components such as the covariation in harmonics in vocalizations. Neurons in the inferior colliculus (ICC) have both linear and non-linear
responses to natural or dynamically modulated sounds [1-3]. However, it remains unknown whether non-linear
responses in ICC fall outside the classic receptive field as observed in primary visual cortex [4]. To examine this,
single unit responses to sounds with dynamically varying spectral and temporal sound modulations are recorded
from a large 3-dimensional volume of cat ICC utilizing dynamic moving ripple (DMR) sounds. Linear and nonlinear
response components are assessed with reverse correlation techniques. The spike-triggered average and covariance for the envelope of the DMR stimulus are computed separately for each sound frequency carrier to assess
frequency-dependence of nonlinearities. More than half of ICC neurons examined display significant nonlinear
response properties generally accompanied by a linear response component. The greatest nonlinearities are
found at frequencies near the“best frequency”i.e., that which drives the maximal linear response. This suggests
a high degree of overlap between the linear and nonlinear receptive field components. In addition, the nonlinear
response components have either the same or opposite temporal receptive field pattern (e.g. on-off) as the linear
response components. We found no relationship to other neural properties such as feature-selectivity or degree
of phase-locking. This study indicates that the nonlinearities are a general property of many ICC neurons. Significantly, it supports the notion that non-linear and linear responses are not due to distinct frequency sensitivities
(e.g., harmonics) but instead have similar frequency-dependence. This could emerge from ascending auditory
pathways known to process different sound features and to converge onto a common frequency-lamina within
ICC [12].

154

COSYNE 2017

II-79 – II-80

II-79. Biophysical constraints of optogenetic inhibition at presynaptic terminals
Mathias Mahn
Katayun Cohen-Kashi-Malina
Matthias Prigge
Shiri Ron
Rivka Levy
Ilan Lampl
Ofer Yizhar

MATHIAS . MAHN @ WEIZMANN . AC. IL
KATAYUN . COHEN - KASHI @ WEIZMANN . AC. IL
MATTHIAS . PRIGGE @ WEIZMANN . AC. IL
SHIRI . RON @ WEIZMANN . AC. IL
RIVKA . LEVY @ WEIZMANN . AC. IL
ILAN . LAMPL @ WEIZMANN . AC. IL
OFER . YIZHAR @ WEIZMANN . AC. IL

Weizmann Institute of Science
Optogenetic inhibition of axonal terminals is a powerful approach for investigating the function of long-range
neuronal projections. We investigated the constraints of optogenetic inhibition at presynaptic terminals using
inward-directed chloride pumps (halorhodopsin), outward-directed proton pumps (archaerhodopsin) and chlorideconducting ion channels (channelrhodopsins). While the use of such optogenetic tools can potentially allow fast
and reversible inhibition of defined projection pathways through illumination of presynaptic terminals, their use in
axonal terminals has not been mechanistically validated. Here we used two-photon imaging and whole-cell patch
clamp recordings in-vitro and in-vivo to evaluate the impact of optogenetic light-driven ion fluxes on the excitability
and function of presynaptic terminals. Temporally-precise activation of both archaerhodopsin and halorhodpsin in
presynaptic terminals indeed significantly attenuated evoked synaptic transmission by 24.3 % and 67.9 %, respectively. However, in acute brain slices, sustained archaerhodopsin activation was paradoxically associated with a
65.1% increase in spontaneous vesicle release. This increase in neurotransmission was mediated by presynaptic alkalization and calcium influx, and resulted in recruitment of local-circuit feed-forward inhibition. Activation
of a chloride-conducting channelrhodpsin in presynaptic terminals evoked vesicle release in-vitro and in-vivo,
consistent with a depolarized chloride reversal potential in presynaptic terminals. Overexpression of the potassium chloride co-transporter KCC2 reduced the incidence of antidromic action potential generation in the axon of
whole-cell patch-clamp recorded neurons expressing anion channelrhodopsins in-vitro. Our results suggest that
the unique biophysical properties of presynaptic terminals dictate boundary conditions for optogenetic modulation.
Both in-vitro and ex-vivo, light-driven proton efflux from presynaptic terminals can paradoxically increase synaptic
transmission through a pH- and calcium-dependent process. Our data therefore suggest that halorhodpsin is
currently the most suitable tool for synaptic terminal silencing.

II-80. Measuring cross-frequency coupling using mutual information and its
application to epilepsy
Rakesh Malladi1
Don Johnson1
Giridhar Kalamangalam2
Nitin Tandon2
Behnaam Aazhang1
1 Rice

RAKESH . MALLADI @ RICE . EDU
DHJ @ RICE . EDU
GIRIDHAR . P. KALAMANGALAM @ UTH . TMC. EDU
NITIN . TANDON @ UTH . TMC. EDU
AAZ @ RICE . EDU

University
of Texas Health Science Center

2 University

Recent studies have shown that interactions between neuronal oscillations at different frequencies plays an important role in neuroscience. These interactions are typically quantified by measuring phase-amplitude, phase-phase
and amplitude-amplitude couplings. The current metrics used to quantify these interactions in frequency cannot
detect statistical independence and are either restricted to multivariate autoregressive models or cannot quantify dependence across phase and amplitude jointly. The main contribution of this work is in defining a powerful
new metric, mutual information in frequency, that detects statistical independence between different frequency
components. This new metric broadly applies to all models and quantifies dependence across phase and am-

COSYNE 2017

155

II-81 – II-82
plitude more generally. This novel metric is based on the Cramer’s representation of stochastic processes. We
also propose a model-free estimator to estimate this metric and validate its performance on linear and nonlinear
models. Finally, we use this metric to investigate the cross-frequency coupling during seizures, and find that highfrequency oscillations become more synchronized during seizures, when compared with preictal and postictal
periods. This analysis can be potentially used to learn the cross-frequency coupling mechanisms that should be
targeted to treat epilepsy.

II-81. A probabilistic model for the neural code using maximum entropy of
random projections
Ori Maoz1
Gasper Tkacik2
Roozbeh Kiani3
Elad Schneidman1

ORI . MAOZ @ WEIZMANN . AC. IL
GTKACIK @ IST. AC. AT
ROOZBEH @ NYU. EDU
ELAD. SCHNEIDMAN @ WEIZMANN . AC. IL

1 Weizmann

Institute of Science
of Science and Technology Austria
3 New York University
2 Institute

Neural codes are often described as probabilistic mappings between population activity patterns and sensory
stimuli or motor outputs. High-dimensionality of the stimulus space, combinatorial nature of population activity
patterns, and neural noise make inference of neural codes intractable unless we can find powerful simplifying
principles. Pairwise maximum entropy models have suggested such a principle, where strongly correlated population activity patterns in tens of neurons were accurately described by minimal models that rely only on pairwise
interactions between cells. Yet, pairwise models are insufficient in describing larger neural populations, and
identifying which interaction terms they should be augmented with is difficult. We present a new family of probabilistic models of neural populations: Maximum Entropy distributions based on Random sparse Projections of the
network’s activity (MERP). Applied to large populations of cortical neurons in behaving primates, we show that
MERP models match or surpass the accuracy of the best existing models when employing a similar number of
constraints. These models are easily extendable by adding more constraints, but we show that they can perform
as well with a significantly smaller number of constraints by adaptively replacing non-contributing constraints with
newly drawn random ones. Importantly, we show that MERP is superior to other models, in particular when the
population activity is severely under-sampled. We demonstrate the equivalence of randomly connected feedforward neural circuits and MERP models, suggesting a plausible way for neural circuits to estimate the saliency
or log-likelihood of their inputs by reweighting the contributions of a large set of random projections. We further
show that because MERP can be implemented by a shallow neural network, multiple models can share a common
set of random projections. Thus, MERP models present an efficient, scalable, and highly accurate probabilistic
representation of their inputs, suggesting it as a computational design principle for neuronal architectures.

II-82. Sparse predictive coding in balanced spiking networks
Mirjana Maras
Sophie Deneve

MARAS . MIRJANA @ GMAIL . COM
SOPHIE . DENEVE @ GMAIL . COM

Ecole Normale Superieure
Recent theoretical and experimental studies are challenging the widely held notion that neural populations represent sensory input via time averaged firing rates. Experimentally observed tight balance between excitation and
inhibition supports the non-traditional view that neurons encode information in their precisely timed spike trains.
While theoretical studies show that this tight balance can be achieved in both sparsely and densely connected
networks with random weights, they do not address why the brain would operate in this regime [Vogels et al,

156

COSYNE 2017

II-83
2011; Renart et al, 2015]. On the other hand, the predictive spike-coding framework shows that the tight balance
between excitation and inhibition can be seen as a direct consequence of a coding efficiency principle [Boerlin
et al, 2013; Deneve and Machens, 2016]. The predictive coding model is an all-to-all connected spiking network
with fast inhibition and symmetric recurrent weights. Relaxation of the fast inhibition constraint has shown that
the model continues to outperform the Poisson network when realistic synaptic dynamics are modeled [Chalk et
al, 2016]. Here, we remove the all-to-all symmetric connectivity constraint, and analyze the performance of the
network with both realistic synaptic dynamics and sparse lateral connections. We train the recurrent weight matrix
by employing a local balance restoring learning rule [Bourdoukan et al, 2015]. Furthermore, we control the population firing rate via a homeostatic adaptation of the firing threshold. We show that the revised model successfully
learns the lateral connection weights with biologically realistic connection probabilities and synaptic delays. The
resulting prediction errors are greater than those of the optimal all-to-all connected network, but the sparse network outperforms the equivalent Poisson and independent leaky integrate-and-fire networks. Furthermore, the
mean of the squared learned weights scales inversely with the total number of synaptic inputs, as experimentally
observed.

II-83. Confidence representations across modalities in orbitofrontal cortex
Paul Masset1
Torben Ott1
Junya Hirokawa1,2
Adam Kepecs1
1 Cold

PMASSET @ CSHL . EDU
TOTT @ CSHL . EDU
RATCORTEX @ GMAIL . COM
KEPECS @ CSHL . EDU

Spring Harbor Laboratory
University

2 Doshisha

Assigning appropriate levels of confidence to decisions based on sensory evidence from different modalities
is essential to perform adaptive behaviors in complex environments. Previously neural correlates of decision
confidence have been found in the orbitofrontal cortex (OFC) in an olfactory decision task (Kepecs et al, 2008)
and OFC inactivation was shown to diminish the ability for confidence reporting in rats (Lak et al, 2014). If OFC
computes a metacognitive signal it should do so irrespective of the modality used to make the decision and a
number of models can be proposed for the encoding of the metacognitive signal in single neurons. Here we set
out to test these hypotheses by recording OFC neural activity during a dual modality decision task. Rats were
trained in a dual modality 2AFC task complemented with a confidence-reporting paradigm. Trials with olfactory
and auditory stimuli were randomly interleaved. Rats reported their choice by entering one of two ports where
they had to wait for a water reward delivered at randomized timings. The willingness to wait for a reward is used
a measure of decision confidence. This task allows us to measure the responses of individual neurons in a single
behavioral session to sensory stimuli from two modalities. We recorded neurons from OFC in rats (N=2) trained
this task. We found two distinct populations of neurons encoding task events. A first population responded to
behavioral events in the task (eg. stimulus sampling, entering choice port) irrespective of the modality of the
stimulus. The responses of the second population were modality specific. We contrast across modalities the
representation of decision variables, such as choice and confidence, within these two populations. Our results
suggest that OFC carries information about decision confidence based on multiple sensory modalities and could
be the seat of computation of a metacognitive confidence signal.

COSYNE 2017

157

II-84 – II-85

II-84. Boosting olfactory cocktail-party performance by semi-supervised learning in mice
Alexander Mathis1
Alan Wei2,1
Alexandra W Ding1
Matthias Bethge3
Venkatesh N Murthy1

AMATHIS @ FAS . HARVARD. EDU
ALANYWEI @ STANFORD. EDU
ALEXANDRADING 01@ COLLEGE . HARVARD. EDU
MATTHIAS . BETHGE @ UNI - TUEBINGEN . DE
VNMURTHY @ FAS . HARVARD. EDU

1 Harvard

University
University
3 University of Tuebingen
2 Stanford

Mice are excellent at detecting single odor components in complex mixtures. Yet, when they are trained on single
odors alone, they fail to reliably detect target odors in mixtures of multiple odorants. This inability was predicted
by a linear readout that was trained using samples from an empirically estimated, nonlinear odor encoding model
at the level of receptors. These results from mouse behavior and the modeling suggested that mice learn the
‘cocktail-party task’ discriminatively (Mathis et al. 2016, Neuron). Another possibility for their inability to generalize
much beyond simple mixtures, is that lab mice are not exposed to mixtures and thus, have not formed a reliable
generative model’ of mixtures. To test this idea, we performed a novel variant of the previous task. As before,
mice were trained on single odor-reward associations with two target odors and fourteen distractor odors until they
reached performance levels above 90%. They were divided in two groups. Outside of the operant-conditioning
task, mice were exposed to odor stimuli in an ‘unsupervised way’. One group was presented with mixtures stimuli
(UM group) and the other group with single odors (US group). Once an animal reached 90% performance,
they were tested on mixture stimuli with 1, 4, 8 and 12 odorants. On the first day, the UM group significantly
outperformed the US group, even for single odors, despite similar performance on the last day of training. Over
multiple days, the UM group then also improved their performance faster than the US group. Thus, passive
exposure to mixtures can aid the detection of single odors in mixtures. We will discuss the implications of this
result for recent models of the olfactory cocktail party task.

II-85. Anticipatory neural activity is driven by a cue-triggered acceleration of
network dynamics
Luca Mazzucato
Giancarlo La Camera
Alfredo Fontanini

LUCA . MAZZUCATO @ STONYBROOK . EDU
GIANCARLO. LACAMERA @ STONYBROOK . EDU
ALFREDO. FONTANINI @ STONYBROOK . EDU

Stony Brook University
Ensemble recordings from alert rat gustatory cortex (GC) demonstrate that taste stimuli can be decoded more
rapidly if their delivery is preceded by an anticipatory cue. This phenomenon has been related to the presence
of anticipatory activity triggered by the cue (1). However, the specific mechanism linking anticipatory activity to
speeding of coding is unknown. Here, we propose a biologically plausible model based on a recurrent network
of spiking neurons with clustered architecture. In the absence of stimulation, the model’s neural activity unfolds
through sequences of metastable states, where each state is a population vector of stable firing activity lasting few
hundreds of milliseconds (2). We modeled taste stimuli and the anticipatory cue (the same for all stimuli) as two
sets of inputs targeting subsets of excitatory neurons. As observed in experiments, taste stimuli evoked specific
state sequences. Analysis of taste-evoked activity revealed the presence of state sequences characterized by
‘coding states’, (i.e., states occurring significantly more often for a particular taste than in other conditions). The
presentation of an anticipatory cue accelerates the rate of transitions between states and shortens their duration.
As a result, when the stimulus presentation is preceded by a cue, coding states show a faster and more reliable
onset. This phenomenon explains why expected stimuli can be decoded more quickly than unexpected ones.
The anticipatory effect is unrelated to changes of firing rates in stimulus-selective neurons and is absent in homo-

158

COSYNE 2017

II-86 – II-87
geneous networks, suggesting that a clustered organization is necessary to mediate the expectation of relevant
events. Altogether our results demonstrate a novel mechanism for speeding up sensory coding in cortical circuits.

II-86. Efficiency despite diversity across retinal ganglion cell types
Lane McIntosh1
David B. Kastner2
Mihai Manu3
Stephen A Baccus1

LMCINTOSH @ STANFORD. EDU
DAVID. KASTNER @ UCSF. EDU
DRMANUMIHAI @ GMAIL . COM
BACCUS @ STANFORD. EDU

1 Stanford

University
of California, San Francisco
3 Medical School Hannover
2 University

The theory of efficient coding has been used to explain a variety of retinal mechanisms and computations, including the construction of receptive fields [1], placement of thresholds [2], and timescales of adaptation [3]. Much of
this work relies on single-neuron analysis, where ganglion cell receptive fields are shown to closely approximate
a single ideal filter shape that maximizes information transmission. In apparent conflict with these ideas is the
observation that ganglion cells have a wide diversity of receptive field shapes, consistent with the large body of
literature supporting the benefits of diversity in neural circuits, from reducing correlations between neurons [4] to
increasing the reliability of the network during decoding [5]. Given that one would expect that deviations from
an ideal shape would reduce information transmission, it is not clear how ganglion cells can both be optimally
informative and diverse. We report here that ganglion cell diversity is in a direction that allows different receptive field shapes to nonetheless maintain near-optimal information transmission. For a dataset of 1,721 recorded
retinal ganglion cells spanning 5 cell types we find that the diversity in retinal ganglion cell receptive fields is well
explained by input from two inhibitory neurons with different spatial scales, horizontal and amacrine cells, the
contributions of which were computed using simultaneous intracellular and multielectrode recording. We find that
differences in the center and surround weighting produce large changes in the efficiency of retinal ganglion cell
receptive fields, but that horizontal and amacrine contributions are tuned to maintain an optimal center-surround
weighting, allowing different receptive field shapes with virtually no cost in information efficiency. We find that the
benefits of diversity are obtained in the retina without sacrificing the efficiency of individual cells, and show the
general utility of using parallel pathways to generate a neural computation.

II-87. Efficient multi-dimensional neural coding via conformal diffusion flow
Daniel McNamee
Daniel Wolpert
Mate Lengyel

D. MCNAMEE @ ENG . CAM . AC. UK
WOLPERT @ ENG . CAM . AC. UK
M . LENGYEL @ ENG . CAM . AC. UK

University of Cambridge
The efficient coding hypothesis posits that neurons are tuned to minimize redundancy and encode information as
e iciently as possible [Barlow1961] and has successfully accounted for a host of neural phenomena [Dayan2001].
However, previous approaches were limited to studying special cases of the hypothesis. Classical work ignored
the effects of noise variability in responses [Laughlin1981,Atick1990,Bell1995]. More recently, [Ganguli2014]
established that neural resources should be allocated in proportion to the stimulus probability density for a variety
of objective functions including maximizing mutual information. This approach is limited by the requirement that
the stimulus be one-dimensional and does not optimize coding efficiency for the discrete nature of a finite neural
population. In contrast, [Mathis2015] shows that the hexagonal symmetry of grid cells is maximally efficient in the
sense of maximizing the trace of the Fisher information matrix (hence minimizing decoding mean-squared-error)
and generalizes this principle to higher dimensions. However, [Mathis2015] requires that the stimulus density

COSYNE 2017

159

II-88 – II-89
being represented be uniform and that the stimulus space itself be isotropic. Here, we describe a method to
integrate these two approaches into a theory of efficient coding based on the concept of conformal mappings that
(i) applies to arbitrary stimulus probability densities, (ii) for any dimensionality and any stimulus geometry, and
(iii) can be combined with a variety of objective functions. Furthermore, our solution is uniquely specified by its
integration of these two approaches. As a proof of principle, we apply our model to place cell data [Hollup2001],
which could not be previously modeled in an efficient coding framework, and our results provide a normative
explanation for the accumulation of place cells at salient spatial locations.

II-88. Adaptive time-averaging for texture perception
Richard McWalter1,2
Josh McDermott3

RMCW @ ELEKTRO. DTU. DK
JHM @ MIT. EDU

1 Technical

University of Denmark
Systems
3 Massachusetts Institute of Technology
2 Hearing

Sensory receptors measure signals over short time scales, but our perception of the natural world often entails
integrating these measurements over much longer extents. Texture provides a paradigmatic example of such
integration – it is believed to be represented by statistics that are time-averages of sensory measurements. We
probed the mechanisms underlying auditory texture integration (i.e., the averaging process), attempting to characterize the time scale of the averaging and the dependence of any such time scale on stimulus stationarity. We
conducted psychophysical experiments using ‘texture steps’ – stimuli whose statistics underwent a subtle shift
partway through their duration. Listeners were asked to compare a texture step to a texture with constant statistics, the idea being that judgments should be biased by the step if the underlying average extended beyond the
step. We found that texture judgments were biased by a step occurring 1s from the endpoint, but that the bias
significantly lessened when the step was 2.5s from the endpoint, suggesting a statistical integration window on
the order of a few seconds. However, the extent of the bias depended on the variability (stationarity) of the texture,
with increased bias for more variable textures (e.g. ocean waves) and reduced bias for less variable textures (e.g.
rain). Biasing effects were also lessened when a silent gap was inserted at the step, suggesting that integration
was partially reset by the change. Together the results suggest a multi-second integration process that adapts
itself to stimulus characteristics, extending integration when it benefits statistical estimation of a variable signal,
and excluding the stimulus history when evidence suggests the generative process has changed.

II-89. Sleep restores slow intrinsic timescales of cortical dynamics after sustained wakefulness in rats
Christian Meisel
Dietmar Plenz

CRISHEN @ YAHOO. COM
CHRISTIAN @ MEISEL . DE

NIMH
Sleep is crucial for daytime functioning and well being. Without sleep optimal brain functioning such as responsiveness to stimuli, information processing, or learning is impaired. The neuronal correlates underlying the impaired
brain functioning after extended wake, however, have been difficult to identify. Recent studies have provided accumulating evidence that cortical dynamics is organized along a temporal gradient, where hierarchically higher cortical areas are governed by increasingly long timescales suitable to integrate inputs over longer times to increase
the signal-to-noise ratio in decision-making and working memory. Here we show that these slow timescales are
progressively shortened during extended wake at the individual neuron level in freely-behaving rats. Specifically,
we implanted microelectrode arrays into superficial layers of prefrontal cortex for chronic recordings in the awake,
behaving rat. Rats were sleep deprived for 6 h during which multiunit and LFP activity was recorded, and slow

160

COSYNE 2017

II-90 – II-91
cortical timescales were derived. Reversing their decay, we observed that sleep restores slow timescales which
support long-range temporal information integration. Experimental findings during sleep deprivation were closely
matched by detailed neural network modeling. The integration of experiment and modeling links our findings
to a dynamical systems framework where slow cortical timescales arise as a natural consequence of network
dynamics being poised near a balanced, critical state. Consequently, the deterioration of slow timescales and
optimal cognitive capabilities is caused by increased average synaptic strength which move the network away
from criticality. Our results point to a network-level function of sleep: to reorganize cortical networks towards a
state governed by slow cortex dynamics to ensure optimal function for the time awake.

II-90. Large-scale spike sorting for the analysis of electrical stimulation
Gonzalo Mena1
Lauren Grosberg2
Sasidhar Madugula2
Pawel Hottowy3
Alan Litke4
John Cunningham1
EJ Chichilnisky2
Liam Paninski1

GEM 2131@ COLUMBIA . EDU
LAUREN . GROSBERG @ GMAIL . COM
SASIDHAR @ STANFORD. EDU
HOTTOWY @ AGH . EDU. PL
ALAN . LITKE @ CERN . CH
JPCUNNI @ GMAIL . COM
EJ @ STANFORD. EDU
LIAMPANINSKI @ GMAIL . COM

1 Columbia

University
University
3 AGH University
4 University of California, Santa Cruz
2 Stanford

Simultaneous electrical stimulation and recording using multi-electrode arrays (MEAs) is critical for studying neural circuits and engineering neural interfaces. However, interpreting these recordings is challenging because spike
sorting (identifying and segregating action potentials from different neurons) is greatly complicated by electrical
stimulation artifacts, which can exhibit complex and nonlinear waveforms. To our knowledge, no current analysis
methods are available for large-scale applications. Also, current approaches are often based on highly restrictive
assumptions. We provide a principled, efficient algorithm for solving this problem. Our model linearly decomposes neural activity and artifact contributions to data, and imposes a structured Gaussian Process (GP) prior
on the artifact for a parsimonious representation of its properties. We develop a scalable algorithm based on
this structured GP to jointly infer the neural activity and the artifact. The effectiveness of our method is demonstrated in both real and simulated 512-electrode recordings with electrical stimulation in primate retina. We use
the algorithm to perform a large-scale analysis showing that the electrical excitability of RGCs varies substantially
depending on the location of stimulation on the cell soma or axon, consistent with results from time-consuming
manual analysis. This technology will be useful in the design of future high-resolution stimulating devices such as
retinal prostheses, and for closed-loop neural stimulation at a much larger scale than is currently possible.

II-91. Biologically plausible learning in recurrent neural networks reproduces
neural dynamics observed during cognitive tasks
Thomas Miconi

THOMAS . MICONI @ GMAIL . COM

The Neurosciences Institute
Neural activity during cognitive tasks exhibits complex dynamics that flexibly encode task-relevant variables. Recurrent neural networks operating in the near-chaotic regime, which spontaneously generate rich dynamics, have
been proposed as a model of cortical computation during cognitive tasks. However, existing methods for training
these networks are either biologically implausible, and/or require a continuous, real-time error signal to guide the

COSYNE 2017

161

II-92
learning process. The lack of a biological learning method currently restricts the plausibility of recurrent networks
as models of cortical computation. Here we show that a biologically plausible learning rule can train such recurrent networks, guided solely by delayed, phasic rewards at the end of each trial. This rule implements the
node-perturbation method (itself a variant of the classical REINFORCE algorithm) in a biologically plausible manner, without requiring a continuous, real-time error signal at each point in time. Using this learning rule, networks
successfully learn nontrivial tasks requiring flexible (context-dependent) associations, memory maintenance, nonlinear mixed selectivities, and coordination among multiple outputs. Furthermore, applying this method to learn
various tasks from the experimental literature, we show that the resulting networks replicate complex dynamics
previously observed in animal cortex, such as dynamic encoding of task features, switching from stimulus-specific
to response-specific representations, and selective integration of sensory input streams. We conclude that recurrent neural networks offer a plausible model of cortical dynamics during both learning and performance of flexible
behavior.

II-92. The role of orbitofrontal cortex in planning: Decision process, learning
process, or both?
Kevin Miller1
Matthew Botvinick2,3
Carlos Brody1,4
1 Princeton

KJMILLER @ PRINCETON . EDU
BOTVINICK @ GOOGLE . COM
BRODY @ PRINCETON . EDU

University

2 DeepMind
3 University
4 Howard

College London
Hughes Medical Institute

Planning can be defined as the use of an internal model of the likely outcomes of one’s actions to guide behavior.
Silencing the orbitofrontal cortex (OFC) is known to impair planning, but the details of its role, as well as of the
neural mechanisms of planning in general, remain unclear. Various theories of orbitofrontal function suggest that
it plays a role in model-based choice (Padoa-Schioppa, 2011), model-guided learning (Schoenbaum, et al, 2009),
or both. Data indicating that units in the OFC encode expected (“economic”) value signals associated with stimuli
and actions support these theories, but do not clearly adjudicate between them. Previously, we have adapted a
multi-step probabilistic decision task (Daw et al, 2011) for use with rodents, and shown that rats adopt a strategy
of model-based planning which depends on the OFC (Miller, Botvinick, & Brody, Cosyne Abstracts, 2016). Here,
we investigate the role of this region in further detail. We present data from optogenetic inactivation experiments
indicating that inhibiting OFC during reward consumption at the end of a trial changes the effect of several past
trials’ rewards on the upcoming choice (suggesting a role in the choice process), but does not change the effect
of that reward on subsequent choices (suggesting no role in the learning process). We also present data from
single-unit recordings, in which the multi-step nature of our task allows us to distinguish value signals associated
with choices from those associated with outcomes. Puzzlingly, we find strong evidence for coding of outcome
value signals (suitable for supporting model-guided learning), but little evidence of choice value signals (which
would be suitable for supporting model-based choice). Taken together, these results indicate that value signals in
the OFC may not be straightforwardly related to its functional role, and suggest that new theories of orbitofrontal
function may be necessary.

162

COSYNE 2017

II-93 – II-94

II-93. Motor cortex engages output circuits in a behaviorally-selective manner
Andrew Miri
Claire Warriner
Jeffrey Seely
Gamaleldin Elsayed
Laurence Abbott
John Cunningham
Mark Churchland
Thomas Jessell

ANDREWMIRI @ GMAIL . COM
CLAIRELWARRINER @ GMAIL . COM
JSSEELY @ GMAIL . COM
GAMALELDIN . ELSAYED @ GMAIL . COM
LFA 2103@ CUMC. COLUMBIA . EDU
JPC 2181@ COLUMBIA . EDU
MC 3502@ COLUMBIA . EDU
TMJ 1@ COLUMBIA . EDU

Columbia University
When and how the motor cortex influences movement remains uncertain. The behavioral consequences of motor cortical inactivation suggest a circumscribed role. Yet across a broad array of behaviors including those that
survive inactivation, motor cortical activity is correlated with muscle activation and motor cortical stimulation activates muscles. To address this ambiguity, we used optogenetic perturbation and neural recording together with
electromyography in mouse to examine the influence of motor cortex on forelimb movements that differ in their
requirement for motor cortical involvement. We developed a behavioral paradigm in which head-fixed mice are
trained to reach toward and grasp a joystick before pulling it a set distance – a precision pull task. Unilateral motor
cortical injection of muscimol, as well as ablation of motor cortex, markedly diminished task performance, demonstrating a requirement for motor cortex in task execution. Optogenetic silencing of motor cortical output during
precision pull caused muscle activity time series to deviate from controls within 10ms, yet such short-latency
effects were not observed during treadmill walking, which survives motor cortical inactivation with muscimol or
ablation. To probe the origins of this differential influence on motor tasks, we performed population-level analysis of neural activity recorded in motor cortex during the two behaviors. Motor cortical firing was correlated with
muscle activity to a similar degree during both behaviors, yet correlations between neuronal firing patterns were
dramatically different. These changes in neuronal correlation imply that motor cortical activity varies along different directions in a neural activity space in which each cardinal dimension represents the firing rate of an individual
neuron. Such changes could enable differential influence of motor cortical output on downstream effector circuits
between behaviors. Our findings indicate a behavioral selectivity in the influence of motor cortex mediated by
changes in firing dynamics, helping to reconcile previous observations.

II-94. Adaptive compression of statistically homogeneous sensory signals
Wiktor Mlynarski
Josh McDermott

MLYNAR @ MIT. EDU
JHM @ MIT. EDU

Massachusetts Institute of Technology
Human observers are believed to represent statistically stationary signals (textures) with summary statistics, and
to be insensitive to the details that compose them. The underlying normative principles for this coding strategy
remain unclear. We noted that stationary stimuli quickly diminish in value to a statistical observer whose goal is to
infer the state of the environment from raw sensory signals. This is because they cease to convey new information
about the world once its properties have been estimated. An efficient sensory system would compress such uninformative signals to minimize coding cost. To assess the extent to which such a strategy would be beneficial, we
first estimated the prevalence of stationarity in natural images and sounds by fitting probabilistic models to signal
excerpts and computing the KL-divergence between fitted distributions. We found that highly stationary signals are
common in natural scenes, necessitating an efficient coding strategy. To test whether adaptive compression could
explain human texture perception, we conducted two perceptual experiments using auditory textures. Previous
work found that the ability to discriminate exemplars of a texture, which entails encoding their details, decreases
with their duration. This phenomenon could reflect coding capacity limits, in which case exemplar discrimination
should be poor only for stimuli with high coding cost (entropy). Alternatively, the hypothesized efficient coding

COSYNE 2017

163

II-95 – II-96
strategy predicts perceptual compression of stationary signals irrespective of their coding cost, and precise encoding of stimuli that provide information about a change in their generative parameters even if the coding cost is
high. Consistent with the latter predictions, we found that texture exemplar discrimination decreased with duration
regardless of the stimulus entropy, but improved for texture stimuli whose parameters changed halfway through.
Our results suggest that sensory systems adaptively compress stimuli which do not generate novel inferences
about the environment.

II-95. Neural coding of context-specific rules in primate prefrontal cortex, caudate, and hippocampus
Morteza Moazami
Scott L Brincat
Earl K Miller

MOAZAMI @ MIT. EDU
SBRINCAT @ MIT. EDU
EKMILLER @ MIT. EDU

Massachusetts Institute of Technology
Flexible expression of behavioral rules in appropriate contexts is a fundamental hallmark of cognition. To understand the underlying neural mechanisms, we simultaneously recorded spiking and local field potential (LFP) activity from up to 104 electrodes in the prefrontal cortex (PFC), hippocampus, and caudate tail of monkeys performing
a context-specific rule-switching task. In each task trial, one of four object cues was shown in one of four locations.
Half of the objects mandated a leftward saccadic response following a brief delay, while the other half mandated
a rightward saccade. Critically, this object-saccade mapping was switched for half of the object locations. Thus,
the correct behavioral response required integration of both object identity and its location context. We examined
how semantic information about task structure—which objects had the same mapping (“object rule”), and which
locations had the same mapping (“location rule”)—was encoded in different neural signals: multiunit spiking rates,
LFP amplitude, and band-specific LFP power. Neural information about both rule types was distributed across all
three brain regions, but different forms of neural coding predominated in each region and different task epochs.
During the object presentation, rule information in spike rates was strongest in PFC, information in LFP amplitude
was strongest in hippocampus, and information in LFP beta-band power was strongest in caudate. During the
memory delay, the representation shifted such that information about both rules was consistently stronger in the
PFC than other areas, and stronger in the LFP beta-band power than other signals, consistent with a proposed
role for beta oscillations in top-down processing. These results suggest a shift in neural coding from retrieval of
behavioral rules to their maintenance in working memory, and that temporal coding may be stronger in subcortical
than neocortical regions.

II-96. Decoupling categorical choice from action reveals pure timing signals
in striatal populations
Tiago Monteiro1,2
Filipe Rodrigues1
Asma Motiwala3
Thiago S Gouvea4
Joseph Paton3

TIAGO. MONTEIRO @ NEURO. FCHAMPALIMAUD. ORG
FILIPE . RODRIGUES @ NEURO. FCHAMPALIMAUD. ORG
ASMA . MOTIWALA @ NEURO. FCHAMPALIMAUD. ORG
TSGOUVEA @ CSHL . EDU
JOE . PATON @ NEURO. FCHAMPALIMAUD. ORG

1 Champalimaud

Research
Centre for the Unknown
3 Champalimaud Neuroscience Programme
4 Cold Spring Harbor Laboratory
2 Champalimaud

Keeping track of time is critical for identifying structure in the environment and knowing when to act. However
the neural mechanisms of timing are poorly understood. Previously, we have shown that striatal populations

164

COSYNE 2017

II-97 – II-98
encoded elapsed time, predicted rats judgments, and were necessary for accurate behavioral performance in
a two alternative forced choice (TAFC) time interval discrimination task. However, in this previous work, as in
many decision-making paradigms in rodents, categorical judgments (long/short) were reported via lateralised
orienting movements (left/right), and the rule that mapped categorical judgment to laterality of movement was
fixed. Thus, striatal populations might have encoded information about action in addition to time. To address if
striatal activity patterns represent the passage of time per se or time-varying representations of specific actions
we developed a task where the mapping between action and temporal categorization was flexible. Consistent with
our previous work, we found a systematic relationship between the speed of striatal trajectories in neural space
and the animals’ estimate of elapsed time – with faster dynamics associated with longer judgments. Strikingly,
this was largely true irrespective of the mapping between categorical judgment and the action used to report that
judgment. Indeed, we were able to identify a subspace within the population that encoded time in a manner
that correlated with judgments irrespective of the choice rule in use, demonstrating that a pure time signal is
encoded in the population. Thus striatal dynamics might form a central neural representation of time that guides
animals’ decisions about duration. More generally, these results highlight how removing confounds in the design
of decision-making paradigms can disentangle the encoding of task-relevant cognitive and motor variables.

II-97. Emergence of goal-driven multi-agent communication in artificial neural
networks
Igor Mordatch1,2

IGOR . MORDATCH @ GMAIL . COM

1 OpenAI
2 University

of California, Berkeley

Ability to communicate and use language to effectively cooperate is one of the key properties of intelligent behavior. In this work, we investigate how communication and language emerges de novo in simulated multi-agent
environments. Importantly, we do not pose communication as an explicit goal in itself (as in language games,
for example). Rather, the goals are grounded in the physical environment that requires multi-agent cooperation,
and communication emerges as a pragmatic tool to effectively cooperate. For example, if one agent has a goal
to meet another agent at a particular location, it must communicate navigation instructions to that location. The
other agent then needs to use physical actions to actually move there. The success of communication is concretely measured by success of the physically-grounded goal. The agents act according to a policy based on
artificial deep neural networks with recurrent memory that map visual and verbal observations to physical and
verbal actions. Reinforcement learning methods are used to train agent policies end-to-end to maximize goalbased reward. For the task of either moving to or directing gaze at a particular location, we observe an automatic
formation of a shared language based on discrete symbols to communicate the task between agents. This emergent language has a compositional structure, with symbols referencing the type of action (move to, or gaze at),
the discrete goal location landmark, and the discrete offset relative to the landmark (left of, below, etc). In contrast
to prior work, policy neural networks and the training algorithm are completely general and there is no special
structure or manually-specified design that encourages a particular style of communication. This work provides
the first steps towards complex communication and action behavior emerging jointly based on fully-automatic
methods and crucially grounded in the physical (and not solely linguistic) environments.

II-98. Visual modulation of layer 6 neurons in mouse auditory cortex
Ryan Morrill
Andrea Hasenstaub

RYAN . MORRILL @ UCSF. EDU
ANDREA . HASENSTAUB @ UCSF. EDU

University of California, San Francisco
Cortical multisensory integration is traditionally held to occur in higher-order association areas; however, work

COSYNE 2017

165

II-99
over the past decade suggests that even primary sensory cortex may receive and integrate information from
other modalities (Wallace et al., 2004; Ghazanfar & Schroeder, 2006). In a variety of organisms, auditory cortex
responds to stimuli from other modalities, both in conjunction with and independent of acoustic stimulation (Kayser
et al., 2008; Bizley et al., 2007). Despite the importance of the mouse as a systems neuroscience model for circuit
dissection, it remains unknown to what degree visual stimuli modulate or drive responses in mouse auditory
cortex. Mouse auditory cortex receives inputs from visual cortical areas which terminate both in both deep and
superficial layers (Banks et al., 2011), suggesting that modulation of activity might also be layer-specific. Here
we describe the temporal and laminar organization of visual influences on neural firing in the auditory cortex
of the awake mouse, using multisite probes to sample across multiple cortical layers. We demonstrate that
these recording sites are located in auditory cortex based on characteristic neural responses to click trains and
pure tones of varied frequencies and sound levels. We then determine the laminar location of recording sites
through electrode track tracing with fluorescent dye and optogenetic identification using layer-specific markers.
We find that spiking responses to visual stimulation occur deep in auditory cortex, primarily in layer 6. Visual
modulation of firing rate occurs more frequently at areas with secondary-like auditory responses than those with
primary-like responses. We further find auditory cortical responses to visual drifting gratings are not directionor orientation-tuned, unlike characteristic visual cortex responses. We speculate as to the role of this finding in
auditory processing and its generalizability to other cortical systems.

II-99. Local generation of errors in multi-layer networks through broad category tuning of each neuron
Hesham Mostafa
Gert Cauwenberghs

HMMOSTAFA @ UCSD. EDU
GERT @ UCSD. EDU

University of California, San Diego
Backpropagation is currently the technique of choice for training artificial multi-layer networks. Several of its aspects, however, are biologically unrealistic such as weight symmetry in the forward and backward pass, and the
need for intermediate layers to maintain their state until error signals arrive from higher layers. For classification
tasks, we describe an alternative multi-layer training technique where each layer only makes use of locally generated error signals to adjust incoming weights without receiving information from higher layers. This is achieved
by pre-assigning each neuron in the network a random category response mask that specifies for each category
if the neuron should respond strongly (selective), respond weakly (anti-selective), or whether its response is noninformative for this category. Each layer uses the activities of its neurons and the response masks to generate a
score for each category and adjusts incoming weights to maximize the score for the category corresponding to
the current input pattern. We evaluate our scheme on the MNIST classification task where it performs on par with
backpropagation when training multi-layer fully connected networks of rectified linear units. Our scheme leads to
significantly sparser representations in the hidden layers compared to backpropagation. Our scheme was motivated by single neuron responses measured from monkey inferior temporal (IT) cortex that indicate each neuron
is broadly tuned to a collection of categories. This tuning is task dependent. Interestingly, we observe a drop
in classification performance if we require each neuron to be selective to one category only and anti-selective
to all the others, which indicates broad category tuning, where each neuron is selective to a random subset of
categories, is indeed beneficial. Broad category tuning is thus a potential biological mechanism for the supervised
or task dependent learning of sparse features in the intermediate layers of multi-layer networks.

166

COSYNE 2017

II-100 – II-101

II-100. Judgement of time is reflected in dopaminergic reward prediction errors
Asma Motiwala1
Sofia Soares1
Bassam Atallah1
Joseph Paton1
Christian Machens2
1 Champalimaud
2 Champalimaud

ASMA . MOTIWALA @ NEURO. FCHAMPALIMAUD. ORG
SOFIA . SOARES @ NEURO. FCHAMPALIMAUD. ORG
BASSAM . ATALLAH @ NEURO. FCHAMPALIMAUD. ORG
JOE . PATON @ NEURO. FCHAMPALIMAUD. ORG
CHRISTIAN . MACHENS @ NEURO. FCHAMPALIMAUD. ORG

Neuroscience Programme
Centre for the Unknown

The basal ganglia appear to implement reinforcement learning (RL) like computations and dopaminergic neurons
encode reward prediction errors (RPE), the key teaching signal in RL algorithms. RPE signals elicited by a cue are
modulated both by the reward expectation associated with the cue as well as the predictability of the cue itself.
We wanted to ask how variable estimates of elapsed time since a cue affect the properties of RPE signals in
response to that cue. We trained a RL agent on an interval discrimination task using temporal difference learning
in an actor-critic framework. To emulate our previous finding that over- or underestimation of elapsed time results
from differences in the speed with which neural activity evolves, we modelled variability in time estimates by
changing the speed with which agents’ state representations evolved. We found that RPE signals elicited by
the cue indicating the end of the delay to be discriminated systematically varied with agents’ internal estimate
of elapsed time. The resulting pattern and variability of RPE signals qualitatively matched previously reported
activity of dopaminergic neurons in the SNc, recorded in mice using fibre photometry, while they performed the
interval discrimination task. Thus dopaminergic RPEs in the SNc and animals’ judgments likely have access to
the same internal representation of time.

II-101. A neural model of motor sequence learning and timing in basal ganglia
James Murray
Sean Escola

JM 4347@ CUMC. COLUMBIA . EDU
SEAN . ESCOLA @ GMAIL . COM

Columbia University
The basal ganglia are known to play a major role in directing learned sequences of behaviors such as reaching
and pressing, particularly in behaviors for which precise timing plays an important role, as experiments in rodents
performing delayed lever press tasks have shown. A complete understanding of the relative roles of cortex and
basal ganglia in learning and performing such behaviors, however, is still lacking. Inspired by recent results showing that motor cortex is necessary for learning new behaviors but not for performing already-learned behaviors
(Kawai et al., Neuron 2015), we develop a model of striatum, the basal ganglia input structure, in which inhibitory
neurons receive cortical input. Sparse sequential firing patterns such as those seen in recent population recordings (Mello et al., Curr. Bio. 2015; Rueda-Orozco and Robbe, Nature Neuro. 2015) arise generically in the model
when lateral inhibition and synaptic depression—both known features of the principal neurons in striatum—are
included. Anti-Hebbian plasticity allows for particular activity sequences to be learned based on repeating external input to the network, for both continuous-variable and spiking-neuron models. After learning, the network can
reproduce the sequence autonomously without patterned cortical input, and can further speed up or slow down
the sequence simply by adjusting the level of tonic external input, with generalization to faster or slower speeds
even if learning occurs only at a single fixed speed. Such temporal scaling of neural activity in proportion to behavioral speed has been observed over a wide range of delays in lever-press experiments (Mello et al., Curr. Bio.
2015) and has remained a challenge to explain theoretically. Thus, in addition to addressing the internalization of
activity patterns by basal ganglia, the model provides a mechanism for encoding flexible and robust neural activity
patterns that may underlie timekeeping and motor sequence performance.

COSYNE 2017

167

II-102 – II-103

II-102. Maximal-length orbits in binary neural networks
Samuel P Muscinelli
Wulfram Gerstner
Johanni Brea

SAMUEL . MUSCINELLI @ EPFL . CH
WULFRAM . GERSTNER @ EPFL . CH
JOHANNI . BREA @ EPFL . CH

Ecole Polytechnique Federale de Lausanne
Humans and some animals are able to produce complex sequential behaviors that require precise coordination
of muscle activities on time scales that range from hundreds of milliseconds to several seconds. How does the
brain achieve this coordination, relying mainly on fast time scales, is an open question. A widespread hypothesis
is that sequential behavior can be read out from autonomously generated sequences of neural activations. This
mechanism is believed to be used in the songbird brain (Hanloser et al., Nature, 2002) and it has inspired several
reservoir approaches to sequence learning (Sussillo and Abbott, Neuron, 2009). In most of these applications
however, the length of the autonomous sequences scales only linearly with the number of units, which makes
their application to slow behavioral sequences unlikely. Here, we investigate how to overcome this limitation,
deriving the conditions under which a network autonomously produces maximally long sequences, without relying
on intrinsic slow time scales. We show that binary neural networks admit stable orbits that form sequences of
maximal length, scaling exponentially with the number of units (Muscinelli et al, Neural Comput., to appear). We
provide an algorithm to explicitly construct the synaptic weight matrices that produce these orbits. The exponential
length of the orbits comes at the cost of an exponential fine tuning of the weights. Importantly, these orbits are
relatively robust to dynamical noise, while perturbation of the weights reveals other periodic orbits that are not
maximal but typically still very long. The resulting dynamics exhibit emerging slow time scales, that can be
used to read out behaviorally relevant, non-trivial output sequences. Our result generalizes the song-generation
mechanism of the songbird and shows how a simple, fast system can produce extremely slow and complex
behavior if properly tuned.

II-103. Single-trial decision can be predicted from population activity of excitatory and inhibitor neurons
Farzaneh Najafi1
Gamaleldin Elsayed2
Eftychios Pnevmatikakis3
John Cunningham2
Anne Churchland1

FARZNAJ @ GMAIL . COM
GAMALELDIN . ELSAYED @ GMAIL . COM
EPNEVMATIKAKIS @ SIMONSFOUNDATION . ORG
JPCUNNI @ GMAIL . COM
CHURCHLAND @ CSHL . EDU

1 Cold

Spring Harbor Laboratory
University
3 Simons Foundation
2 Columbia

Decisions are driven by the coordinated activity of excitatory and inhibitory populations in multiple neural structures. Inhibitory neurons play a critical role in many models of decision-making, but the difficulty in measuring large
inhibitory populations in behaving animals has left their in vivo role mysterious. To understand the contributions of
excitatory and inhibitory neural populations to perceptual decision-making, we measured neural responses in the
posterior parietal cortex (PPC) of transgenic mice expressing tdTomato in inhibitory neurons (GAD2-Cre crossed
with Ai14 reporter line). To record neural activity, mice were injected with AAV9-Synapsin-GCaMP6f in the posterior parietal cortex. Mice were trained to make perceptual decisions about multisensory stimuli. Specifically,
head-fixed mice were presented with a series of multisensory “events” (clicks and flashes). They were trained
to lick to a right (left) port for event rates above (below) an abstract category boundary (16 Hz). We then used
2-photon imaging to measure single-neuron responses during these decisions. In each session, approx. 600
neurons were simultaneously recorded while mice performed approx. 400 trials. Inhibitory neurons were identified using a local contrast measure after correcting for bleed through from the green (pan-neuronal) channel to
the red (inhibitory) channel. Having identified which measured neurons were excitatory vs. inhibitory, we used a

168

COSYNE 2017

II-104 – II-105
linear classifier to determine the information carried by each population about different features of the decision.
We observed that on single trials, both populations could reliably predict the animal’s current as well as previous
choice. We used a novel analysis based on regularized regression to compare the relative contribution of excitatory and inhibitory neurons to decoding current and previous choice. Taken together, our results suggest that
inhibitory neurons are active during decision-making, reflect multiple decision features, and seem to complement
excitatory populations by playing a differential role in carrying information about the current vs. previous choice.

II-104. A computational model providing quantitative description of features
of view-tuned face neurons
Yunjun Nam1
Takayuki Sato
Yuji Yamauchi
Chia-pei Lin
Chou P Hung2
Uchida Go
Manabu Tanifuji1

YUNJUN . NAM @ RIKEN . JP
TAKAYUKI @ BRAIN . RIKEN . JP
YUJI . YAMAUCHI @ RIKEN . JP
GOTO. LIN @ GMAIL . COM
CHOU. P. HUNG . CIV @ MAIL . MIL
GUCHIDA @ BRAIN . RIKEN . JP
TANIFUJI @ POSTMAN . RIKEN . JP

1 RIKEN
2 US

Brain Science Institute
Army Research Laboratory

Face neurons in inferior temporal (IT) cortex respond to particular views of faces, meaning that view invariant face
recognition requires combinations of neurons. However, it is difficult to address how faces are view invariantly
represented by the combinations, because we do not have quantitative description of features encoded by face
neurons so that we cannot predict responses to arbitrary images. The purpose of the present study is to establish
a computational model that provide quantitative description of features and to justify the model by comparing
predicted facial view tuning with experimentally observed responses. We used a previously proposed model to
identify features of neurons diversely responding to objects (Owaki et al., 2012). In this model, we assumed
that we can recover the features from a set of natural image fragments if it is sufficiently large. We created a
massive number (= 560,000) of natural image fragments as a dictionary giving prediction of responses to arbitrary
images. We searched for a feature from the dictionary as the fragment that showed good agreement between
predicted and actual neural responses. First, we recorded localized neural responses to faces and non-face
objects from macaque IT cortex. We identified features for 17 recording sites, and the predicted responses from
the fragments showed 0.50 +- 0.09 of correlation coefficients (c.c.) with the neural responses, which explained
24.7% of variance in the neural responses to these images. Second, we recorded neural responses to faces
with systematically controlled views. For these responses, we investigated correlation in view tuning between the
predicted and recorded responses, and we found the responses from 12 among 17 sites (70%) showed significant
correlation (p< 0.05). The results showed that our model can be used to explain neural representations of different
facial views.

II-105. Burst ensemble multiplexing: Linking neural codes to dendritic spikes
and microcircuits
Richard Naud1
Filip Vercruysse2
Henning Sprekeler2
1 University

RNAUD @ UOTTAWA . CA
FILIPVERCRUYSSE @ GMAIL . COM
H . SPREKELER @ TU - BERLIN . DE

of Ottawa
Universitaet Berlin

2 Technische

Cracking the neural code is to attribute proper meaning to temporal sequences of action potentials. While the

COSYNE 2017

169

II-106
neural code is typically understood as a set of rules to translate features of the external world into trains of action
potentials, anatomical connections show that external input is often combined with internal signals from higher
order areas. A popular view poses that this top-down input merely modulates the encoding of bottom-up information, a signal fusion that leads to an information loss with respect to the two individual signals. Based on the known
properties of cortical neurons and circuits, we suggest that, instead, both signals are represented simultaneously,
in the form of a multiplexed population code consisting of spikes and bursts. We use a computational model of
thick-tufted pyramidal neurons that is constrained by data to understand how bottom-up and top-down signals are
represented in the spiking activity of a population. We show that top-down signals arriving in distal dendrites are
represented by the relative prevalence of bursts, while bottom-up information arriving perisomatically is encoded
in the sum of single spikes and bursts. Using a coherence-based information-theoretical analysis, we show that
the code can more than double the rate of information transfer, even for rapidly changing signals. We then show
that both bottom-up and top-down signals can also be decoded by biologically plausible mechanisms, namely by
combining inhibitory microcircuits with short-term plasticity. These results suggest a novel functional role of both
active dendrites and the stereotypical patterns with which inhibitory cell types interconnect in the neocortex. Burst
ensemble multiplexing, we suggest, is a general code used by the nervous system to flexibly combine two distinct
streams of information.

II-106. Deciphering neural variability within a neocortical microcircuit
Max Nolte
Michael Reimann
Eilif Muller
Henry Markram

MAX . NOLTE @ EPFL . CH
MICHAEL . REIMANN @ EPFL . CH
EILIF. MUELLER @ EPFL . CH
HENRY. MARKRAM @ EPFL . CH

Ecole Polytechnique Federale de Lausanne
Stochastic synaptic transmission is a major source of trial-to-trial spike-timing variability within neocortical circuits.
When recording from an individual postsynaptic neuron, it is unclear to what extent its spike-timing variability is
influenced by the already variable spike-timing of its presynaptic neurons, and to what extent its variability is
influenced by stochastic neurotransmitter release at the synapses it forms with the presynaptic neurons. In this
study, we used simulations of a digital reconstruction of a rat somatosensory microcircuit (Markram et al., 2015)
to analyze the respective impact of variable presynaptic input and stochastic synaptic release on the spike-timing
variability of different types of neurons within the microcircuit. We measured electrical activity in the microcircuit
during different thalamic stimuli, and during a state of in vivo-like spontaneous activity. Spike times of all neurons
were recorded during multiple trials of each stimulus, with different random number seeds for the synapses in each
trial ensuring different synaptic release events. Next, all neurons were simulated individually with presynaptic input
replayed from the previous full microcircuit simulations. We then compared the mean spike- timing variability of
each neuron over simulations with different inputs, where variability arises from both variable presynaptic spike
times and stochastic synaptic release, and over simulations with identical input, where variability arises mainly
from stochastic synaptic release. We found that the response of excitatory cells in layers 5 and 6 is significantly
less variable when receiving identical presynaptic inputs, indicating that much of their variability during thalamic
stimulation of the microcircuit is due to variable presynaptic inputs. Indeed, we found that cells in the deep layers
can become just as reliable as neurons in layer 4 that receive strong thalamic input. As a next step, we will extend
the analysis to a new version of the reconstruction, incorporating data on multi-vesicular synaptic release.

170

COSYNE 2017

II-107 – II-108

II-107. Changing prior probability in perceptual decision-making
Elyse Norton
Luigi Acerbi
Weiji Ma
Michael Landy

EHN 222@ NYU. EDU
LUIGI . ACERBI @ NYU. EDU
WEIJIMA @ NYU. EDU
MICHAEL . LANDY @ NYU. EDU

New York University
Optimal sensory decision-making requires the combination of uncertain sensory signals with prior expectations.
Signal detection theory describes the effect of prior probabilities as a shift in the decision criterion [1]. Previous
studies typically vary category probability between blocks and assume a fixed criterion for each block. Can
observers track sudden changes in probability? Observers performed two tasks in separate sessions. (1) Covertcriterion task: An ellipse was shown from one of two uncertain categories differing in mean orientation. The
observer indicated the category by keypress. (2) Overt-criterion task: The observer adjusted the orientation of
a “criterion line.” Then, an ellipse was presented and categorized based on its orientation relative to the set
criterion. In both tasks, category feedback was provided on each trial and category probabilities were updated
throughout the session using a sample-and-hold procedure. We developed a novel ideal Bayesian change-point
detection model in which the observer marginalizes over both the current run length (time since last change) and
the current category probability. We compared this model with several suboptimal models: (1) Fixed criterion;
(2) Suboptimal Bayesian model with potentially incorrect belief about the distribution of run lengths; (3-4) Two
exponential averaging models; and (5) Reinforcement-learning model. Models had between one and three free
parameters. Models were judged by marginal likelihood, which accounts for goodness of fit and model complexity.
All models outperformed the fixed-criterion model. An exponential averaging model that estimates probability from
a weighted average of recently experienced categories, with an additional bias towards p = 0.5, fit the data best.
Thus, observers track sudden changes in probability with a conservative bias towards equal priors.

II-108. Linking structure and activity in nonlinear spiking networks
Gabriel Ocker1
Kresimir Josic2
Eric Shea-Brown3
Michael Buice1

GABEO @ ALLENINSTITUTE . ORG
JOSIC @ MATH . UH . EDU
ETSB @ UW. EDU
MICHAELBU @ ALLENINSTITUTE . ORG

1 Allen

Institute for Brain Science
of Houston
3 University of Washington
2 University

Recent experimental advances have produced an avalanche of data on both neural connectivity and neural activity. To take full advantage of these two emerging datasets we need a framework that links them and reveals
how collective neural activity arises from the structure of neural circuits and intrinsic neuronal dynamics. Existing methods for relating activity and architecture in spiking networks rely on linearizing activity around a central
operating point, and thus fail to capture the nonlinearities in neural responses that are a hallmark of neural information processing. Here, we overcome this limitation and present a new relationship between connectivity and
activity in networks of nonlinear spiking neurons. We explicitly show how recurrent network structure produces
pairwise and higher-order correlated activity and, in particular, how nonlinearities impact the networks’ spiking
activity. Finally, we demonstrate how the nonlinearity of neural responses leads to a new impact of correlations
on population coding: to affect stimulus discrimination by shifting the mean of populaton response distributions.
We thus provide new tools for investigating how neural nonlinearities -including those expressed across multiple
cell types - in combination with connectivity shape population activity and function.

COSYNE 2017

171

II-109 – II-110

II-109. Evidence for optimal Bayesian cue combination of landmarks and velocity in the entorhinal cortex
Samuel Ocko
Kiah Hardcastle
Lisa Giocomo
Surya Ganguli

SOCKO @ STANFORD. EDU
KHARDCAS @ STANFORD. EDU
GIOCOMO @ STANFORD. EDU
SGANGULI @ STANFORD. EDU

Stanford University
The medial entorhinal cortex (MEC) exhibits striking grid cell (GC) firing patterns reflecting an animal’s spatial
location. Any such internal code for space fundamentally originates from two sources: (1) a past history of
velocity, which can be integrated to yield position, and (2) current sensory landmarks. Recent work [1] revealed
that landmarks continually correct the internal MEC spatial code, by reducing the overall magnitude of errors
in individual GC firing events. However, the detailed principles governing how idiothetic self-motion cues and
allothetic landmark cues are combined to compute spatial location remain poorly understood. To investigate the
nature of such cue combination, we examined how GC firing patterns change depending on which landmark
(one of four borders in an open field) a mouse most recently encountered. In a population of 115 grid cells
recorded in 14 different mice, we found GC firing patterns were systematically shifted towards the most recently
encountered border, by about 0.2 cm (P<1e-3) for east/west borders and 0.3cm (P<1e-6) for north/south borders.
We developed a Bayesian model that can account for these systematic MEC biases. In our model, observations
from border cell firing continuously update internal representations of space derived from path integration of noisy
velocity signals. We find the concavity of border cell firing as a function of distance from the boundary critically
affects the resultant bias: convex (concave) fields yield a shift away from (toward) the border. By examining
several border cells, we found their firing fields are concave, consistent with our observed GC shift toward the
border. Overall, these results connect the shape of border fields to the direction of systematic biases in GC firing,
raising the intriguing possibility that the MEC performs optimal Bayesian combination of idiothetic and allothetic
information to compute where we are in space. [1] Hardcastle, Ganguli, Giocomo Neuron 2015.

II-110. Feature-specific prediction errors in the macaque fronto-striatal system during reversal learning
Mariann Oemisch
Stephanie Westendorff
Thilo Womelsdorf

MOEMISCH @ YORKU. CA
STEPHANIE . WESTENDORFF @ UNI - TUEBINGEN . DE
THIWOM @ YORKU. CA

York University
When only one stimulus feature among many predicts reward, it is pivotal for an agent to learn which specific
feature is causally linked to changes in reward outcomes. This linkage could be achieved through feature-specific
prediction error (PE) signals, but it has remained elusive whether PEs carry sufficiently detailed feature information
to serve as selective learning signals for updating synaptic weights. We set out to find feature-specific PEs and
track their temporal dynamics in neuronal recordings in lateral prefrontal cortex, anterior cingulate cortex, caudate
head and ventral striatum of the macaque performing a feature-based reversal learning task. This task presented,
peripherally to the gaze center, two moving gratings that differed in color, location, and motion direction. The
animal was rewarded for reporting the motion of the stimulus with the reward-associated color following a Go cue.
The rewarded color was reversed uncued between blocks, with motion and location varying independently of color.
PEs were quantified using a statistical learning model (Smith et al., 2004) and a reinforcement learning model
with Bayesian feature-weighting (Niv et al., 2015). We found feature-specific PE signals in all fronto-striatal areas
with similar rapid onset time. Astonishingly, PE selectivity was as ubiquitous for color, which was the only rewardassociated feature, as it was for location and motion features. Anatomically, feature-specific PE signalling was
most likely in ventral striatum after both, positive and negative outcomes. Functionally, anterior cingulate cortex
was unique by hosting neurons linking the feature selective signalling of PEs with the actual reward predictions

172

COSYNE 2017

II-111 – II-112
prior to the outcome. Our results document that across fronto-striatal brain areas neuronal firing carries feature
information in their PEs that could contribute to tag the specific synapses that need updating during learning,
enabling selective credit assignment in feature space.

II-111. Psychophysical reverse correlation reflects both sensory filters and
the decision-making process
Gouki Okazawa
Roozbeh Kiani

OKAZAWA @ NYU. EDU
ROOZBEH @ NYU. EDU

New York University
Goal directed behavior depends on sensory mechanisms that gather information from the outside world and
decision-making mechanisms that select appropriate behavior based on the sensory information. Behavioral
studies have relied on psychophysical reverse correlation to quantify how noisy stimulus fluctuations influence
behavior. However, past studies have commonly attributed the resulting psychophysical kernels to spatiotemporal filters of sensory processes. Here we show that these kernels also reflect decision-making processes and
can deviate significantly from the true sensory spatiotemporal filters. Psychophysical kernels are guaranteed to
match sensory filters only if decisions are made by filtering sensory inputs and subjecting the result to a static
non-linearity, for example, comparison to a decision criterion, as suggested by signal detection theory (SDT).
However, recent advances in our understanding of the decision-making mechanisms suggest that SDT’s characterization is incomplete. In particular, many perceptual decisions depend on integration of sensory information
toward a decision bound, the integration is subject to urgency and prior signals, the decision bound may not be
static, and the time between the stimulus and choice reflects a combination of decision time and non-decision
time (sensory and motor delays). We show that psychophysical kernels can systematically deviate from sensory
filters if one disregards the nuances of the decision-making process. For example, non-decision time implies that
stimuli presented immediately prior to choice do not influence the choice. That creates an artificial downward
trend in psychophysical kernels of a reaction-time task, which may be mistaken with gradual reduction of sensory
weights over time. However, one can recover the true sensory weights by accounting for the decision-making
mechanism. We suggest that psychophysical reverse correlation reflects both sensory and decision-making processes. Ignoring this insight results in interpretational errors but using it properly turns reverse correlation into a
powerful method for studying both sensory and decision-making mechanisms.

II-112. Functional specialization of information flow between single neurons
across behavioral states
Umberto Olcese
Jeroen Bos
Martin Vinck
Laura van Mourik-Donga
Cyriel Pennartz

U. OLCESE @ UVA . NL
JEROENJBOS @ GMAIL . COM
MARTINVINCK @ GMAIL . COM
A . B . DONGA @ UVA . NL
C. M . A . PENNARTZ @ UVA . NL

University of Amsterdam
Neuronal activity does not vary homogeneously across the brain, but depends on factors such as area, neuronal
type and preceding history. The same is true for the way neurons communicate, although most of the current
knowledge is limited to the mesoscopic level. Recently, we provided a first account of how functional coupling
between neurons (measured in terms of coordinated firing rate modulations) varies across brain states (Olcese
et al., 2016). Remarkably, long-range coupling between excitatory neurons appeared as the main determinant
of the previously reported global loss of connectivity during non-REM sleep. Here we expand this by assessing
information flow (quantified by pairwise transfer entropy between spike trains) between single neurons in rat

COSYNE 2017

173

II-113
primary visual and somatosensory cortex, perirhinal cortex and hippocampus. Rats were trained to perform a
sensory discrimination task in a maze, modulating neuronal activity in all four recorded areas. Recordings were
done during resting periods before and after task performance. Behavioral states were subdivided into active
and quiet wakefulness, and non-REM sleep. We found that, at time scales compatible with direct mono- and
poly-synaptic interactions (2-10 ms), information transfer between neurons whose activity was modulated by the
task was higher than that between non-modulated neurons, in both quiet wakefulness and non-REM sleep, and
specifically for neurons located in different brain areas. Conversely, information transfer during quiet wakefulness
and non-REM sleep at longer time scales – in the range of firing rate fluctuations (600-900 ms) – was higher
for non-modulated neurons than for modulated ones. These results challenge our current understanding of how
the brain’s functional architecture varies across behavioral states. Crucially, we demonstrate: a) that functional
specialization of single neurons influences neuronal communication during awake rest and sleep in a major yet
non-uniform way, and b) that multiple directed-communication architectures coexist in the brain at different time
scales.

II-113. High-dimensional inhibitory activity in visual cortex
Carsen Stringer
Marius Pachitariu
Mario Dipoppa
Matteo Carandini
Kenneth Harris

CSTRINGER @ GATSBY. UCL . AC. UK
MARIUS 10 P @ GMAIL . COM
MARIO. DIPOPPA @ GMAIL . COM
MATTEO @ CORTEXLAB . NET
KENNETH . HARRIS @ UCL . AC. UK

University College London
Networks of excitatory and inhibitory neurons in the brain can exhibit multiple modes of activity, in response
to stimuli or spontaneously. Simulations of neural networks can also exhibit multiple modes, but only if there
are separate inhibitory populations stabilizing each mode. This theory thus predicts that the activity patterns of
inhibitory neurons should be high-dimensional, and aligned to the activity patterns of excitatory neurons. This
prediction seems inconsistent with previous studies which have shown that interneurons receive dense, nonspecific inputs from the local excitatory neurons. We tested directly whether interneuron populations have highdimensional activity, in large-scale recordings from visual cortex done with 2-photon calcium imaging. All neurons
expressed GCaMP6, and either all interneurons (GAD) or selected classes of inhibitory neurons (PV, Sst, or Vip)
were labelled with tdTomato. We discovered that inhibitory activity explored around 20–40 dimensions of variability
during spontaneous activity, and about half of these dimensions aligned significantly to dimensions of excitatory
variability. These dimensions of activity had a wide range of timescales (hundreds of milliseconds to tens of
seconds), and were distributed randomly over the ≈ 0.3mmˆ3 imaged volume. Most of the shared dimensions
could be predicted from 10–20 dimensions of behavioral activity, extracted by PCA from movies of the face,
pupil and whiskers. These behavioral components explained more of the activity variance of inhibitory compared
to excitatory neurons. In contrast, stimulus-driven variance was significantly larger in excitatory compared to
inhibitory neurons.

174

COSYNE 2017

III-1 – III-2

III-1. Behavioral discrimination of natural images correlates with average neural activity in higher visual areas
Douglas Ollerenshaw
Marina Garrett
Peter Groblewski
Justin Kiggins
Derric Williams
Sahar Manavi
Stefan Mihalas
Shawn Olsen

DOUGO @ ALLENINSTITUTE . ORG
MARINAG @ ALLENINSTITUTE . ORG
PETERG @ ALLENINSTITUTE . ORG
JUSTINK @ ALLENINSTITUTE . ORG
DERRICW @ ALLENINSTITUTE . ORG
SAHARM @ ALLENINSTITTUE . ORG
STEFANM @ ALLENINSTITUTE . ORG
SHAWNO @ ALLENINSTITUTE . ORG

Allen Institute for Brain Science
The ability to detect moment-to-moment changes in the visual scene is fundamental to an animal’s ability to
navigate the world. Here we describe a behavioral task in mice for studying the cortical underpinnings of natural
image discrimination. This task is a variant of a go/no-go change detection paradigm, and uses natural images as
the stimuli to be compared by the mouse. Mice were trained to discriminate stimuli presented in sequence with an
intervening gray period of 500 ms, requiring comparison of the currently perceived stimulus with one held in visual
working memory. Similar tasks have been shown to be cortically dependent. Using this change detection task
we tested the ability of mice to discriminate between a subset of natural image stimuli from the recently released
Allen Brain Observatory dataset. Mice showed a remarkably stereotyped pattern of responses across subjects,
suggesting unique features of the image set may elicit similar neural response patterns across subjects. In
order to explore the neural underpinnings of these behavioral observations we examined visually-evoked activity
patterns measured in the cortical visual areas by the Allen Brain Observatory. This dataset includes activity
measurement from thousands of neurons in the cortical visual areas V1, PM, AL, and LM. Initial analyses indicate
a strong correlation between average population level neural responses and the pattern of behavioral responses.
Moreover, the correlation is stronger for the higher visual areas compared to V1. Using image analysis and
modeling we seek to identify which natural image features are encoded by distinct visual areas in order to mediate
the perceptual detection of changes in the visual scene.

III-2. Adaptation of spontaneous activity in V1 during exposure to a novel
stimulus statistics
Gergo Orban1,2
Marcell Stippinger3
Andreea Lazar4
Mihaly Banyai1
Wolf Singer4

ORGERGO @ GMAIL . COM
STIPPINGERM @ GMAIL . COM
ANDREEALAZ @ GOOGLEMAIL . COM
HAGYMASBAB @ GMAIL . COM
WOLF. SINGER @ ESI - FRANKFURT. DE

1 MTA

Wigner RCP
Systems Neuroscience Lab
3 MTA Wigner RCP & MTA TKI
4 Ernst Strungmann Institute for Neuroscience
2 Computational

The visual system is hypothesized to construct an internal model of the visual environment and use this model to
make inferences about upcoming stimuli. The internal model adapts to the statistics of the environment throughout
the lifetime of the animal, a process that can be captured in the shaping of the statistics of spontaneous activity
(SA). In particular, statistics of the SA was shown to be matched to that of the (average) evoked activity (EA) in the
mature, but not in the juvenile primary visual cortex. However, it is not known whether and how these principles
apply when the mature animal is exposed to a novel, albeit limited, set of stimuli for a limited time. We investigated
how the activity statistics of V1 neurons in behaving monkeys is shaped when a set of artificial stimuli are shown
over a single session of training. We used a latent variable model of neuronal activity to assess the statistics

COSYNE 2017

175

III-3 – III-4
of EA and SA under different conditions. Analysis of SA before, during, and after exposure to the sequence of
stimuli reveals a gradual transformation of the statistics of SA during the adaptation process. Statistics of neural
responses to stimuli is undergoing similar changes: while EA early during exposure is more similar to SA before
exposure, it becomes more similar to SA after exposure as adaptation to stimuli progresses. Changes are not due
to less selective neural responses at later phases of training since the fidelity of stimulus encoding is left intact
by the transformations. Crucially, we show that adaptation-induced joint transformation of SA and EA is such
that they become more similar. Our results demonstrate that during a limited exposure to a novel stimulus set,
principles of statistical learning theory can be identified in the transformations of population activity during EA and
SA.

III-3. Physiological compensation for animal-to-animal variability in morphology
Adriane Otopalik
Eve Marder

AOTOPALI @ BRANDEIS . EDU
MARDER @ BRANDEIS . EDU

Brandeis University
Much work has explored animal-to-animal variability and compensation in ion channel expression. Yet, little is
known regarding the physiological consequences of morphological variability across animals. We address this
question in identified neurons with conserved physiologies in the crustacean stomatogastric ganglion (STG). Using custom computational tools, we quantify morphological features of four neuron types (N = 4 for each). These
analyses show the expansive nature of STG neurons, with cable lengths > 10 mm, branch point numbers > 1000,
and a great degree of variability within and across cell types. No single metric or combination of metrics distinguished cell types. Numerous theoretical studies suggest that neuronal morphology is tuned to minimize wiring
and conduction delay of synaptic events1-8. We compared these same STG neuronal structures to synthetic
minimal spanning neurite trees constrained by a wiring cost equation6. We find that STG neurons do not adhere
to prevailing hypotheses regarding wiring optimization principles. We addressed the remaining question of how
reliable physiology can arise in these neurons, given the observed animal-to-animal variability in morphology. We
characterized voltage signal propagation in one STG neuron type, the gastric mill (GM) neuron, utilizing focal glutamate photo-uncaging. We examined GM electrotonic structure by measuring the apparent reversal potentials
(Erevs) of local glutamate responses evoked at 7-20 sites varying in distance from the somatic recording site (N
= 10 neurons). Apparent Erevs were surprisingly invariant (mean CV + SD = 0.04 + 0.01), indicating that GM
neurons are electrotonically compact. Passive cable simulations predict electrotonic length constants > 1.5 mm.
STG circuit function is mediated by graded transmission. Such electrotonically compact structures, in consort
with graded transmission, effectively compensate for the observed morphological variability across animals and
the differential propagation of voltage signals that may otherwise arise from variable synaptic site locations across
animals.

III-4. Flexible decision-making in rats
Marino Pagan1
Diksha Gupta1
Alex Piet1
Carlos Brody2,1

MPAGAN @ PRINCETON . EDU
DIKSHAG @ PRINCETON . EDU
PIET @ PRINCETON . EDU
BRODY @ PRINCETON . EDU

1 Princeton
2 Howard

University
Hughes Medical Institute

Our ability to flexibly select, based on context, the information relevant to guiding our decisions is a fundamental
cognitive process, yet its underlying neural mechanisms are still largely unknown. To address this issue, we have

176

COSYNE 2017

III-5
developed a high-throughput, computer-automated procedure to efficiently train rats to perform a task requiring
flexible, context-dependent selection and integration of sensory information (adapted from Mante et al., Nature,
2013). In our task, rats are presented with a train of randomly-timed auditory pulses, where each pulse is either
played from the right or from the left, and each pulse is either low-frequency or high-frequency. On “location
trials” rats are rewarded for discriminating the prevalent location of the pulses, while ignoring their frequency. On
“frequency trials” rats are rewarded for discriminating the prevalent frequency of the pulses, while ignoring their
location. For any given pulse train, correct performance thus requires selecting the relevant information, depending on task context. Using our automated training procedure we have fully trained 10 rats. Because information
is delivered in randomized, yet precisely-known pulses, multiple statistical methods can be applied to precisely
characterize the animals’ behavior. Model-free analyses of rats’ behavior suggest that the rate of accumulation of
both context-relevant and context-irrelevant sensory information is approximately constant throughout the stimulus
presentation. Additionally, we have developed a 2-dimensional accumulation model which estimates the momentby-moment evolution of accumulated location and frequency information. This model will facilitate quantitative
behavioral modeling, and neural population analyses. In summary, our results demonstrate that rats are a viable
model to study context-dependent feature selection for decision-making, and open the door to the use of the rich
experimental tools available in rodents to dissect the neural mechanisms underlying flexible behavior.

III-5. High dimensional chaos and synchronous irregular dynamics in delayed
spiking networks.
Agostina Palmigiano1
Fred Wolf2
1 Max
2 Max

AGOS @ NLD. DS . MPG . DE
FRED @ NLD. DS . MPG . DE

Planck Institute for Dynamics
Planck Institute for Dynamics and Self-Organization

The dynamical stability of neuronal networks, and the possibility of chaotic dynamics in the brain pose profound
questions to the mechanisms underlying perception. Whether the rapid amplification of differences of nearby
states and the finite-time dissolution of encoded information offer an advantage for brain processing or works in
detriment to it, remains to be understood. Here we advance on the tractability of large, highly heterogeneous neuronal networks of exactly solvable neuronal models with delayed pulse-coupled interactions and study its stability
properties. We develop a framework in which pulse-coupled delayed systems with an infinite dimensional phase
space can be studied in an equivalent system of fixed and finite degrees of freedom. By introducing a single
compartment axon for each neuron, delays arise from the extra steps of integration needed for the variable to
reach threshold. The Jacobian of the equivalent system can be analytically obtained and numerically evaluated
in event-based simulations, giving access to an exact calculation of the entropy production rate of the delayed
network. Profiting from neuronal models with a tunable action potential onset rapidness, find that depending on
the speed of the action potential onset and the level of heterogeneities, the asynchronous irregular regime characteristic of balanced state networks loses stability with increasing delays to either a slow synchronous irregular
or a fast synchronous irregular state for slow and fast onset rapidness respectively. In networks of neurons with
slow action potential onset, the transition to collective oscillations leads to an increase of the exponential rate of
divergence of nearby trajectories and of the entropy production rate of the chaotic dynamics, revealing that chaos
is intensified in the slow synchronous irregular state. The attractor dimension, instead of increasing linearly with
increasing delay as reported in many other studies, decreases until eventually the network reaches full synchrony.

COSYNE 2017

177

III-6 – III-7

III-6. Precise estimates of single-trial neural population state in motor cortex
via deep learning methods
Chethan Pandarinath1,2
Jasmine Collins3
Rafal Jozefowicz3
Sergey Stavisky4
Jonathan Kao4
Mark Churchland5
Matthew Kaufman6
Stephen Ryu
Jaimie Henderson4
Krishna Shenoy4
Laurence Abbott5
David Sussillo3,4

CPANDAR @ EMORY. EDU
JLCOLLINS @ GOOGLE . COM
RAFAL @ OPENAI . COM
SERGEY. STAVISKY @ STANFORD. EDU
JCYKAO @ STANFORD. EDU
MC 3502@ CUMC. COLUMBIA . EDU
MKAUFMAN @ CSHL . EDU
SEOULMAN @ STANFORD. EDU
HENDERJ @ STANFORD. EDU
SHENOY @ STANFORD. EDU
LFA 2103@ CUMC. COLUMBIA . EDU
SUSSILLO @ GOOGLE . COM

1 Emory

University
Institute of Technology
3 Google Brain
4 Stanford University
5 Columbia University
6 Cold Spring Harbor Laboratory
2 Georgia

Neuroscience is experiencing a data revolution in which many hundreds or thousands of neurons are recorded
simultaneously. This often reveals structure in the population activity that is not apparent from single neuron
responses. However, understanding this structure on a single-trial basis is often challenging due to limited observations of the neural population, trial-to-trial variability, and the inherent noise of action potential arrival times.
Here we introduce Latent Factor Analysis via Dynamical Systems (LFADS), a deep-learning method to infer latent
dynamics from simultaneously recorded, single-trial, high-dimensional neural spiking data. LFADS is a sequential
model based on a variational auto-encoder (Kingma & Welling, 2013). By making a dynamical systems hypothesis regarding the generation of the observed data, LFADS reduces observed spiking to a set of low-dimensional
temporal factors, per-trial initial conditions, and inferred inputs. Here we apply LFADS to a variety of datasets
from monkey motor cortex. We show that LFADS’s estimates of neural population state are more informative
about behavioral variables than population activity itself. In addition, LFADS uncovers multiple known dynamic
features of single-trial motor cortical firing rates, including slow oscillations (1-3Hz) that accompany the transition from pre- to peri-movement activity (Churchland et al., Nature 2012), and high-frequency oscillations (15-45
Hz) that occur during the pre-movement period (Donoghue et al., J Neurophys 1998). In cases where the neural data’s dynamics cannot be modeled by an initial state alone (e.g., unexpected perturbations), LFADS infers
time-varying external inputs that correlate with behavioral outcomes. Finally, we apply LFADS to an unstructured
dataset (no precise timing, free-paced reaching movements, no repeated conditions) and show that it uncovers
precise state estimates and inputs from unstructured activity. These results showcase the ability of LFADS to infer
precise estimates of single-trial dynamics on multiple timescales and uncover inputs that correlate with behavioral
choices.

III-7. Sequence replay in model neural networks without Hebbian plasticity
Richard Pang
Adrienne Fairhall

RPANG @ UW. EDU
FAIRHALL @ U. WASHINGTON . EDU

University of Washington
The reactivation of neuronal activity patterns outside the context in which they originally occurred is thought to
play a role in mediating memory, but the mechanisms underlying reactivation are not well understood. Especially

178

COSYNE 2017

III-8 – III-9
mysterious is the replay of sequential activity patterns that occurred in the recent past, since plasticity mechanisms
typically associated with sequences are unlikely to have a large effect on short timescales. Here we explore a
model in which short-term memory of sequences is instead maintained in excitability increases of the nodes in
the sequence, thus increasing the probability of later replay of sequences involving those nodes. We first show
how this can be implemented in a non-plastic network of leaky integrate-and-fire neurons. In a simplified model
we then show that the decoding of past stimulus sequences from future neural sequences improves when the
network connectivity reflects the stimulus transitions, and that in sparse networks, replay can have both high
accuracy and high memory content. Finally, we show that with geometrically organized bidirectional connectivity,
reverse sequence replay can occur as well. Our model highlights fast excitability changes as a possible means to
encode complex spatiotemporal memories.

III-8. Optimal unsupervised Hebbian learning rules for attractor neural networks
Ulises Pereira
Nicolas Brunel

ULISES @ UCHICAGO. EDU
NBRUNEL @ UCHICAGO. EDU

University of Chicago
The attractor neural network (ANN) scenario has been a popular scenario for memory storage in association
cortex, but there is still a large gap between these models and experimental data. In particular, two issues have
received surprisingly little attention: whether the distribution of the learned patterns is compatible with data; and
whether the learning rules used in such models are compatible with data. A recent study (Lim et al, 2015) has
found in IT cortex a distribution of neuronal responses close to lognormal, at odds with bimodal distributions of
firing rates used in the vast majority of theoretical studies; and a Hebbian learning rule dominated by depression
with a non-linear dependence on postsynaptic firing rate. In this work, we study an attractor neural network model
in which external inputs defining the stored patterns have a unimodal distribution, with a family of generalized
Hebbian rules that captures these findings. Using a mean field approach, we show that the learning rule that
optimize storage capacity has: (1) a highly non-linear dependence on the pre and postsynaptic firing rate; (2) a
bias towards depression of the post-synaptic non-linearity; (3) A threshold between depression and potentiation
that is much higher than the mean firing rate. All these features are consistent with the learning rules derived in
(Lim et al, 2015). We derive distributions of firing rates for novel and familiar stimuli during both presentation and
delay period, finding unimodal distributions consistent with data from delay match to sample (DMS) experiments.
Our theory is in good agreement with simulations of large neuronal networks. Our results suggest that learning
rules inferred from data in IT cortex are optimal for memory storage.

III-9. Coding of visual stimuli and attentional state across layers of area V4
Warren Pettine1
Nicholas Steinmetz2
Tirin Moore1
1 Stanford

WARREN . PETTINE @ GMAIL . COM
NICK . STEINMETZ @ GMAIL . COM
TIRIN . MOORE @ GMAIL . COM

University
College London

2 University

Neurons in area V4 are organized in cortical columns and are known to integrate sensory information with behavioral states. Processing of these two disparate inputs within columns of V4 is poorly understood. We examined
the coding of visual stimulus information and the influence of two forms of attentional deployment by single neurons and neuronal populations across superficial and deep cortical layers. Monkeys performed a task in which
they covertly attended to one oriented stimulus while preparing eye movements to another. Here we show that
superficial layer neurons provide greater information about stimulus orientation, whereas deep layer neurons pro-

COSYNE 2017

179

III-10 – III-11
vide greater information about attentional state. In particular, deep layer neurons provided greater information
about eye movement preparation. These results reveal functional divisions between superficial and deep neurons
in coding of visual stimulus and the preparation of eye movement.

III-10. Synaptic inhibition shapes forward suppression in the auditory cortex
of awake mice
Elizabeth AK Phillips
Andrea Hasenstaub

ELIZABETHAKPHILLIPS @ GMAIL . COM
ANDREA . HASENSTAUB @ UCSF. EDU

University of California, San Francisco
Perception is not invariant; sensory systems do not create an exact neural representation of stimuli, but rather
modify these representations on the basis of local-global comparisons, in both time and space (Simons, 1985;
Allman et al., 1985; Bregman, 1990). A simple paradigm in which to study such context-dependent representations is forward suppression (a.k.a forward masking), in which neural responses to a “probe” tone are suppressed,
and sometimes enhanced, by a preceding “masker” tone (Shamma & Symmes, 1985; Calford & Semple, 1995;
Brosch & Schreiner, 1997). Although forward suppression occurs subcortically (Shore, 1995; Malone & Semple,
2001; Nelson et al., 2009), and even in the auditory nerve (Harris & Dallos, 1979), forward suppression is stronger
and lasts for longer durations in cortical neurons (Creutzfeldt, 1980; Fitzpatrick et al., 1999; Miller et al., 2001),
suggesting forward suppression is generated, partly, within the auditory cortex. Several mechanisms have been
proposed to explain forward suppression in the cortex (e.g. synaptic depression, intrinsic adaptation, and synaptic inhibition), but the relative contributions of these mechanisms, especially in awake animals, have not been
identified. Here, we describe forward suppression in the awake mouse, and use optogenetic and computational
tools to disambiguate the possible cellular and synaptic mechanisms. We found that forward suppression is far
more diverse than previously appreciated and varies by cell type. We further combined analysis of variability with
a simple computational model to determine that spike-frequency adaptation plays only a minimal role in shaping
forward suppression. Finally, we identified differential contributions of parvalbumin and somatostatin interneurons
in enhancing, versus reducing, forward suppression, suggesting generalizable mechanisms for context processing
in other cortical systems.

III-11. Diverse timescales of population coding in cortex
Eugenio Piasini1
Caroline Runyan2
Stefano Panzeri1
Christopher Harvey2
1 Istituto

EUGENIO. PIASINI @ IIT. IT
CAROLINE RUNYAN @ HMS . HARVARD. EDU
STEFANO. PANZERI @ IIT. IT
HARVEY @ HMS . HARVARD. EDU

Italiano di Tecnologia
University

2 Harvard

The cortex represents information across widely varying timescales. For instance, sensory cortex encodes stimuli
that fluctuate over milliseconds, whereas in association cortex behavioral choices can require the maintenance
of information over seconds. It is poorly understood how the cortex achieves such diverse coding timescales.
While recent work has identified different timescales in features intrinsic to individual neurons, coding timescales
in populations of neurons have not been studied, and population codes have not been compared in depth across
cortical regions. Here we discovered that population codes are essential to achieve long and diverse coding
timescales and that codes differ fundamentally between sensory and association cortices. We compared coding
for sensory stimuli and behavioral choices in auditory cortex (AC) and posterior parietal cortex (PPC) as mice
performed a sound localization task. Information about the auditory stimulus was present in AC but not PPC,
whereas both regions contained information about the choice. Although both regions coded information by tiling

180

COSYNE 2017

III-12 – III-13
in time neurons that were transiently informative for less than approx. 200 milliseconds, the areas had major
differences in functional coupling between neurons, measured as activity correlations that could not be explained
by task events. Coupling among PPC neurons was strong, extended over long time lags, and contributed to a
long timescale population code characterized by consistent representations of choice lasting over two seconds.
In contrast, coupling among AC neurons was weak, shorter-lived, and resulted in moment-to-moment fluctuations
in stimulus and choice information. Our results suggest that population coupling is a variable property that affects coding timescales: relatively uncoupled activity in sensory cortex is key for signals that change rapidly to
code temporally variable stimuli, whereas highly coupled activity in association cortex appears critical to form a
consistent signal from which temporally integrated information can be read out instantaneously to drive behavior.

III-12. Rats can optimally discount evidence for decision-making in a dynamic
environment
Alex Piet1
Carlos Brody2,1
Ahmed El Hady1

ALEXPIET @ GMAIL . COM
BRODY @ PRINCETON . EDU
AHADY @ PRINCETON . EDU

1 Princeton
2 Howard

University
Hughes Medical Institute

How are choices made within constantly-changing noisy environments? Previous work has characterized evidence accumulation for decision-making in static environments, finding that rats and humans can accumulate
evidence over long timescales (Brunton 2013). However, real-world decision-making involves environments with
statistics that change over time. In a changing environment, the optimal evidence accumulation strategy involves
the additional task of discounting old evidence that may no longer inform the current state of the world (Glaze
2015, Veliz-Cuba 2016) and, if necessary, changing one’s mind to reflect the changing world. Can animals adapt
the timescale over which they accumulate evidence to optimally reflect the statistics of a changing world? What
neural mechanisms could underlie this capacity? In a changing environment, the optimal timescale for evidence
discounting is a function of variability in the environment and the reliability of the evidence within each environment. We trained rats on an auditory pulse-based decision-making task in a dynamically changing environment.
Using a pulse-based task allows us to separate evidence reliability into stimulus statistics and noise in the sensory transduction process. Using high-throughput behavioral training and quantitative modeling, we found that
rats can discount evidence in a dynamic environment. Furthermore, once we account for sensory noise, their discounting timescale is close to optimal. This study establishes a quantitative behavioral framework with which one
can investigate neural mechanisms underlying changes of mind and adaptive nature of evidence accumulation
timescales.

III-13. A mechanism for self-organized error-correction of grid cells by border
cells
Eli Pollock1
Niral Desai2
Xuexin Wei3
Vijay Balasubramanian4

EPOLLOCK @ MIT. EDU
DESAINI @ SAS . UPENN . EDU
WEIXXPKU @ GMAIL . COM
VIJAY @ PHYSICS . UPENN . EDU

1 Massachusetts

Institute of Technology
of Texas at Austin
3 Columbia University
4 University of Pennsylvania
2 University

Grid cells are thought to be involved in path integration, and noise in this process is expected to disrupt grid

COSYNE 2017

181

III-14
firing patterns, e.g. by producing stochastic drift in grid attractor networks. Recently, Hardcastle et al. (Neuron,
2015) proposed that border cells may provide one mechanism for correcting such drift. We construct a model
in which experience-dependent Hebbian plasticity during exploration allows border cells to self-organize their
responses, while also learning connectivity to grid cells which maintain their activity through an attractor network.
We show that border cells in this learned network effectively correct for grid drift despite stochasticity of border
cell firing. This error-correction is robust with respect to environmental shape including squares and circles.
Furthermore, it survives insertion of barriers within an enclosure (consistent with Solstad et al., 2008) even though
a given border cell can fire ambiguously at multiple boundaries with the same allocentric orientation (Solstad et
al, 2008; Lever et al, 2009). In our mechanism, the learned border-grid connectivity pattern compensates for
such ambiguities. Upon deformation of an environment, e.g., by shrinking or changing shape, the error correction
initially fails and grid drift resumes. However, in our model the border-grid connectivity adapts to boundaries of the
deformed environment, restoring error correction after a characteristic timescale. Our results demonstrate a class
of self-organized mechanisms that achieve robust path integration. These mechanisms predict that: (a) disrupting
synaptic plasticity between grid cells and border cells will cause grid patterns to drift in a random walk, and (b)
deforming an environment will initially lead to grid drift, which is subsequently stabilized.

III-14. Time-warped PCA: simultaneous alignment and dimensionality reduction of neural data
Ben Poole1
Alexander Williams1
Niru Maheswaranathan1
Byron Yu
Gopal Santhanam2
Stephen Ryu
Stephen A. Baccus1
Krishna Shenoy1
Surya Ganguli1

POOLE @ CS . STANFORD. EDU
AHWILLIA @ STANFORD. EDU
NIRUM @ STANFORD. EDU
BYRONYU @ CMU. EDU
GOPALS @ STANFORD. EDU
SEOULMAN @ STANFORD. EDU
BACCUS @ STANFORD. EDU
SHENOY @ STANFORD. EDU
SGANGULI @ STANFORD. EDU

1 Stanford
2 Google

University
X

Analysis of multi-trial neural data often relies on rigid alignment of neural activity to stimulus triggers or behavioral
events. However, activity on a single trial may be shifted and skewed in time due to differences in attentional
state, biophysical kinetics, and other unobserved latent variables. This temporal variability can inflate the apparent dimensionality of data and obscure our ability to recover inherently simple, low-dimensional structure. For
example, small temporal shifts on each trial introduces illusory dimensions as revealed by principal component
analysis (PCA). We demonstrate the prevalence of these issues in spike-triggered analysis of retinal ganglion
cells and in primate motor cortical neurons during a reaching task. To address these challenges, we develop a
novel method, time-warped PCA (twPCA), that simultaneously identifies time warps of individual trials and lowdimensional structure across neurons and time. Our method contains a single hyperparameter that trades off
complexity of the temporal warps against the dimensionality of the aligned data. Furthermore, we identify the
temporal warping in a data-driven, unsupervised manner, removing the need for explicit alignment with predefined variables. We apply twPCA to motor cortical data recorded from a monkey performing a center-out delayed
reaching task. The learned warpings can explain 70% of the variability in reaction time. Time-warped PCA is
broadly applicable to a variety of neural systems as a method for disentangling temporal variability across trials
as well as discovering underlying neural dynamics and structure of interest.

182

COSYNE 2017

III-15 – III-16

III-15. Linking normative models and methods for neural systems identification
Johannes Burge1
Priyank Jaini2
1 University
2 University

JBURGE @ SAS . UPENN . EDU
PJAINI @ UWATERLOO. CA

of Pennsylvania
of Waterloo

Understanding how the nervous system exploits task relevant properties of sensory stimuli to perform natural
tasks is central to the study of perceptual systems. Recently, a Bayesian ideal observer method was developed
for task-specific dimensionality reduction called Accuracy Maximization Analysis (AMA). AMA returns the encoding filters (receptive fields) that extract the most useful stimulus features for specific estimation and categorization
tasks. Unfortunately, in its original form, AMA’s compute time is quadratic in the number of stimuli in the training
set, rendering it impractical for large scale problems without specialized computing resources. Here, we develop
AMA-Gauss, a new more practical form of AMA that reduces compute time from quadratic to linear in the number
of stimuli by incorporating the assumption that the conditional filter responses are Gaussian distributed. First,
we verify the expected compute time decreases with three fundamental tasks in early vision: binocular disparity
estimation, retinal speed estimation, and motion-in-depth estimation. Second, we demonstrate that the taskspecific receptive fields returned by AMA-Gauss closely approximate the properties of receptive fields in cortex.
Third, we show that the Gaussian assumption is justified for all three tasks with natural stimuli and biologically
realistic contrast normalization. Fourth, we show that quadratic computations are required to compute the likelihood function and posterior probability distribution over the latent variable. Fifth, we make explicit the formal
similarities between AMA-Gauss and the Generalized Quadratic Model (GQM), a recently developed method for
neural systems identification. Together, these results provide a normative explanation for why energy-model-like
(i.e. quadratic) computations account well for the response properties of neurons involved in these tasks. These
developments should help accelerate research with natural stimuli, deepen our understanding of why classic descriptive models have proved successful, and improve our ability to evaluate results from subunit model fits to
neural data.

III-16. Optimal population coding depends on the source of noise
Kai Roeth
Julijana Gjorgjieva

KAI . ROETH @ BRAIN . MPG . DE
GJORGJIEVA @ BRAIN . MPG . DE

Max Planck Institute for Brain Research
Information in neuronal circuits is processed by many different cell types, but it is unknown how these cell types
work together to achieve that. We use information theoretic measures to offer a plausible explanation: neurons
diversify their responses to maximize the information about stimuli they encode given biophysical constraints.
Previous work has made specific assumptions about how noise at different locations in the circuit limits stimulus encoding. We show that different optimal coding solutions of a neuronal population (of more than two cells)
emerge as we change the location and strength of the noise. We use mutual information (MI) to examine population coding by neurons that jointly code the same stimulus with monotonically increasing firing rate nonlinearities.
Maximizing MI is technically difficult, so previous studies have focused on single neurons, or assumed infinite encoding times. We derive optimal population coding in the limit of short coding windows appropriate for biological
systems and in the presence of different noise sources. We examine two sources of noise: output noise following
Poisson statistics, and input noise arising from shared presynaptic inputs. Similar to the case of one neuron, the
structure of the optimal nonlinearity for each neuron in a population is discrete, and the number of firing levels
increases through a set of bifurcations as a function of the noise. When only output noise is present, all neurons
simultaneously transition to more firing levels as the output noise decreases. With increasing input noise, the
nonlinearities transition gradually: first all neurons have the same nonlinearity, then one by one the individual
neurons diversify their nonlinearities. Our results show that threshold distributions that are optimal when noise is

COSYNE 2017

183

III-17 – III-18
only due to Poisson variability in the spiking output, may be suboptimal when input noise also corrupts the signal
even before reaching the nonlinearity.

III-17. Neural implementation of change rate inference in dynamic environments
Adrian Radillo1
Alan Veliz-Cuba2
Kresimir Josic1
Zachary P Kilpatrick3

ADRIAN @ MATH . UH . EDU
AVELIZCUBA 1@ UDAYTON . EDU
JOSIC @ MATH . UH . EDU
ZPKILPAT @ COLORADO. EDU

1 University

of Houston
of Dayton
3 University of Colorado
2 University

In nature, an organism’s most rewarding choices can vary in time in ways that are a priori unknown. Recent
experiments have probed how mammals make decisions when they must learn the statistics of the environment
to make a choice based on noisy evidence. Such experimental settings improve decision-making theories to
increase their relevance for natural environments. Normative models of evidence accumulation are a first step
in quantifying decision-making processes. Earlier optimal online algorithms describe ideal observers that detect
changepoints to infer the environmental hazard rate. Using similar principles, we recently derived an online optimal
algorithm wherein an observer infers both the present state and the transition rates of a Hidden Markov Model
(HMM). However, the storage space needed to implement our algorithm, for a general transition matrix, increases
polynomially in time. Equivalent continuous-time models do not lead to practical neural implementations as they
are described by an infinite set of stochastic differential equations. It is thus unclear how such inference may
be implemented in neural networks. To address this problem we apply moment closure techniques to drastically
reduce the dimensionality of our model. We show that this approximation is valid over a wide parameter range.
Our approach also leads to a plausible neural implementation of the algorithm, in the limit of low observational
noise: We describe a neural population rate model which uses rate-correlation based plasticity to encode the
change rates of the environment in the excitatory connections between populations. We thus illustrate a general
approach to developing approximations to evidence accumulation processes that can be implemented in neural
circuits.

III-18. Hunger-dependent enhancement of food cue responses in mouse postrhinal cortex and lateral amygdala
Rohan Ramesh1
Christian Burgess2
Arthur Sugden2
Kirsten Levandowski2
Margaret Minnig2
Henning Fenselau2
Bradford Lowell2
Mark L Andermann2,1

RAMESH @ FAS . HARVARD. EDU
CBURGESS @ BIDMC. HARVARD. EDU
ASUGDEN @ BIDMC. HARVARD. EDU
KLEVANDO @ GMAIL . COM
MINNIG . M @ HUSKY. NEU. EDU
HFENSELA @ BIDMC. HARVARD. EDU
BLOWELL @ BIDMC. HARVARD. EDU
MANDERMA @ BIDMC. HARVARD. EDU

1 Harvard
2 Beth

University
Israel Deaconess Medical Center

The needs of the body can direct behavioral and neural processing toward motivationally relevant sensory cues.
For example, humans attend more to food cues in hungry versus sated states. In modern society, where food cues
are ubiquitous, this can lead to increased food intake and adverse health effects. Human neuroimaging studies

184

COSYNE 2017

III-19
consistently demonstrate hunger-dependent enhancement of neural responses to visual food cues versus nonfood cues in temporal association cortex, but not in early visual cortex. To obtain a cellular-level understanding
of these hunger-dependent cortical response biases, we performed two-photon calcium imaging in postrhinal
association cortex (POR) and primary visual cortex (V1) of behaving mice across days and across slowly changing
hunger states. As in humans, POR, but not V1, exhibited a bias toward food-associated cues that was abolished
by satiety. This emergent bias was mirrored by the innervation pattern of amygdalo-cortical feedback axons. We
imaged the same amygdalo-cortical feedback axons across slowly changing hunger states and found that these
axons exhibited even stronger food cue biases and sensitivity to hunger state, suggesting a role for POR and its
inputs from the lateral amygdala in biasing sensory processing to motivationally relevant stimuli. The observed
POR bias to motivationally-relevant cues was consistent across different drive states and different visual cueoutcome associations. Our data suggest that POR integrates information about basic stimulus features and
motivational salience from early visual areas and from subcortical sources such as the amygdala in order to
enhance processing of motivationally relevant sensory cues.

III-19. What can we learn about natural vision from color tuning curves?
Pavan Ramkumar1,2
Hugo Fernades1
Matthew Smith3
Konrad Kording1

PAVAN . RAMKUMAR @ GMAIL . COM
HUGOGUH @ GMAIL . COM
SMITHMA @ PITT. EDU
KOERDING @ GMAIL . COM

1 Northwestern

University
Institute of Chicago
3 University of Pittsburgh
2 Rehabilitation

Neural computations in the visual cortex transform patterns of light into visual percepts. Relative to early (e.g.
oriented edges in striate cortex) or late (e.g. objects in inferior temporal cortex) stages, computations in the
intermediate stages (e.g. colors or textures in V4) are the least understood. Here, we ask how color tuning in V4
neurons characterized using full-field hue images generalizes to monkeys freely viewing natural scenes. Mapping
color tuning in natural scenes is hard because we do not know the computational rules that transform the foveated
image patch to a single perceived hue. To circumvent this problem, we take a prediction first approach by fitting
a model based on (1) hue histograms, and (2) convolutional neural networks (CNN) trained on object recognition
tasks to predict spike counts from natural images. Once fit, we ask the model to predict spike counts to a sweep
of previously unseen full-field hue images (artificial stimuli). This prediction can then be directly compared against
experiments using the same artificial stimuli. We present three main findings using this approach. First, we could
significantly predict spike counts of 25/90 V4 neurons from natural images. A CNN-based model explained up to
3 times as much variance in spike counts than models based on hue histograms. Second, all these neurons were
significantly tuned to artificial stimuli. However, on average the CNN-model based tuning curve predictions did
not correlate with experimental tuning curves (Pearson’s r = 0.01 ± 0.05; mean ± SEM ). Third, these neurons
clustered into two groups: those that were more strongly driven by artificial stimuli, and those that were more
strongly driven by natural images. Our results suggest that findings in higher cortical areas such as V4 obtained
with artificial stimuli may not meaningfully generalize to natural images.

COSYNE 2017

185

III-20 – III-21

III-20. Spatiotemporal characteristics of brain regions modulated by the difficulty of mental arithmetic
Michael Randazzo
Youssef Ezzyat
Michael Kahana

MICHAEL . RANDAZZO @ UPHS . UPENN . EDU
YEZZYAT @ SAS . UPENN . EDU
KAHANA @ PSYCH . UPENN . EDU

University of Pennsylvania
Mental arithmetic is a critical ability for daily life and may be a central component of human intelligence. Understanding how the brain carries out mental arithmetic could offer insight into the involvement and coordination
of other essential cognitive functions, including working memory, semantic retrieval, and numerical perception.
A substantial body of research, predominantly utilizing functional magnetic resonance imaging, has been dedicated to identifying active brain areas during varying types of calculation; however, the neural dynamics have not
yet been completely elucidated. Here, we use intracranial electroencephalography to determine how arithmetic
problem difficulty modulates neural activity across the brain. Two hundred twenty-nine patients with intractable
epilepsy undergoing seizure monitoring performed arithmetic problems, in which they were required to add three
single-digit numbers. Response time to correctly answer the presented expression correlated with factors considered to be related to trial difficulty, for example the total sum of the digits, and was subsequently used to separate
problems as easy or difficult. We compared spectral power in low and high frequencies between easy and difficult
equations and found a pattern consistent across regions within frontal, temporal, and parietal cortices. Difficult
trials exhibited reduced low frequency power (3 – 17 Hz) and enhanced high frequency power (56 – 180 Hz)
throughout the response interval, most prominent in the inferior and middle frontal gyri. Other areas previously
shown to support mental arithmetic, including hippocampus and medial temporal lobe cortex, did not appear to
be significantly modulated by difficulty. Our results provide evidence from a large subject population with extensive electrode coverage that areas associated with working memory and numerical interpretation display greater
activation and disengagement from resting rhythms during difficult arithmetic, while areas responsible for memory
retrieval are unchanged. These findings help further determine the neural basis of mental arithmetic and the
contributions of other basic cognitive abilities.

III-21. The hippocampus as a predictive recurrent neural network
stefano recanatesi1
Greg D Wayne2
Mattia Rigotti3

STEFANO. RECANATESI @ GMAIL . COM
GREGWAYNE @ GOOGLE . COM
MR 2666@ COLUMBIA . EDU

1 weizmann

institute of science
DeepMind
3 IBM TJ Watson Research Center
2 Google

Modern investigations on the role of the hippocampal system are typically being pursued along two largely independent venues: a spatial navigation view, positing that the hippocampus is part of a complex dedicated to localization in the environment, and a declarative memory view, focusing on rapid encoding and retrieval of episodic
experience. Here we investigate a theoretical proposal that tries to reconcile these apparently contrasting views:
that the hippocampus supports a semantic relational network organizing semantically related episodes in the service of sequential planning (Eichenbaum and Cohen, 2014). In particular, we provide an algorithmic grounding to
this idea in the form of a Recurrent Neural Network (RNN) whose task is to predict future observations in partially
observable environments, based on current observations and the actions taken by an actor module. We tested our
model by simulating an agent navigating a linear track and a squared 2d arena, and receiving only partial observations about its location. We hypothesized that, in order to accurately predict future observations, our RNN model
had to learn to generate recurrent states that usefully summarize the history of its inputs, which, in a Markovian
navigation task, would correspond to representing the current location. Indeed, our predictive training procedure
leads to recurrent activations that are reminiscent of the well-known hippocampal physiological observations of

186

COSYNE 2017

III-22 – III-23
place cells, border cells, and head-direction cells. Moreover, we show that these learned representations provide
good generalization performance in reinforcement learning navigation tasks in partially observable environments.
Our model supports the unifying view that both, place-related activity observed in the hippocampus during spatial
navigation, as well as its involvement in episodic memory formation, could be a consequence of its role as a
semantic relational network (Eichenbaum and Cohen, 2014), specifically via the computational mechanism of a
recurrent system trained with predictive coding.

III-22. Quantifying the information rate of sensory feedback
Julien Rechenmann
Joseph E O’Doherty
Philip Sabes

RECHENMANN @ GMAIL . COM
JOEYO @ PHY. UCSF. EDU
SABES @ PHY. UCSF. EDU

University of California, San Francisco
A recent goal in neuroprosthesis development is to create closed-loop Brain-Machine Interfaces (BMIs) that both
decode movement intentions from brain activity and encode artificial sensory feedback with stimulation. While it
has become routine to estimate the amount of information we can decode from the brain, there are still no tool to
quantify the amount of information the brain can extract from various feedback modalities. To address this gap,
we have employed the Critical Stability Task (CST) to quantify closed-loop sensorimotor performance (Jex, 1966).
This task has been recently used in the field of BMI (Quick et al., 2014). Our innovation is to experimentally
manipulate the quality of visual feedback, both to diminish its reliability and to allow for the delivery of quantifiable
information rates. We tested the performance of human and non-human primate subjects in these feedback
regimes and estimated the mapping between the task-relevant sensory feedback information rate (in bits/s) and
task performance. For all subjects, we found that CST performance depends on the information rate of the signal,
validating the approach. The functional dependence is well modeled by a sigmoid, with a peak inflection at about
15 bit/s. Based on these results, and those of Quick et al (2014), we propose that the CST can be used to evaluate
the sensory performance of any closed-loop control system, providing a generic, validated estimate of information
rate and a common currency for comparing the performance of different feedback schemes.

III-23. Evidence for a dynamical network underlying self-timed actions in macaque
frontal cortex
Evan Remington
Mehrdad Jazayeri

EREMING @ MIT. EDU
MJAZ @ MIT. EDU

Massachusetts Institute of Technology
Recent work has provided support for a hypothesis that brain networks responsible for planning and generating
actions can be understood as dynamical systems that generate “neural trajectories” of firing rates in low dimensional subspaces. Two common assumptions of such models is that (1) activity is governed by recurrent dynamics,
and (2) the evolution of the activity is fully determined by current firing rates. Consequently, absent a change in
input, averaged neural trajectories generating different actions must have different initial conditions and should be
non-intersecting thereafter. We aim to validate these assumptions in recordings from dorsomedial frontal cortex
(dMFC) of monkeys trained to produce self-timed saccades at delays equal to randomly presented “sample” intervals. The task provides a particularly powerful test for these model assumptions because the dynamics precede
the action and the action produced is held constant. We found that dMFC population responses associated with
different produced intervals evolved along non-intersecting trajectories ordered in a smooth manifold toward a
putative generalized threshold near the time of response. Crucially, trial-by-trial variability of produced intervals
was associated with activity which respected the geometric organization of the trajectories within the manifold.
Finally, neural activity in trials with longer response intervals evolved more slowly, suggesting that the speed of

COSYNE 2017

187

III-24 – III-25
the dynamics depended on the position within the manifold. Together, these findings suggest that neural activity
associated with the temporal control of self-initiated movements can be powerfully captured within the framework
of dynamical systems.

III-24. An inhibitory neural circuit for reliable coding in mouse visual cortex
Rajeev V Rikhye
Ming Hu
Mriganka Sur

RVRIKHYE @ MIT. EDU
MING HU @ MIT. EDU
MSUR @ MIT. EDU

Massachusetts Institute of Technology
Cortical neurons often respond to repetitions of identical stimuli with highly variable spike trains. Under certain
conditions, however, neurons can generate reliable responses, implying that this variability can be modulated to
suit the processing needs of the network. Here we sought to elucidate the circuit mechanisms underlying this
process; specifically, the role of different interneuron (IN) subtypes in reducing shared noise. To this end, we
developed a novel intersectional genetic strategy and used dual-color calcium imaging to record from PV and
SST-INs simultaneously in the same neural population in layer 2/3 of V1 in awake mice. We found that SSTINs responded more reliably to full-field natural movies than PV-INs. Additionally, in reliably-processed movies,
SST and PV-INs were strongly temporally correlated, with SST activity appearing after PV activity. To test the
necessity of this temporal relationship for reliable coding, we conditionally expressed ChR2 in either PV or SSTINs and activated these cells at different times during a movie. Activating SST-INs when pyramidal neurons were
unreliable increased reliability and reduced noise correlations. Additionally, increasing SST inhibition caused a
strong, transient suppression of PV-INs. Mimicking this suppression by expressing Arch in PV-INs also improved
pyramidal cell reliability, but did not alter noise correlations. Therefore, these results suggest that SST-INs act via
the SST-PV circuit to modulate variability through a combined effect of increased dendritic inhibition and reduced
somatic inhibition. Finally, we trained mice on a natural movie discrimination task, and introduced uncertainty
in the stimulus by altering the statistics of the target movie. Activating SST-INs improved the ability of mice
to discriminate the target movie, whereas increasing PV inhibition decreased accuracy. This improvement in
discriminability correlated well with an increase in reliability. Taken together, our work identifies an essential role
for the SST-PV circuit in modulating the fidelity of visual processing.

III-25. Closed-loop stimulation reveals modulation of somatosensory cortex
excitability by active sensing
Jason Ritt1
Smrithi Sunil1
Joseph Schroeder1
Vincent Mariano1
Gregory Telian2
1 Boston

JRITT @ BU. EDU
SSUNIL @ BU. EDU
JOE . SCHROEDER @ GMAIL . COM
VINN 1017@ BU. EDU
GITELIAN @ BERKELEY. EDU

University
of California, Berkeley

2 University

Active sensing allows closed-loop behavioral selection of information during sensory acquisition. The rodent
whisker system provides a well-characterized model, analogous to active touch in humans, for studying active
touch. We developed a real time feedback system that delivers optogenetic stimulation to somatosensory cortex
(S1), time locked to whisking as estimated through facial electromyography (EMG). Water-restricted mice were
trained to alternately traverse a linear track to obtain a water reward. Mice were implanted over SI with custom
hyperdrives allowing neural recording and stimulation, and with bilateral EMG electrodes to estimate whisker
motions. Using real-time processing of EMG and SI activity, we delivered closed-loop optogenetic stimulation to SI

188

COSYNE 2017

III-26 – III-27
locked to whisker motion, creating an artificial channel of sensory input. In particular, we investigated differential
processing of inputs during different phases of active sensing motions: whisker protractions and retractions.
Stimulation of excitatory neurons in S1 lead to an increase in regularity of whisker motions, with fast effects
on the next whisk following stimulation. Stimulation also induced a rapid increase in neural activity when timed
with protractions, but not when timed with retractions. This cyclic modulation of S1 may suggest information is
more likely encoded while the whiskers are in forward motion. We further investigated stimulation of inhibitory
interneurons, which resulted in a decrease in neural activity during protractions, without significant change in
neural activity during retractions. Together, these results reveal cyclic modulation of both excitation and inhibition
in S1 with each whisk, such that downstream areas may show preferential tuning to S1 timing relative to selfmotion, and a “window of opportunity” effect. Whisker motion did not show strong changes with recruitment of
inhibition, possibly due to the local innervation of interneurons. Overall, sensory cortex likely plays a role in guiding
sensory motions, enhanced by encoding incoming information within the context of self-motion.

III-26. Markov transitions between attractor states in a recurrent neural network
David Rolnick1
Jeremy Bernstein2
Ishita Dasgupta3
Haim Sompolinsky4

DROLNICK @ MATH . MIT. EDU
BERNSTEIN @ CALTECH . EDU
ISHITADASGUPTA @ G . HARVARD. EDU
HAIM @ FIZ . HUJI . AC. IL

1 Massachusetts

Institute of Technology
Institute of Technology
3 Harvard University
4 Hebrew University of Jerusalem
2 California

The development of probabilistic theories of human cognition [Griffiths et al. 2010] has raised the question of how
the necessary probabilistic transitions could be represented neurally. In many tasks, the necessary transitions are
Markov, where the next state depends stochastically on the previous one. This stochasticity carries information
and is distinct from noise in our perception and representations. Hopfield networks represent an effective model for
storage and representation that is largely immune to such noise: partial or corrupted sensory percepts converge to
the uncorrupted percept as long as they lie within the percept’s basin of attraction. We present a network capable
of making Markov transitions between temporarily stable attractor states, providing a step towards stochastic
computation that still ensures noise-robust representations of states. Our architecture involves conditioning state
transitions on the state of a separate part of the network that samples its activity uniformly at random. This is
analogous to the reparameterization trick [Kingma, Welling 2014] used in machine learning. Since transitions are
dependent on the combined state of two separate subnetworks, a random expansion as in [Barak, Rigotti, Fusi
2013] is used to increase discriminability. Markov chain Monte Carlo methods [Neal 1993] allow us to engineer
a Markov chain with stationary distribution equal to any distribution of interest. Therefore our work could be
extended to form a basis for neurally plausible probabilistic inference on a discrete state space.

III-27. 3D rotations with recurrent neural networks
Herve Rouault1,2
Alon Rubin3
Sandro Romani1,2

ROUAULTH @ JANELIA . HHMI . ORG
ALON . RUBIN @ WEIZMANN . AC. IL
ROMANIS @ JANELIA . HHMI . ORG

1 Janelia

Research Campus
Hughes Medical Institute
3 Weizmann Institute of Science
2 Howard

COSYNE 2017

189

III-28
Spatial navigation is an ethologically relevant behavior that requires online memory processing to guide decisions
ultimately leading to an intended goal. The integration of linear and angular self-motion apt to maintain an updated
representation of spatial location relative to a reference point (path integration), is a fundamental component of
navigation for species ranging from insects to mammals. A recent study in bats uncovered the existence of head
direction (HD) cells representing the orientation of bats in 3D, a major component of path integration theories.
While orientation coding in 2D has been thoroughly studied, extensions of models to 3D orientation coding are
hindered by computational challenges; for instance, unlike 2D rotations, 3D rotations are non-commutative: the
same rotations applied in a different order would result in different outcomes. We leveraged methods from the
representation theory of groups and developed a neural network model that exhibits activity patterns mapped continuously to the group of 3D rotations (SO(3)). The choice of a family of synaptic couplings between the neurons
allows us to obtain exact expressions for the stationary solutions of the network dynamics in the form of persistent localized activity. In addition, we are analyzing neuronal recordings with the aim of discriminating between
alternative representation of 3D orientations. To our knowledge this is the first example linking the theory of noncommutative Lie groups to the theory of neural networks, a link that allowed us to study a behaviorally relevant
computation. Our model is not restricted to the encoding of orientations of an animal. We took advantage of the
generality of 3D rotations encoding to implement a simple model of recognition of 3D shapes, which accounts for
the psychophysics observation of “mental rotations”.

III-28. Balanced excitation and inhibition are needed for robust neuronal selectivity and associative memory
Ran Rubin1
Laurence Abbott1
Haim Sompolinsky2

RR 2980@ CUMC. COLUMBIA . EDU
LFA 2103@ CUMC. COLUMBIA . EDU
HAIM @ FIZ . HUJI . AC. IL

1 Columbia
2 Hebrew

University
University of Jerusalem

Many neural circuits in the cerebral cortex operate in a regime in which strong excitatory synaptic currents are
roughly balanced by inhibition, resulting in a net input that is considerably smaller than either of these components individually. However, the benefit of excitation-inhibition balance has been demonstrated mostly in specific
circumstances but not for general information processing tasks, raising the question why would neural circuits
evolve to generate strong excitation and inhibition currents simply to be canceled by the circuit dynamics. Here,
we provide a novel constructive computational reason for strong, balanced excitation and inhibition. We characterize a neuron’s basic task as discriminating between patterns of input to which it should respond by firing action
potentials and other patterns which should leave it quiescent. To study this form of neuronal response selectivity
we use the well known perceptron model, analyzing the implications of adding four realistic features not previously considered together: 1. non-negative input, corresponding to firing rates; 2. a fixed membrane potential
threshold for neuronal firing; 3. synapses that are either excitatory or inhibitory and have limited total synaptic
strength; and 4. two sources of noise, input and output noise, representing, respectively, fluctuations arising from
variable stimuli as well as processes within the neuron or noisy inputs unrelated to the task. Our work shows
that strong and balanced synaptic input is needed for neurons to generate selective responses that are robust not
only to input noise but also to output noise and is crucial for the stability of memory states in associative memory
networks. Unlike previous theoretical work, in which the recurrent feedback adjusts dynamically to ensure the balance of excitatory and inhibitory synaptic currents, this novel type of balanced networks emerges automatically
from synaptic learning in the presence of noise.

190

COSYNE 2017

III-29 – III-30

III-29. To see or not to see: Cortico-collicular circuit dynamics in visual perception
Sarah Ruediger1,2
Massimo Scanziani1,2
1 Howard

SRUEDIGERLEE @ UCSF. EDU
MASSIMO @ UCSF. EDU

Hughes Medical Institute
of California, San Francisco

2 University

Behavioral responses to sensory stimuli are variable. It has been hypothesized that behavioral variability may result from random fluctuations of brain activity that could add noise to the processing of sensory stimuli. However,
changes of internal brain states that modulate behavior based on demand, also contribute to behavioral variability.
There is still a major gap in our understanding of the link between behavioral variability and the dynamics of brain
activity. We trained mice to detect and report luminance contrast changes in the visual scene. This behavior is
driven by the superior colliculus (SC), a central node in the transformation of sensory information to motor commands. The SC receives a direct retinal input as well as inputs from sensory and higher-order cortical areas and
thus provides an ideal entry point to address whether the origin of behavioral variability arises from noisy sensory
processes or from the impact of cortical-collicular projections. We obtained psychometric functions to determine
the threshold contrast eliciting behavioral responses and combined in vivo electrophysiology with optogenetics.
We found that the SC plays a critical role in contrast change detection while visual cortex, through its impact on
the SC, modulates the sensitivity by decreasing the threshold of reported contrast changes. Furthermore, we
identify a link between the dynamics of neural activity in the SC and the variability of behavior. Together, our
results shed light on the circuit mechanisms underlying behavioral variability and the function of cortico-collicular
interactions.

III-30. Limits on fast, high-dimensional information processing in recurrent
circuits
Virginia Rutten
Guillaume Hennequin

VMSR 2@ CAM . AC. UK
G . HENNEQUIN @ ENG . CAM . AC. UK

University of Cambridge
Fast and reliable processing of information in the brain is critical for decision-making. Local cortical circuits
receive high-dimensional inputs, and produce high-dimensional outputs [Gao & Ganguli (2015)]. What are the
fundamental limits on rapid multiplexing in recurrent neuronal networks in the presence of either strong noise or
weak signals, and what are the main tradeoffs? Previous work has characterized the information capacity of model
neuronal networks in the context of short-term memory, and only for one-dimensional input signals [Ganguli et al.
(2008); Lim and Goldman (2012)]. Here, we give a complete characterization of the Shannon information capacity
for the broad class of normal linear networks. These networks are equivalent to a set of decoupled information
channels that fade and/or oscillate with varying time constants. We calculate the optimal time constants for each
channel as a function of the fraction of the total input signal variance that goes through it, and the amount of time
the stimulus must be optimally represented. We then explore the behaviour of nonnormal networks, which include
the family of excitation-inhibition networks that more naturally model the dynamics of cortical circuits. These
networks can outperform normal networks on short timescales, by transiently increasing the signal-to-noise ratio
(SNR) through fast, selective amplification of the input. This is not possible in normal networks, in which we
could show that the SNR must decay with time. Finally, directly optimizing network connectivity for information
transmission yields nonnormal networks very similar to those we have previously proposed to explain movementrelated activity in primary motor cortex. We therefore suggest that the cortex might employ nonnormal dynamics
to process fast-varying signals that are either weak or mixed with task-unrelated activity.

COSYNE 2017

191

III-31 – III-32

III-31. Diverse weighting of shared input noise prevents information saturation in a population code
Pratik Sachdeva
Michael DeWeese

PRATIK . SACHDEVA @ BERKELEY. EDU
DEWEESE @ BERKELEY. EDU

University of California, Berkeley
Noise is a prominent feature of neural systems: neural responses will vary trial-to-trial despite constant experimental stimuli. In sensory cortex, response variability is often correlated - these noise correlations are of theoretical interest because their structure can strongly influence the fidelity of a population code. Previous work has
demonstrated that shared input noise can induce specific noise correlations, called differential correlations, that
cause information to saturate in a neural population. We present a network of linear-nonlinear neurons in which
we induce differential correlations by injecting noise to model, for instance, shared synaptic noise from irrelevant
upstream action potentials. We show that by applying a diverse set of synaptic weights to the injected noise, the
network can prevent information saturation and further improve the accuracy of its population code, despite an
overall increase of noise in the system. This improvement results because the noise correlations are restructured
in a way that is beneficial for decoding. Thus, by diversifying synaptic weights, a population of neurons can remove the harmful effects imposed by afferents that are uninformative about a stimulus. Interestingly, we also find
cases where an abundance of weight diversity is harmful to the network, implying that there is some balanced
regime where diversifying synaptic weight is optimal.

III-32. Coordinated activation of value-related computations across human
orbitofrontal cortex
Ignacio Saez1
Jack Lin2
Edward Chang3
Josef Parvizi4
Gerwin Schalk5
Robert Knight1
Ming Hsu1

ISAEZ @ BERKELEY. EDU
LINJJ @ UCI . EDU
EDWARD. CHANG @ UCSF. EDU
JPARVIZI @ STANFORD. EDU
GSCHALK @ NEUROTECHCENTER . ORG
RTKNIGHT @ BERKELEY. EDU
MHSU @ HAAS . BERKELEY. EDU

1 University

of California, Berkeley
of California, Irvine
3 University of California, San Francisco
4 Stanford University
5 Wadsworth Center
2 University

The human orbitofrontal cortex (OFC) is known to be involved in goal-directed behavior and reward processing.
Neural activity in OFC activity reflects a variety of computations necessary for estimating the value of available options, including expected reward, risk, and learning signals. Based on these data, an influential account proposes
that OFC encodes economic value in the form of a common neural currency used to guide action selection. At
the same time, data from model organisms increasingly suggest that OFC responses encompass a much larger
range of computations than those necessary for signaling value. However, disentangling the nature of these
computations in the human OFC remains challenging due to spatiotemporal limitations in current neuroimaging
techniques. Here we address this issue by combining neurosurgical electrocorticography (ECoG) recordings and
computational methods to observe neural activity directly from human OFC during economic decision-making.
We find that ongoing electrophysiological activity in the high gamma band (HG, 70-200Hz) reflects a diverse collection of computational components, containing information about current and past task states that go beyond
valuation per se. These computations are distributed widely across the OFC and organized into three volleys of
coordinated activity across electrodes. Two of these volleys are time-locked to choice and outcome presentation
and contain information related to decision and outcome processing, respectively. Both types of information are

192

COSYNE 2017

III-33 – III-34
reactivated as a third, longer-lasting volley in the immediately following, but not subsequent, trials. Strikingly, this
reactivation of past task states was significantly more widely represented than any other class, involving up to half
of all electrodes. Taken together, these results demonstrate that human OFC representations go beyond those
necessary for signaling value, and provide evidence for a dynamic task state representation that incorporates
multiple sources of internal and external information related to valuation and learning.

III-33. Latent manifold analysis reveals parallels in hierarchical patterning of
birdsong and human speech
Tim Sainburg
Brad Theilman
Marvin Thielk
Timothy Gentner

TIMSAINB @ GMAIL . COM
BRADTHEILMAN @ GMAIL . COM
MARVIN . THIELK @ GMAIL . COM
TGENTNER @ UCSD. EDU

University of California, San Diego
A rigorous neurobiology of pattern perception requires detailed understanding of acoustically rich communication
systems, such as birdsong, that are amenable to invasive neuroscience techniques. Such systems are challenging to represent, however, in a way that allows for straightforward acoustic and sequential analysis. Here, we
demonstrate a set of tools that solve this problem. We first use a syllabic segmentation algorithm to create large
datasets of spectral representations of birdsong syllables from single male European Starlings. We then use
deep-convolutional autoencoders with a central bottleneck layer to produce a low-dimensional manifold (latent
space) capturing the entire syllabic repertoire of individual European Starlings. In this low-dimensional space,
movement corresponds to smooth changes in spectral-space, allowing us to psychophysically map our neural
network learned low-dimensional manifold to the songbird’s perceptual manifold. We then discretize this space
using density based clustering, mapping birdsong to sequences of discrete syllables (e.g. ‘G-K-D-A-B-M-P...’).
This allows us to analyze our sequences using the same techniques used in the analysis of text-corpora. We
compare our results to a recent analysis on language corpora, which shows that human language cannot be
described by low-order Markov processes, due to its hierarchical nature. Our results show that birdsong and language have a qualitatively similar decay in mutual information as a function of sequential distance, suggesting that
birdsong is similarly hierarchically patterned as human language. We thus provide a number of advances to the
study of animal communication. (1) We create a technique for reducing the dimensionality of complex acoustic
stimuli for controlled analysis in behavior. (2) We provide a fully automated method for translating syllables of
birdsong to discrete sequences of elements. (3) We provide evidence for hierarchically structured sequencing in
natural birdsong.

III-34. Complementary organization of responses to apparent motion in Drosophila
elementary motion detectors
Emilio Salazar
Bara Badwan
Margarida Agrochao
Damon Clark

EMILIO. SALAZARCARDOZO @ YALE . EDU
BARA . BADWAN @ YALE . EDU
MARGARIDA . AGROCHAO @ YALE . EDU
DAMON . CLARK @ YALE . EDU

Yale University
Sighted animals use visual motion for tasks ranging from tracking prey and mates to course correcting during
navigation. Motion estimation systems must integrate information over both space and time, and combine that
information nonlinearly. Classical motion detection models correlate contrast information at two points in space
across time, yet many biological systems split the motion detection algorithm into two distinct pathways. These
two pathways appear in vertebrates and invertebrates and separately compute moving light edge and moving

COSYNE 2017

193

III-35 – III-36
dark edge motion. They are typically referred to as ‘ON’ and ‘OFF’ motion pathways because they are thought to
receive rectified inputs reporting local contrast increments and decrements, respectively. In this study, we used
calcium imaging to study the motion encoding in these two pathways in the optic lobe of the fruit fly Drosophila.
We presented flies with apparent motion stimuli consisting of pairwise local, sequential contrast changes, and we
recorded the response properties of the complete set of ON and OFF elementary motion detectors (EMDs) to
all pairings of horizontal motion. This allowed us to distinguish how neurons in each motion detection pathway
respond to individual contrast changes and paired contrast changes. We found that the ON and OFF EMDs in
Drosophila responded to correlation pairings in a complementary form, so that the population of EMDs responded
to all possible pairwise contrast patterns. Responses to paired contrasts depended on inputs that spanned three
distinct locations in space, and, counterintuitively, indicated that ON pathways possess information about contrast
decrements and OFF pathways possess information about contrast increments. These complementary responses
to apparent motion stimuli form the basis of the cells’ preferred and null direction correlation processing. Our results reveal how fly EMDs process complex and complementary correlation information early in motion detection.

III-35. Predictive information in retinal ganglion cell responses to natural movies
Jared Salisbury1
Olivier Marre2
Michael Berry3
Stephanie Palmer1

JSALISBURY @ UCHICAGO. EDU
OLIVIER . MARRE @ GMAIL . COM
BERRY @ PRINCETON . EDU
SEPALMER @ UCHICAGO. EDU

1 University

of Chicago
de la Vision
3 Princeton University
2 Institut

Predicting the future state of the environment is an essential task for the brain to perform, in order to generate appropriate behavior in the face of significant sensory and motor delays. At the same time, high metabolic
costs dictate that neural representations of stimuli be as efficient as possible. The solutions to both problems are
intimately linked to the statistical structure of natural scenes. Here we address interrelated questions of the predictability and efficiency of the neural code by analyzing responses of populations of retinal ganglion cells (RGCs)
to natural and artificial stimuli. Natural movies have a complex structure that can be described at different levels
of abstraction; we examine various statistics of our stimuli, from basic contrast power spectra to velocity autocorrelation functions describing moving objects, estimated with a novel optical flow-based pixel tracking algorithm.
We relate these measures of stimulus predictability to response predictability by estimating the internal predictive
information—the amount of information that past responses carry about future responses—of subpopulations of
RGCs. We find that responses to natural stimuli are significantly predictable for hundreds of milliseconds. This
cannot be attributed to strong stimulus-independent neural correlations, since the predictability of responses to an
uncorrelated checkerboard stimulus decays within 100 milliseconds. While the amount of predictive information
varies across stimuli, these differences largely collapse after normalizing by firing rate. This phenomenon suggests there is a fundamental timescale relevant to prediction that the retina maintains despite broad changes in
the structure of visual input. Significant predictive information is not an inevitable feature of sensory systems; in
fact, one consequence of maximizing efficiency is temporal decorrelation, which can drive predictive information
to zero. Thus, the significant predictive information observed here may be indicative of a competing optimization
that maximizes information about the future by selectively encoding predictable components of the stimulus.

III-36. Flexible normalization in deep convolutional neural networks
Luis G Sanchez-Giraldo
Odelia Schwartz

LGSANCHEZ @ CS . MIAMI . EDU
ODELIA @ CS . MIAMI . EDU

University of Miami

194

COSYNE 2017

III-37
Spatial context effects are prevalent in visual neural processing and in perception. In primary visual cortex, neural
responses are modulated by stimuli spatially surrounding the classical receptive field in a rich way. These effects
have been modelled with divisive normalization approaches. From a scene statistics perspective, spatial normalization may be seen as a nonlinear operation that reduces statistical dependencies between filter activations to
scenes. Recent models propose that spatial normalization is not fixed, and that normalization is recruited only
to the degree that filter activations in center and surround locations are deemed statistically dependent. However, extending these studies to understanding when normalization is recruited in mid-level visual areas, such as
secondary visual cortex, has been so far an elusive problem. This may partly be due to a lack of understanding
of what might be the optimal stimulus space or filter set. In this work, we propose to use deep convolutional
networks (CNNs) to study flexible normalization mechanisms. CNNs have shown intriguing similarities to visual
cortex, and they provide a potentially tractable way to obtain representations at mid-levels of a visual processing
hierarchy. We introduce a computational model that incorporates flexible normalization into the second layer of the
network. This model is able to capture non-trivial spatial dependencies among mid-level features such as textures
and more geometric tiling of the space. This approach makes predictions about when spatial normalization might
be recruited in mid-level areas. In addition, it has been shown that performance of deep CNNs on supervised
tasks have benefited from incorporating nonlinearities such as local contrast normalization, which are inspired by
neuroscience. Our work suggests that flexible normalization has further potential to improve the performance of
deep CNNs for visual recognition tasks.

III-37. Confidence results from internal monitoring of information integration
Hannah Schewe
Windy Torgerud
Dominic Mussack
Ganesh Rakate
Paul Schrater

SCHEW 012@ UMN . EDU
LYNCH 174@ UMN . EDU
MUSS 0080@ UMN . EDU
RAKAT 001@ UMN . EDU
SCHRATER @ UMN . EDU

University of Minnesota
The role of confidence in decision-making has garnered increasing attention both because it captures a salient
aspect of decision making and critically challenges established models. Interpreted as response uncertainty,
confidence should be linked to the evidence available at decision time, predicting that equi-evidence decisions
should have similar confidence. Instead, a series of recent results show that confidence varies substantially with
reaction time for equal evidence (Kiani, et al 2014), findings that have led to several proposed modifications to
standard decision making models, including priors on information usage, and suggestion that subjects expect
longer trials to have lower performance. All these modifications still attribute confidence to aspects of a decision
making process assumed constant across trials. However, it is widely understood that subjects experience trialby-trial fluctuations in attention. Attentional fluctuations should manifest as variations in the gain in information
integration - we hypothesis that the brain monitors these fluctuations in information processing efficacy and incorporates them into its estimate of performance. To test this hypothesis, we used a version of the random dot
motion discrimination task that incorporated brief pulses of coherence and simultaneous measures of decision
and confidence. The paradigm allowed us to non-parametrically estimate subject’s time-varying weighting profile
on information using separate hazard analyses on reaction time data and logistic regression on performance data.
Our results show that confidence is a direct function of overall information gain - low confidence trials simply have
lower gains. Congruent with previous results, information integration is non-uniform - a concave function with
mid-trial information having the highest weight and information near decisions increasingly discarded. Intriguingly,
hazard analysis show that subjects actively delayed decisions at high confidence to ensure a minimum of integrated information. These patterns are more consistent with confidence incorporating knowledge of attentional
allocation- i.e. confidence better reflects meta-cognitive processes than simple decision uncertainty.

COSYNE 2017

195

III-38 – III-39

III-38. Perceptual categorization of infant vocalizations in the auditory cortex
of maternal mice
Jennifer Schiavo
Robert Froemke

JKS 368@ NYU. EDU
ROBERT. FROEMKE @ MED. NYU. EDU

New York University
Learning requires generalization from exemplars to enable animals to respond reliably to distinct stimuli with the
same behavioral significance. Sounds with the same meaning typically vary along a continuum of low-level features, such as rhythm. Categorization of sounds enables adaptive responses to novel stimuli, allowing animals
to generalize based on categorical knowledge. Categorization facilitates comprehension of communicative vocalizations, such as the ability of humans to differentiate phonemes despite variability in speech features While the
neural correlates of categorization in auditory cortex are well studied, how neural computations enable animals to
generalize from learned perceptual categories to direct behavior is poorly understood. Here, we take advantage
of a maternal behavior, termed pup retrieval, in which mouse mothers retrieve nest-isolated pups based on distress ultrasonic vocalizations (USVs). This behavior requires females to perceptually categorize pup USVs, and
serves as a model to examine the neural correlates of category-directed behavior. Because USVs are variable
between individuals, dams must form categories based on core features of calls to generalize over a range of
vocalizations and reliably retrieve any pup. Using a Y-maze, we assessed the perceptual boundary for pup USVs
in dams. We confirmed previous findings that dams approach speakers playing USVs, and we also showed that
dams categorize USVs based on inter-motif duration (‘repetition rate’). Using in vivo two-photon imaging, we
found that USV-responsive neurons respond invariably to USVs morphed in the temporal domain within a natural
distribution. These results demonstrate that the ability of dams to generalize over a range of vocalizations may be
based on invariant responses to USVs in core auditory cortex.

III-39. Synaptic architecture of visual space in ferret visual cortex
Benjamin Scholl
Daniel Wilson
David Fitzpatrick

SCHOLL . BEN @ GMAIL . COM
DAN . WILSON @ MPFI . ORG
DAVID. FITZPATRICK @ MPFI . ORG

Max Planck Florida Institute for Neuroscience
Understanding how single neurons integrate synaptic inputs from myriad sources to generate somatic responses
remains a challenge. Neurons receive thousands of synaptic inputs, and the organization of those inputs with
respect to diverse functional properties remains largely unknown. As retinotopic maps are highly conserved
across mammals, we examine the representation of visual space on the dendrites of neurons in ferret visual
cortex. Specifically we address whether there is an ordered global or distributed local organization of visual space
within the dendritic tree. Measuring spatial receptive fields (RFs) of dendritic spines with 1-dimensional noise
stimuli and two photon calcium imaging, we were surprised to discover individual synaptic inputs were often tuned
for regions of space outside the somatic RF and there was considerable diversity in synaptic RF properties for
individual neurons. However, we find no evidence for a global retinotopic organization of synaptic inputs within the
dendritic field. Instead, dimensionality reduction and cluster analysis reveal a nonrandom anatomical organization
of synaptic inputs based on the principal components of their diverse responses to spatial stimuli. Individual
dendritic branches exhibited clusters of spines with a high degree of similarity in spatial selectivity and forming
synaptic aggregates for each principle component cluster yield spatial RFs localized to specific regions of visual
space, suggesting a local organization of synapses distributed across the dendrites. Consistent with this notion,
we also find a distance dependent correlation of response properties and trial-to-trial variability for neighboring
spines with a spatial length constant of a few microns. Distributed local functional clustering of synaptic inputs
carrying similar sensory information suggests that dendritic compartments may play an important role in forming
spatial interactions that contribute to cortical neuron response properties.

196

COSYNE 2017

III-40 – III-41

III-40. Inter-areal synchronization in visual cortex during a divided attention
paradigm
Marieke Scholvinck
Jarrod Dowdall
Georgios Spyropoulos
Pascal Fries

MARIEKE . SCHOLVINCK @ ESI - FRANKFURT. DE
JARROD. DOWDALL @ ESI - FRANKFURT. DE
GEORGIOS . SPYROPOULOS @ ESI - FRANKFURT. DE
PASCAL . FRIES @ ESI - FRANKFURT. DE

Ernst Strungmann Institute for Neuroscience
Typical visual attention paradigms try to direct attention to one stimulus for extended periods of time. Yet in real
life, we often pay attention to multiple objects simultaneously. It is unknown how dividing attention over several
stimuli impacts interactions between neuronal populations in visual cortex. We simultaneously recorded local
field potential (LFP) from areas V1 and V4, as well as single and multi-unit activity (MUA) from area MT, in one
macaque while he was engaged in a divided attention task. Two drifting gratings were presented, both of which
had an equal probability of transiently changing their drift orientation, after which the monkey was required to move
his eyes to this grating. The two gratings were placed such that they activated separate neuronal populations in
V1 and fell within the same receptive field of a chosen V4 or MT site. Therefore, the coherence between the
separate V1 sites and the V4 or MT site could be studied as both stimuli were attended to. To date, only the V1V4 data have been analysed sufficiently. When the gratings were shown separately, a large proportion of V1 sites
were selectively activated by either stimulus, whereas V4 sites were activated by both stimuli. We then trained
a classifier on the LFP power in V4 during these separate presentations of the gratings, which resulted in 73%
accuracy of determining which grating had been shown. We are now applying this classifier to V4 LFP power
recorded during presentation of both gratings to estimate which one was attended to, and to then investigate
the influence of this spontaneous allocation of attention on V1-V4 coherence. These analyses will enable us to
determine whether attention can be truly divided over, or instead switches between, multiple visual stimuli. As
such, they will provide important insights into the neural implementation of attention.

III-41. Task states are represented in OFC during task performance and replayed in hippocampus at rest
Nicolas Schuck
Yael Niv

NSCHUCK @ PRINCETON . EDU
YAEL @ PRINCETON . EDU

Princeton University
How humans learn to flexibly and efficiently perform arbitrary decision-making tasks is largely unknown. In previous work, we demonstrated that multivariate activity in the orbitofrontal cortex (OFC) reflects the partially observable states of a task (Schuck et al., 2016, Neuron). Here, we replicate and extend our previous work by
showing the existence of task-related state representations in the OFC that become more pronounced as the task
is practiced and performance improves. Strikingly, we also found that the order of successive activation patterns
in the hippocampus during rest reflected the order of previously experienced task states. Our results survive stringent control analyses, and extensive simulations support the general possibility to investigate sequential neural
pattern activations with fMRI despite low sampling rates and the seemingly disadvantageous properties of the
hemodynamic response function. These results suggest that the encoding of a cognitive map of task states might
employ the same neural mechanisms involved in encoding a cognitive map of space, and establish the feasibility
of investigating ordered pattern reactivations with fMRI.

COSYNE 2017

197

III-42 – III-43

III-42. Neural noise improves path representation in a simulated network of
grid, place, and time cells
David Schwartz
O Ozan Koyluoglu

SCHWARTZ . DAVID. MICHAEL @ GMAIL . COM
OZAN . KOYLUOGLU @ GMAIL . COM

University of Arizona
Place cells in the hippocampus are active when an animal visits a certain location (referred to as a place field)
within an environment, and remain silent otherwise. Grid cells in the medial entorhinal cortex (MEC) respond
at multiple locations, and the firing fields follow a hexagonally symmetric periodic pattern. Time cells fire at
successive moments in an interval of time with a well defined start and end. The joint activity of grid, place,
and time cell populations forms a neural code for path representation (i.e. a temporally ordered sequence of
visited locations). We study simulated networks of phenomenologically modeled noisy grid, place, and time cells
connected via interneurons. The networks’ synaptic connections are trained with an anti-Hebbian learning rule on
activity evoked by stimulation with locations and times drawn from recordings of real rat motion during a spatial
navigation task. In [Schwartz and Koyluoglu, submitted] and [Schwartz and Koyluoglu, 2016], we show that when
grid and place cells collaborate in such a network, some errant activity is almost always corrected, and that
the modular organization of grid cells can be exploited to improve de-noising and decoding accuracy. Here, we
extend that work by including time cells so that temporal ordering of states of network activity can be decoded. We
observe that low amplitude noise impairs specific decoding accuracy of space and time (i.e. accuracy of decoding
either variable alone, for one moment), but - via biologically plausible de-noising operations implemented by
interneurons - bolsters accuracy of decoding a full path. We generate testable predictions that may support the
existence of de-noising communication between grid, place, and time cells.

III-43. A miniature ultra-widefield microscope for imaging global brain dynamics in GCaMP6 transgenic rats
Benjamin Scott1
Stephan Thiberge1
Caiying Guo2,3
Dougal Tervo2,3
Carlos Brody1,3
Alla Karpova2,3
David Tank1

BBSCOTT @ PRINCETON . EDU
THIBERGE @ PRINCETON . EDU
GUOC @ JANELIA . HHMI . ORG
TERVOD @ JANELIA . HHMI . ORG
BRODY @ PRINCETON . EDU
KARPOVAA @ JANELIA . HHMI . ORG
DWTANK @ PRINCETON . EDU

1 Princeton

University
Research Campus
3 Howard Hughes Medical Institute
2 Janelia

Widefield imaging of calcium dynamics in transgenic animals is an emerging method for measurement of neural
activity at the mesoscopic scale. Head-mounted microscopes enable calcium imaging during an expanded range
of behaviors, however, existing devices only offer a small window onto global brain dynamics due to limited field
of view (FOV <1mm^2). Here we describe a head-mounted ultra-widefield microscope developed to image largescale brain dynamics in rats during natural behavior. The microscope consists of a lightweight 3D-printed plastic
body that houses a folded optical path consisting of a series of lenses and filters, two sets of LEDs for respectively
fluorescence excitation and reflectance-based intrinsic imaging, and a CMOS imaging sensor for high sensitivity
light measurements. The optical assembly was designed in Zemax using off the shelf components, while the
first-generation imaging sensor and readout electronics is based on the UCLA miniscope[1]; this allows each
microscope to be constructed for less than 1,000 USD. The complete microscope weighs 22g and provides a
7mm by 7mm FOV at 20-micron resolution. To facilitate large-scale calcium imaging we also developed a strain of
transgenic rats expressing the genetically encoded calcium indicator GCaMP6f in cortical neurons and tested the
microscope by imaging GCaMP6f dynamics in the dorsal neocortex of these animals. Blue illumination alternated

198

COSYNE 2017

III-44 – III-45
with green illumination at 30hz allowing software-based correction of calcium dependent fluorescence changes
from changes due to the hemodynamic response[2]. Image stability was sufficient to record cortical dynamics
during exploratory navigation, feeding and social behavior. Residual motion was corrected offline by standard
motion correction software. The microscope can be detached from the head allowing two-photon imaging of
the same cortical region. Our results extend the domain of head-mounted microscopes to larger-scale cortical
dynamics. [1]Cai et al. Nature 534,115 (2016); [2]Ma Y et al. Philos Trans B. 371,1705 (2016)

III-44. Population coupling in the mouse visual cortex
Madineh Sedigh-Sarvestani1
Philip Mardoum2
Max Nolte3

MADINEH @ UPENN . EDU
PMARDOUM @ UW. EDU
MAX . NOLTE @ EPFL . CH

1 University

of Pennsylvania
of Washington
3 Ecole Polytechnique Federale de Lausanne
2 University

Recordings from increasingly large populations of neurons have resulted in improved understanding of how ensembles of neurons process information. A recent electrophysiology study found that a simple metric can capture
a neuron’s coordination with the overall population activity, with some neurons weakly coordinated with the population (‘soloists’) and others strongly coordinated (‘choristers’, Okun et al. 2015). This population coupling
(PC) metric was found to be an invariant property of each neuron for a variety of experimental conditions. This
invariance, as well as a series of in vitro patch recordings, suggests that PC is determined by the underlying
connectivity to the local population. If valid across various measurement modalities this metric will improve our
understanding, and simplify our characterization, of complex population codes. We were therefore motivated to
validate this metric using the largest public dataset of simultaneously recorded sensory neurons recently made
available by the Allen Institute for Brain Science (the Brain Observatory). We calculated PC for 2-photon calcium
imaging responses of hundreds of simultaneously recorded neurons. We looked for variations in PC as a function
of different visual stimuli, different visual areas and cortical layers, different excitatory cell type populations, and
various behavioral conditions. Our results overwhelmingly confirm and expand on Okun et al.’s findings. We find
that PC extracted from 2-photon calcium responses appears to estimate the local coupling of a population: PC
variations with cortical cell-type are consistent with the density of the cell-type and a model constrained by PC
outperforms a null model in predicting pairwise correlations between neurons. More importantly, we find that PC
is invariant to visual stimulus, across all studied regions and transgenic lines. Our study not only validates the
metric described by Okun et al., but also validates the utility of the Brain Observatory for the study of neuronal
correlations.

III-45. Cortical spatial representations for solving the cocktail party problem
Kamal Sen1
Howard Gritton1
Jun Ma1
Nicholas James2
Xue Han1
1 Boston

KAMALSEN @ BU. EDU
HGRITTON @ BU. EDU
MAXXX 425@ BU. EDU
NICHOLAS . JAMES @ UCSF. EDU
XUEHAN @ BU. EDU

University
of California, San Francisco

2 University

The Cocktail Party Problem (CPP) has been a longstanding problem in a diverse range of fields e.g., neuroscience,
computer science, speech recognition and engineering. More than 50 years after the problem was named, it
remains a focus of intensive research in many fields. Although a difficult problem for machines to solve, humans

COSYNE 2017

199

III-46 – III-47
with normal hearing solve it with relative ease. This simple fact is like an “existence proof” in mathematics. Even
though we do not know the solution, we know it exists somewhere in the brain. Here, we report auditory cortical
neurons in mouse A1 with spatial response properties that are well suited for segregating competing sounds from
different spatial locations. We found that auditory cortical neurons are broadly tuned to single “target” sound
sources from different spatial locations; however, when the target is presented at the same time as a competing
“masker” from a different location, cortical neurons sharpen their spatial tuning. Specifically, we quantified neural
discrimination performance over a spatial grid of all possible combinations of target and masker locations and
found that cortical neurons display “hotspots” of high performance at particular positions on the spatial grid.
Thus, such neurons are sensitive to the spatial configuration of the target and the masker. Similar findings have
been previously reported for cortical level neurons in songbirds. Songbirds and mice have different frequency
ranges of hearing. Cues for spatial processing, e.g., interaural time-difference (ITD), and interaural level difference
(ILD) are frequency dependent, and the peripheral representations of these cues is likely to be different across
species with different frequency ranges of hearing. Our findings suggest the emergence of general cortical spatial
representations for solving the CPP, despite different peripheral representations of acoustic cues.

III-46. A biologically plausible neural network for whitening-free ICA
Anirvan Sengupta1
Dmitri Chklovskii2

ANIRVANS . PHYSICS @ GMAIL . COM
DCHKLOVSKII @ SIMONSFOUNDATION . ORG

1 Department
2 Simons

of Physics and Astronomy
Foundation

Our brains solve the cocktail party problem – unsupervised extraction of sources from mixtures – seemingly effortlessly. As this problem is solved for auditory, olfactory, and visual stimuli, understanding neural circuits capable
of solving it may shed light on general principles of neural computation. We focus on the cocktail party problem formulation using linear generative model known as Independent Component Analysis (ICA), which has been
studied extensively in signal processing. For ICA algorithms to be biologically plausible they must be implemented
in the online (or streaming) setting with local learning rules. Moreover, many ICA algorithms in signal processing
start with prewhitening the data. While such operation can be neurally implemented, it requires a dedicated neural layer. To maintain adaptivity, the prewhitening layer must be regulated in tandem with the second layer, which
complicates the circuits.

III-47. Model-based inference of nonlinear subunits underlying responses of
primate retinal ganglion cells
Nishal Shah1
Nora Brackbill1
Colleen Rhoades1
Alexandra Tikidji-Hamburyan1
Georges Goetz1
Alexander Sher2
Alan Litke2
Liam Paninski3
Eero P Simoncelli4,5
EJ Chichilnisky1

NISHALPS @ STANFORD. EDU
NBRACK @ STANFORD. EDU
RHOADES @ STANFORD. EDU
ALEXTH . NN @ GMAIL . COM
GGOETZ @ STANFORD. EDU
SASHAKE 3@ UCSC. EDU
ALAN . LITKE @ CERN . CH
LIAM @ STAT. COLUMBIA . EDU
EERO. SIMONCELLI @ NYU. EDU
EJ @ STANFORD. EDU

1 Stanford

University
of California, Santa Cruz
3 Columbia University
4 New York University
2 University

200

COSYNE 2017

III-48
5 Howard

Hughes Medical Institute

Information processing in neural circuits relies on interneurons. In the retina, visual input is transduced by photoreceptors, processed by bipolar interneurons, and combined to drive responses of retinal ganglion cells (RGCs).
Bipolar cell nonlinearities produce “subunits” in the RGC receptive field (RF) (Hochstein & Shapley, 1976) that may
mediate responses to texture and movement. However, understanding precisely how subunits affect the retinal
output is challenging because direct recordings from bipolar cells are difficult. We present a model-based method
to infer functional properties of subunits indirectly from RGC responses. The method confers advantages over
previous approaches: fewer assumptions, more efficient estimation, and more accurate response predictions. We
test the model and estimation procedure using stimuli designed to highlight nonlinearities. ON and OFF parasol
cells in macaque retina were recorded using a 512-electrode recording system. Responses were modeled as a
linear-nonlinear-linear (LNL) cascade: outputs of spatio-temporal filters (subunits) are exponentiated and summed
to generate a time-varying rate for Poisson spike generation. A maximum-likelihood estimate of subunit filters corresponds to soft-clustering the ensemble of stimuli preceding spikes and identifying their centroids. The number
of reliably estimated subunits was determined by cross-validation. In simulation, the algorithm correctly identified
subunits, or with limited data, aggregates of subunits. With recorded data at coarse spatial resolution, estimated
subunits were larger than bipolar cell receptive fields, spatially localized, and non-overlapping, as would be expected from aggregates of bipolar cells. To test whether estimated subunits captured a significant component of
spatial nonlinearity, we designed “null” stimuli, orthogonal to the linear RF measured by reverse correlation. By
construction, a linear RGC would not respond to these stimuli. Recorded RGCs exhibited strong responses to
null stimuli, especially OFF cells, consistent with previous reports of stronger nonlinearities in OFF cells. Subunits
fitted with the above approach substantially improved predictions of responses to null stimuli.

III-48. Learning nonlinear models for visual computation in populations of
retinal ganglion cells
Nishal Shah1
Nora Brackbill1
Colleen Rhoades1
Alexandra Tikidji-Hamburyan1
Georges Goetz1
Alexander Sher2
Alan Litke2
Vineet Gupta3
Yoram Singer3
EJ Chichilnisky1
Jon Shlens3
1 Stanford

NISHALPS @ STANFORD. EDU
NBRACK @ STANFORD. EDU
RHOADES @ STANFORD. EDU
ALEXTH . NN @ GMAIL . COM
GGOETZ @ STANFORD. EDU
SASHAKE 3@ UCSC. EDU
ALAN . LITKE @ CERN . CH
VINEET @ GOOGLE . COM
SINGER @ GOOGLE . COM
EJ @ STANFORD. EDU
SHLENS @ GOOGLE . COM

University
of California, Santa Cruz

2 University
3 Google

The computations performed by a population of neurons are shaped by the underlying circuitry. In the retina,
the photoreceptor signal is transformed nonlinearly by bipolar cells before being transmitted to retinal ganglion
cells (RGCs) and then to the brain. This nonlinearity, which produces ‘subunits’ in the RGC receptive field, poses
a challenge for modeling the activity of RGC populations. Therefore, a tractable large-scale model that can
account for the nonlinear properties of an entire RGC population is essential for understanding how the retina
encodes visual images. Here we describe a parsimonious and scalable model of RGC population activity that
is rich enough to capture nonlinear spatial properties of a neural population, but is formulated to be tractable
using modern optimization techniques. Parasol RGCs in macaque retina were recorded using a 512-electrode
recording system. The responses of the RGC population were modeled with an almost-convolutional structure,
with a softmax normalization, and model parameters were inferred so that the likelihood of observed data was

COSYNE 2017

201

III-49 – III-50
maximized. The model architecture led to sparseness in the weights of subunits. Specifically, each RGC strongly
connected to 3-8 nearby subunits out of about 3000 possible, reflecting the homogeneity and spatial localization
of RGCs. Inferred subunits were finely spaced in a regular lattice, consistent with a single underlying mosaic of
bipolar cells. However, the model was also able to capture the irregularities within the receptive field of cells. The
resulting model predicted spiking activity in a held out data set with greater accuracy than single neuron LNP
models. Finally, 11% of all strongly connected subunits provided input to more than one nearby parasol cell, and
the strength of the subunit input was predictive of the observed RGC correlated activity. This model architecture
may generalize to capture the underlying neural circuitry of other populations of neurons.

III-49. Neural adaptation may explain anomalous diffusion in fixational eye
motion
Nimrod Shaham
Nadav Ben-Shushan
Yoram Burak

NIMROD. SHAHAM @ MAIL . HUJI . AC. IL
NADAVBENS @ GMAIL . COM
YORAM . BURAK @ ELSC. HUJI . AC. IL

Hebrew University of Jerusalem
While we fixate on stationary targets, our eyes undergo smooth diffusive motion, called fixational drift. Detailed
quantification shows that fixational drift is superdiffusive over timescales up to approximately 100 ms: the variance
of the eye’s motion increases as a power law of the time interval with an exponent ranging between 1.1 and 1.4,
depending on the subject (Engbert and Kliegl, 2004). Therefore, the eyes are more likely to move away from their
recent locations than to go back to a previously visited location. The mechanistic source of this superdiffusive
motion is still unknown. Here we identify a neural mechanism that may account for superdiffusive fixational drift:
we show that noisy neural networks with continuous attractor dynamics, combined with neural adaptation, can
produce anomalous diffusive motion along the attractor, in which the variance grows as a power of the time
interval with an exponent larger than unity. An intuitive way to understand this result is that depletion of neural
resources biases the bump of activity generated by the attractor network, to travel away from recently visited
locations. We first consider continuous attractor networks with ring connectivity. Such networks can represent a
periodic variable (an angle or orientation) in working memory. We analyze the dynamics of a ring network in which
we explicitly model neural adaptation and the stochasticity of spikes. The position of the activity bump exhibits
superdiffusive motion, with an exponent that increases in proportion to the amount of adaptation. Additionally,
we demonstrate a similar effect in a more realistic model of the oculomotor integrator, which includes adaptation
and noise. Our work reveals a new form of superdiffusive dynamics which can be realized by continuous attractor
networks, and provides a possible mechanistic explanation for the statistics of human fixational drift.

III-50. Fine-grained characterization and modeling of decision behavior with
optimal motion-pulse sequences
Victoria Shavina1,2
Valerio Mante1,2
1 ETH

SHAVINA @ INI . UZH . CH
VALERIO @ INI . UZH . CH

Zurich
of Zurich

2 University

Studies of perceptual decision-making have been singularly influential in shaping our understanding of the neural
mechanisms underlying cognition (e.g. Usher & McClelland 2001). Simple models based on diffusion-to-bound
or evidence accumulation can explain many aspects of decision behavior as well as many properties of neural responses observed during such decisions across a variety of brain areas. However, the insights provided by such
studies can be limited by the properties of the employed stimuli, which often lack rich temporal dynamics, and
contain various amounts of noise, whose effect on single choices is difficult to predict. Here we overcome these

202

COSYNE 2017

III-51
shortcomings with a novel motion-direction discrimination task, where the visual stimuli consist of sequences of
randomly timed, short motion pulses (in various directions) and breaks (static images). Each pulse has a precisely
defined motion energy that optimally matches the receptive field of a single population of pattern-direction selective neurons in MT (Schrater & Simoncelli 2010). Crucially, the strength of each pulse can be adjusted without the
addition of any noise. The resulting stimulus is well suited to study the neural dynamics underlying the decision
process at a very fine time-scale, and in particular can reveal the relaxation of the neural responses following
a pulse (Mante et al 2013). Moreover, the rich but controlled structure of each stimulus allows a fine-grained
characterization of the behavior both with logistic regression approaches and fits of stochastic differential models
(Brunton et al 2013). For human subjects these methods reveal large, unexpected differences across subjects in
the properties of the inferred sensory and accumulation processes underlying the observed decisions. Moreover,
while classic models based on optimal, leaky, or unstable evidence accumulation can explain coarse measures of
the behavior like psychometric curves, in many subjects they cannot replicate the finer structure of the behavior
revealed by our task.

III-51. Can you teach an old monkey a new trick?
Noa Shinitski1
Yingzhuo Zhang2
Daniel Gray3
Sara Burke4
Anne Smith3
Carol Barnes3
Demba Ba2

NOA @ BCCN - BERLIN . DE
YINGZHUOZHANG @ G . HARVARD. EDU
DTGRAY @ EMAIL . ARIZONA . EDU
BURKES @ UFL . EDU
ASMITH 3142@ GMAIL . COM
CAROL @ NSMA . ARIZONA . EDU
DEMBA @ SEAS . HARVARD. EDU

1 Technische

Universitaet Berlin
University
3 University of Arizona
4 University of Florida
2 Harvard

Understanding how learning differs between groups of subjects is important in many areas of neuroscience.
Obtaining an objective measure can be challenging because learning is dynamic and varies greatly between
individuals. We introduce a method for analyzing two-dimensional (2D) binary response data from monkeys
performing a sequence of tasks across multiple days that enables us to compare performance between two age
groups. The data were obtained from 14 female macaque monkeys (6 young) performing a reversal learning task–
a modified Wisconsin Card Sort Task. Conventional methods ignore the 2D nature of these data by aggregating
the responses across days or tasks, thus failing to capture subtle changes in learning dynamics. We propose a
separable 2D random field (RF) model wherein the probability of making correct choices on a given day depends
on two latent Markovian state sequences that evolve separately but in parallel. We use a Laplacian prior for
the latent process capturing the learning dynamics across days which, unlike its Gaussian counterpart, allows
abrupt transitions such as occur in reversal learning. An efficient Monte Carlo Expectation-Maximization algorithm
is employed to estimate the parameters of the RF by maximum likelihood, followed by a provably convergent
iteratively re-weighted least-squares Maximum a Posteriori algorithm for change point detection. The approach
obviates the need for aggregating across either of the dimensions, produces a within-day performance metric by
task for each group, and a learning rate across days for each monkey. We show that the older monkeys find the
tasks harder, and that the cognitive flexibility (ability to successfully relearn tasks after reversal) of the younger
group is higher. Further demonstrating the strength of our method, we show how the monkeys can be clustered
based on a cognitive flexibility metric computed from our model.

COSYNE 2017

203

III-52 – III-53

III-52. Orthogonal spatial features and homogeneous temporal dynamics in
larval Drosophila olfactory coding
Guangwei Si1
Jessleen Kanwal1
Yu Hu1
Matthew E Berck2
Christopher Tabone3
Aravinthan Samuel1

GSI @ FAS . HARVARD. EDU
JKANWAL @ FAS . HARVARD. EDU
HUYUPKU @ GMAIL . COM
MEBERCK @ GMAIL . COM
CHRISTABONE @ GMAIL . COM
ADTSAMUEL @ GMAIL . COM

1 Harvard

University
Research Campus
3 Flybase
2 Janelia

Odorants are coded combinatorially at the olfactory periphery. To crack the code, many studies tried to map the
physiological atlas of odor responses. But because of the complex nature of the odorant space and combinatorial
coding, it will be hard to fully collect and understand the map. Finding any structures of the map or conservation
laws of the process will simplify this effort. Here, we report the overall structure of population coding of odor
identity, intensity and temporal dynamics from a comprehensive experimental and computational study of the
Drosophila larval antennal lobe, the first relay of its olfactory system. The Drosophila larva as a model system for
olfaction study benefits from its small nerve system and the recently reconstructed wiring diagram. We collected
the dose-response data from all larval olfactory receptor neurons (ORNs) to a variety of odorants across orders of
magnitude in concentration. Projecting the data into a lower dimensional neuronal activity space, the trajectories
of each odor, with increasing of concentration, formed a vector: vector direction represents odor type, and length
represents concentration; odors with similar molecular structures projected to vectors in similar directions. This
emerging structure allows the odor identity and intensity to be coded in orthogonal spatial features. Simulation
of the downstream circuit suggested the encoded intensity and identity are indeed extracted by two different
populations of second order neurons. We also studied the ORNs’ temporal coding property using flickering
white-noise odor stimuli and a linear-non-linear model. The results showed ORNs’ filters were similar for variant
ORN-odor pairs and across concentrations, which suggested a homogeneous temporal coding property. These
spatial structures and temporal properties suggest a simple and low-dimensional odor coding scheme at the
sensory periphery, and also suggest constraints for downstream circuits that process and decode primary sensory
information.

III-53. Distributed temporal processing in human prefrontal cortical neurons
during cognitive control
Elliot Smith1
Guillermo Horga2
Charles Mikell3
Mark Yates1
Garrett Banks1
Yagna Pathak1
Seth Pullman1
Qiping Yu1
Catherine Schevon1
Shraddha Shrinivasan1
Guy McKhann1
Matthew Botvinick4,5
Sameer Sheth1

EHS 2149@ CUMC. COLUMBIA . EDU
GH 2297@ CUMC. COLUMBIA . EDU
CHUCK . MIKELL @ GMAIL . COM
MJY 2115@ CUMC. COLUMBIA . EDU
GPB 2111@ CUMC. COLUMBIA . EDU
YP 2385@ CUMC. COLUMBIA . EDU
SP 31@ CUMC. COLUMBIA . EDU
QY 12@ CUMC. COLUMBIA . EDU
CAS 2044@ CUMC. COLUMBIA . EDU
SS 4219@ CUMC. COLUMBIA . EDU
GM 317@ COLUMBIA . EDU
BOTVINICK @ GOOGLE . COM
SS 4451@ CUMC. COLUMBIA . EDU

1 Columbia
2 New

204

University
York State Psychiatric Institute

COSYNE 2017

III-54
3 Stonybrook

University

4 DeepMind
5 University

College London

Our everyday lives require monitoring our environment for relevant cues, ignoring irrelevant ones, and adjusting
expectations based on outcome. These cognitive control processes are thought to arise in the prefrontal cortex,
particularly in the dorsal anterior cingulate cortex (dACC). Cognitive control signals have appeared in EEG studies
as modulations of frontal midline theta (4-8 Hz) power. Here we examine single neuron recordings from the human dACC (136 neurons from 6 subjects) and dorsolateral prefrontal cortex (dlPFC; 367 neurons from 9 subjects)
recorded from neurosurgical patients performing a Stroop-like task with both spatial and flanker sources of cognitive interference. We examined firing rate in each area using a generalized linear model (GLM) with factors: spatial
interference, flanker interference, response identity, feedback valence, and a nuisance variable for reaction time.
We found firing rate coding of conflict type is greater than chance in both dACC and dlPFC neurons (10 % and 11
%, respectively). However, no significant change in LFP power occurred across conditions (2-way ANOVA over
frequency band and conflict condition, p > 0.05 for all post hoc tests among conflict conditions). Nevertheless,
significant temporal coding of conflict was prevalent in dlPFC neuronal firing as measured with phase coherence
between action potential timing and theta-band LFP (permutation test with cluster correction, p<0.01). This thetarange temporal code could be a means by which cognitive control signals are broadly distributed across effector
regions such as dlPFC. To address this hypothesis, we examined mean pairwise mutual information for spike timing coincidence across many (>100) simultaneously recorded neurons in dlPFC. Mutual information correlated
with reaction time and conflict level across all single trials. These results illuminate the neuronal mechanisms of
cognitive control initiation in dACC and subsequent distributed processing across populations of neurons in dlPFC
via a temporal code.

III-54. Learning Bayesian prior and loss function in interval timing
Hansem Sohn
Mehrdad Jazayeri

HANSEM @ MIT. EDU
MJAZ @ MIT. EDU

Massachusetts Institute of Technology
Bayesian decision theory (BDT) has had remarkable success in describing human behavior in the presence of
prior information and reinforcement. Based on this success, it has been hypothesized that the brain performs
Bayesian computations explicitly by using representations of prior distribution and loss function. However, the
Bayesian models have been challenged on the grounds of model identifiability: different choices of the prior and
loss function can lead to nearly identical optimal decision policies. Indeed, Bayes-optimal behavior can result from
developing a lookup table (LUT), a deterministic estimator that emulates the decision policy without independent
encoding of the prior and loss function. Therefore, it is not known whether humans learn priors and loss functions
independently, or instead, acquire and use the corresponding deterministic decision rule. Here, we addressed this
question by building on previous work on a time-interval reproduction task for which behavior can be accurately
explained by BDT. We devised pairs of priors and loss functions that produced the same optimal decision policy,
and analyzed behavior during uncued transitions between those pairs. Results show that subjects relearn each
pair of prior and loss function even when the combined effect of the two corresponds to the same decision policy.
These results put BDT beyond a descriptive model and suggest avenues for teasing apart the neural structures
and mechanisms involved in independent learning of the prior and loss function.

COSYNE 2017

205

III-55

III-55. Uncovering the whole-brain electrical networks that underlie human
cognition
Ethan Solomon1
James Kragel1
Michael Sperling2
Ashwini Sharan2
Gregory Worrell3
Michal Kucewicz3
Robert Gross4
Bradley Lega5
Kathryn Davis1
Joel Stein1
Barbara Jobst6
Kareem Zaghloul7
Sameer Sheth8
Daniel Rizzuto1
Michael Kahana1

ESOLO @ UPENN . EDU
JKRAGEL @ SAS . UPENN . EDU
MICHAEL . SPERLING @ JEFFERSON . EDU
ASHWINI . SHARAN @ JEFFERSON . EDU
WORRELL . GREGORY @ MAYO. EDU
KUCEWICZ . MICHAL @ MAYO. EDU
RGROSS @ EMORY. EDU
BRADLEY. LEGA @ UTSOUTHWESTERN . EDU
KATHRYN . DAVIS @ UPHS . UPENN . EDU
JOEL . STEIN @ UPHS . UPENN . EDU
BARBARA . C. JOBST @ HITCHCOCK . ORG
KAREEM . ZAGHLOUL @ NIH . GOV
SS 4451@ CUMC. COLUMBIA . EDU
DRIZZUTO @ PSYCH . UPENN . EDU
KAHANA @ PSYCH . UPENN . EDU

1 University

of Pennsylvania
Jefferson University
3 Mayo Clinic
4 Emory University
5 University of Texas Southwestern Medical Center
6 Dartmouth Medical Center, Neurology
7 NIH
8 Columbia University
2 Thomas

The brain has long been thought of as a network of interacting components, but our understanding of the electrophysiological activity which gives rise to those networks is weak. Recent studies using direct brain recordings
in neurosurgical patients have made it possible to ask how specific brain regions participate in broader networks
of electrical activity. Through the lens of memory encoding and retrieval processes, we constructed the first
whole-brain connectivity maps of fast gamma (45-95 Hz) and slow theta (3-8 Hz) neural activity, based on a
large dataset of 300 patients. For all pairs of intracranial electrodes, we computed phase synchronization at individual frequencies in the gamma and theta ranges, and asked whether synchronization values were correlated
with a subject’s successful encoding or retrieval of a word during a free-recall task. The result was a network
of memory-related connectivity in the gamma and theta frequency bands, spanning 74 regions across the brain.
Parsing these networks with graph-theoretic tools, we found that gamma band was characterized by generally
desynchronized activity during successful memory encoding and retrieval, while theta networks were highly synchronous. Both showed the presence of spatial “hubs“ of activity that change their participation in the network over
time, including prefrontal, posterior parietal, and medial temporal regions. We also showed that the gamma-band
spectral power was inversely correlated with gamma-connectivity of a brain region, but positively correlated with
theta-connectivity – establishing broadband gamma as a largely asynchronous phenomenon but theta as a likely
mechanism that binds information across the brain. By significantly extending prior work that could only assess
connectivity dynamics via noninvasive measures – or invasively among just a small subset of brain regions – the
whole-brain nature of our analysis quantifies the general relationships between spectral activity, connectivity, and
cognitive processing.

206

COSYNE 2017

III-56 – III-57

III-56. A systematic study of variation of response sparseness across rat visual cortical areas
Liviu Soltuzu
Davide Zoccolan

LIVIU. SOLTUZU @ SISSA . IT
ZOCCOLAN @ SISSA . IT

International School for Advanced Studies
Leading theories of visual processing suggest that visual cortical representations are optimally adapted to code
the inherently sparse structure of natural scenes. This notion is supported by several studies showing how lifetime sparseness of neuronal responses in primary visual cortex (V1) is larger for natural scenes, as opposed to
phase-scrambled scenes. However, only few studies have tested whether this relationship holds also for higherlevel visual areas, and no study has investigated whether sparseness depends on the dynamical properties of
natural movies (e.g. speed). Here, we addressed these issues by recording from V1 and the succession of extrastriate areas that, in the rat, run laterally to it: area LM, whose attribution to dorsal or ventral processing is still
unresolved, and areas LI and LL, which are considered part of the rodent ventral stream. Recordings were performed in anesthetized animals, presented with 20s-long natural movies spanning a wide range of speeds, their
phase-scrambled versions and a white noise movie. For natural movies, we found that sparseness decreased
significantly along the areas’ progression while, at the same time, globally increasing as a function of the speed
neurons experienced in their receptive field. Area and speed also showed a significant interaction, driven by the
large increase of sparseness in LM at high speeds. Finally, sparseness was lower for the phase-scrambled and
white noise movies, despite their large speed values. Overall, these trends: 1. confirm the specialization of visual
neurons for the sparse coding of natural scenes, not only in V1 but also in higher-level areas; 2. support the
involvement of LI and LL in ventral-like processing, since their lower sparseness, compared to V1, is consistent
with the growth of invariance expected for ventral stream representations; and 3. reinforce the attribution of LM to
dorsal processing, given its increase of selectivity during fast-varying visual inputs.

III-57. Two-photon microscopy simulation for optics optimization and benchmarking
Alexander Song
Adam S Charles
Jeff Gauthier
Sue Ann Koay
David Tank
Jonathan Pillow

AS 31@ PRINCETON . EDU
ADAMSC @ PRINCETON . EDU
JLGAUTHI @ PRINCETON . EDU
KOAY @ PRINCETON . EDU
DWTANK @ PRINCETON . EDU
PILLOW @ PRINCETON . EDU

Princeton University
The recent successes of two-photon microscopy (TPM) in neural imaging has spurred the development both of
novel optics configurations for volumetric imaging and automated cell-finding algorithms. Despite these advances,
optimization of the optics and validation of the algorithms remains a difficult process. Microscopes are typically
laboriously hand-tuned and empirically tested for useful configurations. Projection volume imaging techniques
involve tradeoffs between point-spread function (PSF) shape/size and the signal/noise ratio (SNR). For example,
increasing the scan area reduces per-neuron SNR and the ability to detect weaker activity. Automated algorithms
can be validated by comparing found neural profiles (cell shapes) to manually located profiles. “Ground-truth” calcium transients obtained via simultaneous electrophysiology recordings, however, are unavailable outside of few,
specific optical configurations and samples. Here we provide a detailed simulation framework for TPM capturing
many of the nuances in calcium imaging. This framework generates neural volumes with adjustable cell/dendrite
densities and neuropil, and scans the volumes with simulated PSFs representing the optical setup. By adjusting
the PSF, these simulations can explore optical configuration parameters and assess cell-finding algorithms on
realisitic ground-truth data. We generate neural volumes with randomly placed neurons, modeled as isotropic
Gaussian processes (GPs) for the cell shapes and a stochastic process to grow dendrites, and neuropil (modeled

COSYNE 2017

207

III-58 – III-59
as 3D GPs) occupying the space between neurons. Our model also provides substantial computational improvements over other neural simulators. We simulate each neuron’s activity as a sparsely active autoregressive (AR-2)
process. The resulting volume is scanned with the mathematically defined PSF. We demonstrate that simulations
using diffraction-limited PSFs closely match TPM data collected in L2/3 mouse cortex. Furthermore, automated
cell-finding discovers similar spatial profiles to L2/3 mouse cortex, indicating that our simulator captures TPM
statistics and can be useful for validation of TPM techniques.

III-58. Differences between “statistical“ and “dynamical“ criticality in multineuron activity
Martino Sorbaro Sindaci1
Gerrit Hilgen2
Hayder Ibraheem Hussein A Amin3
Alessandro Maccione3
Luca Berdondini3
Evelyne Sernagor2
Matthias Hennig1

MARTINO. SORBARO @ ED. AC. UK
GERRIT. HILGEN @ NEWCASTLE . AC. UK
HAYDER . AMIN @ IIT. IT
ALESSANDRO. MACCIONE @ IIT. IT
LUCA . BERDONDINI @ IIT. IT
EVELYNE . SERNAGOR @ NEWCASTLE . AC. UK
M . HENNIG @ ED. AC. UK

1 University

of Edinburgh
of Newcastle
3 Istituto Italiano di Tecnologia
2 University

We investigate criticality in energy-based models of neural activity, with a focus on Restricted Boltzmann Machines
(RBMs). Recently, it was shown that statistical models fit to neural data often seem to be poised at a critical point
between a disordered and a spin-glass phase (Tkacik et al., 2015). Here, we fit RBMs, with different numbers of
hidden units to control their expressive power, to data generated by a simple network model that can be tuned
to or away from criticality (Gautam et al., 2015, Larremore et al., 2011). Our results show that the specific heat
diverges with the number of neurons considered, consistent with spin glass criticality, almost regardless of the
network parameters. This effect is less pronounced for the sub-critical phase, but does not disappear by adding
more hidden units to the RBM, which also does not increase the fit quality. Hence this effect is unlikely due to the
inability of the models to capture relevant correlation structure (Mastromatteo and Marsili, 2011). We next fit multineuron recordings from retinas and cultures, under normal conditions, and during pharmacological manipulations.
While we observe a similar divergence in the RBM specific heat, the relation between divergence and model size
differs between conditions, with stronger correlations in activity causing an earlier divergence. In sum, we provide
evidence that “statistical” criticality, related to the probability of codewords, and “dynamical” criticality, related to
propagation of neural activity, are different concepts. Criticality indicated by diverging energy fluctuations for spike
patterns is unrelated to critical system dynamics, but instead indicates a privileged property of the codebook
utilised by the circuit (Tkacik et al., 2015). Our analysis does not refute the claim that neural activity shows critical
properties, as other means of assessing criticality remain unaffected.

III-59. Amortized inference for fast spike prediction from calcium imaging
data
Artur Speiser1
Srini Turaga2
Evan Archer3
Jakob Macke1

ARTUR . SPEISER @ CAESAR . DE
TURAGAS @ JANELIA . HHMI . ORG
EVANARCHER @ GMAIL . COM
JAKOB . MACKE @ CAESAR . DE

1 research

center caesar
Research Campus
3 Columbia University
2 Janelia

208

COSYNE 2017

III-60

Calcium imaging allows neuronal activity measurements from large populations of spatially identified neurons invivo. However, spike inference algorithms are needed to infer spike times from fluorescence measurements of
calcium concentration. Bayesian model inversion can be used to infer spikes, using carefully designed generative
models that describe how spiking activity in a neuron influences measured fluorescence. Model inversion typically requires either computationally expensive MCMC sampling methods, or faster but approximate maximuma-posteriori estimation. We present a method for efficiently inverting generative models for spike inference. Our
method is several orders of magnitude faster than existing approaches, allowing for generative-model based spike
inference in real-time for large-scale population neural imaging, and can be applied to a wide range of linear and
nonlinear generative models. We use recent advances in black-box variational inference (BBVI, Ranganath 2014)
and ‘amortize’ inference by learning a deep network based recognition-model for fast model inversion (Mnih 2016).
At training time, we simultaneously optimize the parameters of the generative model as well as the weights of a
deep neural network which predicts the posterior approximation. At test time, performing inference for a given
trace amounts to a fast single forward pass through the network at constant computational cost, and without the
need for iterative optimization or MCMC sampling. On simple synthetic datasets, we show that our method is just
as accurate as existing methods. However, the BBVI approach works with a wide range of generative models
in a black-box manner as long as they are differentiable. In particular, we show that using a nonlinear generative model is better suited to describe GCaMP6 data (Chen 2013), leading to improved performance on real
data. The framework can also easily be extended to combine supervised and unsupervised objectives enabling
semi-supervised learning of spike inference.

III-60. Predictive plasticity in dendrites: from a computational principle to
experimental data
Dominik Spicher1
Claudia Clopath2
Walter Senn1

SPICHER @ PYL . UNIBE . CH
CLOPATHLAB . IMPERIAL @ GMAIL . COM
SENN @ PYL . UNIBE . CH

1 University
2 Imperial

of Bern
College London

Plasticity of excitatory cortical synapses is thought to be the main mediator of mammalian learning. Due to the
selective advantage that the ability to adapt to a changing environment grants, it is rea- sonable to assume that the
processes governing these changes in connection strength are in some sense optimal. Yet, this has been difficult
to reconcile with an “embarrassment of riches” of the LTP and LTD phenomenology. Here, we present an attempt
at bridging this gap by showing that a mathematically derived model can exhibit some of these experimentally
observed effects, while still retaining functional capabilities under diverse learning paradigms. Our work is based
on a published plasticity model that postulates that learning is driven by an intraneuronal prediction error where
the weights of “student inputs” onto a dendritic compartment change in order to reproduce voltage changes
imposed on a somatic compartment by “teacher inputs.” We show here that this two-compartment model of a
pyramidal neuron can be extended in some simple ways, such as using conductance-based instead of currentbased inputs, bringing it closer to the biophysics of pyramidal neurons. This allows us to reproduce a diverse set of
experimental observations on cortical plasticity, such as different characteristics of the spike-timing dependence of
plasticity. Additionally, we show within a simple setup of a pattern recognition task that the extended model, while
being less analytically tractable, can still perform well under unsupervised and reinforcement learning paradigms.
Therefore, a single learning rule derived from the optimization of a well-defined cost function can be brought into
correspondence with a large body of experimental evidence on synaptic plasticity, while still providing a diverse
set of relevant functionality.

COSYNE 2017

209

III-61 – III-62

III-61. Superior limb-movement decoding from cortex with a new, unsupervisedlearning algorithm
Joseph Makin
Joseph E O’Doherty
Philip Sabes

MAKIN @ PHY. UCSF. EDU
JOEYO @ PHY. UCSF. EDU
SABES @ PHY. UCSF. EDU

University of California, San Francisco
Brain-machine interfaces use neurological recording devices and a set of “decoding” algorithms to transform brain
activity directly into real-time control of a machine, archetypically a robotic arm or a cursor. The standard procedure treats neural activity—vectors of spike counts in small temporal windows—as noisy observations of the
kinematic state (position, velocity, acceleration) of the arm. Inferring the state from the observations then takes
the form of a dynamical filter, typically some variant on Kalman’s (KF). The KF, however, although fairly robust in
practice, is optimal only when the relationships between variables are linear and the noise is Gaussian, conditions
usually violated in practice. We overcome these limitations with a new filter, the “recurrent exponential-family
harmonium” (rEFH). Like the KF, the rEFH treats spike counts as noisy observations of a latent state. Unlike the
KF, the recurring motif in the rEFH is an undirected graphical model—a variant on the RBM—which underwrites
three critical differences: (1) The conditional distribution of spike counts can be correctly modeled as Poisson,
rather than approximated as Gaussian. (2) Unsupervised learning: the latent variables are not assigned to the
arm kinematics, but rather come to encode whatever dynamical variables are required to explain the observations. (3) The hidden units, being numerous and binary, acquire a distributed representation of kinematic state,
admitting arbitrary nonlinear relationships among states and observations. We compare cross-validated offline
reconstruction of reaches in the plane, in three sessions, one monkey, implanted with Utah arrays in M1 and S1.
Velocities and accelerations are more accurately reconstructed from the hidden units of the rEFH than with the
KF, though the opposite holds for position decoding. However, Kalman filtering the rEFH’s estimates yields the
best overall decoder for position (18% average improvement in R2), velocity (29%), and acceleration (118%).

III-62. Dopamine reward prediction errors reflect hidden state inference across
time
Clara Starkweather
Naoshige Uchida
Samuel Gershman

SKARALC @ GMAIL . COM
UCHIDA @ MCB . HARVARD. EDU
GERSHMAN @ FAS . HARVARD. EDU

Harvard University
Midbrain dopamine neurons are thought to drive associative learning by signaling reward prediction error (RPE), or
actual minus expected reward. In particular, the temporal difference (TD) learning model has been a cornerstone
in understanding how dopamine RPEs could drive associative learning. Classically, TD learning imparts value to
features that serially track the passage of elapsed time relative to observable stimuli. In the real world, however,
sensory stimuli provide ambiguous information about the hidden state of the environment, leading to the proposal
that TD learning might instead operate over an inferred distribution of hidden states (a ‘belief state’). In this
work, we asked whether dopaminergic signaling supports a TD learning framework that incorporates hidden state
inference. We recorded optogenetically-identified dopamine neurons as mice performed either one of two classical
conditioning tasks. In both tasks, the timing of reward relative to cue was randomly varied across trials (Fig 1a,b).
In Task 1 (n = 3 mice, 30 dopamine neurons), reward was always given (100% rewarded). We found that postreward RPEs decreased as a function of time (F 8, 232 = 5.56, P = 1.9 × 10ˆ−6, 2-way ANOVA - factors: time,
neuron; Fig 1c). In Task 2 (n = 4 mice, 43 dopamine neurons), reward was occasionally omitted (90% rewarded).
In contrast to Task 1, post-reward RPEs increased as a function of time (F 8, 336 = 8.23, P = 3.48 × 10ˆ−10,
2-way ANOVA - factors: time, neuron; Fig 1d). In both Tasks 1 and 2, pre-reward RPEs decreased as time elapsed
(P <2.0 × 10ˆ−5 in both Tasks; black line, Fig 1c,d). Through computational modeling, we show that a TD learning
rule that incorporates hidden state inference uniquely recapitulates our empirical results (Fig 1e,f). These results

210

COSYNE 2017

III-63 – III-64
provide evidence in favor of an associative learning rule that combines cached values with hidden state inference.

III-63. Exactly solvable nonlinear recurrent networks for context-dependent
information processing
Christopher Stock
Subhaneil Lahiri
Alexander Williams
Surya Ganguli

CHSTOCK @ STANFORD. EDU
SULAHIRI @ STANFORD. EDU
AHWILLIA @ STANFORD. EDU
SGANGULI @ STANFORD. EDU

Stanford University
An increasingly common approach to confronting large-scale neural recordings during complex tasks involves
training large neural networks to solve the same task. This has led, for example, to highly accurate models
of the ventral visual stream, prefrontal cortex, and retina. However, such network models themselves can be
so complex that they defy intuitive understanding, raising profound questions about the nature of explanation
in systems neuroscience. For example, what design principles govern how the connectivity and dynamics of
these networks endow them with their computational capabilities? Moreover, what would a codification of these
design principles even look like? Here we follow a cue from Feynman’s dictum: “What I cannot create, I do not
understand.” In essence, if we really understand how connectivity and dynamics give rise to a computation, we
should be able to analytically write down, using pencil and paper, a variety of exactly solvable neural networks
that accomplish a given task, instead of obtaining them indirectly through an opaque process of neural network
training. We accomplish this goal for a wide variety of context-dependent information processing tasks in which
the same stimulus or behavioral condition can yield different actions depending on prior context. Our framework
enables us to take such a task, specified as a set of input-dependent internal state transitions and actions, and
generate an ensemble of nonlinear recurrent neural networks that accomplish the requisite transitions through
their connectivity and dynamics. As an illustration of our method, we show how to analytically generate a diversity
of networks that accomplish a canonical context-dependent task: the Wisconsin card sorting test. Overall, our
newly discovered framework for generating exactly solvable nonlinear networks, through a process of analytical
forward engineering, provides a foundation for extracting design principles governing how neural connectivity and
dynamics can conspire to generate emergent computations.

III-64. Distinct roles of basolateral amygdala and orbitofrontal cortex in valuelearning under uncertainty.
Alexandra Stolyarova
Alicia Izquierdo

ASTOLYAROVA @ PSYCH . UCLA . EDU
AIZQUIE @ PSYCH . UCLA . EDU

University of California, Los Angeles
In dynamic environments, value updating in response to surprising outcomes is critical for reward maximization.
Optimal performance relies on the ability to distinguish volatility (i.e., fundamental changes in reward distribution)
from predictable fluctuations (expected uncertainty or risk). Previous findings demonstrated that humans are capable of segregating these forms of uncertainty (Behrens et al., 2007; Diederen and Schultz, 2015). In the present
study, we developed a delay-based task to assess such cognitive function in rodents. At baseline, rats were required to respond to one of two options, each identical in mean reward rate but different in the variance of delay
distributions (high variability, HV vs. low variability, LV). Following the establishment of stable performance, they
experienced value upshifts and downshifts on each option independently, followed by return to baseline conditions. We fit different versions of two major classes of models commonly utilized in human studies, approximately
Bayesian delta-rule (Behrens et al., 2007; Nassar et al., 2010) and reinforcement learning models (Diederen
and Schultz, 2015), to trial-by-trial choices for each animal. Our results show that rodents, similar to humans,

COSYNE 2017

211

III-65
take distinct types of uncertainty into account when making decisions based on expected values and updating
representations in response to surprising outcomes to maximize the rate of reward acquisition. Such behavioral
complexity in rats allowed us to investigate causal contributions of basolateral amygdala (BLA) and medial orbitofrontal cortex (mOFC), brain regions consistently implicated in adaptive surprise-driven learning and outcome
representation (Mainen and Kepecs, 2009; Wassum and Izquierdo, 2015), to value updating in uncertain environments. Our results indicate that functionally-intact mOFC is required for adaptations to expected uncertainty
and stabilization of value representation in stochastic environments. Conversely, BLA is critical for facilitation of
learning in response to volatility.

III-65. How much to gain: targeted gain modulation facilitates learning in recurrent motor circuits
Jake Stroud1
Guillaume Hennequin2
Mason Porter3
Tim P Vogels1

JAKE . STROUD @ WADH . OX . AC. UK
G . HENNEQUIN @ ENG . CAM . AC. UK
MASON @ MATH . UCLA . EDU
TIM . VOGELS @ CNCB . OX . AC. UK

1 University

of Oxford
of Cambridge
3 University of California, Los Angeles
2 University

Primary motor cortex (M1) can be viewed as a “dynamical engine“ that produces multiphasic activity transients
from specific input patterns (Churchland et al., 2012). Such temporally structured neuronal population activity
can be explained by recent network models with optimised recurrent architectures (Hennequin et al., 2014 and
Sussillo et al., 2015). However, it remains unclear how network activity within such models can be refined during
behavioural learning. Here, we demonstrate that reward-based learning of single-neuron input-output gains is
an effective mechanism for refining network activity in two commonly used recurrent network models. We show
that gain modulation affects neuronal firing rate activity in a predictable manner that can be exploited to reduce
errors in network outputs. Interestingly, we find that a relatively small number of modulatory control units provide
sufficient flexibility to adjust high-dimensional network activity on biologically relevant time scales. Such coarse
neuromodulatory control is consistent with the sparse and diffuse dopaminergic projections to M1 observed in
both primates and humans (Huntley et al., 1992 and Hosp et al., 2011) as well as many other neuromodulatory
systems. Additionally, traditional Hebbian synaptic plasticity mechanisms work in concert with reward-based
learning of neuronal gains - allowing memories to be permanently imprinted on slower time scales after the
desired activity is achieved through reward-mediated gain modulation. Furthermore, a novel network output
can be intuited as a linear combination of previously learned gain patterns and refined thereafter - substantially
reducing training time. Our results demonstrate that reward-based gain modulation is a viable method of learning
in recurrent cortical circuits - providing several advantages as well as complementing traditional synaptic plasticity
mechanisms.

212

COSYNE 2017

III-66 – III-67

III-66. Two-photon imaging of functional diversity of trigeminal meningeal afferents in awake mice
Arthur Sugden1
Jun Zhao1
Helaine Gariepy1
Kirsten Levandowski2
Dan Levy1
Mark L Andermann2,1

ARTHUR . SUGDEN @ GMAIL . COM
JZHAO 1@ BIDMC. HARVARD. EDU
HGARIEPY @ BIDMC. HARVARD. EDU
KLEVANDO @ GMAIL . COM
DLEVY 1@ BIDMC. HARVARD. EDU
MANDERMA @ BIDMC. HARVARD. EDU

1 Harvard
2 Beth

University
Israel Deaconess Medical Center

One in eight people suffers from debilitating migraines, yet the mechanisms underlying headaches and migraines
remain unclear. Indirect evidence has suggested a role for trigeminal afferents that innervate the meninges, particularly the dura mater. Electrophysiological recordings of trigeminal ganglion afferent somata, while informative,
have been difficult to correlate with afferent innervation patterns or genetic markers. Here, we describe an in
vivo preparation we have developed for simultaneously imaging the activity of different trigeminal afferents in
awake mice across hours and days using 3D two-photon calcium imaging. A chronic 3 mm cranial window is
implanted over posteromedial cortex and an adeno-associated virus encoding the calcium indicator GCaMP6s is
injected into the trigeminal ganglia. Red dextran is injected into the tail vein to allow visualization of dural blood
vessels. This preparation yields a unique platform to simultaneously measure the activity of multiple afferents
in addition to the animal’s behavior, arousal, brain motion and torsion along multiple dimensions, and intracranial pressure. With this preparation, we are addressing the hypothesis that populations of trigeminal afferents
collectively monitor the overall mechanical state of the dura and of dural vasculature. We have identified one
subset of afferents that primarily respond to brain acceleration, and other afferents that respond primarily to locomotion and vasodilation. We are decomposing the activity of different afferent subtypes using a generalized
linear model (GLM), and measuring afferent responses during pharmacological models of headache. We hope to
combine these imaging methods with anatomical and genetic markers to better classify functionally diverse sets
of simultaneously-recorded trigeminal afferents. Thus, by directly visualizing activity in populations of trigeminal
afferents in awake mice, we can extend analyses of population coding to a highly understudied set of neurons
regulating the meninges that surround, protect, and interact with the cerebral cortex.

III-67. Central circuits encoding wind direction in the fruit fly
Marie Suver
Katherine Nagel

SUVER . MARIE @ GMAIL . COM
KATHERINE . NAGEL @ NYUMC. ORG

New York University
Wind direction cues are crucial for navigation in many insects. For example, wind direction information is used
by both fruit flies (Bhandawat et al. 2010) and mosquitoes (Carde and Gibson, 2010) to navigate towards an
attractive odor. Flies detect wind using antennal mechanoreceptors (Yorozu et al. 2009), however, where and
how wind direction information is encoded in the central brain is not known. We developed a novel preparation
that enables us to simultaneously record from single identified neurons in Drosophila and to present wind stimuli
to the fly from multiple directions. We conducted an electrophysiological screen for central wind-direction tuned
neurons, based on anatomical criteria. Using this approach, we have identified putative second- and third-order
neurons that encode wind direction relative to the fly. Among the third-order neurons are two pairs of bilateral
neurons that are reliably depolarized by ipsilateral wind and hyperpolarized by contralateral wind. These neurons
project contralaterally, where they appear to contact their sister neurons on the opposite side of the brain. By
surgically manipulating the antennae, we found that these neurons receive their sensory input from antennal
mechanoreceptors, and that they combine information from the two antennae in a nonlinear manner that increases
their dynamic range. We are currently using optogenetics, genetic silencing, and laser ablation to determine the

COSYNE 2017

213

III-68 – III-69
origin of this nonlinearity, and to test the role of these neurons in wind-guided behaviors.

III-68. Optimal policy leads to “irrational” choice behavior under multiple alternatives
Satohiro Tajima1,2
Jan Drugowitsch3
Alexandre Pouget1

SATOHIRO. TAJIMA @ GMAIL . COM
JDRUGO @ GMAIL . COM
ALEXANDRE . POUGET @ UNIGE . CH

1 University

of Geneva
PRESTO
3 Harvard University
2 JST

Previous physiological and behavioral experiments have revealed puzzling properties of human/animal decisions
involving more than two options. For example, your preference of “beef” over “chicken” in-flight meals can be
perturbed by the presence of a third “pasta” option even if you never choose that option. More specifically,
the presence of a third option makes it harder to choose between the other two. This and other observations
seem to reflect interactions among choice options (e.g., Louie et al., 2013). Why the nervous system ought
to have such properties remains poorly understood. We address these conundrums by deriving the normative
strategies for general value-based decisions with multiple options that identify optimal stopping-rules for valuebased evidence accumulation. The resulting complex nonlinear and time-dependent decision-boundaries in a
high-dimensional belief space does not seem to lend itself to a simple implementation in neural circuits. However,
we find that a geometric symmetry in those boundaries allows the optimal strategy to be implemented by a
remarkably simple neural mechanism, with time-dependent activity-normalization in a recurrent circuit controlled
by an urgency signal. Importantly, this neural model reproduces the apparently “irrational” interaction between 3
choices mentioned above, suggesting that it is not an unfortunate consequence of noisy neurons and biological
constraints (Louie & Glimcher, 2012). Our model also captures Hick’s law, i.e., the fact that reaction times are
proportional to the log of the number of options. Therefore, our results suggest that several puzzling aspects
of the neurophysiology and behavior of decision making with more than two options may be side effects of the
optimal policy.

III-69. Task demand modulates collective attractor dynamics in visual cortex
Satohiro Tajima1,2
Kowa Koida3
Chihiro I Tajima4
Hideyuki Suzuki5
Kazuyuki Aihara4
Hidehiko Komatsu6

SATOHIRO. TAJIMA @ GMAIL . COM
KOIDA @ EIIRIS . TUT. AC. JP
CHIHI . AT. HEART @ GMAIL . COM
HIDEYUKI @ IST. OSAKA - U. AC. JP
AIHARA @ SAT. T. U - TOKYO. AC. JP
KOMATSU @ NIPS . AC. JP

1 University

of Geneva
PRESTO
3 Toyohashi University of Technology
4 University of Tokyo
5 Osaka University
6 National Institute for Physiological Sciences
2 JST

Context-dependent behavior of animals has been related to flexible attractor dynamics of neural populations in the
prefrontal areas (e.g., Mante, Sussillo et al., 2013; Stokes et al., 2013). Here we report that the context-dependent
attractor dynamics during task-switching are present also at the level of inferior temporal (IT) cortex in macaque
monkeys performing task-switches between fine color discrimination and categorization. By reconstructing the

214

COSYNE 2017

III-70
collective neural dynamics with nonlinear decoders, we demonstrate the emergence of bistable attractor dynamics
in the perceptual space during stimulus categorization. The slow and stimulus dependent evolution of the neural
dynamics are naturally accounted for by a recurrent mechanism in which the structure of collective attractor
dynamics in IT neurons is modulated by task context, but not fully explained with conventional models of taskdependent gain-modulations. The dynamical modulation selectively increases the information relevant to task
demands, indicating that such modulation is beneficial for perceptual decisions. A computational model that
features nonlinear recurrent interaction among neurons with a task-dependent background input replicates the
key properties of neural dynamics in the data. These results suggest that the recurrent dynamical mechanisms
underlying the flexible perceptual abilities can involve the sensory cortex.

III-70. Coordination in the hippocampal-prefrontal network during awake and
sleep sharp wave ripple events
Wenbo Tang1
Justin Shin1
Loren Frank2
Shantanu Jadhav1
1 Brandeis

WBTANG @ BRANDEIS . EDU
JDSHIN @ BRANDEIS . EDU
LOREN @ PHY. UCSF. EDU
SHANTANU @ BRANDEIS . EDU

University
of California, San Francisco

2 University

Interactions between the hippocampus and prefrontal cortex (PFC) are critical for learning and memory. Coordinated reactivation of hippocampal-prefrontal activity during sharp-wave ripple (SWR) events is thought to play
an important role in these interactions. We and others have shown that SWRs during both behavior (awake
SWRs) and slow-wave sleep (sleep SWRs) are necessary for learning, and both awake and sleep SWRs engage
PFC activity (Girardeau et al., 2010; Wierzynski et al, 2010; Jadhav et al., 2012, 2016). But whether sleep and
awake reactivation represent the same phenomena, or if there are any differences between the two, indicating
different roles in memory processes, is still unclear. To address this question, we recorded neuronal ensembles
simultaneously in PFC and in the CA1 region of the hippocampus across sleep and behavior sessions as rats
learned a novel spatial alternation task. Surprisingly, we found that SWR-related activity in PFC during waking
is distinct from that during sleep, with PFC neurons showing significantly different patterns of modulation in the
two cases. Further, awake SWRs reactivated spatial experiences in the CA1-PFC network with stronger strength
and higher fidelity than sleep SWRs. Finally, we found that this structured CA1-PFC reactivation is strongest
during initial learning, especially for awake SWRs. In contrast, hippocampal reactivation maintained significance
throughout the duration of learning and performance of the task. Our results demonstrate for the first time that
hippocampal-cortical reactivation during awake and sleep SWRs are distinct phenomena. Further, they suggest
that coordination during awake SWRs is prevalent early during task acquisition, and could provide a mechanism
for the formation of hippocampal-prefrontal associations in novel environments to support behavioral learning.

COSYNE 2017

215

III-71 – III-72

III-71. Generalized leaky integrate-and-fire models classify multiple neuron
types
Corinne Teeter
Ramakrishnan Iyer
Vilas Menon
Nathan W Gouwens
Nicholas Cain
David Feng
Jim Berg
Christof Koch
Stefan Mihalas

CORINNET @ ALLENINSTITUTE . ORG
RAMI @ ALLENINSTITUTE . ORG
VILASM @ ALLENINSTITUTE . ORG
NATHANG @ ALLENINSTITUTE . ORG
NICHOLASC @ ALLENINSTITUTE . ORG
DAVIDF @ ALLENINSTITUTE . ORG
JIMB @ ALLENINSTITUTE . ORG
CHRISTOFK @ ALLENINSTITUTE . ORG
STEFANM @ ALLENINSTITUTE . ORG

Allen Institute for Brain Science
In the mammalian neocortex, there is a high diversity of neuronal types (Markram et al. 2004, Tasic et al. 2016).
Here we make two contributions to understanding cell types using simple point neuron models. First, to facilitate
construction of system models with multiple cell types, we generate a database of generalized linear integrate
and fire (GLIF) point models of five levels of increasing complexity aiming to reproduce the spike times of the
recorded neurons associated with the Allen Institute Cell Types Database (celltypes.brain-map.org). We tested
the performance of these GLIF models on test data from 748 neurons from 14 transgenic lines. The standard
LIF model already captures 70% of total temporal variance of spiking (at 10 ms temporal granularity). The more
sophisticated model captures 76% of total variance (with interneurons always performing significantly better than
excitatory neurons). Second we show that these GLIF point models provide a useful dimensionality reduction
of the most relevant aspects of neuronal input/output transformations. We perform unsupervised clustering on
two data sets 1. the parameters extracted from the GLIF models and the optimization procedure and 2. sixteen
common electrophysiological features. We show that classification on GLIF parameters yields similar clustering
power to that of electrophysiological features. We show the ability of simple generalized leaky integrate and fire
point models to both reproduce the spike times of biological data and effectively reduce the biological space to
a set of useful parameters. This hierarchy of models of increasing complexity will facilitate the development of
system-wide models with multiple cell types.

III-72. Environmental adaptation of olfactory receptor distributions
Tiberiu Tesileanu1
Simona Cocco2
Remi Monasson2
Vijay Balasubramanian3

TTESILEANU @ GMAIL . COM
COCCO @ LPS . ENS . FR
MONASSON @ LPT. ENS . FR
VIJAY @ PHYSICS . UPENN . EDU

1 CUNY

Graduate Center
Normale Superieure
3 University of Pennsylvania
2 Ecole

Olfaction is unique among mammalian senses in that olfactory sensory neurons are replaced regularly, on the
timescale of weeks. Each sensory neuron in the olfactory epithelium expresses one of several hundred receptor genes, each yielding a different binding profile to a wide array of volatile molecules (odorants). Experiments
show that after replacement, the proportions of the neurons expressing different receptor types can change. We
propose that these changes reflect adaptation of newly-born neurons to the olfactory experience of the animal in
order to enhance detection of natural odors. We build a model for olfactory adaptation in which the distribution
of receptor types is chosen so that the receptor responses form a maximally-accurate representation of odorant
concentrations given a fixed total number of sensory neurons. When the number of neurons is small, the optimal representation is achieved when a single receptor type is populated. When the total number of neurons is
very large, the optimal receptor distribution is close to uniform. In intermediate cases, only some of the available

216

COSYNE 2017

III-73 – III-74
receptor types should be present in significant proportions. In this regime, our model predicts that a variation in
olfactory environment should lead to a significant change in the abundances of various receptor types. Such an
effect has recently been observed in mice. In our model, the optimal receptor distribution depends on the affinity
profile of each receptor against possible odorants, and on the probabilities of encountering the odorants. The
model can thus be used to gain insight about the olfactory environment given receptor affinities and abundances,
which can be measured experimentally. Conversely, given additional assumptions regarding the structure of natural olfactory scenes, our model can predict how a change in environment would change the receptor distribution.
This can be directly compared with experimental results.

III-73. Choice-supportive asymmetry in learning rates
Heloise Thero
Henri Vandendriessche
Valerian Chambon
Stefano Palminteri

THERO. HELOISE @ GMAIL . COM
HENRI . VANDENDRIESSCHE @ ENS . FR
VALERIAN . CHAMBON @ GMAIL . COM
STEFANO. PALMINTERI @ GMAIL . COM

Ecole Normale Superieure
Reinforcement learning theories assume that action values are learnt via the calculation of a reward prediction
error, i.e. the difference between the obtained and the expected outcome (Sutton et Barto 1998). These theories
suppose that subjects learn similarly, regardless of the valence (positive or negative) of the prediction error. Yet,
recent findings support a different view: participants have a positive bias, i.e. they tend to disregard bad news
by integrating worse-than-expected outcomes at a lower rate compared to better-than-expected ones (Lefebvre
et al. 2016). This result has been interpreted as a general valence-induced bias, but it could as well be explained
by another cognitive heuristic, i.e., a choice-supportive bias (ascribing a positive value to an option one has
chosen). To disentangle these two alternatives, we implemented a simple instrumental learning task in which
the participant’s choice was either “free“ or “forced“. In free choice trials, the subject freely chose between two
symbols, whereas in forced choice trials the computer preselected one symbol and the subject was forced to
match this choice. Model comparison using the Bayesian information criterion showed that a model allowing for
different learning rates for positive and negative outcomes on free and forced choice trials best fitted the data.
We found that participants updated action values more in response to positive than to negative outcomes on
free choice trials, while forced trials were characterized by the reverse pattern (valence-by-choice interaction,
p=0.01). Together, these results suggest that what has been previously interpreted as a valence-induced bias
(Sharot et Garret 2016) is better characterized by a choice-supportive bias, whereby participants disregard both
negative outcomes from freely chosen actions and positive outcomes resulting from forced choices. These results
constitute a new step in understanding how the most salient aspect of information - valence - affects our choices
in a self-centered manner.

III-74. Topological analysis of neural population activity in the auditory system of the European starling
Bradley Theilman
Timothy Gentner

BTHEILMA @ UCSD. EDU
TGENTNER @ UCSD. EDU

University of California, San Diego
Neurons in higher-order sensory regions represent complex naturalistic stimuli through the coordinated spatiotemporal structure of neural population activity. The responses of these neurons are often difficult to characterize individually using techniques such as the receptive field computations because their activity is often highly selective,
sparse, and temporally precise. Population activity is frequently quantified by studying the pairwise correlation
structure of neurons in the population, but it is infeasible to compute statistical correlations beyond second order.

COSYNE 2017

217

III-75
Thus, to understand how secondary sensory regions represent complex, naturalistic stimuli, new techniques for
quantifying this spatiotemporal structure are needed. Here, we adapt techniques from the mathematical field of
Algebraic Topology to quantify the spatiotemporal structure of neural population activity from higher-order sensory
regions by converting this structure into a topological space known as a simplicial complex. We then characterize
this space by computing topological invariants that capture the structure of the space as sequences of integers.
To determine what kinds of information about complex, naturalistic stimuli are carried by this topological representation of population activity, we trained European starlings, a songbird, to classify patterned complex acoustic
stimuli built from natural birdsong. We apply these topological techniques to simultaneously recorded population
activity in the secondary auditory regions of the bird’s brain while presenting the learned stimuli. We demonstrate
that these topological measures carry significant behaviorally relevant information sufficient to decode behavior
class and stimulus familiarity/unfamiliarity at a high level of accuracy, exceeding that attainable from naive population firing rates. Using recent developments in graph theory, we propose information-theoretic measures of this
neurally derived topological structure. These measures allow for the fitting of spiking network models to the topological structure of neural systems, and open a novel, powerful, and previously unexplored avenue to investigate
the coordinated spatiotemporal dynamics of neural populations.

III-75. Shared perceptual spaces for high-dimensional natural acoustic signals
Marvin Thielk1
Tim Sainburg1
Tatyana Sharpee2
Timothy Gentner1

MTHIELK @ UCSD. EDU
TIMSAINB @ GMAIL . COM
SHARPEE @ SNL . SALK . EDU
TGENTNER @ UCSD. EDU

1 University
2 Salk

of California, San Diego
Institute for Biological Studies

A longstanding impediment to understanding the perception of communication signals is the inability to parameterize complex natural stimuli. We solve this problem using machine learning, using a generative deep convergent
network to represent and parameterize the space of natural starling birdsong. We train starlings on a standard
operant conditioning paradigm to classify short song segments. We then project these segments into the latent
space of the deep network, and generate morphs between pairs of song segments and ask subjects to classify
these novel stimuli. From the set of responses to these intermediate signals, we generate psychometric functions
that describe the animals’ perception of the natural stimulus space. These psychometric function reveal remarkable similarities in both the threshold and sensitivities across multiple subjects on specific segment contrasts.
This convergent perception indicates a shared perceptual space. In addition, these boundaries are experience
dependent; arbitrarily assigning song segments to different responses alters the shape of the resulting psychometric functions for different birds in similar ways. We then train a decoder on the neural activity recorded from
populations of neurons in CM, a secondary auditory region, to classify the presented template song segment and
predict the identity on the interpolated morphed stimuli. We find that the boundaries predicted from the neural
activity does not match the boundaries derived from the behavioral data in all cases. These results are consistent
with a mixed selectivity model of neural responses and makes the existence of a shared perceptual space more
remarkable.

218

COSYNE 2017

III-76 – III-77

III-76. A data-driven approach to identify ion channel correlations in homeostatic regulation
Kun Tian
Cengiz Gunay
Astrid Prinz

IO. KUNTIAN @ GMAIL . COM
CENGIQUE @ GMAIL . COM
ASTRID. PRINZ @ EMORY. EDU

Emory University
Intrinsic and synaptic homeostatic plasticity mechanisms have been examined extensively in isolation in both invertebrate and vertebrate nervous systems, yet how multiple cellular and synaptic currents interact during homeostatic regulation is still unclear. The pyloric circuit in the crab Cancer borealis is a central pattern generator
that produces a stereotyped rhythm. This rhythm is maintained by homeostatic plasticity in face of perturbations.
Under normal conditions, the rhythm is maintained in a neuromodulator-dependent manner. When descending
neuromodulators to the pyloric circuit are blocked, a process called decentralization, the pyloric rhythm is disrupted but can recover 3-4 days later in a cell-autonomous manner1. Recent studies suggest that pyloric rhythm
recovery is related to changes in the linear ratios of ion channel densities, i.e. ion channel correlations, yet a
systematic examination of ion channel correlations in the pyloric circuit is still missing due to experimental limitations. To systematically identify ion channel correlations both before and 4 days after decentralization in the
pyloric circuit, we combine a multi-objective evolutionary algorithm (MOEA) called NSGA II with a biophysical
model of the pyloric circuit2,3. Our preliminary results identify eight ion channel correlations with correlation coefficient >= 0.8 both within neurons and between cellular and synaptic currents before decentralization (n=60)
(Table 1). However, no strong ion channel correlations were found 4 days after decentralization (n=84). This in
general matches experimental results4 and together experiment and modeling suggest that the cell-autonomous
homeostatic mechanism might be independent of ion channel correlations. This study is novel because the datadriven approach enables a systematic examination of ion channel correlations at the circuit level and generates
experimentally testable predictions. Nonlinear correlation measures and dimensionality reduction methods can
be applied to elucidate the nonlinearities underlying the cell-autonomous homeostatic mechanisms.

III-77. Modeling network oscillations in precise subcallosal cingulate white
matter deep brain stimulation
Vineet Tiruvadi1,2
Allison Waters1
Liangyu Tao2
Rohit Konda2
Andrea Crowell1
Patricio Riva-Posse1
Robert Butera2
Helen Mayberg1
1 Emory

VTIRUVADI 3@ GATECH . EDU
ALLIWATERS @ EMORY. EDU
LTAO 31@ GATECH . EDU
RKONDA 6@ GATECH . EDU
ANDREA . CROWELL @ EMORY. EDU
PRIVAPO @ EMORY. EDU
RBUTERA @ GATECH . EDU
HMAYBER @ EMORY. EDU

University
Institute of Technology

2 Georgia

Deep brain stimulation (DBS) is being increasingly used to treat patients with complex psychiatric disorders, including treatment resistant clinical depression (TRD) [1]. Precise stimulation of white matter tracts passing through
the subcallosal cingulate (SCCwm) is linked to improvements in TRD symptoms but a mechanistic understanding
of SCCwm-DBS effects on pathological brain signaling remains incomplete [1,2]. Recent hardware advances
allow for direct study of network-level electrophysiology in patients implanted with SCCwm-DBS. This enables
characterization and data-driven modeling of SCCwm-DBS modulation of brain network signaling that can then
be systematically linked to TRD symptom relief and/or remission. In this study, we characterize network oscillatory
power changes in three TRD patients treated with precise DBS of critical white matter tracts [2]. Using a prototype DBS stimulator capable of recording local field potentials (LFPs) (via Medtronic Activa PC+S) in conjunction

COSYNE 2017

219

III-78 – III-79
with dense-array scalp EEG (dEEG), we identify specific oscillatory bands within specific brain network nodes
directly modulated by precise SCCwm-DBS [3]. We then develop a phase-oscillator network model for three likely
DBS mechanisms in order to hypothesis-test the patient data. Two results are presented here: a network-level
oscillatory signal indicating adequate stimulation of critical SCCwm tracts, and a data-driven model of SCCwmDBS mechanism. This investigation is a critical first step in understanding how DBS of SCCwm engages and
modulates brain network activity, and how that engagement induces TRD remission. The results presented here
will directly improve clinical implementation of SCCwm-DBS to optimize anatomical and functional engagement
of brain networks associated with depression, enable further scientific studies into the link between brain network
electrophysiology and complex higher-level behavioral dynamics, and inform engineering of closed-loop DBS devices able to automatically and rationally program stimulation settings to optimally modulate network-level brain
states.

III-78. MAP inference in linear models with sparse connectivity using sister
mitral cells.
Sina Tootoonian1,2
Peter Latham1,2
1 Gatsby

SINA . TOOTOONIAN @ GMAIL . COM
PEL @ GATSBY. UCL . AC. UK

Computational Neuroscience Unit
College London

2 University

Sensory processing is hard because the variables of interest are encoded in spike trains in a relatively complex
way. A major goal in sensory processing is to understand how the brain extracts those variables. Here we
revisit a common encoding model (Olshausen and Field, 1996) in which variables are encoded linearly. Although
there are typically more variables than neurons, this problem is still solvable because only a small number of
variables appear at any one time (sparse prior). However, previous solutions usually require all-to-all connectivity,
inconsistent with the sparse connectivity seen in the brain. Here we propose a principled algorithm that trades
speed for connection sparsity to provably reach the MAP inference solution. Our algorithm is inspired by the
mouse olfactory bulb, but our approach is general enough to apply to other modalities; in addition, it should be
possible to extend it to nonlinear encoding models.

III-79. Decoding arm force from neural population dynamics in PMd and M1
during reaching
Eric Trautmann
Sergey Stavisky
Jonathan Kao
Stephen Ryu
Krishna Shenoy

ETRAUT @ STANFORD. EDU
SERGEY. STAVISKY @ STANFORD. EDU
JCYKAO @ STANFORD. EDU
SEOULMAN @ STANFORD. EDU
SHENOY @ STANFORD. EDU

Stanford University
A number of experiments have characterized the tuning properties of individual neurons in PMd and M1 of rhesus
monkeys when loads are applied to the arm [e.g., Sergio 2005], but the relationship between population-level
neural dynamics and forces generated during reaching is less well understood. Here, we report how neural
dynamics in PMd and gyral M1 change when controlling the arm while it supports different external weights.
We demonstrate that the arm’s output force along the anti-gravity (vertical) axis can be decoded from the neural
population before and during movement. Importantly, neural correlates of force are small and heterogeneous
across individual electrodes, but form an orderly readout at the level of the motor cortical population that can be
easily decoded. We used a support vector machine to identify a vertical force axis (VFA) in the high-dimensional
neural population state space using pre-movement threshold crossings from two 96-channel arrays in PMd and

220

COSYNE 2017

III-80 – III-81
M1 for reaches with either 200g or 0g attached to the arm. The cross-validated classification performance on
single trials was 77% for monkey J (monkey R: 59%). To test whether this axis meaningfully encodes force
information throughout the trial, we made several predictions regarding projections of neural state onto the VFA.
First, when moving to upward targets, neural activity should move towards the high-force end of the force axis
when accelerating, then to the low-force end when decelerating. The opposite should be true for reaches to
downward targets, which require little vertical force early but higher vertical forces to decelerate. Lastly, we predict
that intermediate targets should display graded VFA projections between the upward and downward targets during
acceleration and deceleration. The data support each of these predictions, suggesting that the VFA serves as a
meaningful and reliable readout of vertical arm force while reaching.

III-80. Selective involvement of mouse prefrontal activity during an auditory
discrimination task
Hua-an Tseng
Xue Han

HUAANTSENG @ GMAIL . COM
XUEHAN @ BU. EDU

Boston University
Goal-orientated decision making is crucial for animal survival, and the prefrontal cortex is known to play a central
role in the decision making process. Many of our understandings of prefrontal cortex are from human, monkey,
and rat; yet, relatively little is known about the prefrontal areas in mouse, a popular animal model with many
genetic tools. Here we recorded and characterize the spike activity and local field potential (LFP) from mouse
prefrontal area while mice were performing a 3-choice auditory discrimination task. Our results showed that the
spike activity exhibited a transient, task stage-dependent modulation. Moreover, the neurons could carry the
information regarding either the presence or absence of a specific sound. Interestingly, while LFP oscillations
were modulated across various frequency bands at different task stages, the spike-field coherence showed celltype specific modulation at beta frequency (∼ 15 − 30Hz). Overall, we demonstrated that mouse prefrontal
area is involved in auditory discrimination, and auditory evoked prefrontal responses are selectively modulated
by behavior. While different oscillations are dynamically regulated during auditory discrimination, interneuron
network is likely selectively involved in beta oscillations and associated behavioral outcomes.

III-81. Neural coding of leg proprioception in Drosophila
John Tuthill
Akira Mamiya

JOHNCTUTHILL @ GMAIL . COM
AKIRAMAMIYA @ GMAIL . COM

University of Washington
Proprioception, the sense of self-movement and body position, is critical for the effective control of motor behavior.
In the absence of proprioceptive feedback, animals are unable to maintain limb posture or coordinate fine-scale
movements of the arms and legs. However, despite the importance of proprioception to the control of movement
in all animals, little is known about the neural computations that underlie limb proprioception in any animal. We
have developed new methods to record from proprioceptive neural circuits in the fruit fly, Drosophila. Each fly leg
contains approx. 135 proprioceptive stretch receptor neurons positioned at the joint between the femur and the
tibia. We used in vivo 2-photon calcium imaging to record from the axons of this proprioceptor population while
manipulating leg position and movement with a magnetic control system. With unsupervised clustering methods,
we have identified anatomically distinct subpopulations of proprioceptor neurons that encode specific kinematic
variables such as leg position, velocity, acceleration, and direction. Imaging from more specific genetic driver lines,
we found that single sensory neurons are sharply tuned for combinations of these variables. We then identified
two populations of second-order neurons that process sensory information from leg proprioceptors. Targeted
whole-cell recordings revealed that these two populations are specialized for encoding leg position and directional

COSYNE 2017

221

III-82
movement. Overall, our results illustrate how a low-dimensional stimulus—proprioception of a single leg joint—is
encoded by an overcomplete population of sensory neurons. Narrowly tuned proprioceptive signals converge
onto central pathways that separately represent leg movement and position. This circuit architecture may help
to reduce sensory noise while minimizing delays in neural processing. Speed and robustness are critical for
feedback control of the limbs during locomotion.

III-82. The Fruit Fly Brain Observatory: from structure to function
Nikul H Ukani1
Chung-Heng Yeh1
Adam Tomkins2
Yiyin Zhou1
Yu-Chi Huang3
Dorian Florescu2
Carlos Luna Ortiz2
Cheng-Te Wang3
Paul Richmond2
Chung-Chuan Lo3
Daniel Coca2
Ann-Shyn Chiang3
Aurel Lazar1

NIKUL @ EE . COLUMBIA . EDU
CHYEH @ EE . COLUMBIA . EDU
A . TOMKINS @ SHEFFIELD. AC. UK
YIYIN @ EE . COLUMBIA . EDU
HYC @ LOLAB - NTHU. ORG
DORIAN . FLORESCU @ SHEFFIELD. AC. UK
CARLOS . LUNA @ SHEFFIELD. AC. UK
CHENG - TE @ LOLAB - NTHU. ORG
P. RICHMOND @ SHEFFIELD. AC. UK
CCLO @ MX . NTHU. EDU. TW
D. COCA @ SHEFFIELD. AC. UK
ASCHIANG @ LIFE . NTHU. EDU. TW
AUREL @ EE . COLUMBIA . EDU

1 Columbia

University
of Sheffield
3 National Tsing Hua University
2 University

The Fruit Fly Brain Observatory (FFBO) is a collaborative effort between experimentalists, theorists and computational neuroscientists at Columbia University, National Tsing Hua University and Sheffield University with the
goal to (i) create an open platform for the emulation and biological validation of fruit fly brain models in health
and disease, (ii) standardize tools and methods for graphical rendering, representation and manipulation of brain
circuits, (iii) standardize tools for representation of fruit fly brain data and its abstractions and support for natural
language queries, (iv) create a focus for the neuroscience community with interests in the fruit fly brain and encourage the sharing of fruit fly brain structural data and executable code worldwide. NeuroNLP and NeuroGFX,
two key FFBO applications, aim to address two major challenges, respectively: i) seamlessly integrate structural
and genetic data from multiple sources that can be intuitively queried, effectively visualized and extensively manipulated, ii) devise executable brain circuit models anchored in structural data for understanding and developing
novel hypotheses about brain function. NeuroNLP enables researchers to use plain English (or other languages)
to probe biological data that are integrated into a novel database system, called NeuroArch, that we developed
for integrating biological and abstract data models of the fruit fly brain. With powerful 3D graphical visualization,
NeuroNLP presents a highly accessible portal for the fruit fly brain data. NeuroGFX provides users highly intuitive
tools to execute neural circuit models with Neurokernel, an open-source platform for emulating the fruit fly brain,
with full data support from the NeuroArch database and visualization support from an interactive graphical interface. Brain circuits can be configured with high flexibility and investigated on multiple levels, e.g., whole brain,
neuropil, and local circuit levels. The FFBO is publicly available and accessible at http://fruitflybrain.org from any
modern web browsers, including those running on smartphones.

222

COSYNE 2017

III-83 – III-84

III-83. Finding order within chaos in neural representations: A trajectory model
of sensorimotor computation
Cevat Ustun

CEVAT. USTUN @ GMAIL . COM

University of California, Los Angeles
Visually guided movements are critical to how we interact with our environment, yet the mechanisms underlying
them remain poorly understood. Insights have been sought in recent decades by recording single-unit activity in
monkeys as these subjects prepare, but withhold, goal-directed movements. These studies have shown that in
addition to pure reference frames, the brain also represents motor goals in intermediate reference frames. Despite their ubiquity, intermediate representations have no obvious bio-mechanical justification, and their discovery
therefore presents a challenge to traditional models of sensorimotor processing. More fundamentally, intermediate representations suggest that neural computation may not in general need to be carried out in an orderly
fashion. However, there are indications that a core assumption of traditional models – and hence the current
interpretations of neurophysiological data – may not be well founded. These models assume that motor planning
is concerned with the intended destination of movement, even though behavioral studies suggest that motor planning considers the intended trajectory. Might the neurophysiological data be better explained by a trajectory-based
model? I postulate an expression for intended trajectories and cast it as a network by using a sequence of basis
function representations, a mechanism that is thought to carry out spatial transformations. The resulting network
leads to responses that simulate reach-related responses in posterior parietal cortex (PPC). Eye-centered, handcentered, and intermediate reference frames emerge before movement onset, and these gradually shift towards
an eye-centered frame during movement itself. The model thus provides a framework for interpreting neural representations within which computation appears systematic and orderly. It further suggests a link between the
currently distinct subjects of reference frames for motor planning and internal representations of movement. I
conclude with additional predictions of the model and its implications for several ongoing controversies.

III-84. How dentate gyrus place cells represent distinct place memories
Milenna van Dijk
Andre Fenton

MV 780@ NYU. EDU
AFENTON @ NYU. EDU

New York University
Memory discrimination relies on hippocampal dentate gyrus (DG), but how does DG disambiguate similar memories? DG neural discharge encodes locations (place cells; PCs). DG PCs are reported to change location-specific
tuning (“global remapping”) and/or firing rate (“rate remapping”) across similar environments more than PCs in
downstream hippocampus subfields CA3/1. Remapping is standardly hypothesized to be the mechanism of memory discrimination but the critical experiment linking remapping to memory discrimination was never performed.
We report replicating the remapping results in DG and CA3/1 PCs by standardly manipulating environments without explicit memory discrimination. We then investigated the remapping hypothesis in DG neurons while mice
performed a DG-dependent memory discrimination task. We observed no global remapping but DG cells rate
remapped across both DG-insensitive and DG-dependent memory discrimination trials; neither form of remapping accounts for the DG-dependent memory discrimination. We noticed that DG PC spike trains express a
non-stationary place code; they fluctuate between representing locations in distinct spatial fames, demonstrating multistable place representations. Consequently, we examined the temporal organization of discharge within
networks of DG cells. Our temporal coding “coordination hypothesis” asserts representational information in the
temporal organization of coactivity across spike trains. Because DG PCs are weakly connected and inhibitory
interneurons (INH) control temporal patterns of discharge, we focus on the temporal coordination of PC-INH spike
trains. While PC-PC pairwise correlations were insensitive to changed task conditions as expected, PC-INH correlations were sensitive, as predicted. Specifically, previously uncorrelated cell pairs became positively correlated,
whereas positively correlated pairs maintained their coupling. In fact, PC-INH correlations increased selectively
during the few seconds when mice explicitly express DG-dependent memory discrimination. These findings reject

COSYNE 2017

223

III-85 – III-86
the remapping hypothesis for memory discrimination, and demonstrate memory discrimination is mediated by
increased excitation-inhibition coupling to stabilize DG network discharge and the place code.

III-85. Unbiased log-likelihood estimation with inverse binomial sampling
Bas van Opheusden
Luigi Acerbi
Weiji Ma

SVO 213@ NYU. EDU
LUIGI . ACERBI @ NYU. EDU
WEIJIMA @ NYU. EDU

New York University
The fate of behavioral, cognitive, or neural hypotheses often relies on a model’s ability to explain data, quantified by
the likelihood function. The log-likelihood is the key element for parameter estimation and comparison between
competing models. However, the log-likelihood of complex models is often intractable to compute analytically
or numerically. In those cases, one commonly resorts to sampling. The default method to estimate the loglikelihood of a subject’s response on a single trial is to draw a fixed number of samples from the behavioral
model, count how many samples match the response, and take the logarithm of the ratio. This method produces
accurate log-likelihood estimates with hundreds to thousands of samples per trial, but drawing that many samples
can be computationally expensive. With fewer samples, the log-likelihood estimator becomes severely biased.
Here, we explore another method, inverse binomial sampling (IBS), which can estimate log-likelihoods without
any bias. IBS draws samples from the behavioral model until one matches the subject’s response. The loglikelihood estimate is then a function of the number of samples drawn. The variance of this estimator is uniformly
bounded achieves the minimum variance for an unbiased estimator (Cramer-Rao bound). Its computational time
scales with the number of samples used, which is stochastic. We compare IBS and fixed sampling, used for
maximum-likelihood estimation of parameters of simple behavioral models in simulated data for two example
tasks: orientation discrimination and change localization. In both tasks, IBS causes lower root mean squared error
in estimated parameters than fixed sampling with the same average number of samples. Our results demonstrate
the potential of IBS as a practical, robust, and easy to implement method for log-likelihood evaluation when exact
techniques are not available.

III-86. Inference by reparameterization using population codes
Rajkumar Vasudeva Raju1,2
Xaq Pitkow1,2
1 Rice

RV 12@ RICE . EDU
XAQ @ RICE . EDU

University
College of Medicine

2 Baylor

Behavioral experiments suggest that humans and animals weigh uncertainty when interpreting sensory data. The
central hypothesis is that the brain creates an internal model of the world that represents this uncertainty as a
probability distribution that brain computations transform to implement probabilistic inference. Here we present
a new general-purpose, biologically-plausible neural implementation of approximate inference. The neural network represents uncertainty using Probabilistic Population Codes (PPCs), which are distributed neural representations that naturally encode probability distributions, and support marginalization and evidence integration
in a biologically-plausible manner. By connecting multiple PPCs together as a probabilistic graphical model, we
represent multivariate probability distributions. Approximate inference in probabilistic graphical models can be
accomplished by message-passing algorithms that disseminate local information throughout the graph. An attractive and often accurate example of such an algorithm is Loopy Belief Propagation (LBP), which uses local
marginalization and evidence integration operations to perform approximate inference efficiently even for complex
models. Unfortunately, a subtle feature of LBP renders it neurally implausible. However, LBP can be elegantly
reformulated as a sequence of Tree-based Reparameterizations (TRP) of the graphical model. We re-express the

224

COSYNE 2017

III-87 – III-88
TRP updates as a nonlinear dynamical system with both fast and slow timescales, and show that this produces
a neurally plausible solution. By combining all of these ideas, we show that a network of PPCs can represent
multivariate probability distributions and implement the TRP updates to perform probabilistic inference. Simulations with Gaussian graphical models demonstrate that the neural network inference quality is comparable to the
direct evaluation of LBP and robust to noise, and thus provides a promising mechanism for general probabilistic
inference in the population codes of the brain.

III-87. Compressive approaches for neuronal reconstruction using DNA barcodes
Alexander Vaughan
Xiaoyin Chen
Anthony Zador

AVAUGHAN @ CSHL . EDU
XICHEN @ CSHL . EDU
ZADOR @ CSHL . EDU

Cold Spring Harbor Laboratory
Reconstruction of neuronal morphology have historically relied on methods with low throughput (such as singleneuron tracing) or low-resolution (such as viral methods). One recent alternative relies on labeling neurons with
RNA barcodes, which are then extracted from putative target regions to trace individual neurons throughout the
brain (Kebschull et al., 2016). An extension of this technique is the use of barcodes as in-situ markers, akin to
an infinite-color Brainbow (Livet et al., 2007). In this approach, individual barcodes can be sequenced across n
bases using fluorescent in situ sequencing (FISSEQ) to give rise to a 4*n-color signal. Barcodes can be identified
throughout the 3D volume using standard base-calling on each pixel. One problem arising in this framework is that
the FISSEQ signal from multiple barcodes may overlap, either due to co-expression of two barcodes within a given
neuron, or due to overlapping neurons. When this happens, these mixtures cannot be read out using standard
base-calling, and the underlying information is lost. Here, we present a framework, known as mixSEQ, that
enables the neuroanatomical reconstruction under dense barcode labeling. The main insight is that overlapping
DNA sequences can be disentangled by assuming that the sequencing signal arises as a sparse mixture of DNA
sequences drawn from a known dictionary. In addition, for cases in which the sequence dictionary is unavailable,
dictionary learning methods can be used to identify neuronal barcodes de novo. Although it is particularly wellsuited to reconstruction of local neuronal morphology, mixSEQ also has applications to long-range connectivity,
such as identification of individual neurons contained in a dense axon bundles. The general approach is also
applicable to a wide variety of problems in in situ transcriptomics and standard DNA sequencing pipelines.

III-88. Experimental isolation of parallel information pathways embedded in
distributed, convergent systems
Nicholas Wall1
Peter Neumann1
Kevin Beier1
Ava Mokhtari2
Liqun Luo1
Robert Malenka1
1 Stanford

NRWALL @ STANFORD. EDU
PNEUMANN @ STANFORD. EDU
BEIER @ STANFORD. EDU
AVA . MOKHTARI 5830@ CNSU. EDU
LLUO @ STANFORD. EDU
MALENKA @ STANFORD. EDU

University
Northstate University

2 California

The mammalian dorsal striatum plays a central role in the regulation of motivated behavior, and represents a site
of massive information convergence; in addition to major dopaminergic and thalamic input, the dorsal striatum
samples information from almost every region of cortex. How are these diverse inputs integrated at the level of
dorsal striatum in order to modulate the wide range of motivated behaviors relevant to the animal? A series of

COSYNE 2017

225

III-89 – III-90
advanced genetic screens was applied to examine the degree to which distributed ensembles of cortical neurons, activated by a strong cocaine experience, selectively target co-active neurons in the striatum to transmit
cocaine-relevant information. Transgenic mice expressing CreERT2 under the control of the immediate early
gene promoter Arc were utilized in combination with monosynaptic rabies virus tracing to selectively identify the
brain-wide inputs to cocaine-activated neurons in the striatum. Widely-distributed cortical neurons, activated by
a strong cocaine experience, preferentially wired onto co-activated striatal neurons at a level of 50-100% above
chance, and repeated prior cocaine exposure further refined the connectivity between co-active neurons from
a subset of cortical input regions. This increased selectivity in wiring between co-active neurons is achieved
through the specific elimination of non-cocaine-activated inputs onto activated striatal neurons. Furthermore,
selective optogenetic stimulation of corticostriatal projections from cocaine-activated neurons revealed unique
synaptic properties selectively at synapses between co-active neurons. These results indicate a privileged circuit that links cocaine-activated neurons from diverse sources to co-active striatal neurons, providing a selective
substrate for information transmission that can be dissociated from the bulk corticostriatal projection.

III-89. The speed of neural dynamics as a neural code for motor timing
Jing Wang
Mehrdad Jazayeri
Eghbal Hosseini
Devika Narain

JINGWANG . PHYSICS @ GMAIL . COM
MJAZ @ MIT. EDU
EHOSEINI @ MIT. EDU
DNARAIN @ MIT. EDU

Massachusetts Institute of Technology
Internal control of neural dynamics is at the core of mental capacities such as anticipation, motor coordination and
imagination. One of the simplest behavioral setting where such internal control is operative is when we flexibly
adjust movement initiation time to produce a desired time interval. Recent advances suggest that motor timing is
controlled by the dynamics of neural population activity. However, the exact nature of the neural code established
by such dynamics remains largely unknown. We recorded from the dorsomedial frontal cortex (dMFC) of monkeys
trained to flexibly produce different time intervals with different effectors. The activity profile of individual neurons
was nonlinear, complex and heterogeneous. However, responses displayed an unexpected form of invariance:
activity profiles were stretched or compressed in time in correspondence with the produced interval. This property
allows motor timing to be understood in terms of a simple neural code: the speed with which the population neural
dynamics unfold in time. These findings highlight the potential for a novel and general computational scheme in
which the brain sets the speed of neural trajectory to flexibly control the timing of voluntary movements.

III-90. Early motion processing circuit uses gap junctions to achieve efficient
stimuli encoding
Siwei Wang1
Noga Zaslavsky1
Alexander Borst2
Naftali Tishby1
Idan Segev1

SIWEI . WANG @ MAIL . HUJI . AC. IL
NOGA . ZASLAVSKY @ MAIL . HUJI . AC. IL
ABORST @ NEURO. MPG . DE
TISHBY @ CS . HUJI . AC. IL
IDAN @ LOBSTER . LS . HUJI . AC. IL

1 Hebrew
2 Max

University of Jerusalem
Planck Institute of Neurobiology

Successful processing of millisecond scale motion information is crucial for survival. Here, we show that, in the
blowfly’s visual system, efficient stimuli encoding emerges at the earliest stage of global motion perception to cope
with this challenge. Moreover, the uniquely strong axonal gap junctions (GJ) in this circuit are essential for achieving such near-optimal efficiency. We focus on the VS network in the lobula plate of the blowfly’s compound eyes. It

226

COSYNE 2017

III-91
consists of 10 vertical sensitive (VS) cells (VS1-VS10), each set tiling the visual world of a given hemisphere. This
network integrates responses from a large set of local motion detectors and sends the resulting global motionsensitive signal to downstream descending neurons which in turn target neck and wing muscles for head and
body movements, respectively. This VS network is designated to encode rotational motion, i.e., the rotational axis
θ, with an intriguing structure: first, each VS cell connects with neighboring cells via strong (∼ 1µS) axonal gap
junctions. Second, only subpopulations on both sides connect with downstream pathways, i.e., only the VS5,6,7
send output to the descending premotor neurons. Previous work [Trousdale et.al, J.Neuroscience,2014] shows
that with GJs, this subpopulation encoding can successfully estimate the horizontal θ. What makes the presence
of GJs so helpful? By modeling the VS network, we find that these strong GJs help the VS5,6,7 encoding to
obtain at least 90% efficiency as compared to the theoretical physical limit, determined by the input statistics, for
both natural and checkerboard stimuli. When estimating θ (with the natural stimuli), the VS5,6,7 encoding can
represent both axes (“roll” and “pitch”) successfully with GJs as opposed to only one axis (“roll”) without GJs.
When discriminating between θ and θ’ (with the checkerboard stimuli), hyperacuity emerges in the presence of
strong GJs.

III-91. Graph clustering with coupled oscillators: A retinal model.
Christopher Warner
Friedrich Sommer

CWARNER @ BERKELEY. EDU
FSOMMER @ BERKELEY. EDU

University of California, Berkeley
It has been proposed that the retina not only encodes activation by local image features in the firing rate of individual ganglion cells but additionally may use its intricate network structure to also extract and encode more
extended features such as segment membership in the phase of firing of ganglion cells relative to the oscillatory
sub-threshold local field potential (LFP) signal. To explore that claim, we construct an abstract physical model of
coupled oscillators in which the phase of firing of neurons can is adaptable based on feature similarity and spatial
proximity between pairs of ganglion cell receptive fields while leaving firing rates, known to encode local contrast,
un altered. By multiplexing a second phase-coded segmentation information channel on top of the rate-coded local contrast signal, the retina can increase the amount of information sent down the optic nerve bottleneck without
increasing its energy consumption. While state-of-the art image segmentation uses sophisticated image features,
like edge and spatial frequency detectors; here we constrain our model to include only image features available to
the retina, like non-oriented local contrast, and ask whether coupled oscillator models for periodic neural activity in
retina can use such simple filters to perform image segmentation at an interesting performance level. For setting
the phase interactions we compare existing models for graph clustering such as laplacian (Shi 2000), association
(Sarkar 1998), and modularity (Newman 2006) with proposed a new one, the topographic modularity. We find
that the phase-interaction model with couplings set by topographic modularity achieves significant performance
levels, better than competing graph clustering methods. Moreover, this coupled-oscillator phase relaxation computation outperforms eigenvector segmentations, i.e. “spectral methods” (Chung 1997). Here we present the
model, highlight the results and discuss implications for neuroscience and neuromorphic computing.

COSYNE 2017

227

III-92 – III-93

III-92. Deep multi-view representation learning of brain responses to natural
stimuli
Leila Wehbe1
Anwar Nunez-Elizalde1
Alexander Huth1
Fatma Deniz1,2
Natalia Bilenko1
Jack Gallant1
1 University

LWEHBE @ BERKELEY. EDU
ANWARNUNEZ @ BERKELEY. EDU
ALEX . HUTH @ BERKELEY. EDU
FATMA @ BERKELEY. EDU
NBILENKO @ BERKELEY. EDU
GALLANT @ BERKELEY. EDU

of California, Berkeley
Computer Science Institute

2 International

One of the central goals of cognitive neuroscience is to understand how information is represented in the brain.
Most neuroimaging studies focus on only a single cognitive domain such as vision and language, and use relatively
simple, highly controlled stimuli. Here we construct a statistical model that learns a common latent “cognitive“
space that accounts for results obtained in many different naturalistic neuroimaging experiments, such as watching
movies or listening to stories. We cast this problem as a multi-view modeling problem. In this framework, the
perception of a stimulus of any type, such as a movie or a story, is an event that can be captured from multiple
views. We consider two types of views: (1) views corresponding to the brain activity elicited in each one of N
subjects who are exposed to that stimulus, and (2) views that represent the properties of that same stimulus in
different feature spaces. We design a multi-view autoencoder with a shared bottleneck layer. The bottleneck layer
is a low dimensional representation of the latent cognitive event, estimated from one or more input views. Then,
our model reconstructs any one view from that bottleneck layer estimate alone. Using data from experiments in
which subjects listened to spoken stories and watched natural movies, we show that this model can accurately
reconstruct: (1) any one subject’s brain activity any stimulus feature view; (2) any stimulus feature view from any
subject’s brain activity; and (3) any subject’s brain activity from any other subject’s brain activity. Our one model
provides a common representation of cognitive events across experiments, subjects and feature spaces.

III-93. Layer-specific differences between spontaneous and visually evoked
spiking correlations in V1
Jacob Westerberg
Michele Cox
Kacie Dougherty
Alexander Maier

JACOB . A . WESTERBERG @ VANDERBILT. EDU
MICHELE . A . COX @ VANDERBILT. EDU
KACIE . DOUGHERTY @ VANDERBILT. EDU
ALEX . MAIER @ VANDERBILT. EDU

Vanderbilt University
Correlated spiking between cortical neurons affects sensory coding and perceptual decision making. Previous
work has established that neurons across the layers of visual cortex differ systematically in their spontaneous activity as well as in the degree to which they are correlated in their visual response. However, we know little about
how spontaneously occurring neuronal correlations are related to those evoked under sensory stimulation. Here
we investigate correlated population spiking responses within and across the layers of macaque primary visual
cortex (V1), and compare their spatial structure during fixation and under visual stimulation. We placed linear
multielectrode arrays spanning all layers of V1 in two monkeys that were trained to passively fixate on a screen
while grating stimuli were presented to the receptive field of neurons under study (n=63 penetrations). Electrodes
were aligned and laminar compartments delineated using current source density analysis. We extracted multiunit
activity as a measure of population activity within the supragranular, granular and infragranular laminar compartments and computed the degree of correlated spiking across electrode contacts for the average spiking activity
before and after visual stimulation. We found that spontaneously correlated spiking across all layers is significantly
greater than that obtained from a randomized trial-shuffle control. However, the degree of the spontaneously generated correlated spiking varied systematically between layers. Visual stimulation both increased and decreased

228

COSYNE 2017

III-94 – III-95
spike rate correlations in a layer-specific manner even though spiking increased uniformly across the entire cortical column. Taken together, these findings suggest that the laminar profile of visually evoked spiking correlations
differs from the spatial structure of spontaneously occurring correlated activity. We will discuss these findings in
the context of the differential impact of feedforward and feedback afferents on V1’s laminar microcircuitry.

III-94. Nonlinear latent variable approaches for understanding population activity in sensory cortex
Matthew Whiteway1
Karolina Socha2
Vincent Bonin2
Daniel A Butts1
1 University

THEMATTINTHEHATT @ GMAIL . COM
KAROLINA . ZOFIA . SOCHA @ GMAIL . COM
VINCENT. BONIN @ NERF. BE
DAB @ UMD. EDU

of Maryland
Research Flanders

2 Neuro-Electronics

In the awake animal, sensory neuron activity often represents more than just information about the stimulus, and is
often influenced by experimentally uncontrolled variables such as alertness, attention, and other elements of brain
state. The ability to record from large numbers of neurons simultaneously provides an opportunity to infer the time
courses of such “latent variables”, and thereby learn how they affect sensory representations. Here, we present
two advances to current “latent variable” methods that account for what are likely critical nonlinearities affecting
the ability to properly estimate latent variables and untangle their separate influences. We first introduce a linear
factor model called the “rectified latent variable model” (RLVM), where the latent variables are constrained to be
non-negative, and demonstrate its use with two-photon data from mouse barrel cortex during a discrimination
task (Peron et al. 2015). We show that the rectification is necessary to “demix” distinct, potentially sparse inputs
such as whisker touches and responses to a decision cue in an unsupervised fashion. Secondly, we introduce
a “stacked” RLVM which incorporates hidden intermediate layers in between rectified latent variables and neural
activity, allowing for potentially arbitrary nonlinear interactions between them. While such a model does not
provide a different description of the barrel cortex dataset, it leads to significant improvements when applied to a
second two-photon dataset, which contains simultaneously imaged LGN afferents in mouse primary visual cortex
in response to visual stimulation. The resulting latent variables are then used as inputs into a systems identification
method in order to demonstrate how the activity of single afferents nonlinearly combines different sets of latent
variables. These results demonstrate general methods for understanding activity of sensory (and potentially nonsensory) cortex, and suggests that capturing these nonlinear interactions will be crucial for understanding the role
of such variables in sensory cortical function.

III-95. High cellular and columnar variability underlies the absence of early
orientation selectivity
David E Whitney1
Gordon B Smith1
Bettina Hein2
Matthias Kaschube2
David Fitzpatrick1
1 Max

DAVID. WHITNEY @ MPFI . ORG
GORDON . SMITH @ MPFI . ORG
HEIN @ FIAS . UNI - FRANKFURT. DE
KASCHUBE @ FIAS . UNI - FRANKFURT. DE
DAVID. FITZPATRICK @ MPFI . ORG

Planck Florida Institute for Neuroscience

2 FIAS

Selectivity for stimulus orientation is a fundamental property of primary visual cortex in primates and carnivores,
where it is organized into a smoothly varying columnar map that emerges in an activity-dependent manner during
early postnatal life [1-3]. In order to examine how orientation selectivity develops within this highly distributed

COSYNE 2017

229

III-96 – III-97
network of neurons, we visualized grating-evoked responses in ferret visual cortex using longitudinal imaging of
GCaMP6s at both the mesoscopic and cellular level. We show that prior to eye opening gratings already evoke
robust population responses with modular structure. Surprisingly the spatial location and pattern of domains activated by a single orientation varies substantially across trials, resulting in low orientation selectivity and absent
orientation maps. Thus, a major factor limiting orientation map emergence is the early disassociation between
stimulus orientation and population responses in visual cortex. Yet variability in orientation responses is not a
general feature of the developing cortex, as responses evoked by uniform luminance steps are selective. At the
cellular level, we confirm that individual neural responses pre-eye opening exhibit higher response variability, but
are already spatially coherent with no evidence of an intermixing of differently tuned populations. Notably activity patterns evoked by gratings 1-2 days prior to eye opening do not necessarily represent transitory cortical
states (i.e. random patterns), as we show that the absolute strength of pair-wise response correlations (>1mm)
are significantly higher than surrogate data and that trial-averaged responses show similarity to the mature orientation map. Additionally, at all ages examined, the correlation structure of evoked activity resembles that of
endogenously-generated, spontaneous activity, suggesting that both evoked and spontaneously activity reflect a
common functional architecture. Taken together these results indicate that the lack of orientation selectivity prior
to eye opening reflects highly variable responses in a developing cortex already exhibiting a highly structured
functional organization.

III-96. Efficient supervised learning of hierarchical cortical networks in the
predictive coding framework
James Whittington
Rafal Bogacz

JAMES . WHITTINGTON @ MAGD. OX . AC. UK
RAFAL . BOGACZ @ NDCN . OX . AC. UK

University of Oxford
To efficiently learn from feedback, cortical networks need to update synaptic weights on multiple levels of cortical
hierarchy. Artificial neural networks have gained much popularity recently, however they are trained using the
back-propagation algorithm where the change in synaptic weights are a complex function of weights and activities
of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses
are determined only by the activity of pre-synaptic and post-synaptic neurons. Several models have been proposed that approximate the back-propagation algorithm with local synaptic plasticity, but these models require
complex external control over the network or relatively complex plasticity rules. Here we show that a network
developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple local Hebbian plasticity. Furthermore, we draw parallels between our network and artificial
neural networks, showing the networks make identical predictions given they have the same weights, and that for
certain parameters they learn in an approximately identical manner. This suggests that it is possible for cortical
networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in
areas on multiple levels of hierarchy are modified to minimize the error on the output. This work is an important
step in the pathway of discovering biological networks that are simple, efficient and have the ability to perform
powerful computations.

III-97. Cortical projection neurons support a parallel and complementary analysis of stimulus features
Ross S Williamson1,2
Daniel Polley1

ROSS WILLIAMSON @ MEEI . HARVARD. EDU
DANIEL POLLEY @ MEEI . HARVARD. EDU

1 Massachusetts
2 Harvard

230

Eye and Ear Infirmary
University

COSYNE 2017

III-98
Neurons in layers (L) 5 and 6 of the auditory cortex (ACtx) give rise to a massive subcortical projection that innervates all levels of the central auditory pathway as well as non-auditory areas including the amygdala and striatum.
L5 and L6 neurons feature distinct morphology, connection patterns, intrinsic membrane and synaptic properties,
yet little is known about how these differences relate to sensory selectivity in vivo. Here, we performed cell-typespecific recordings from two classes of ACtx L5 and L6 projection neurons; L5 corticocollicular neurons (L5CC),
and L6 corticothalamic neurons (L6CT). We utilized an antidromic optogenetic “phototagging” strategy to isolate
single L5CC and L6CT neurons from extracellular recordings in awake, head-fixed mice. In order to functionally
identify L6CT neurons, we injected an adeno-associated viral construct (AAV) encoding channelrhodopsin (ChR2)
into the ACtx of Ntsr1-Cre mice, in which cre-recombinase is expressed only in L6CT neurons. We then implanted
an optic fiber near the medial geniculate body (MGB), which could be used to evoke antidromic spikes. Injecting
a non-specific AAV encoding ChR2 and implanting an optic fiber near the inferior colliculus (IC) allowed functional identification of L5CC neurons. We constructed a bank of complex stimuli by endowing noise tokens with
a randomly chosen modulation frequency, level, spectral bandwidth, and center frequency, and then quantified
each neuron’s lifetime sparseness. We found that L5CC neurons had a lower lifetime sparseness index, indicative
of reduced stimulus selectivity and a broader response distribution than L6CT neurons. Linear spectrotemporal
receptive field fits were also able to explain a higher percentage of response variance in L6CT neurons, indicating a higher degree of linearity in their responses when compared to L5CC neurons. These findings suggest a
functional dichotomy in the form of stimulus-related modulation imposed by deep layer cortical projection neurons
that make distinct subcortical targets.

III-98. Rapid and concentration-invariant coding of olfactory identity using
early neural activity
Christopher Wilson
Dmitry Rinberg

CDW 291@ NYU. EDU
DMITRY. RINBERG @ NYUMC. ORG

New York University
Odors’ perceptual identities are mostly stable to shifts in concentration despite considerable variability to the sensory input that these shifts create. We propose that the brain uses the temporal relationship of olfactory receptor
responses to identify features that are stable across concentrations. Specifically, we hypothesize that responses
evoked at relatively low latencies represent a sensitive and stable substrate to represent odors’ perceptual features in a coding scheme we call ‘primacy’ coding. We tested two predictions of this coding scheme. First, we
tested whether animals can use only the earliest odor-evoked neural activity to guide olfactory decisions using an
optogenetic masking paradigm. In this task, we find that animals can discriminate between odors with no significant behavioral penalty when input occurring later than 100 ms is masked. This finding describes a time window
in which the minimal information necessary for odor identification is transduced. Next, we tested whether neural
activity in the olfactory system occurring within the time window contains concentration-invariant odor information.
Using multielectrode arrays, we recorded odor responses of mitral/tufted cells in the mouse olfactory bulb while
presenting odor stimuli across a range of concentrations. A subset units responded reliably to odor across all concentrations tested, and these concentration invariant units predominantly responded at latencies within the first
100 ms of inhalation onset. Together, these data suggest that low-latency odor-evoked neural activity is stable to
concentration shifts and that the olfactory system uses this low latency information to guide olfactory decisions.

COSYNE 2017

231

III-99 – III-100

III-99. Cellular and synaptic mechanisms of direction selectivity in visual cortex
Daniel Wilson
Benjamin Scholl
David Fitzpatrick

DAN . WILSON @ MPFI . ORG
BEN . SCHOLL @ MPFI . ORG
DAVID. FITZPATRICK @ MPFI . ORG

Max Planck Florida Institute for Neuroscience
Individual neurons in primary visual cortex respond selectively to the direction of motion of visual stimuli. In simple
cells, previous studies have shown that weak biases in synaptic inputs are amplified by a spike threshold nonlinearity to generate robust direction selectivity. These biases in membrane potential direction selectivity derive from
differences in the relative timing of synaptic inputs rather than differentially tuned excitatory and inhibitory inputs.
Mechanisms of direction selectivity in other cortical neurons throughout the visual system have remain largely
unexplored. In vivo two photon imaging of spine calcium signals in single neurons in ferret visual cortex revealed
direction selective neurons receive excitatory synaptic inputs tuned to both the preferred and null directions. Despite the prevalence of null-tuned excitatory synaptic inputs, in vivo whole-cell patch clamp recordings show that
neurons with strong spiking direction selectivity are driven by subthreshold responses with strong direction selectivity. Highly selective membrane potential tuning may arise from null-direction tuned inhibition. Consistent with
this notion, we find that membrane potential variance decreases at the null direction in these neurons, and that
this reduction in noise at the null direction is strongly correlated with membrane potential direction selectivity. Hyperpolarizing highly selective neurons strongly reduces membrane potential direction selectivity, suggesting that
inhibitory input tuned for the null direction enhance direction selectivity in complex cells. Preliminary experiments
mapping inhibitory to excitatory connectivity using patterned optogenetic stimulation and whole cell patch clamp
recording reveal the existence of long-range, intercolumnar inhibitory input to single excitatory neurons. These
findings have broad implications beyond the visual cortex. This circuit architecture supports models that use mutually inhibitory circuits as a way for populations to dynamically modulate firing without changing connectivity; we
propose that feature-selective inhibition may allow neurons to flexibly cancel out subsets of excitatory synaptic
input to achieve functionally specific spiking output.

III-100. Reinforcement learning over time: effects of spacing on the mechanisms supporting feedback learning
G Elliott Wimmer
Russ Poldrack

GELLIOTTWIMMER @ STANFORD. EDU
RUSSPOLD @ STANFORD. EDU

Stanford University
Over the past few decades, neuroscience research has illuminated the neural mechanisms supporting learning from reward feedback, demonstrating a critical role for the striatum and midbrain dopamine system. Simple
feedback learning paradigms are increasingly being extended to understand learning dysfunctions in mood and
psychiatric disorders as well as addiction (“computational psychiatry”). However, one potentially critical characteristic that this research ignores is the effect of time on learning: events in human feedback learning paradigms
are only separated by several seconds, while learning events in the everyday environment are almost always separated by longer periods of time. Importantly, early animal studies found that spacing of learning events strongly
increased the rate of learning, suggesting a quantitative or qualitative shift in the underlying learning mechanisms.
Remarkably, the effect of spacing between learning events on human reinforcement learning has not been investigated. In Experiment 1, we examined learning for “massed” stimuli (presented on consecutive trials) and “spaced”
stimuli (presented on rare interleaved trials). To avoid ceiling performance, we included a concurrent WM task.
During learning, performance was significantly higher for the massed stimuli. Critically, at test, performance fell in
the massed condition and increased in the spaced condition to yield equivalent behavior (interaction, p<0.001).
In Experiment 2, we examined long-term learning completed across weeks vs. matched training completed in a
short pre-fMRI session. While performance was equivalent in the subsequent fMRI session, in a final test 3 weeks

232

COSYNE 2017

III-101 – III-102
later, we found that spaced stimuli exhibited significantly greater maintenance of learned value. Additionally, our
neural results suggest that in contrast to spaced value associations, massed associations elicit greater PFC engagement. Overall, these studies begin to address a large gap in our knowledge of fundamental processes of
reinforcement learning, with potentially broad implications for our understanding of learning in mood disorders
and addiction.

III-101. Visual cortical circuits in a state of flux
Fred Wolf1,2
David E Whitney3
Justin C Crowley4
Juan Daniel Florez Weidinger1

FRED - WL @ NLD. DS . MPG . DE
DAVID. WHITNEY @ MPFI . ORG
JCROWLEYPA @ GMAIL . COM
CHEPE @ NLD. DS . MPG . DE

1 Max

Planck Institute for Dynamics and Self-Organization
Center for Computational Neuroscience
3 Max Planck Florida Institute for Neuroscience
4 Advisestream
2 Bernstein

The cerebral cortex is a primary learning center in the mammalian brain. To assimilate the stream of incoming
information substantial circuit-turnover is therefor expected, even in primary sensory cortices. It is a long-standing
theoretical prediction that visual cortical circuits are in a “state of flux”, such that e.g. the preferred orientation of
the neurons represents a non-equilibrium steady state of circuit turnover. Neurons in the primary visual cortex
are selective for the orientation of light/dark edges in their receptive fields and in primates and carnivores these
neurons form iso-orientation domains ordered around point-like topological defects called pinwheel centers. If the
circuit-turnover prediction is true and multiple steady states coexist, then signatures of circuit-turnover should be
observable as a rearrangement of the domains over time. We experimentally tested this by a large-scale screen
for the largest conceivable change in the arrangement of orientation domains: the generation and annihilation of
pinwheel pairs. We conducted acute high-accuracy large-scale intrinsic signal imaging experiments in 30 ferrets.
In 29 of the ferrets we further employed an adapted pairing protocol between imaging sessions to drive the cortical
circuits out of a potential stationary state. To harvest significant topological changes we quantified measurement
precision using re-sampling methods, the probability of pinwheel existence by tracking their position between
bootstrap samples, and the accuracy orientation preference estimation from inter-sample tuning distributions.
We analyzed the dynamics of 1590 pinwheels by comparing the measured layouts in 4 subsequent imaging
sessions of 42 minutes each. Using this extensive data set we found rare but conclusive examples of pinwheel
rearrangement. The rate of these events was increased when the pairing paradigm was used. Our results
demonstrate for the first time dynamical changes of visual cortical architecture establishing conclusively that
visual cortex networks are in a persistent state of flux.

III-102. Essential nonlinear properties in neural decoding
Qianli Yang1,2
Xaq Pitkow1,2
1 Rice

QY 8@ RICE . EDU
XAQ @ RICE . EDU

University
College of Medicine

2 Baylor

To decode task-relevant information from sensory observations, the brain must eliminate nuisance variables that
affect those observations. For natural tasks, this generally requires nonlinear computation. Here we contribute
new concepts and methods to characterize behaviorally relevant nonlinear computation downstream of recorded
neurons. Linear decoding weights can be inferred from correlations between neurons and behavior (Haefner et
al. [2013],Lakshminarasimhan et al. [2014]). However, these weights do not adequately describe the neural

COSYNE 2017

233

III-103
code when, due to nuisance variation, mean neural responses are poorly tuned to the task while higher-order
statistics of neural responses are well tuned. The task-relevant stimulus information can then be extracted only
by nonlinear operations. For example, detecting an object boundary in an image requires contrast invariance: an
edge appears when the foreground object is darker or lighter than the background, yet any linear function will
exhibit opposite responses in these two cases. We generalize past weight-inference methods to determine the
brain’s nonlinear neural computations from joint higher-order statistics of neural activity and behavioral choices in
perceptual tasks. This method is based on a new statistical measure we call nonlinear choice correlation, defined
as the correlation coefficient between behavioral choices and nonlinear functions of measured neural responses.
Importantly, the exact neural transformations may not be uniquely identifiable, since many neural nonlinearities
can generate the same behavioral output. This is expected when sensory signals are expanded into a larger
cortical response space, creating a redundant code. We exploit this redundancy to define a new concept of
equivalence classes for neural transformations. We then demonstrate how to quantify essential properties of
these equivalence classes, and provide simulations that show how these properties can be extracted using neural
data from behaving animals. Finally, we explain the functional importance of these nonlinearities in specific
perceptual tasks.

III-103. A neural circuit for efficient analysis-by-synthesis in the primate face
processing system
Ilker Yildirim1
Winrich Freiwald2
Josh Tenenbaum1

ILKERY @ MIT. EDU
WFREIWALD @ ROCKEFELLER . EDU
JBT @ MIT. EDU

1 Massachusetts
2 Rockefeller

Institute of Technology
University

Vision is often well characterized as the solution to an inverse problem: inverting the process of how threedimensional (3D) scenes give rise to images to reconstruct the scene that best explains a given observed image.
Yet the neural circuits underlying this inverse mapping are poorly understood. Leading neural models of vision
concentrate on the task of object identification or categorization, and do not attempt to explain how vision also
constructs rich descriptions of scenes with detailed 3D shape and surface appearance. Alternative “inverse
graphics“ or “analysis-by-synthesis“ approaches aim to recover 3D scene content, but are typically implemented
using iterative computations that are hard to map onto neural circuits and much too slow for the speed of natural
perception. Here, we propose a novel computational architecture for efficient analysis-by-synthesis that combines
a probabilistic generative model based on a 3D graphics program with a recognition model based on a deep
convolutional neural network. This approach is as fast as the leading feedforward network accounts of visual
object recognition, but its output is the most plausible 3D scene description. As a test case, we apply this model
in the domain of face recognition where, as a rarity, data from brain imaging, single-cell recordings, and behavior
all come together to constrain models. We show that this model, but not the alternatives, explains the primate
ventral face patch system at the same time as it predicts human gradations in various face recognition tasks.
Taken together, this model provides an efficient analysis-by-synthesis account of visual face processing, bridging
levels of analysis from behavior to physiology and with the potential to generalize across vision.

234

COSYNE 2017

III-104 – III-105

III-104. Decoding of replay events from unsorted spike data using a particle
filter applied to a marked point
Ali Yousefi1,2
Loren Frank3
Uri Eden2

AYOUSEFI @ MGH . HARVARD. EDU
LOREN @ PHY. UCSF. EDU
TZVI @ MATH . BU. EDU

1 Harvard

University
University
3 University of California, San Francisco
2 Boston

Replay of specific patterns of population spiking activity during sharp-wave ripple (SWR) events in the hippocampus is thought to be critical in memory consolidation and retrieval. During SWRs recorded in a rat performing a
navigation task, the hippocampus may replay patterns of place cell activity corresponding to multiple different past
experiences or patterns that do not seem to correspond to any past experience directly. To further explore the role
of such various replay patterns, we have developed a collection of algorithms to allow for closed-loop, directed
manipulation of SWRs based on information extracted from patterns of population spiking activity in real time. We
define a state process that represents a continuous path of positions that are represented in the population place
field activity during a replay event and use a point process filter to estimate the state trajectory for each SWR.
Since real-time estimation does not allow for spike sorting, we construct a marked point process model [1] for a
population of simultaneously recorded units that relates the waveform of each spike to the position of the rat when
that spike occurs. Our current models only allow for decoding of one the animal’s position along one spatial dimension, and here we develop an approach compatible with movement in two-dimensional spaces. We developed
a sequential Monte Carlo (particle filter) approach [2] to numerically estimate the state distribution in a computationally efficient manner. We compare existing bootstrap particle filter methods to one that intermittently uses a
mixture-of-Gaussians proposal density and show that the latter allows for real-time estimation of multimodal filter
distributions. Finally, we demonstrate this filter framework by decoding two-dimensional replay trajectories using
partially simulated data based on actual movement and place cell activity during active exploration of an open
field environment.

III-105. Three-dimensional spatiotemporal receptive field structure in macaque
area MT
Andrew Zaharia1,2
Robbe Goris3
J. Anthony Movshon1
Eero P Simoncelli1,2

ADZ 213@ NYU. EDU
ROBBE . GORIS @ UTEXAS . EDU
MOVSHON @ NYU. EDU
EERO. SIMONCELLI @ NYU. EDU

1 New

York University
Hughes Medical Institute
3 University of Texas at Austin
2 Howard

Neurons in area MT are selective for direction and speed of visual motion. Some of these neurons are “pattern
invariant,” meaning they are velocity-selective regardless of spatial pattern. These properties distinguish them
from neurons in primary visual cortex (V1), which are selective for orientation and spatial frequency. Historically, pattern invariance has been quantified by comparing neurons’ responses to plaids (two sinusoidal gratings
superimposed) with the summed responses to the constituent gratings presented in isolation. More generally,
moving images can be described in terms of the distribution of their 3D spatiotemporal frequency content: translating patterns consist of frequencies that lie on a plane. Recent efforts have characterized MT responses using
stimuli rich in frequency content, including “motion-enhanced” natural movies (Nishimoto and Gallant, 2011) and
“hyperplaids” (multiple, overlapping gratings; Inagaki et al, 2016). These two studies did not focus on pattern invariance, and arrived at models differing in their descriptions of MT receptive field properties. We recorded single
unit responses in alert macaque MT to novel hyperplaid stimuli, consisting of superpositions of multiple moving

COSYNE 2017

235

III-106 – III-107
gratings that continuously and efficiently sampled 3D frequency space. To increase the dynamic range of neural
responses, the stimulus was tailored to each neuron’s estimated frequency preferences so that at least half of the
components were sampled near the neuron’s preferred frequencies. We use a two-stage model of V1 and MT to
capture the selectivity and invariance for spatiotemporal frequency in MT. Both stages include linear weights in
3D frequency space, followed by a nonlinearity. We find this model can both predict hyperplaid responses and
account for pattern invariance in responses to single gratings and plaids. Moreover, we find that positive linear
weights concentrated near the plane corresponding to the preferred velocity of the neuron, as well as nonlinear
tuned suppression, are necessary elements of MT pattern computation.

III-106. A model-based analysis of the interaction between pre-stimulation
neural state and stimulation
Syed Zaidi1,2
Mark Connolly1,3
Babak Mahmoudi1
Robert Gross1

SYED. ZAIDI 3@ EMORY. EDU
MARK . CONNOLLY @ EMORY. EDU
B . MAHMOUDI @ EMORY. EDU
RGROSS @ EMORY. EDU

1 Emory

University
Translational Neuroengineering Labs
3 Georgia Institute of Technology
2 Gross

Neural modulation presents a complex problem, where the goal is to control the state of the dynamic, and ever
changing, brain. This is most clear when the hippocampus is exposed to high-amplitude stimulation, where
identical stimuli can elicit vastly different changes in neural state. Our previous work used machine learning
approaches to classify the pre-stimulation states that could be driven to produce epileptiform afterdischarges,
and those that were resistant to this change in state. However, while this black-box approach was effective at
identifying features that were predictive of neural state change, it did not provide any insight into the underlying
physiology. We further investigate the pre-stimulation neural state using a biophysical model of the hippocampal
CA3 and CA1 network. By fitting the model to pre-stimulation recordings of local field potentials, we were able
to identify model parameters – specifically those related to the excitability of axodendritic inhibitory neurons –
that were associated with a susceptibility to transitioning to an AD state. This higher excitability agrees with the
hypothesis that the intermittent AD response is a manifestation of synaptic vesicle depletion, and the fact that the
axodendritic inhibitory projects can act to disinhibit excitatory neurons leading to epileptiform activity. Biophysical
modeling offers a powerful tool to gain insight into the neural state, further our understanding of neurophysiology,
and control neural state for basic neuroscience and clinical applications.

III-107. Structure and dynamics of robust associative networks operating at
maximum capacity
Danke Zhang
Armen Stepanyants

D. ZHANG @ NORTHEASTERN . EDU
A . STEPANYANTS @ NEU. EDU

Northeastern University
Cortical networks share many ubiquitous structural and dynamical properties. These networks are dominated by
excitatory neurons and synapses, have sparse connectivity, function with stereotypically distributed connection
weights, and exhibit irregular spontaneous activity. We hypothesized that these basic network features arise
readily from associative memory storage. To test this hypothesis, we considered McCulloch and Pitts network with
multiple classes of excitatory and inhibitory neurons, sign-constrained synaptic connections, and homeostatically
constrained synaptic weights. We used the replica theory from statistical physics to solve this problem analytically,
in its most general formulation, and validated this theoretical solution with numerical simulations. Our results show

236

COSYNE 2017

III-108
that various network features emerge at maximum (critical) associative memory storage capacity of individual
neurons. (1) Critical networks have sparse connectivity with probabilities of excitatory connections less than
50% and probabilities of inhibitory connections greater than 50%. (2) The distributions of connection weights in
such networks resemble truncated Gaussians, but can have much heavier tails if neurons in the network have
heterogeneous properties. (3) Robust memory storage leads to irregular network dynamics with large firing
rate fluctuations. These results are consistent with published experimental data from cerebellum to neocortex
to hippocampus. Our theory makes two salient predictions about the dependence of connection weight and
connection probability on neuron type and firing rate. (i) We predict that connection weight is a decreasing function
of presynaptic neuron firing rate. (ii) Connection probabilities, on the other hand, decrease with increasing firing
rate if the presynaptic neuron is excitatory and increases if it is inhibitory.

III-108. Optimizing information transmission in an array of diverse sensory
neurons
Yilun Zhang1,2
David Kastner3
Stephen A Baccus4
Tatyana Sharpee1

YIZHANG @ SALK . EDU
DBKASTNER @ GMAIL . COM
BACCUS @ STANFORD. EDU
SHARPEE @ SALK . EDU

1 Salk

Institute for Biological Studies
of California, San Diego
3 University of California, San Francisco
4 Stanford University
2 University

The retina provides arguably the best system for understanding the tradeoffs that influence dis- tributed information processing across multiple neuron types. We focus here on the problem faced by the nervous system of
allocating a limited number neurons to encode different sensory features at dif- ferent spatial locations. Three
competing goals faced by the retina are the need to 1. encode different visual features, 2. maximize spatial
resolution for each feature, and 3. maximize accuracy for each feature at each location. There is no current
understanding of how these goals are optimized together. While information theory provides a platform for theoretically solving these problems [1–6], evaluating information for large retinal arrays is challenging [1,2]. Here
we account for natural scene statistics and use methods from sufficient statistics to transform these high dimensional entropy calculations into multiple feasible low dimensional calculations. Applying this approach to multiple
overlapping retinal mosaics we identify a transition that determines when it is optimal for a system to shift from
one type of neuron in a high density mosaic to two or more neuron types at lower density. This transition occurs
based on the noise in individual neurons, with lower noise favoring more cell types. These results extend previous
work analyzing a few representative neurons [3–5] and make possible an exploration of how a full set of neural
mosaics behave and interact. For example, our analysis predicts that the relative number of low threshold and
high threshold neurons should differ depending on the noise level, which in turn depends on a cell’s temporal
frequency. These predictions are consistent with experimental measurements showing that salamander ganglion
cells with different temporal filters have different fractions of low threshold (sensitizing) and high threshold (adapting) cells. Overall, the framework makes it possible to evaluate information conveyed in parallel by multiple large
neural populations.

COSYNE 2017

237

III-109 – III-110

III-109. A two-dimensional seperable random field model of within and crosstrial neural spiking dynamics
Yingzhuo Zhang1
Noa Shinitski2
Stephen Allsop3
Kay Tye3
Demba Ba1

YINGZHUOZHANG @ G . HARVARD. EDU
NOA @ BCCN - BERLIN . DE
SA 3@ MIT. EDU
KAYTYE @ MIT. EDU
DEMBA @ SEAS . HARVARD. EDU

1 Harvard

University
Universitaet Berlin
3 Massachusetts Institute of Technology
2 Technische

A fundamental problem in neuroscience is to characterize the dynamics of spiking from the neurons in a circuit
that is involved in learning about a stimulus or a contingency. A key limitation of current methods to analyze neural
spiking data is the need to collapse neural activity over time or trials, which may cause the loss of information
pertinent to understanding the function of a neuron or circuit. We introduce a new method that can determine
not only the trial when a neuron learns a contingency, but also the latency of this learning with respect to the
onset of a cue within that trial. The backbone of the method is a separable two-dimensional (2D) random field
(RF) model of neural spike rasters, in which the joint conditional intensity function of a neuron over time and trials
depends on two latent Markovian state sequences that evolve separately but in parallel. We develop efficient
statistical and computational tools to estimate the parameters of the separable 2D RF model, and apply these to
data collected from neurons in the anterior cingulate cortex (ACC) in an experiment designed to characterize the
neural underpinnings of the observational learning of fear in mice. We find that the trial at which ACC neurons
exhibit a conditioned response to the auditory cue (learning trial) is robust across cells, occurring 3 to 4 trials
into the conditioning period. Following the learning trial, we find that the time with respect to cue onset when we
observe a significant change in neural spiking compared to baseline activity (learning time) varies significantly
from cell to cell, occurring between 20 to 600 ms after cue onset. Overall, the separable 2D RF model provides a
detailed characterization of the dynamics of neural spiking of ACC neurons during observational learning of fear.

III-110. Neural network models of the tactile system develop first-order units
with complex receptive fields
Charlie Zhao1,2
Mark Daley2
Andrew Pruszynski2
1 Yale

WEIGE . ZHAO @ YALE . EDU
MARK . DALEY @ UWO. CA
ANDREW. PRUSZYNSKI @ UWO. CA

University
of Western Ontario

2 University

First-order tactile neurons have distal axons that branch in the skin and form many transduction sites, yielding
complex receptive fields (RFs) with many highly sensitive zones. We have recently shown that this arrangement
permits first-order tactile neurons to signal high-level geometric features of touched objects such as the orientation
of an edge. Here we use machine learning to determine (1) the constraints under which such complex RFs
arise and (2) what computational benefit they yield. We developed a simple four-layer feedforward neural network
abstracting the tactile processing pathway. Importantly, we introduced three biologically-inspired constraints. First,
non-negative regularization in the first weight layer to represent the fact that first-order tactile neurons can only
be excited when their transduction sites are stimulated. Second, convergence from the input to the first hidden
layer to simulate the many-to-one convergence from transduction sites to first-order neurons. Third, two distinct
unsupervised and supervised training phases, representing the encoding and interpreting aspects of the tactile
processing pathway respectively. Our training set included a range of simple stimuli including single points, two
points, as well as Roman and Braille characters. Our network developed complex RFs for almost all training sets
and constraints. The exceptions were edge-cases, when the training set only consisted of single points. We

238

COSYNE 2017

III-111 – III-112
investigated the computational benefit of complex RFs by testing performance on various spatial discrimination
and classification tasks, comparing the performance of learned networks with complex RFs to networks that
we engineered with 2D Gaussian (i.e. simple) RFs. We found that learned networks outperformed engineered
networks, especially on complex classification tasks and that this benefit was accentuated with fewer units in the
first hidden layer (i.e. more convergence) and when we introduced noise to the inputs.

III-111. Gotta infer’em all: dynamical features from neural trajectories
Yuan Zhao
Il Memming Park

YUAN . ZHAO @ STONYBROOK . EDU
MEMMING . PARK @ STONYBROOK . EDU

Stony Brook University
Continuous dynamical systems theory lends itself as a framework for both qualitative and quantitative understanding of how neural systems compute [1, 2, 3, 4]. For example, attractor dynamics are often proposed as
mechanisms for decision-making or working memory, where the convergence to an attractor represents the result
of computation. Thus, a central challenge in systems neuroscience is solving the inverse problem: reconstructing
meaningful dynamics from neural time series. Here we tackle this problem by fitting a nonlinear time series model
amenable to human interpretation under the continuous dynamical systems theory. Unlike typical approaches
for time series analysis using nonlinear autoregressive models [7, 8] which can produce wild extrapolations not
suitable for scientific study, our goal is to confidently recover key dynamical features that reflect the nature of the
underlying computation. We propose a model that directly learns the velocity field that governs the local time
evolution of the neural state. We further parameterize the velocity field using a finite number of basis functions,
in addition to a global contractional component. These features encourage the model to focus on interpolating
dynamics within the support of the training trajectories. Critically, our particular parameterization yields to better
interpretations by recover qualitative features of the phase portrait such as fixed points, continuous attractors, oscillation, and bifurcations, while also producing reliable future predictions in a variety of dynamical models and in
real neural data far beyond a linear model. Furthermore, the model provides straightforward identification of ghost
points and linearization of the dynamics around those points for stability and manifold analyses. We envision that
this flexible model can be used to reverse-engineer the neural implementation of computation from the inferred
phase portrait.

III-112. Non-equilibrium driven dynamics of continuous attractors in place
cell networks
Weishun Zhong1
Hyun Jin Kim2
David Schwab2
Arvind Murugan1
1 University

WZHONG @ UCHICAGO. EDU
HYUNKIM 2015@ U. NORTHWESTERN . EDU
DAVID. SCHWAB @ NORTHWESTERN . EDU
AMURUGAN @ UCHICAGO. EDU

of Chicago
University

2 Northwestern

The concept of dynamical attractors has found much use in neuroscience as a means of information processing,
storage, statistical inference, and decision making. Examples include Hopfield’s model of associative memory
with point attractors, short-term memory storage in continuous attractors such as the oculomotor system, spatial
navigation and planning using grid and place cell networks, and dynamic pattern recognition. To perform many of
these tasks, external signals must be able to modulate persistent dynamical activity and yet, most theoretical work
on attractors has been in the limit where these forces are static, weak or absent. The lack of time- and spacedependent driving forces is a particularly important omission for continuous attractors such as place cell networks;
here, driving forces provide crucial sensory input or locomotion signals to move a bump of activity through the

COSYNE 2017

239

III-113
environment. In this work, we study how continuous attractors in networks of place cell neurons perform under
the action of space- and time-varying external driving forces. We obtain analytical results for the performance
and capacity of such non-equilibrium networks as a function of the spatial and temporal rate of change of driving
forces. We derive such results using an intuitive ‘equivalence principle’ between a time-dependent external drive
and a static modified drive, much like the equivalence principle in Newtonian physics. Interference due to multiple
stored attractors results in a ‘bumpy’ continuous attractor, and we find that the effects of this bumpy landscape
on a driven activity bump can be well approximated by an effective temperature. Using this approach, we are
able to study how memory retrieval in rapidly driven non-equilibrium place cell networks depends on fundamental
parameters such as the speed and strength of the force and the number of stored memories.

III-113. Interdigitating subnetworks of intracortical projection neurons in mouse
V1
Petr Znamenskiy1
Meanhwan Kim1
Maria Florencia Iacaruso2
Thomas Mrsic-Flogel1

PETER . CSHL @ GMAIL . COM
MEANHWAN . KIM @ UNIBAS . CH
FLORENCIA . IACARUSO @ GMAIL . COM
THOMAS . MRSIC - FLOGEL @ UNIBAS . CH

1 University
2 Oxford

of Basel
University

In mouse primary visual cortex, direct cortico-cortical projections convey information about the visual scene to
several higher visual areas. How do these projections act in concert to deconstruct the visual scene? To answer
this question, we examined the local connectivity and functional properties of V1 neurons projecting to anterolateral (AL) and posteromedial (PM) visual areas. Using multiple whole-cell patch-clamp recordings we found that
AL and PM projection neurons in layer 2/3 preferentially receive local inputs from neurons projecting to the same
long-range target, while connections between neurons projecting to different targets are rare. Previous work has
shown that similarity of responses to visual stimuli strongly predicts synaptic connectivity in mouse V1. Can the
specific connectivity of AL and PM projection neurons be explained by differences in their functional properties?
To test this hypothesis, we simultaneously recorded the activity of AL and PM projection cells in awake mice using
two-photon calcium imaging. These populations differed in terms of their spatial frequency, temporal frequency,
speed, and direction selectivity. However, individual AL and PM projection neurons were highly heterogeneous,
spanning the full range of functional diversity found in mouse V1. Similarly, although responses of neurons targeting the same area were more correlated than those projecting to different targets, the correlation distributions
overlapped heavily. Consequently, synaptic connectivity rates predicted by response correlations between AL
and PM projection neurons fell short of explaining their exclusive connectivity, suggesting that other mechanisms
must play a role. These results demonstrate that projection neurons in the neocortex are organized in functionally
distinct interdigitating subnetworks.

240

COSYNE 2017

Author Index

B

Author Index
Aazhang B., 155
Abbara A., 49
Abbott L., 43, 142, 151, 163, 178, 190
Acerbi L., 171, 224
Adam V., 49, 150
Adams R., 150
Adler W., 45
Agarwal G., 50
Agnes E. J., 50
Agrochao M., 193
Ahamed T., 51
Ahilan S., 52
Ahissar M., 150
Ahmad S., 119
Ahmadian Y., 62
Aihara K., 214
Aimon S., 52
Aitchison L., 53
Aizenberg M., 56, 69
Akrami A., 35, 143
Albanna B., 32
Albeanu D. F., 60
Alemi A., 47, 49
Aljadeff J., 54
Allen W. E., 42, 54
Allsop S., 238
Alonso J. M., 140
Amarante L., 55
Amin H. I. H. A., 208
Ammer G., 96
Amoroso M., 55
Andalman A. S., 42
Andermann M. L., 38, 184, 213
Anderson A., 56
Angeloni C., 56
Angulo-Garcia D., 116
Anumanchipalli G. K., 80
Arai K., 57
Archer E., 208
Arenz A., 96
Atallah B., 167
Athalye V., 57, 135
Averbeck B., 63
Axel R., 43
Azouz R., 58

Baccus S. A., 159, 182, 237
Badwan B., 193
Bai Y., 81
Bak J. H., 143
Bakhurin K., 114
Balasubramanian V., 131, 134, 181, 216
Ballard I., 59
Balle J., 66
Banerjee A., 60
Bang D., 120
Banks G., 204
Banyai M., 175
Baqai F., 60
Barabasi D., 133
Barak O., 63, 94
Baram A., 61
Barbosa J., 36
Barbour B., 61
Barello G., 62
Barnes C., 203
Bartolo R., 63
Barzelay O., 63
Bassetto G., 64, 113
Batista A., 68, 112, 120
Behabadi B. F., 129
Behrens T., 145
Behrens T. E., 61
Beier K., 225
Beierholm U., 64
Ben-Shushan N., 65, 202
Bengio Y., 31
Benjamin A., 65
Berardino A., 66
Berck M. E., 204
Berdondini L., 208
Berens P., 66
Berg J., 216
Bernacchia A., 67
Bernstein J., 189
Berry M., 194
Bethge M., 66, 75, 158
Beyeler M., 68
Bijsterbosch J., 145
Bilenko N., 228
Bimbard C., 61
Bishop S., 145

Ba D., 203, 238

COSYNE 2017

241

C

Author Index
Bishop W., 68
Blackwell J., 69
Blei D., 150
Blot A., 78
Boahen K., 101
Bogacz R., 230
Boie S., 69
Bolding K., 70
Bonin V., 229
Bonnen K., 71
Boos M., 71
Borghuis B. G., 72
Borst A., 96, 147, 226
Bos J., 173
Botvinick M., 44, 162, 204
Bouvier G., 61
Boyden E., 82
Boynton G. M., 68
Brackbill N., 200, 201
Brea J., 168
Bredenberg C., 72
Brincat S. L., 36, 164
Brinkman B., 73
Brody C., 35, 90, 162, 176, 181, 198
Browning P., 63
Broxton M., 52
Brunel N., 54, 61, 179
Buice M., 73, 171
Buonomano D., 114
Burak Y., 202
Burge J., 183
Burgess C., 38, 184
Burke S., 203
Buschman T., 98
Butera R., 219
Butts D. A., 74, 229
Cadena S., 75
Caetano M., 55
Cain N., 216
Calaim N., 75
Calangiu I., 76
Calhoun A., 76
Carandini M., 34, 174
Carcea I., 32, 77
Carmena J. M., 57, 135
Carrasco M., 45
Castonguay P., 53
Cauwenberghs G., 166
Cayco Gajic A., 77
Chacron M., 124
Chadwick A., 78
Chae H., 60
Chalk M., 78
Chambon V., 217

242

Chang C., 100
Chang E., 80, 137, 192
Charles A. S., 79, 107, 207
Chartier J., 80
Chase S., 68, 112, 120
Chattoraj A., 144
Chaudhuri R., 80
Chen C., 154
Chen M. Z., 54
Chen S., 81
Chen X., 82, 225
Chen Y., 81
Cheung B., 83
Chiang A., 222
Chiang F., 83
Chichilnisky E., 161, 200, 201
Chklovskii D., 84, 107, 110, 200
Choi H., 84
Choi J. Y., 143
Chong E., 85
Chowdhury R., 110
Chowdhury S., 85
Christensen A., 86
Chung S., 86
Churchland A., 133, 168
Churchland M., 163, 178
Clark D., 92, 193
Clemens J., 87
Clopath C., 61, 77, 209
Coca D., 87, 222
Cocco S., 216
Coen P., 76
Coen-Cagli R., 88
Cohen M., 123
Cohen-Kashi-Malina K., 155
Cohn R., 88, 117
Collins J., 178
Compte A., 36
Conen K., 89
Connolly M., 90, 236
Conover K., 52
Constantinidis C., 36
Constantinople C., 90
Cormack L. K., 71
Cossart R., 116
Costa R., 57
Costa R. M., 135
Costa R. P., 91
Cowley B., 91
Cox M., 228
Creamer M. S., 92
Crevecoeur F., 92
Crimaldi J., 69
Crowell A., 219
Crowley J. C., 233

COSYNE 2017

Author Index
Cui Y., 74, 119
Cunningham J., 161, 163, 168
Currier T., 93
Czuba T. B., 71
D’Amour J., 91
Dai B., 85
Daley M., 238
Dasgupta I., 189
Davis K., 206
Davis M., 106
Davis Z., 93
Dawes H., 137
Dayan P., 52, 152
De La Rocha J., 126
Debanne D., 54
Decker Harris K., 43
Deco G., 111
Degenhart A., 68
Deisseroth K., 42, 52, 54
Demb J., 74
Deneve S., 47, 75, 156
Denfield G., 75
Denison R., 45
Deniz F., 94, 228
Derdikman D., 94
Desai N., 181
Deverett B., 110
DeVries S., 73
DeWeese M., 192
Dhawale A., 95
DiCarlo J., 131
Ding A. W., 158
Ding J., 120
Dipoppa M., 34, 174
Doiron B., 28, 72, 123, 146
Domnisoru C., 115
Dougherty K., 228
Dowdall J., 197
Drebitz E., 95
Drews M., 96
Drugowitsch J., 214
Dulac C., 30
Duncker L., 41, 49, 97
Dvorak D., 97
Ebitz B., 98
Echeveste R., 98
Ecker A., 75
Eden U., 57, 235
Effenberger F., 99
Egger S., 100
Egner T., 120
Ehrlich D., 100
El Hady A., 181

COSYNE 2017

F–G
Eliasmith C., 126
Elsayed G., 163, 168
Emptage N., 91
Engel T., 101
Engelken R., 101
Engert F., 116
Epstein R., 134
Ermentrout B., 69
Escabi M., 136, 154
Escola S., 167
Esnaola I., 87
Ezzyat Y., 186
Fairhall A., 178
Farhoodi R., 102
Feldheim D., 127
Feng D., 73, 216
Fenk L., 46
Fenselau H., 38, 184
Fenton A., 97, 223
Fernades H., 65, 185
Ferreira M., 50
Festa D., 102
Fiebelkorn I., 103
Field G., 97
Fiete I., 80
Fiete I. R., 41
Figueroa Velez D., 106
Fine I., 68
Fiquet P., 75
Fisek M., 32
Fiser J., 67
Fitzpatrick D., 48, 196, 229, 232
Florescu D., 222
Foley N. C., 103
Fontanini A., 158
Forest C., 82
Frank L., 57, 215, 235
Franks K., 70
Freeman J., 66
Freiwald W., 234
Fridman M., 104
Friedrich J., 105, 110
Fries P., 197
Froemke R., 32, 77, 91, 196
Fukai T., 121
Furst M., 63
Gage G., 27
Gagne C., 145
Galbiati G., 105
Galgali A., 106
Gallant J., 94, 228
Gandhi S., 106
Ganguli S., 27, 40, 118, 172, 182, 211

243

I

Author Index
Gariepy H., 213
Garrett M., 175
Gauthier J., 107, 115, 207
Geffen M., 28, 56, 69
Geisler W., 81
Genkin A., 107
Gentner T., 193, 217, 218
Gepner R., 108
Gercek B., 80
Gershman S., 210
Gershow M., 108
Gerstner W., 109, 148, 149, 168
Ghanbari A., 108
Giahi A., 109
Gilra A., 109
Gilson M., 111
Giocomo L., 172
Giovannucci A., 110
Gjorgjieva J., 183
Glaser J., 43, 110
Glomb K., 111
Go U., 169
Goetz G., 200, 201
Goldey G., 38
Gollisch T., 112
Golub M., 112, 120
Goncalves N., 113
Goncalves P. J., 113
Goo W., 41
Goodman N., 59
Goris R., 114, 235
Gottlieb J., 103
Goudar V., 114
Gouvea T. S., 164
Gouwens N. W., 216
Gray D., 203
Greenspan R., 52
Griffiths B., 64
Gritton H., 199
Groblewski P., 175
Grosberg L., 161
Grosenick L., 52
Gross R., 90, 206, 236
Gu Y., 115
Guggiana-Nilo D., 116
Gugig E., 58
Gunay C., 219
Guo C., 198
Gupta D., 176
Gupta V., 201

Handler A., 117
Hardcastle K., 172
Harpaz R., 117
Harris K., 34, 174
Hart E., 118
Harvey C., 180
Harvey S., 118
Hasenstaub A., 42, 122, 165, 180
Hausser M., 53
Hawkins J., 119
Hein B., 48, 229
Henderson J., 178
Hennequin G., 67, 98, 102, 191, 212
Hennig J., 120
Hennig M., 208
Herce Castanon S., 120
Herman P., 36
Hermoso-Mendizabal A., 126
Hermundstad A., 121
Hilgen G., 208
Hillar C., 99
Hiratani N., 121
Hirokawa J., 157
Hisey E., 37
Hochberg M., 144
Hocker D., 122
Hofer S., 137
Hofer S. B., 78
Hoglen N., 122
Horga G., 204
Horst N., 55
Hosseini E., 226
Hottowy P., 161
Howard J., 130
Hsu M., 192
Hu B., 123
Hu M., 188
Hu X., 149
Hu Y., 204
Huang C., 123, 124, 146
Huang L., 124
Huang Y., 222
Huelsdunk P., 48
Huh D., 125
Huk A., 71, 118, 139
Humplik J., 125
Hung C. P., 169
Hunsberger E., 126
Huth A., 94, 228
Hyafil A., 126

Haefner R., 144
Haimerl C., 116
Hakim V., 61
Han X., 199, 221

Iacaruso M. F., 137, 240
Inglebert Y., 54
Insanally M., 32, 77
Ishii S., 148

244

COSYNE 2017

Author Index
Ismakov R., 94
Issa E. B., 131
Ito S., 127
Itskov V., 142
Iyer R., 216
Izquierdo A., 211
Jacot-Guillarmod A., 127
Jadhav S., 215
Jadi M., 129
Jaini P., 183
James N., 199
Jaramillo J., 128
Jaramillo S., 126
Jazayeri M., 35, 100, 187, 205, 226
Jeanne J., 32
Jeffery K., 94
Jegminat J., 128
Jessell T., 163
Jikomes N., 38
Jin J., 140
Jin L., 129
Jobst B., 206
Johnson D., 155
Johnson M., 150
Jonas E., 46
Josic K., 127, 171, 184
Jozefowicz R., 178
K Namboodiri V. M., 129
Kadmon J., 130
Kahana M., 186, 206
Kahnt T., 130
Kalamangalam G., 155
Kang L., 131
Kanitscheider I., 41
Kanwal J., 204
Kao J., 178, 220
Kappel J., 85
Kar K., 131
Karpas E., 132
Karpova A., 198
Kaschube M., 48, 229
Kastner D., 237
Kastner D. B., 159
Kastner S., 103
Kato S., 133
Katsuki T., 52
Katz L., 118
Kaufman M., 133, 178
Kauttonen J., 60
Kauvar I., 42, 54
Kearney M., 37
Kebschull J., 124
Keinath A., 134

COSYNE 2017

L
Kell A., 134
Kepecs A., 157
Kepple D., 135
Keramati M., 146
Khan A., 78
Khani M. H., 112
Khanna P., 135
Khatami F., 136
Khorsand P., 136
Kiani R., 156, 173
Kiggins J., 175
Kilpatrick Z. P., 127, 184
Kim A., 46
Kim H., 40
Kim H. J., 239
Kim M., 137, 240
Kinkhabwala A., 115
Kirkby L., 137
Kirst C., 138
Klyachko V., 139
Knight R., 192
Knoell J., 139
Koay S. A., 207
Kobak D., 104
Koch C., 216
Koch E., 140
Kodandaramaiah S., 82
Kohn A., 71, 88, 91
Koida K., 214
Kolb I., 82
Kolda T., 40
Komatsu H., 214
Konda R., 219
Konrad S., 140
Kopec C., 35
Kording K., 43, 46, 65, 102, 110, 185
Koulakov A., 135
Koyluoglu O. O., 198
Kragel J., 206
Kraynyukova N., 141
Kreiter A., 95
Kriegeskorte N., 33
Kubilius J., 131
Kucewicz M., 206
Kuczala A., 141
Kuhlman S., 60
Kumaran D., 44
Kunin A., 142
Kurth-Nelson Z., 44
Kuruppath P., 58
La Camera G., 158
Lahiri S., 118, 211
Lalazar H., 142
Lampl I., 155

245

M

Author Index
Landau I., 59
Landy M., 171
Langdon A., 143
Lange R., 144
Laparra V., 66
Larimer P., 122
Laskar M. N. U., 144
Latham P., 220
Laubach M., 55
Lawlor P., 43
Lawrance E., 145
Lazar A., 175, 222
Lecoq J., 73
Lee D., 86, 100
Lee J., 146
Lee M., 137
Lee T. S., 60
Lefevre P., 92
Lega B., 206
Lehmann M., 149
Leibo J., 44
Lengyel M., 67, 98, 102, 159
Leone M., 146
Leonhardt A., 147
Levandowski K., 38, 184, 213
Levenstein D., 45
Levina A., 147
Levy D., 213
Levy R., 155
Lewis M., 119
Li H. L., 148
Li Y., 105, 148
Liakoni V., 149
Liao F., 149
Lichtman J., 99
Lieder I., 150
Lin C., 169
Lin J., 192
Linderman S., 150
Lindsay G., 39
Liou J., 151
Litke A., 127, 161, 200, 201
Litwin-Kumar A., 43
Liu B., 152
Liu D., 57
Liu J. K., 112
Livneh Y., 38
Lloyd K., 152
Lo C., 222
Lomber S., 82
Lottem E., 153
Lovett-Barron M., 42
Lowell B., 38, 184
Lu X., 99
Luecke J., 71

246

Lueckmann J., 113, 153
Luna Ortiz C., 87, 222
Lund P., 120
Lundqvist M., 36
Luo L., 54, 225
Luongo F., 137
Lyu C., 46
Lyzwa D., 154
Ma J., 199
Ma W., 45, 105, 171, 224
Maccione A., 208
Macellaio M., 152
Machens C., 47, 75, 104, 167
Macke J., 64, 113, 153, 208
Madara J., 38
Madugula S., 161
Maheswaranathan N., 182
Mahmoudi B., 90, 236
Mahn M., 155
Maier A., 228
Maimon G., 46
Mainen Z., 50, 153
Makin J., 210
Malenka R., 225
Malladi R., 155
Malone B. J., 122
Malvache A., 116
Mamiya A., 221
Manavi S., 175
Mano O., 92
Mante V., 76, 106, 202
Manu M., 159
Maoz O., 156
Maras M., 156
Marder E., 176
Mardoum P., 199
Mariano V., 188
Markram H., 170
Marques T., 104
Marre O., 78, 194
Martinez D., 124
Martinez-Trujillo J., 93
Masmanidis S., 114
Masset P., 157
Mastrogiuseppe F., 34
Mathis A., 55, 158
Mauss A., 147
Mayberg H., 219
Mazzucato L., 158
McClure S., 59
McDermott J., 134, 160, 163
McHugh M., 69
McIntosh L., 159
McKhann G., 204

COSYNE 2017

Author Index
McNamee D., 159
McWalter R., 160
Mehrer J., 33
Meisel C., 160
Mel B. W., 129
Memoli F., 85
Mena G., 161
Menon V., 216
Metzen M. G., 124
Miconi T., 161
Miesenbock G., 31
Mihalas S., 175, 216
Mikell C., 204
Miller A., 150
Miller E., 59
Miller E. K., 36, 164
Miller K., 162
Miller K. D., 39
Miller L., 110
Minnig M., 184
Miri A., 163
Mitz A., 63
Miyamoto Y., 95
Mlynarski W., 121, 163
Moazami M., 164
Mohan S., 84
Mokhtari A., 225
Monasson R., 216
Monteiro T., 164
Mooney R., 37
Moore T., 82, 98, 101, 179
Moran R., 120
Morantte I., 88, 117
Mordatch I., 165
Morrill R., 165
Mostafa H., 166
Motiwala A., 164, 167
Movshon J. A., 114, 235
Mrsic-Flogel T., 78, 137, 240
Muller E., 170
Muller L., 93
Muller T. H., 61
Munoz W., 45
Murray J., 167
Murray J. D., 100
Murthy M., 76, 87
Murthy V. N., 158
Murugan A., 239
Murugan M., 38
Musall S., 133
Muscinelli S. P., 168
Mussack D., 195
Nadal J., 61
Nagel K., 69, 93, 213

COSYNE 2017

O–P
Nahum M., 137
Najafi F., 168
Nakae K., 148
Nam Y., 169
Naoki H., 148
Narain D., 35, 226
Natan R., 69
Naud R., 169
Neumann P., 225
Newsome W., 48
Niebur E., 123
Nienborg H., 153
Niv Y., 143, 197
Niyogi R., 52
Nolte M., 170, 199
Nonnenmacher M., 113
Norton E., 171
Nunez-Elizalde A., 228
O’Doherty J. E., 187, 210
O’Reilly J., 145
O’Shea D., 41
Oby E., 68
Ocker G., 171
Ocko S., 172
Oemisch M., 172
Oganov A., 85
Ogmen H., 127
Okazawa G., 173
Olcese U., 173
Ollerenshaw D., 175
Olsen S., 175
Olshausen B., 56, 83
Olveczky B., 95
Onken A., 112
Orban G., 175
Osborne L., 152
Ostojic S., 34
Otopalik A., 176
Ott T., 157
Oude Lohuis M., 153
Ozeri N., 87
Pachitariu M., 34, 174
Packer A., 53
Padamsey Z., 91
Padoa-Schioppa C., 89
Pagan M., 176
Palmer S., 194
Palmigiano A., 177
Palminteri S., 217
Pandarinath C., 178
Pandey B., 80
Pang R., 178
Paninski L., 105, 150, 161, 200

247

S

Author Index
Panzeri S., 112, 180
Park I. M., 39, 122, 239
Parker N., 38
Parvizi J., 192
Pasupathy A., 84
Pathak Y., 204
Paton J., 164, 167
Pedroza C., 127
Pehlevan C., 84, 107
Pennartz C., 173
Pereira U., 179
Perich M., 110
Perrin G. E., 74
Petreanu L., 104
Pettine W., 179
Pfister J., 128
Phelps E. A., 29
Phillips E., 42
Phillips E. A., 122, 180
Piantadosi S., 59
Piasini E., 180
Piet A., 176, 181
Pillow J., 30, 76, 79, 86, 97, 107, 139, 143, 207
Pinsk M., 103
Pitkow X., 109, 224, 233
Plenz D., 160
Pnevmatikakis E., 110, 168
Poldrack R., 232
Polley D., 230
Pollock E., 181
Ponce-Alvarez A., 111
Poole B., 182
Poort J., 78
Porter M., 212
Pouget A., 214
Preuschoff K., 149
Priesemann V., 147
Prigge M., 155
Prinz A., 219
Pruszynski A., 238
Pullman S., 204
Quendera T., 50
Quick K., 112, 120
Radillo A., 184
Radwan B., 97
Rakate G., 195
Ramesh R., 38, 184
Ramkumar P., 43, 102, 185
Randazzo M., 186
Rao W., 69
Ratliff C., 72
Ratnam K., 56
Rausch L., 95

248

Ravi S., 97
Read H., 136, 154
recanatesi s., 186
Rechenmann J., 187
Reimann M., 170
Remington E., 187
Resch J., 38
Reynolds J., 93
Rhoades C., 200, 201
Richman E., 54
Richmond P., 222
Richter F., 96
Rieger J. W., 71
Riegler C., 116
Rieke F., 73
Rigotti M., 186
Rikhye R. V., 188
Rinberg D., 85, 231
Ritt J., 188
Ritter P., 111
Riva-Posse P., 219
Rizzuto D., 206
Robbe D., 126
Rodrigues F., 164
Rodriguez Romaguera J., 129
Roesch M., 143
Roeth K., 183
Rokem A., 68
Rolnick D., 189
Romani S., 189
Romano N., 119
Ron S., 155
Roorda A., 56
Rouault H., 189
Rozenblit F., 112
Rubin A., 189
Rubin D. B., 39
Rubin R., 190
Rudy B., 45
Rueda-Orozco P. E., 126
Ruediger S., 191
Ruff D., 123
Runyan C., 180
Russell L., 53
Ruta V., 31, 88, 117
Rutten V., 191
Ryu S., 112, 120, 178, 182, 220
Sabes P., 187, 210
Sachdeva P., 192
Sadeghi M., 136
Sadtler P., 112, 120
Saez I., 192
Sahani M., 41, 49, 78, 150
Sainburg T., 193, 218

COSYNE 2017

Author Index
Salazar E., 193
Salisbury J., 194
Samuel A., 204
Samulak A., 88
Sanchez-Giraldo L. G., 144, 194
Santhanam G., 182
Santos F., 57
Sarra D., 153
Sato T., 169
Saunders R., 63
Scanziani M., 191
Schalek R., 99
Schalk G., 192
Schevon C., 151, 204
Schewe H., 195
Schiavo J., 196
Schmidt K., 131
Schneidman E., 117, 132, 156
Schnitzer M., 40
Schoenbaum G., 143
Scholl B., 196, 232
Scholvinck M., 197
Schrater P., 195
Schreyer H. M., 112
Schroeder J., 188
Schroeder S., 34
Schuck N., 197
Schwab D., 239
Schwartz D., 198
Schwartz O., 144, 194
Scott B., 198
Sebastian S., 81
Sedigh-Sarvestani M., 199
Seely J., 163
Segev I., 226
Segraves M. A., 43
Seidemann E., 81
Sejnowski T., 52, 93, 125
Semedo J., 91
Sen K., 199
Sengupta A., 200
Senn W., 209
Seo H., 100
Sernagor E., 208
Shah N., 200, 201
Shaham N., 202
Sharan A., 206
Sharpee T., 141, 218, 237
Shavina V., 202
Shea-Brown E., 73, 84, 171
Shenoy K., 40, 41, 178, 182, 220
Sher A., 200, 201
Sheth S., 204, 206
Shin J., 215
Shinitski N., 203, 238

COSYNE 2017

S
Shizgal P., 52
Shklarsh A., 132
Shlens J., 201
Shohamy D., 30
Shrinivasan S., 204
Si G., 204
Siliciano A., 117
Silver R. A., 77
Simoncelli E. P., 66, 114, 200, 235
Singer A. C., 82
Singer J., 74
Singer W., 175
Singer Y., 201
Slotine J., 47
Smith A., 203
Smith E., 204
Smith G. B., 48, 229
Smith M., 91, 95, 185
Soares S., 167
Socha K., 229
Sofroniew N., 66
Sohal V., 137
Sohn H., 205
Solomon E., 206
Solomon R. B., 52
Soltani A., 136
Soltuzu L., 207
Sommer F., 227
Sompolinsky H., 43, 59, 86, 130, 189, 190
Song A., 207
Song H. F., 48
Song S., 149
Sorbaro Sindaci M., 208
Soyer H., 44
Sparks F. T., 97
Speiser A., 208
Sperling M., 206
Spicher D., 209
Sprekeler H., 169
Spyropoulos G., 197
Stan P., 60
Staneva V., 110
Starkweather C., 210
Stavisky S., 178, 220
Stein J., 206
Steinmetz N., 34, 101, 179
Stemmann H., 95
Stepanyants A., 236
Stephens G., 51
Stevenson I., 108, 136
Stippinger M., 175
Stock C., 211
Stolyarova A., 211
Stone J. T., 66
Storrs K., 33

249

V–W
Stringer C., 34, 174
Stroud J., 212
Stuber G., 129
Sugden A., 184, 213
Summerfield C., 120
Sunil S., 188
Sur M., 188
Sussillo D., 178
Suver M., 213
Suzuki H., 214
Tabone C., 204
Tajima C. I., 214
Tajima S., 214
Takahashi Y., 143
Talbot Z., 97
Talei Franzesi G., 82
Tandon N., 155
Tang W., 215
Tanifuji M., 169
Tank D., 107, 115, 198, 207
Tao L., 219
Tchumatchenko T., 140, 141
Teeter C., 216
Telian G., 188
Tenenbaum J., 234
Tervo D., 198
Tesileanu T., 216
Theilman B., 193, 217
Theis L., 66
Thero H., 217
Thiberge S., 198
Thielk M., 193, 218
Tian K., 219
Tikidji-Hamburyan A., 200, 201
Tirumala D., 44
Tiruvadi V., 219
Tishby N., 226
Tkacik G., 78, 117, 125, 156
Tolias A., 66, 75
Tomkins A., 222
Tootoonian S., 220
Torcini A., 116
Torgerud W., 195
Trautmann E., 220
Tremblay R., 45
Tseng H., 221
Tsodyks M., 65
Turaga S., 53, 208
Tuthill J., 221
Tye K., 29, 238
Tyler-Kabara E., 112, 120
Uchida N., 29, 55, 210
Ukani N. H., 222

250

Author Index
Ustun C., 223
Vaadia E., 142
van Dijk M., 223
van Mourik-Donga L., 173
van Opheusden B., 105, 224
Van Vleet T., 137
Vandendriessche H., 217
Varela C., 44
Vasudeva Raju R., 224
Vaughan A., 225
Veliz-Cuba A., 184
Vercruysse F., 169
Vertechi P., 153
Victor J., 69
Vinck M., 173
Vogels T. P., 50, 91, 212
von der Heydt R., 123
Vyas S., 40
Wadekar D. S., 108
Wahlstrom Helgren S., 139
Walker E., 75
Wall N., 225
Wallis J., 83
Walther A., 33
Wang C., 222
Wang F., 40
Wang J., 44, 226
Wang M., 82
Wang S., 226
Wang X., 48, 128
Wang Y., 127
Warden M., 36
Warner C., 227
Warriner C., 163
Waters A., 219
Wayne G. D., 186
Wehbe L., 228
Wei A., 158
Wei X., 181
Weidinger J. D. F., 233
Weiss E., 83
Welchman A., 113
Westendorff S., 172
Westerberg J., 228
Whiteway M., 74, 229
Whitney D. E., 48, 229, 233
Whittington J., 230
Williams A., 40, 182, 211
Williams D., 175
Williamson R. S., 230
Wilson C., 231
Wilson D., 196, 232
Wilson M., 44

COSYNE 2017

Author Index

Z

Wilson R., 32
Wimmer G. E., 232
Witten I., 38, 143
Wolf F., 101, 177, 233
Wolk J., 108
Wolpert D., 159
Womelsdorf T., 172
Wood D., 43
Worrell G., 206
Yamauchi Y., 169
Yan J., 53
Yang G. R., 48
Yang Q., 233
Yates J., 39, 144
Yates M., 204
Yeh C., 222
Yildirim I., 234
Yizhar O., 155
Yousefi A., 235
Yu B., 68, 91, 112, 120, 182
Yu Q., 204
Zador A., 124, 225
Zaghloul K., 206
Zaharia A., 235
Zaidi Q., 140
Zaidi S., 236
Zandvakili A., 91
Zaslavsky N., 226
Zhang D., 236
Zhang Y., 203, 237, 238
Zhao C., 238
Zhao J., 213
Zhao Y., 39, 239
Zhong W., 239
Zhou P., 105
Zhou Y., 222
Ziemba C. M., 114
Zirnsak M., 82
Znamenskiy P., 137, 240
Zoccolan D., 207
Zolin A., 88

COSYNE 2017

251

