COSYNE 2012

The 9th annual Computational and Systems Neuroscience meeting
Main meeting program
February 23-26
Salt Lake City

www.cosyne.org

Program Summary
Thursday, 23 February
4:00 pm

Registration opens

5:30 pm

Welcome reception

6:20 pm

Opening remarks

6:30 pm

Session 1: Spikes - correlations and oscillations
Invited speakers: F. Rieke, E. Brown

8:30 pm

Poster Session I

Friday, 24 February
7:30 am

Breakfast

8:30 am

Session 2: Motor systems
4 accepted talks

10:00 am

Session 3: M INISYMPOSIUM: Olfaction
Invited speakers: N. Sobel, Z. Mainen; 2 accepted talks

12:00 pm

Lunch break

2:00 pm

Session 4: Neural representations – models and theory
Invited speaker: T. Griffiths; 3 accepted talks

4:00 pm

Session 5: Navigation and Movement
Invited speaker: D. Ballard; 2 accepted talks

5:15 pm

Dinner break

6:30 pm

Special workshop: “NSF: Theory and Applications”, Ken Whang, NSF

7:30 pm

Poster Session II

Saturday, 25 February
7:30 am

Breakfast

2:00 pm

Session 6: Linking neural activity and behavior–optogenetics
4 accepted talks

10:00 am

Session 7: M INISYMPOSIUM: Cognitive representation
Invited speakers: J. Assad, S. Fusi; 2 accepted talks

12:00 pm

Lunch break

2:00 pm

Session 8: Dendrites and synapses
Invited speaker: N. Brunel; 3 accepted talks

4:00 pm

Session 9: Representing beliefs
Invited speaker: R. Saxe; 1 accepted talk

5:00 pm

Dinner break

7:30 pm

Poster Session III

COSYNE 2012

i

Sunday, 26 February

ii

7:30 am

Breakfast

8:30 am

Session 10: Visual circuitry
Invited speaker: T. Hensch; 3 accepted talks

10:30 am

Session 11: Auditory processing
Invited speaker: S. Wooley; 3 accepted talks

12:00 pm

Lunch break

2:00

Session 12: Visual representation
4 accepted talks

3:00 pm

Closing remarks

COSYNE 2012

Poster Session Topics

Session I
Thursday

Session II
Friday

Session III
Saturday

1–11
12–24
25–27
28–29
30–31
32–39
40
41–45
46–56
57–58
59–63
64–66
67–68
69–73
74–76
77–78
79–92
93–98

1–11
12–24
25–28
29–31
32–33
34–41
42–43
44–47
48–58
59–61
62–65
66–68
69–71
72–75
76–77
78–80
81–93

1–11
12–24
25–28
29–30
31–33
34–41
42–44
45–48
49–58
59–60
61–64
65–68
69–70
71–74
75–77
78–79
80–92

Topic Area
Bayesian / optimality / sparse coding
Computational / modeling
Dynamical systems
Oscillations
Information theory
Neural coding / decoding
Neural correlations
Cognition: attention, memory
Cognition: decision making, reward
Cognition: objects, categories
Learning / plasticity
Motor systems
Basal ganglia, bird song, grid cells
Sensory: adaptation, statistics, perception
Sensory: chemical / multi / somatosensory
Sensory: hearing
Sensory: vision
Behavior
Stimulation: electrodes, optogenetics, TMS
Techniques

COSYNE 2012

94–99
93–99

iii

celebrating
Biological Learning
and Control
How the Brain Builds Representations,
Predicts Events, and Makes Decisions
Reza Shadmehr
and Sandro Mussa-Ivaldi
“This exciting book provides a coherent
framework for understanding how
the brain learns to control the body.
By synthesizing recent advances with
historical perspectives, it provides an
accessible entry point for both biological and engineering students, as well
as a valuable resource for professionals
seeking to understand the workings of
the brain.” — Daniel Wolpert, University
of Cambridge
Computational Neuroscience series
400 pp., 133 illus., $40 cloth

50 years

Infectious Behavior

How We Remember

Brain-Immune Connections in Autism,
Schizophrenia, and Depression
Paul H. Patterson

Brain Mechanisms of Episodic
Memory
Michael E. Hasselmo

“In his paradigm-shifting book, Paul
Patterson explains the dynamic interaction between the immune system,
the brain, and development, unveiling
an important new understanding of
what may underlie many devastating brain disorders.” — Portia Iversen,
cofounder, Cure Autism Now Foundation (CAN), cofounder, Autism Genetic
Resource Exchange (AGRE), founder,
International Meeting for Autism
Research (IMFAR)

“Students and researchers alike will
want to read this approachable yet
richly detailed treatment of the brain
mechanisms supporting our ability to
recollect prior events and experiences.”
— Jay McClelland, Stanford University

176 pp., 3 color illus., 24 b&w illus., $24.95 cloth

Visual Population Codes

Neural Control
Engineering
The Emerging Intersection between
Control Theory and Neuroscience
Steven J. Schiff
“For many years, Steve Schiff’s pioneering work has led the way toward
a deeper understanding of the brain
as a dynamical system. In this beautifully written and groundbreaking
book, he presents a new synthesis
of neuroscience, computation, and
engineering. Neural Control Engineering will be a welcome resource for all
working in this emerging field, and it
will guide and inspire the next generation of students.” — Olaf Sporns,
Indiana University, author of Networks
of the Brain

Toward a Common Multivariate
Framework for Cell Recording
and Functional Imaging
edited by Nikolaus Kriegeskorte
and Gabriel Kreiman
“Everything you wanted to know about
neural coding in the visual system is
in this book, and it is clearly presented
by some of the best people in this
important field of research.”
— Leo M. Chalupa, The George Washington University
Computational Neuroscience series
632 pp., 14 color plates, 151 b&w illus., $55 cloth

Neural Basis of
Motivational and
Cognitive Control

336 pp., 8 color plates, 111 b&w illus., $40 cloth

Work Meets Life
Exploring the Integrative Study
of Work in Living Systems
edited by Robert Levin,
Simon Laughlin,
Christina De La Rocha,
and Alan Blackwell
Work as fundamental to life, explored
at different levels of organization from
the perspectives of a variety of biological and nonbiological disciplines.
272 pp., 37 illus., $30 cloth

Networks of the Brain
Olaf Sporns
“In Networks of the Brain, Olaf Sporns
synthesizes two of the most exciting
topics in science today, and links the latest breakthroughs to their deep historical roots. A graceful, authoritative, and
fascinating book.” — Steven Strogatz,
Cornell University, and author of Sync
375 pp., 15 color illus., 100 b&w illus., $40 cloth

Computational Neuroscience series
504 pp., 31 color plates, 203 b&w illus., $55 cloth

edited by Rogier B. Mars,
Jérôme Sallet,
Matthew F. S. Rushworth,
and Nick Yeung

Visit our

Statistical Analysis
of fMRI Data

A multidisciplinary overview of key
approaches in the study of cognitive
control and decision making.

for a 30%

F. Gregory Ashby
An overview of statistical methods for
analyzing data from fMRI experiments.
368 pp., 67 illus., $45 cloth

472 pp., 52 illus., $50 cloth

booth

discount
The MIT Press

To order call 800-405-1619 • http://mitpress.mit.edu • Visit our e-books store: http://mitpress-ebooks.mit.edu

Empowered by
imec, KU Leuven and VIB

is the new centre
for Neuro-Electronics
Research Flanders.
NERF is a joint basic research initiative,
set up by VIB, imec and KU Leuven to
understand the function of brain circuits.
NERF labs are located in the heart of
Europe in Leuven Belgium on imec’s
nanotechnology campus. At NERF, worldclass neuroscientists perform innovative,
collaborative, interdisciplinary
research, combining
nanoelectronics with
neurobiology.

Currently, NERF hosts three research
laboratories:
• chemosensory processing in zebraﬁsh
(Yaksi Lab)
• visual processing in mice (Bonin Lab)
• memory processing and spatial
navigation in rodents (Kloosterman Lab)
and two visiting groups:
• Optical control of neural activity in
behaving animals (Battaglia Group)
• Novel neural probes and large-scale
anatomy (McNaughton Group)
NERF is looking for enthusiastic,
energetic early career researchers (at
the PhD and postdoc level) with strong
interests in systems neuroscience
to join our laboratories. PhD students
will have the opportunity to enroll in KU
Leuven international PhD programs. For
more information, visit our website
(www.nerf.be). For applications, please
contact individual PIs directly.

Imec - NERF,
Kapeldreef 75,
3001 Leuven Belgium

Also visit us during
the upcoming NERF 2012
Neurotechnology Symposium
on April 23-24
(www.nerf.be/symposium).

Great companies
never stop moving
forward
Qualcomm is a world leader in providing mobile
communications, computing and network solutions that are
transforming the way we live, learn, work, and play.
As a pioneering technology innovator, we are committed to
continuous evolution and exploration. Our researchers and
computational neuroscientists engage in a wide variety of
exciting and technically challenging projects—including the
application of biologically inspired learning machines to a new
breed of intelligent devices.
We help you work smarter, move faster and reach further. We
see success in your future. In fact, we’re passionate about it.

We are futurists.
www.qualcomm.com

AdPAck_Consumer_science.indd 1

1/28/10 5:10 PM

connect to

neural interface system
neuroscience research
neuroprosthetic development
Mac OSX, Windows, Linux
up to 512 electrodes

amplifier front ends for electrodes

amplify, filter, and digitize biopotential signals

surf D

16-channel differential pairs - EMG

surf S

32-channel single reference - EEG, ECog, ENG

micro

32-channel single reference - spikes and LFP

nano

rodent microelectrode applications

front ends for digital & analog signals
connect to other instruments and audio devices
40 channels LVTTL i/o
56 channels ±5 V analog i/o
with audio in/out

Contact us for a
demonstration
in your lab.

toll free
local
fax
email
© 2012 ripple LLC, patents pending

(866) 324-5197
(801) 413-0139
(801) 413-2874
sales@rppl.com

®

ELECTROPHYSI
OLOGYSUI
TEOFPRODUCTS
Fr
om mi
ni
at
ur
eel
ect
r
odesandi
nt
el
l
i
gentmi
cr
osyst
ems,
t
ocompl
exacqui
si
t
i
onsyst
emsandgr
oundbr
eaki
ngal
gor
i
t
hms
,
wedr
i
v
es
uc
c
es
s
f
ul
c
ommer
c
i
al
i
z
at
i
onofcut
t
i
ngedget
echnol
ogy.

Dat
aacqui
s
i
t
i
ons
yt
em
Forani
malandhuman
neur
ophys
i
ol
ogy

Heads
t
ages
/
Adapt
or
s

Mi
cr
oel
ect
r
odear
r
ays

Fr
om At
oZ

Forext
r
acel
l
ul
arneur
al
r
ecor
di
ngands
t
i
mul
at
i
on

St
i
mul
at
i
on

Pr
ogr
ammabl
es
t
i
mul
us
cont
r
ol

Vi
deos
ys
t
em

T
r
ack,r
ecor
d,andpai
r
wi
t
hdat
aacqui
s
i
t
i
on

TEL:+1.
801.
582.
5533
© 2012Bl
ackr
ockMi
cr
os
ys
t
ems™.Al
lr
i
ght
sr
es
er
ved.Bl
ackr
ockMi
cr
os
ys
t
ems
’l
ogoi
sar
egi
s
t
er
edt
r
ademar
kofBl
ackr
ockMi
cr
os
ys
t
ems™.

x

COSYNE 2012

About Cosyne

About Cosyne
The annual Cosyne meeting provides an inclusive forum for the exchange of experimental and theoretical/computational approaches to problems in systems neuroscience. It has
attracted a growing number of participants, rising to over 550 in 2011. The Cosyne 2011
meeting featured 43 invited and contributed talks, 297 poster presentations, and 14 workshops incorporating a total of 116 presentations. This year, the conference has received a
record number of abstracts.
To encourage interdisciplinary interactions, the main meeting is arranged in a single track.
A set of invited talks are selected by the Executive Committee and Organizing Committee, and additional talks and posters are selected by the Program Committee, based on
submitted abstracts.
Cosyne topics include (but are not limited to): neural coding, natural scene statistics, dendritic computation, neural basis of persistent activity, nonlinear receptive field mapping,
representations of time and sequence, reward systems, decision-making, synaptic plasticity, map formation and plasticity, population coding, attention, and computation with spiking
networks. Participants include pure experimentalists, pure theorists, and everything in between.

Cosyne 2012 Leadership
Organizing Committee:
• Program Chairs: Jonathan Pillow (University of Texas at Austin), Nicole Rust (University of Pennsylvania)
• General Chairs: Jim DiCarlo (MIT), Rachel Wilson (Harvard Medical School)
• Workshop Chairs: Jessica Cardin (Yale), Brent Doiron (University of Pittsburgh)
• Communications Chair : Mark Histed (Harvard Medical School)
Executive Committee:
• Alexandre Pouget (University of Geneva)
• Zachary Mainen (Champalimaud Neuroscience Programme)
• Anthony Zador (Cold Spring Harbor Laboratory)

COSYNE 2012

xi

About Cosyne
Advisory Board:
•
•
•
•
•
•
•
•

Matteo Carandini (University College London)
Anne Churchland (Cold Spring Harbor Laboratory)
Peter Dayan (University College London)
Steven Lisberger (UC San Francisco and HHMI)
Bartlett Mel (University of Southern California)
Maneesh Sahani (University College London)
Eero Simoncelli (New York University and HHMI)
Karel Svoboda (HHMI Janelia Farm)

Program Committee:
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•

xii

Matthias Bethge (Max Planck Institute, Tubingen)
Marlene Cohen (University of Pittsburgh)
Ila Fiete (University of Texas at Austin)
David Freedman (University of Chicago)
Surya Ganguli (Stanford University)
Maria Geffen (University of Pennsylvania)
Tim Gollisch (University of Gottingen)
Andrea Hasenstaub (Salk Institute)
Greg Horwitz (University of Washington)
Mate Lengyel (University of Cambridge)
Mike Long (New York University Medical School)
Wei Ji Ma (Baylor College of Medicine)
Christian Machens (Champalimaud Foundation, Lisbon)
Bruno Olshausen (University of California, Berkeley)
Liam Paninski (Columbia University)
Elad Schneidman (Weizmann Institute)
Tatyana Sharpee (Salk Institute)
Marshall Shuler (Johns Hopkins University)
Garrett Stanley (Georgia Institute of Technology)
Naoshige Uchida (Harvard University)

COSYNE 2012

About Cosyne
Reviewers:
Yashar Ahmadian, Emre Aksay, Wyeth Bair, Sliman Bensmaia, Joseph Bergan, James Bisley, Tim Blanche, Michael Buice, Yoram Burak, Timothy Buschman, Daniel Butts, Matthew
Chafee, Jay Coggan, Jeremiah Cohen, Sophie Deneve, Michael DeWeese, Kamran Diba,
Anita Disney, Eizaburo Doi, Shaul Druckmann, Gidon Felsen, David Foster, Robert Froemke,
Jeff Gavornik, Leif Gibb, Mark Goldman, Daniel Goldreich, Robert Gutig, Rafi Haddad, Ralf
Haefner, Mark Histed, Mark Humphries, Santiago Jaramillo, Mehrdad Jazayeri, Dezhe Jin,
Kresimir Josic, Matthias Kaschube, Steve Kennerley, Garrett Kenyon, Jason Kerr, Roozbeh
Kiani, David Knill, Kilian Koepsell, Adam Kohn, Urs Koster, Bart Krekelberg, Nikolaus
Kriegeskorte, Rodrigo Laje, Hakwan Lau, Christian Leibold, Nicholas Lesica, Robert Liu,
Yonatan Loewenstein, Jason Maclean, Patrick Mineault, Eran Mukamel, Kristina Nielsen,
Hendrikje Nienborg, Shawn Olsen, Gergo Orban, Stephanie Palmer, Joe Paton, Bijan Pesaran, Jean-Pascal Pfister, Xaq Pitkow, Eftychios Pnevmatikakis, Daniel Polley, Nicholas
Price, Heather Read, Alfonso Renart, Jason Ritt, Ari Rosenberg, Constantin Rothkopf,
Chris Rozell, Doug Ruff, Cristina Savin, Andreas Schaefer, Bob Schafer, Geoff Schoenbaum, David Schoppik, Paul Schrater, Odelia Schwartz, Ronen Segev, Maoz Shamir, Harel
Shouval, Matt Smith, Sam Sober, Bill Softky, Sara Solla, Alireza Soltani, Greg Stephens,
Ian Stevenson, Gasper Tkacik, Andreas Tolias, Ivana Tosic, Richard Turner, Jonathan Victor, Matt Wachowiak, Felix Wichmann, Ilana Witten, Byron Yu

Conference Support
Program and Publicity Support:
• Tomáš Hromádka, Cold Spring Harbor Laboratory
Administrative Support, Registration, Hotels:
• Denise Soudan, Conference and Events Office, University of Rochester

COSYNE 2012

xiii

About Cosyne

xiv

COSYNE 2012

About Cosyne

Travel Grants
The Cosyne community is committed to bringing talented scientists together at our annual
meeting, regardless of their ability to afford travel. Thus, a number of travel grants are
awarded to students, postdocs, and PIs for travel to the Cosyne meeting. Each award
covers at least $500 towards travel and meeting attendance costs. Three award granting
programs were available in 2012.
The generosity of our sponsors helps make these travel grant programs possible.

Cosyne Presenters Travel Grant Program
These grants support early career scientists with highly scored abstracts to enable them to
present their work at the meeting.
This program is supported by the following corporations and foundations:

• The Gatsby Charitable Foundation
• Qualcomm Incorporated
• Brain Corporation
• Cell Press/Neuron
• Evolved Machines
The 2012 recipients and their individual sponsors are:
Alberto Bernacchia (Gatsby), Farran Briggs (Gatsby), Engin Bumbacher (Qualcomm), Johannes Burge (Gatsby), Kris Chaisanguanthum (Gatsby), Ruben Coen-Cagli (Gatsby),
Yuwei Cui (Neuron/Cell Press), Jan Drugowitsch (Qualcomm), Jeremy Freeman (Evolved
Machines), Junya Hirokawa (Gatsby), Jonathan Hunt (Gatsby), Shantanu Jadhav (Gatsby),
Santiago Jaramillo (Gatsby), Mehrdad Jazayeri (Gatsby), Jenia Jitsev (Gatsby), Gerhard
Jocham (Qualcomm), David Kastner (Gatsby), Daniel Knudsen (Qualcomm), O. Ozan Koyluoglu (Gatsby), Alex Kwan (Brain Corp), Seung-Hee Lee (Gatsby), Laura Lewis (Neuron/Cell Press), Najib Majaj (Neuron/Cell Press), Il Memming Park (Brain Corp), Shaun
COSYNE 2012

xv

About Cosyne
Patel (Qualcomm), Dominique Pritchett (Gatsby), Alireza Soltani (Gatsby), Corinne Teeter
(Gatsby), Andreas Thomik (Gatsby), Greg Wayne (Neuron/Cell Press), Quan Wen (Gatsby),
Yang Yang (Brain Corp), KiJung Yoon (Gatsby), Adam Zaidel (Neuron/Cell Press), and
Mengchen Zhu (Brain Corp).

Cosyne New Attendees Travel Grant Program
These grants help bring scientists that have not previously attended Cosyne to the meeting
for exchange of ideas with the community.

This program is supported by a grant from the National Science Foundation.
The 2012 recipients are:
Mina Aliakbari Khoei, Aaron Bornstein, Nicole Carlson, Izumi Fukunaga, Atiyeh Ghoreyshi,
Shirin Hadizadeh, Michele Insanally, Jose Jara-Ettinger, Marja-Leena Linne, Katherine
Morrison, Sherika Sylvester, Alan Veliz-Cuba, and Corette Wierenga.

Cosyne Mentorship Travel Grant Program
These grants provide support for early-career scientists of underrepresented minority groups
to attend the meeting. A Cosyne PI must act as a mentor for these trainees and the program
also is meant to recognize these PIs (“NSF Cosyne Mentors”).

This program is supported by a grant from the National Science Foundation.
The 2012 NSF Cosyne Mentors are listed below, each followed by their mentee:
Carina Curto and Ashley Sullivan; Jonathan Pillow and Evan Archer; Ronen Segev and
Irina Segal; Eric Shea-Brown and Natasha Cayco Gajic; Christopher Rozell and Allison
Del Giornio; Jonathan Wallis and Jen Sloan; Michael Wenger and Julie Hammons.

xvi

COSYNE 2012

Program

Program
Note: Printed copies of this document do not contain the abstracts; they can be downloaded at:
http://cosyne.org/c/index.php?title=Cosyne2012_Program.
Institutions listed in the program are the primary affiliation of the first author. For the complete list, please consult
the abstracts.

Thursday, 23 February
4:00 pm

Registration opens

5:30 pm

Welcome reception

6:20 pm

Opening remarks
Welcome from the General Co-Chairs (Jim DiCarlo and Rachel Wilson)
Overview of the program selection from the Program Co-Chairs (Jonathan Pillow and
Nicole Rust)

Session 1: Spikes - correlations and oscillations
(Chair: Tatyana Sharpee)
6:30 pm

Noise correlations and neural coding
Fred Rieke, University of Washington (invited) . . . . . . . . . . . . . . . . . . . . . . . 23

7:15 pm

The Neurophysiology of the Unconscious Brain Under General Anesthesia
Emery Brown, Massachusetts Institute of Technology (invited) . . . . . . . . . . . . . . . 23

8:30 pm

Poster Session I

Friday, 24 February
7:30 am

Continental breakfast

Session 2: Motor systems
(Chair: Michael Long)
8:30 am

Electrophysiological Foundations of Human Speech Production
K. Bouchard, N. Mesgarani, M. Babiak, K. Johnson, E. Chang, University of California,
San Francisco . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

8:45 am

On the Origins of Motor Noise
K. Chaisanguanthum, H. Shen, P. Sabes, University of California, San Francisco . . . . . 24

9:00 am

Neural dynamics of reaching following incomplete or incorrect planning
K. Ames, S. Ryu, K. Shenoy, Stanford University . . . . . . . . . . . . . . . . . . . . . . 25

9:15 am

Energy-conservation and generalized power-law for curved hand movements
D. Huh, T. Sejnowski, Salk Institute for Biological Studies . . . . . . . . . . . . . . . . . . 25

10:00 am

Coffee break

COSYNE 2012

1

Program

Session 3: M INISYMPOSIUM: Olfaction
(Chair: Jonathan Pillow)
10:00 am

Predicting odorant perception and neural activity from odorant structure
Noam Sobel, Weizmann Institute of Science (invited) . . . . . . . . . . . . . . . . . . . 26

10:45 am

Odor-guided decisions in the rat: The origins and uses of uncertainty
Zach Mainen, Champalimaud Neuroscience Programme (invited) . . . . . . . . . . . . . 26

11:30 am

Temporal segregation of olfactory bulb output
I. Fukunaga, M. Berning, M. Kollo, A. Schaefer, Max Planck Institute for Medical Research 27

11:45 am

Do rats make optimal olfactory decisions under perceptual uncertainty when rewards are
unstable?
J. Hirokawa, A. Kepecs, Cold Spring Harbor Laboratory . . . . . . . . . . . . . . . . . . 27

12:00 pm

Lunch break

Session 4: Neural representations – models and theory
(Chair: Matthias Bethge)
2:00 pm

Monte Carlo as mechanism: Sampling and human cognition
Tom Griffiths, University of California, Berkeley (invited) . . . . . . . . . . . . . . . . . . 28

2:45 pm

Noise correlations in population codes with finite information
J. Beck, R. Moreno, P. Latham, A. Pouget, University of Rochester . . . . . . . . . . . . . 28

3:00 pm

Emergence of selectivity in recurrent random networks with balanced excitation and inhibition
C. Pehlevan, H. Sompolinsky, Harvard University . . . . . . . . . . . . . . . . . . . . . . 29

3:15 pm

A general, accurate, closed-form rate model derived as an approximation to spiking network dynamics
E. Schaffer, L. Abbott, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . 29

3:30 pm

Coffee break

Session 5: Navigation and Movement
(Chair: Surya Ganguli)
4:00 pm

A new look at human motor control
Dana Ballard, University of Texas at Austin (invited) . . . . . . . . . . . . . . . . . . . . 30

4:45 pm

Tuning to 3-D head-direction in the bat presubiculum
D. Derdikman, A. Finkelstein, J. Foerster, N. Ulanovsky, Technion, Israeli Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

5:00 pm

Awake hippocampal sharp-wave ripples support spatial working memory
S. Jadhav, C. Kemere, W. German, L. Frank, University of California, San Francisco . . . 31

5:15 pm

Dinner break

6:30 pm

Special workshop: “NSF: Theory and Applications”, Kenneth Whang, Program Director, Division of Information and Intelligent Systems, National Science Foundation.
Location: Deer Valley Room, Marriott
This will be part tutorial, but mostly informal discussion. Ken will be pleased to take
questions in advance about any aspects of NSF, or your experiences with NSF, by email to kwhang@nsf.gov or anonymously to tomas.hromadka@gmail.com. Please use
*Subject: NSF Cosyne*

7:30 pm

2

Poster Session II

COSYNE 2012

Program

Saturday, 25 February
7:30 am

Continental breakfast

Session 6: Linking neural activity and behavior–optogenetics
(Chair: Naoshige Uchida)
8:30 am

Bending waves during C. elegans locomotion are driven and organized by proprioceptive
coupling
Q. Wen, E. Hulme, S. Chen, X. Liu, M. Gershow, A. Leifer, V. Butler, C. Fang-Yen, W.
Schafer, G. Whitesides, M. Wyart, D. Chklovskii, S. Aravinthan, Harvard University . . . . 32

8:45 am

Activation of Specific Interneurons Improves V1 Feature Selectivity and Visual Perception
S. Lee, A. Kwan, V. Phoumthipphavong, S. Zhang, J. Flannery, S. Masmanidis, H. Taniguchi,
J. Huang, E. Boyden, K. Deisseroth, Y. Dan, University of California, Berkeley . . . . . . 33

9:00 am

Entraining gamma oscillations in somatosensory cortex enhances performance in tactile
detection
D. Pritchett, J. H. Siegle, C. Moore, Massachusetts Institute of Technology . . . . . . . . 33

9:15 am

Phosphene induction in monkeys using optogenetics
M. Jazayeri, Z. Lindbloom-Brown, G. Horwitz, University of Washington . . . . . . . . . . 34

9:30 am

Coffee break

Session 7: M INISYMPOSIUM: Cognitive representation
(Chair: Nicole Rust)
10:00 am

Categories, decisions and parietal cortex
John Assad, Harvard Medical School (invited) . . . . . . . . . . . . . . . . . . . . . . . 34

10:45 am

The role of disordered representations in neural dynamics
Stefano Fusi, Columbia University Medical Center (invited) . . . . . . . . . . . . . . . . 35

11:30 am

Separable Influences of Reward on Prefrontal Control of Attention and Target Selection
A. Soltani, B. J. Schafer, B. Burrows, T. Moore, Stanford University . . . . . . . . . . . . . 36

11:45 am

Human single-unit responses in the nucleus accumbens during a financial decision-making
task
S. Patel, S. Sheth, M. Mian, J. Gale, J. Gerrard, B. Greenberg, D. Dougherty, E. Eskandar,
Massachusetts General Hospital . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

12:00 pm

Lunch break

COSYNE 2012

3

Program

Session 8: Dendrites and synapses
(Chair: Elad Schneidman)
2:00 pm

Dynamics of a calcium-based plasticity rule: from single synapses to networks
Nicolas Brunel, Centre National de la Récherche Scientifique, Université Paris Dèscartes
(invited) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

2:45 pm

Long-term modification of cortical synapses improves sensory perception
I. Carcea, K. Yuan, A. Barker, B. A. Seybold, A. R. Martins, N. Zaika, H. Bernstein, D.
Polley, M. Merzenich, C. Schreiner, R. Froemke, New York University . . . . . . . . . . . 37

3:00 pm

Plasticity and stability in motor cortex during learning
D. Huber, D. Gutnisky, S. Peron, D. O’Connor, L. Tian, L. Looger, K. Svoboda, University
of Geneva . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

3:15 pm

Nonlinear dendritic processing during active sensing produces an object localization signal
N. Xu, M. Harnett, S. Williams, D. Huber, D. O’Connor, K. Svoboda, J. Magee, HHMI,
Janelia Farm Research Campus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

3:30 pm

Coffee break

Session 9: Representing beliefs
(Chair: Máté Lengyel)

4

4:00 pm

Thinking about thinking about thought
Rebecca Saxe, Massachusetts Institute of Technology (invited) . . . . . . . . . . . . . . 39

4:45 pm

The beads task, information sampling and impulsivity
B. Averbeck, N. Furl, A. Djamshidian, A. Lees, National Institute of Mental Health . . . . . 40

5:00 pm

Dinner break

7:30 pm

Poster Session III

COSYNE 2012

Program

Sunday, 26 February
7:30 am

Continental breakfast

Session 10: Visual circuitry
(Chair: Greg Horwitz)
8:30 am

Shaping neural circuits by early experience
Takao Hensch, Harvard University (invited) . . . . . . . . . . . . . . . . . . . . . . . . . 40

9:15 am

Patterns of activity initiated by individual photoreceptors in primate retina
P. Li, G. Field, M. Greschner, L. Jepson, D. Gunning, K. Mathieson, A. Litke, E. Chichilnisky,
Salk Institute for Biological Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

9:30 am

Multiple spectral inputs improve motion discrimination in the Drosophila visual system
M. Juusola, T. J. Wardill, O. List, X. Li, S. Dongre, M. E. McCulloch, C. Ting, C. O’Kane,
S. Tang, C. Lee, R. Hardie, University of Sheffield . . . . . . . . . . . . . . . . . . . . . . 41

9:45 am

Visual attention increases synaptic efficacy in thalamocortical circuits
F. Briggs, G. R. Mangun, W. M. Usrey, Dartmouth Medical School . . . . . . . . . . . . . 42

10:00 am

Coffee break

Session 11: Auditory processing
(Chair: Christian Machens)
10:30 am

Mechanisms and consequences of transforming dense codes to sparse codes in the
auditory system
Sarah Woolley, Columbia University (invited) . . . . . . . . . . . . . . . . . . . . . . . . 43

11:15 am

Changes in cortico-striatal connectivity strength during flexible sound-action associations
in rats
S. Jaramillo, P. Znamenskiy, A. Zador, Cold Spring Harbor Laboratory . . . . . . . . . . . 43

11:30 am

Emergence of pitch from natural sound statistics in a hierarchical, dual-pathway sparse
coding model
V. Ming, E. Bumbacher, University of California, Berkeley . . . . . . . . . . . . . . . . . . 44

11:45 am

Sound encoding in the neocortex by combinations of discrete activity patterns in local
neuronal ensembles
B. Bathellier, L. Ushakova, S. Rumpel, Research Institute of Molecular Pathology . . . . . 44

12:00 pm

Lunch break

Session 12: Visual representation
(Chair: Garrett Stanley)
2:00 pm

A unified neuronal population code fully explains human object recognition
N. Majaj, H. Hong, E. Solomon, J. DiCarlo, Massachusetts Institute of Technology . . . . 45

2:15 pm

Linking physiology and perception in V2
J. Freeman, C. M. Ziemba, T. Movshon, E. P. Simoncelli, New York University . . . . . . . 45

2:30 pm

Visual Coding in the primary visual cortex is enhanced during active navigation
A. Saleem, A. Ayaz, K. Harris, K. Jeffery, M. Carandini, University College London . . . . 46

2:45 pm

Linear and non-linear receptive fields for optimal disparity estimation in natural stereoimages
J. Burge, W. S. Geisler, University of Texas at Austin . . . . . . . . . . . . . . . . . . . . 46

3:00 pm

Closing remarks

COSYNE 2012

5

Posters I

Poster Session I

8:30 pm Thursday 25th February

I-1. Kinematic limitations to texture discrimination by whiskers
Andreas AC Thomik, Aldo Faisal, Imperial College London . . . . . . . . . . . . . . . . . . . . . . . . . 47
I-2. Fast encoding model estimation via expected log-likelihoods
Alexandro Ramirez, Liam Paninski, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
I-3. Unconstrained Gaussian mixture models: the best models for natural image statistics?
Daniel Zoran, Yair Weiss, Hebrew University of Jerusalem . . . . . . . . . . . . . . . . . . . . . . . . . . 48
I-4. Biophysically accurate inhibitory interneuron properties in a sparse coding network
Mengchen Zhu, Bruno Olshausen, Chris Rozell, Georgia Institute of Technology . . . . . . . . . . . . . . 48
I-5. Sparse coding model of binocular receptive field development reproduces changes in abnormal rearing
Jonathan Hunt, Peter Dayan, Geoffrey Goodhill, Queensland Brain Institute . . . . . . . . . . . . . . . . 49
I-6. Analysing fixations using latent Gaussian fields
Simon Barthelmé, Hans Trukenbrod, Ralf Engbert, Felix Wichmann, Technische Universität Berlin . . . . 50
I-7. Semantic organization of a neural population codebook and accurate decoding using a neural thesaurus
Elad Ganmor, Ronen Segev, Elad Schneidman, Weizmann Institute of Science . . . . . . . . . . . . . . 50
I-8. Internal model estimation for feedback control in brain-computer interfaces
Matthew Golub, Steven Chase, Byron Yu, Carnegie Mellon University . . . . . . . . . . . . . . . . . . . . 51
I-9. Circuit- and systems-level contributions to successful memory retrieval in the hippocampus
Cristina Savin, Peter Dayan, Mate Lengyel, University of Cambridge . . . . . . . . . . . . . . . . . . . . 51
I-10. How plastic is the “slow speeds prior”?
Grigorios Sotiropoulos, Aaron Seitz, Peggy Series, University of Edinburgh . . . . . . . . . . . . . . . . . 52
I-11. Hamiltonian Monte Carlo sampling and oscillatory activity in V1
Gergo Orbán, Laurence Aitchison, Mate Lengyel, University of Cambridge . . . . . . . . . . . . . . . . . 52
I-12. Sparse and expansive representations in network models of associative memory
Baktash Babadi, Haim Sompolinsky, Harvard University . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
I-13. A computational role for noise in the cortex
David Barrett, Peter Latham, University College London . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
I-14. Quadratic networks for invariant perceptual discrimination
Yoram Burak, SueYeon Chung, Haim Sompolinsky, Harvard University . . . . . . . . . . . . . . . . . . . 54
I-15. Significance of graph theoretic measures in predicting neuronal network activity
Tuomo Mäki-Marttunen, Jugoslava Acimovic, Keijo Ruohonen, Marja-Leena Linne, Tampere University of
Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
I-16. Sequences and the emergence of continuous attractor networks
Alan Veliz-Cuba, Carina Curto, Vladimir Itskov, University of Nebraska, Lincoln . . . . . . . . . . . . . . . 55
I-17. Short-Term Memory Capacity in Recurrent Networks via Compressed Sensing
Adam Charles, Han Lun Yap, Chris Rozell, Georgia Institute of Technology . . . . . . . . . . . . . . . . . 56
I-18. Norepinephrine, neural gain, and “first one wins” network dynamics
Eran Eldar, Angela Radulescu, Yael Niv, Jonathan Cohen, Princeton University . . . . . . . . . . . . . . 56
I-19. Recurrent vs. feedforward networks: differences in neural code topology
Vladimir Itskov, Carina Curto, University of Nebraska, Lincoln . . . . . . . . . . . . . . . . . . . . . . . . 57
I-20. Diffusion of nodal sodium channels can restore function in multiple sclerosis
Ali Neishabouri, Aldo Faisal, Imperial College London . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

6

COSYNE 2012

Posters I
I-21. Perceptual grouping and figure-ground segregation arising from short-term plasticity in a spiking
network
Jason Fleischer, Alexandar Kozarev, The Neurosciences Institute . . . . . . . . . . . . . . . . . . . . . . 58
I-22. The multi-class tempotron: a neuron model for processing of sensory streams
Robert Gütig, Max Planck Institute of Experimental Medicine . . . . . . . . . . . . . . . . . . . . . . . . 58
I-23. Biologically plausible learning of sparse-coding dictionary in a neural network
Ziqiang Wei, Tao Hu, Dmitri Chklovskii, HHMI, Janelia Farm Research Campus . . . . . . . . . . . . . . 59
I-24. Embodied Exploration
Daniel Little, Fritz Sommer, University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . . . 59
I-25. Critical exponents derived for neuronal avalanches in alert non-human primate
Shan Yu, Hongdian Yang, Dietmar Plenz, National Institute of Mental Health . . . . . . . . . . . . . . . . 60
I-26. Functional Connectivity of the Neural Integrator in Larval Zebrafish
Kayvon Daie, Mark Goldman, Emre Aksay, Cornell University . . . . . . . . . . . . . . . . . . . . . . . . 60
I-27. Dynamic grouping in neuronal networks by inhibition induced neuronal excitability transitions
Christoph Kirst, Julian Ammer, Felix Felmy, Andreas VM Herz, Martin Stemmler, Max Planck Institute for
Dynamics and Self-Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
I-28. Phase coherence of field potentials facilitates prediction of single-trial outcome in a memory task
Beth Lopour, Abtine Tavassoli, Itzhak Fried, Dario Ringach, University of California, Los Angeles . . . . . 62
I-29. Two modes of phase-amplitude coupling in human cortical electrical dynamics under general anesthesia
Eran Mukamel, Kin Foon Kevin Wong, Eric T Pierce, P. Grace Harrell, John L Walsh, Emery Brown,
Patrick Purdon, University of California, San Diego . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
I-30. Retinal adaptation and invariance to changes in higher-order stimulus statistics
Gasper Tkacik, Anandamohan Ghosh, Elad Schneidman, Ronen Segev, IST Austria . . . . . . . . . . . 63
I-31. Inhibition in mouse IC affects the rate code but not the timing code when processing vocalizations
Alexander Dimitrov, Graham Cummins, Zachary Mayko, Christine Portfors, Washington State University,
Vancouver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
I-32. Hierarchical generalized linear models of dendritic integration and somatic membrane potential
DJ Strouse, Mate Lengyel, University of Cambridge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
I-33. Structure-preserving model reduction of the passive and quasi-active neuron
Kathryn Hedrick, Steve Cox, Rice University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
I-34. Using dimensionality reduction to explore muscle synergies and torque encoding during insect flight
Simon Sponberg, Tom Daniel, Adrienne Fairhall, University of Washington . . . . . . . . . . . . . . . . . 65
I-35. Retinal metric: a stimulus distance measure derived from population neural responses
Gasper Tkacik, Einat Granot-Atedgi, Ronen Segev, Elad Schneidman, IST Austria . . . . . . . . . . . . . 66
I-36. Bayesian spike-triggered covariance and the elliptical LNP model
Il Memming Park, Jonathan W Pillow, University of Texas at Austin . . . . . . . . . . . . . . . . . . . . . 66
I-37. Using a doubly-stochastic model to analyze neuronal activity in the visual cortex
Robbe Goris, Eero P Simoncelli, Tony Movshon, New York University . . . . . . . . . . . . . . . . . . . . 67
I-38. A feedback error learning approach to online-adaptive decoding for dynamic prosthetic arm control
Matthew Bodenhamer, Francis R Willett, Aaron Suminski, Nicholas Hatsopoulos, Andrew Fagg, School of
Computer Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
I-39. Inferring eye position and saccade direction from populations of LIP neurons
Arnulf Graf, Richard Andersen, California Institute of Technology . . . . . . . . . . . . . . . . . . . . . . 68
I-40. Adaptation to spectro-temporal correlation in the primary auditory cortex
Ryan Natan, Isaac Carruthers, Maria Geffen, University of Pennsylvania . . . . . . . . . . . . . . . . . . 69

COSYNE 2012

7

Posters I
I-41. Decision-making and attention in a sampling-based neural representation
Ralf M Haefner, Pietro Berkes, Jozsef Fiser, Brandeis University . . . . . . . . . . . . . . . . . . . . . . 69
I-42. Attention, Information, Normalization and Correlations
Vikranth Bejjanki, Jeffrey Beck, Alexandre Pouget, University of Rochester . . . . . . . . . . . . . . . . . 70
I-43. Short-term memory with balanced excitation and inhibition based on derivative feedback control
Sukbin Lim, Mark Goldman, University of California, Davis . . . . . . . . . . . . . . . . . . . . . . . . . . 70
I-44. Selective Allocation of attention is crucial in setting a capacity limit in visual short-term memory
Antonio Lara, Jonathan Wallis, University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . 71
I-45. Dynamic networks in frontal cortex support the cognitive flexibility to switch between rules
Timothy J Buschman, Eric Denovellis, Cinira Diogo, Daniel Bullock, Earl Miller, Massachusetts Institute of
Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
I-46. A winner-take-all mechanism of decision rule discrimination by the supplementary eye field
Supriya Ray, Stephen Heinen, The Smith-Kettlewell Eye Research Institute . . . . . . . . . . . . . . . . 72
I-47. How prior probability influences decision making: A unifying probabilistic model
Yanping Huang, Abram Friesen, Rajesh Rao, University of Washington . . . . . . . . . . . . . . . . . . . 72
I-48. Semi-Markov models of the molecular psychophysics of brain stimulation reward
Ritwik K. Niyogi, Yannick-Andre Breton, Kent Conover, Rebecca Solomon, Peter Shizgal, Peter Dayan,
University College London . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
I-49. Neuronal activity in anterior cingulate cortex predicts susceptibility to distraction
R Ebitz, Michael Platt, Duke University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
I-50. Cortico-basal ganglia computations in controlled decision making: an extended diffusion model
analysis
Thomas Wiecki, Michael J Frank, Brown University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
I-51. The importance of being slow: Extreme-value theory of cognitive representations.
Alberto Bernacchia, Xiao-Jing Wang, Yale University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
I-52. Executive control and arbitration in reinforcement learning
Anya Skatova, Seth Madlon-Kay, Nathaniel Daw, University of Nottingham . . . . . . . . . . . . . . . . . 75
I-53. Natural grouping of neural responses reveals spatially segregated clusters in prearcuate cortex
Roozbeh Kiani, Christopher Cueva, John Reppas, William Newsome, Stanford University . . . . . . . . . 75
I-54. Neural threshold for patch leaving decisions in posterior cingulate cortex
David Barack, Benjamin Hayden, John Pearson, Michael Platt, Duke University . . . . . . . . . . . . . . 76
I-55. Trial-by-trial perceptual learning during odor category decisions: Value, uncertainty and dopamine
Thiago Gouvea, Gil Costa, Eric DeWitt, Zachary Mainen, Champalimaud Neuroscience Programme . . . 76
I-56. Generic and stimulus-dependent value signals are encoded in human ventromedial prefrontal cortex
Daniel McNamee, Antonio Rangel, John O’Doherty, California Institute of Technology . . . . . . . . . . . 77
I-57. Spatial heterogeneity in visual perception; a new conceptual framework for translation invariance.
Arash Afraz, Maryam Vaziri-Pashkam, Edith Reshef, James DiCarlo, Patrick Cavanagh, Massachusetts
Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
I-58. Visual Object Classification is Consistent with Bayesian Generative Representations
Feryal Mehraban Pour Behbahani, Aldo Faisal, Imperial College London . . . . . . . . . . . . . . . . . . 78
I-59. Event timing in associative learning: From biochemical reaction dynamics to behavioral observations
Ayse Yarali, Johannes Nehrkorn, Hiromu Tanimoto, Andreas VM Herz, Max Planck Institute for Neurobiology 79
I-60. Bridging the gap: A third time-scale between plasticity and homeostasis?
Friedemann Zenke, Henning Sprekeler, Tim P Vogels, Wulfram Gerstner, École Polytechnique Fédérale
de Lausanne . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
I-61. The incorporation of new information into prefrontal cortical activity after learning new tasks
Ethan Meyers, Xue-Lian Qi, Christos Constantinidis, Massachusetts Institute of Technology . . . . . . . . 80

8

COSYNE 2012

Posters I
I-62. Sequence Learning in Primary Visual Cortex
Jeff Gavornik, Mark Bear, HHMI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
I-63. BMI learning results in highly precise cell-specific coherence in corticostriatal networks
Aaron C Koralek, Jose Carmena, University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . 81
I-64. Dendritic processing underlying temporal integration
Melanie Lee, Kayvon Daie, Sherika Sylvester, Dimitry Fisher, Mark Goldman, Emre Aksay, Cornell University 81
I-65. Loom sensitive neurons link computation to action in the Drosophila visual system
Saskia de Vries, Thomas Clandinin, Stanford University . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
I-66. Identifying the neural initiation of a movement
Biljana Petreska, Matthew T Kaufman, Mark Churchland, Stephen Ryu, Krishna Shenoy, Maneesh Sahani, University College London . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
I-67. How does pacemaking in the globus pallidus affect striatal microcircuits?
Arpiar Saunders, Kevin Beier, Bernardo Sabatini, Harvard Medical School . . . . . . . . . . . . . . . . . 83
I-68. Spike time-dependent plasticity can organize a recurrent network to generate grid cell responses
John Widloski, Ila Fiete, University of Texas at Austin . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
I-69. Which edge probabilities reveal long-range horizontal connections in visual cortex?
Matthew Lawlor, Steven Zucker, Yale University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
I-70. What kinds of local motion signals are present in naturalistic movies?
Eyal Nitzany, Jonathan Victor, Cornell University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
I-71. Sparse coding neurons encode individual vocalizations in complex auditory scenes
David Schneider, Sarah Woolley, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
I-72. A normative theory of Weber’s law
Jeffrey Beck, Ingmar Kanitscheider, Alexandre Pouget, University of Rochester . . . . . . . . . . . . . . 86
I-73. Efficient coding of visual motion signals in the smooth pursuit system
Leslie Osborne, Patrick Stinson, University of Chicago . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
I-74. Stereotyped and diverse computations in third order olfactory circuits
Mehmet Fisek, Rachel Wilson, Harvard Medical School . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
I-75. Sensation of a “noisy” whisker vibration in rats: Psychometric and neurometric analysis
Arash Fassihi, Athena Akrami, Vahid Esmaeili, Mathew E Diamond, SISSA . . . . . . . . . . . . . . . . . 87
I-76. Neural processing of social signals in the medial amygdala
Joseph F Bergan, Yoram Ben-Shaul, Catherine Dulac, Harvard University . . . . . . . . . . . . . . . . . 88
I-77. Summary statistics in auditory perception
Josh McDermott, Eero P Simoncelli, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . 88
I-78. The neural representation of behaviorally relevant acoustical sequences
Justin Kiggins, Timothy Gentner, University of California, San Diego . . . . . . . . . . . . . . . . . . . . 89
I-79. Feedback from retinal ganglion cells to the inner retina
Hiroki Asari, Markus Meister, Harvard University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
I-80. Evidence for a class of dLGN neurons with extra-strong classical surrounds in the awake rat
Balaji Sriram, Pamela Reinagel, University of California, San Diego . . . . . . . . . . . . . . . . . . . . . 90
I-81. Probing mechanisms of contrast adaptation in retina with modeling of synaptic currents and spikes
Daniel A Butts, Yanbin V. Wang, Jonathan B Demb, University of Maryland . . . . . . . . . . . . . . . . . 90
I-82. Decorrelation of retinal response to natural scenes by fixational eye movements
Irina Yonit Segal, Michael Gedalin, Ronen Segev, Ben-Gurion University of the Negev . . . . . . . . . . . 91
I-83. The role of nonlinear phase processing in photoreceptors
Uwe Friederich, S.A. Billings, Mikko Juusola, Daniel Coca, University of Sheffield . . . . . . . . . . . . . 92
I-84. Humans integrate motion information using noise adaptive filters
Justin Ales, Anthony Norcia, Stanford University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92

COSYNE 2012

9

Posters I
I-85. Adaptive sampling of visual stimuli in cortical neurons
Jens Kremkow, Jianzhong Jin, Yushi Wang, Reza Lashgari, Stanley Jose Komban, Jose-Manuel Alonso,
State University of New York . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
I-86. Presaccadic modulation of visual responses in area V4 measured simultaneously across cortical
layers
Nicholas Steinmetz, Tirin Moore, Stanford University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
I-87. Predicting functional connectivity in primary visual cortex
David Schulz, Maneesh Sahani, Matteo Carandini, University College London . . . . . . . . . . . . . . . 94
I-88. Visual cortex learns novel representations under anesthesia
Andreea Lazar, Wolf Singer, Danko Nikolic, Max Planck Institute for Brain Research . . . . . . . . . . . . 94
I-89. Co-variability of spontaneous synaptic excitation and inhibition in visual cortex
Andrew Y Tan, Nicholas Priebe, University of Texas at Austin . . . . . . . . . . . . . . . . . . . . . . . . 95
I-90. Model-based analysis of 3D surface representations in human visual cortex
Andrew Welchman, Hiroshi Ban, University of Birmingham . . . . . . . . . . . . . . . . . . . . . . . . . 95
I-91. Inferred functional circuitry in a microcolumn of cat visual cortex
Urs Koster, Christopher Hillar, Bruno Olshausen, Charles Gray, University of California, Berkeley . . . . . 96
I-92. Precise decoding of dynamical motion from a large retinal population
Olivier Marre, Gasper Tkacik, Michael Berry, Princeton University . . . . . . . . . . . . . . . . . . . . . . 96
I-93. Dealing with sequential dependencies in psychophysical data
Ingo Fründ, Felix Wichmann, Jakob Macke, Bernstein Center for Computational Neuroscience Berlin . . . 97
I-94. Diverse Network Representations of Risky Decision-Making in mPFC
Shirin Hadizadeh, Nina C. Di Pietro, James M Hyman, Stan B Floresco, Eldon Emberly, Jeremy K. Seamans, University of British Columbia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
I-95. How criticality of visuo-motor control behaviour depends on task objective.
Klaus Pawelzik, Felix Patzelt, University of Bremen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
I-96. Maximum entropy models of social behavior reveal high-order interactions in groups of mice
Oren Forkosh, Yair Shemesh, Yehezkel Sztainberg, Alon Chen, Elad Schneidman, Weizmann Institute of
Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
I-97. A retinotopic systems identification method reveals parallel visual streams in the fruit fly
Jacob Aptekar, Jessica Fox, Patrick Shoemaker, Mark Frye, HHMI . . . . . . . . . . . . . . . . . . . . . 99
I-98. Odor detection vs. mixture categorization: crucial differences in behavioral and learning strategies
Maria Vicente, Andre Mendonca, Alexandre Pouget, Zachary Mainen, Champalimaud Neuroscience Programme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100

10

COSYNE 2012

Posters II

Poster Session II

7:30 pm Friday 26th February

II-1. Efficient coding of natural images and movies with populations of noisy nonlinear neurons
Yan Karklin, Eero P. Simoncelli, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
II-2. Optimally adapting heuristics: humans quickly abandon the constant bearing angle strategy
Constantin Rothkopf, Paul Schrater, Frankfurt Institute for Advanced Studies . . . . . . . . . . . . . . . . 101
II-3. The simultaneous silence of neurons explains higher-order interactions in ensemble spiking activity
Hideaki Shimazaki, Kolia Sadeghi, Yuji Ikegaya, Taro Toyoizumi, RIKEN Brain Science Institute . . . . . . 102
II-4. Neural representations that are good for both generalization and discrimination
Omri Barak, Mattia Rigotti, Stefano Fusi, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . 102
II-5. Semi-supervised learning of high-level representations of natural video sequences.
Steven P Brumby, Michael I Ham, Garrett T Kenyon, Los Alamos National Laboratory . . . . . . . . . . . 103
II-6. Implicit representation of high-dimensional stimulus distributions
Xaq Pitkow, University of Rochester . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
II-7. Fano factor constancy and scale-invariant sampling in recurrent networks with probabilistic synapses
Ruben Moreno, Foundation Sant Joan de Deu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
II-8. On the precision of sensory encoding in visual search
Helga Mazyar, Ronald Van Den Berg, Wei Ji Ma, Baylor College of Medicine . . . . . . . . . . . . . . . . 104
II-9. Neural implementation of Bayesian inference using efficient population codes
Deep Ganguli, Eero P Simoncelli, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
II-10. Change detection as probabilistic inference under variable resources
Shaiyan O Keshvari, Ronald Van Den Berg, Wei Ji Ma, Massachusetts Institute of Technology . . . . . . 105
II-11. Change localization: a new paradigm for visual short-term memory
Hongsup Shin, Ronald Van Den Berg, Wei Ji Ma, Baylor College of Medicine

. . . . . . . . . . . . . . . 106

II-12. Phase precession in a network model of entorhinal cortex stellate cells without external pacemaker
Kay Thurley, Franziska Hellmundt, Christian Leibold, Ludwig-Maximilians-Universität München . . . . . . 106
II-13. Neurally plausible reinforcement learning of memory representations in delayed-response tasks
Jaldert Rombouts, Sander Bohte, Pieter Roelfsema, Centrum Wiskunde & Informatica, Amsterdam . . . 107
II-14. Modeling and analysis of rhythm generation mechanisms in excitatory neural networks
Yaroslav Molkov, Patrick Jasinski, Natalia Shevtsova, Jeffrey Smith, Ilya Rybak, Indiana University . . . . 108
II-15. Fluctuations in attractor networks
Michael A Buice, Ila Fiete, University of Texas at Austin . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
II-16. A neural system for motor planning and control
Greg Wayne, Larry Abbott, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
II-17. Structure of stimulus induced correlations in random networks with distance dependent connectivity
Alejandro Fernandez Bujan, Ad Aertsen, Arvind Kumar, Bernstein Center Freiburg . . . . . . . . . . . . 109
II-18. Perturbative memory encoding in recurrent networks
Carina Curto, Anda Degeratu, Vladimir Itskov, University of Nebraska, Lincoln . . . . . . . . . . . . . . . 110
II-19. Efficient horizontal and vertical information processing in neural networks
Yen-Nan Lin, Yu-Chi Huang, Yi-Hsuan Lee, Chung-Chuan Lo, National Tsing Hua University . . . . . . . 110
II-20. Scale-invariant effective connectivity in spontaneously active monkey V1 cortical networks
Iñigo Romero Arandia, Jan Drugowitsch, Adam Kohn, Alexandre Pouget, Ruben Moreno, University of
Barcelona . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
II-21. Synaptic consolidation: from synapses to behavioral modeling
Lorric Ziegler, Wulfram Gerstner, École Polytechnique Fédérale de Lausanne . . . . . . . . . . . . . . . 112
II-22. An adaptive spiking neural network for decision making in partially observable environments
Mattia Rigotti, Daniel Ben Dayan Rubin, Nathaniel Daw, Stefano Fusi, Columbia University . . . . . . . . 112

COSYNE 2012

11

Posters II
II-23. Learning precisely timed spiking responses
Raoul-Martin Memmesheimer, Ran Rubin, Haim Sompolinsky, Radboud University Nijmegen . . . . . . . 113
II-24. Synaptic scaling generically stabilizes circuit connectivity
Christian Tetzlaff, Christoph Kolodziejski, Marc Timme, Florentin Wörgötter, University of Goettingen . . . 113
II-25. Regularisation reveals smooth dynamics of shared variability in neural population activity
Lars Buesing, Jakob Macke, Maneesh Sahani, University College London . . . . . . . . . . . . . . . . . 114
II-26. Identifying endogenous rhythmic spatio-temporal patterns in micro-electrode array recordings
Michel Besserve, Fanis Panagiotaropoulos, Britni Crocker, Vishal Kapoor, Andreas Tolias, Stefano Panzeri, Nikos K Logothetis, Max Planck Institute for Biological Cybernetics . . . . . . . . . . . . . . . . . . . 114
II-27. Embracing disorder: making sense of complex population codes
Omri Barak, David Sussillo, Misha Tsodyks, Ranulfo Romo, Larry Abbott, Columbia University . . . . . . 115
II-28. The Coherence of Brain and Environment, not Input Statistics, Determines Neural Correlations
Christopher Buckley, Taro Toyoizumi, RIKEN Brain Science Institute . . . . . . . . . . . . . . . . . . . . 116
II-29. Sparse gamma rhythms arising via clustering in adapting neuronal networks
Zachary Kilpatrick, Bard Ermentrout, University of Pittsburgh . . . . . . . . . . . . . . . . . . . . . . . . 116
II-30. Spatial properties of the hippocampal theta rhythm in the hippocampus
Gautam Agarwal, Gyorgy Buzsaki, Fritz Sommer, Redwood Center for Theoretical Neuroscience . . . . . 117
II-31. Human cortical neurons form functionally isolated networks during propofol-induced unconsciousness
Laura D Lewis, Veronica Weiner, Eran Mukamel, Jacob Donoghue, Emad Eskandar, Joseph Madsen,
William S Anderson, Leigh Hochberg, Sydney Cash, Emery Brown, Patrick Purdon, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
II-32. Information theoretic limits on performance in short-term memory tasks
O. Ozan Koyluoglu, Ila Fiete, University of Texas at Austin . . . . . . . . . . . . . . . . . . . . . . . . . . 118
II-33. Information processing changes during development in primary auditory cortex
Badr F Albanna, Michele Insanally, Hania Kover, Heesoo Kim, Shaowen Bao, Michael DeWeese, University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
II-34. The neural mechanisms involved in finding specific objects and switching between targets
Marino Pagan, Luke Urban, Margot Wohl, Nicole Rust, University of Pennsylvania . . . . . . . . . . . . . 119
II-35. Second order dimensionality reduction using minimum and maximum mutual information models
Ryan Rowekamp, Jeffrey Fitzgerald, Lawrence Sincich, Tatyana Sharpee, Salk Institute for Biological
Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
II-36. Optimal neural tuning for arbitrary stimulus priors
Zhuo Wang, Kevin Shi, Alan Stocker, Daniel Lee, University of Pennsylvania . . . . . . . . . . . . . . . . 120
II-37. A generative Model for Adaptation in Primary Visual Cortex Neurons derived from Movie Statistics
Michoel Snow, Ruben Coen-Cagli, Odelia Schwartz, Albert Einstein College of Medicine . . . . . . . . . 121
II-38. Identifying dendritic processing in Drosophila OSNs
Aurel Lazar, Yevgeniy B Slutskiy, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
II-39. Support Vector Machines in Spiking Neurons with Non-Linear Dendrites
Ran Rubin, Raoul-Martin Memmesheimer, Haim Sompolinsky, Hebrew University of Jerusalem . . . . . . 122
II-40. Combinatorial neural codes from a mathematical coding theory perspective
Katherine Morrison, Vladimir Itskov, Zachary Roth, Judy Walker, Carina Curto, University of Nebraska,
Lincoln . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
II-41. A concurrent brain-machine interface for enhanced motor function
Maryam Shanechi, Rollin Hu, Marissa Powers, Gregory W Wornell, Emery Brown, Ziv Williams, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
II-42. Global Synchronous Spontaneous Activity in Xenopus Optic Tectum
Kazuo Imaizumi, Hamilton Farris, Louisiana State University . . . . . . . . . . . . . . . . . . . . . . . . . 123

12

COSYNE 2012

Posters II
II-43. The Distinct Behavior of Membrane Potential and Spike Train Statistics
Robert Rosenbaum, Kresimir Josic, University of Pittsburgh . . . . . . . . . . . . . . . . . . . . . . . . . 124
II-44. The spatiotemporal structure of learned and recalled information in whole frontal cortical networks
Ziv Williams, Robert Haslinger, Rollin Hu, Harvard Medical School . . . . . . . . . . . . . . . . . . . . . 124
II-45. Inhibition of return in natural vision revealed by non-parametric analysis of gaze
Paul Bays, Masud Husain, University College London . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
II-46. Evidence for Attention-dependent inactivation of Sodium Channels
Emily Anderson, Jude Mitchell, John Reynolds, Salk Institute for Biological Studies . . . . . . . . . . . . 125
II-47. Cognitive efficiency explains intelligence effects on risk sensitivity and temporal discounting
Nisheeth Srivastava, Paul Schrater, University of Minnesota . . . . . . . . . . . . . . . . . . . . . . . . . 126
II-48. Modeling maladaptive decision-making in a rat version of the Iowa Gambling Task
Vincent Valton, Alain Marchand, Francoise Dellu-Hagedorn, Peggy Series, University of Edinburgh . . . . 126
II-49. A mechanism for value-guided choice based on the excitation-inhibition balance in prefrontal cortex
Gerhard Jocham, Laurence Hunt, Jamie Near, Tim Behrens, University of Oxford . . . . . . . . . . . . . 127
II-50. An optimal control perspective of the competition hypothesis
Vasileios Christopoulos, Paul Schrater, California Institute of Technology . . . . . . . . . . . . . . . . . . 127
II-51. Decision making and working memory in a parietal-prefrontal loop model
John Murray, Xiao-Jing Wang, Yale University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
II-52. Control allows confidence learning
Jacqueline Fulvio, C Shawn Green, Paul Schrater, University of Minnesota

. . . . . . . . . . . . . . . . 128

II-53. A rodent model for studying mechanisms of behavioral response variability
Dougal Tervo, Mayank Kabra, Kristin Branson, Alla Y Karpova, HHMI, Janelia Farm Research Campus . 129
II-54. Adaptive reinforcement learning in dynamic environments
Chaohui Guo, Peter Bossaerts, Kerstin Preuschoff, University of Zürich . . . . . . . . . . . . . . . . . . . 129
II-55. Scientists are suboptimal in judging scientific data
Ronald Van Den Berg, Jeffrey Beck, Wei Ji Ma, Baylor College of Medicine

. . . . . . . . . . . . . . . . 130

II-56. A hippocampal-cortical network underlies model-based planning in humans
Aaron Bornstein, Nathaniel Daw, Thomas Geib, New York University . . . . . . . . . . . . . . . . . . . . 131
II-57. Push-pull neural architecture naturally arises from optimal sensory stimulus detection
Tomoki Tsuchida, Angela J Yu, University of California, San Diego . . . . . . . . . . . . . . . . . . . . . 131
II-58. Concurrent integration and gating of sensory information with orthogonal mixed representations
Valerio Mante, David Sussillo, Krishna Shenoy, William Newsome, Stanford University . . . . . . . . . . . 132
II-59. Object completion along the ventral visual stream: neural signatures and computational mechanisms
Dean Wyatte, Hanlin Tang, Calin Buia, Joseph Madsen, Randall O’Reilly, Gabriel Kreiman, University of
Colorado, Boulder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
II-60. Decoding semantic content from fMRI responses to natural movies
Alexander G Huth, Tyler Lee, Shinji Nishimoto, An Vu, Jack Gallant, University of California, Berkeley

. . 133

II-61. Direction vs. category selectivity in LIP and MT neurons in delayed match-to-category task
Warasinee Chaisangmongkon, David Freedman, Xiao-Jing Wang, Yale University . . . . . . . . . . . . . 134
II-62. Recovery of a shared spike-timing-dependent synaptic plasticity resource in natural spike trains
Jason Hunzinger, Victor H Chan, Robert Froemke, Qualcomm . . . . . . . . . . . . . . . . . . . . . . . 134
II-63. Pairwise analysis can account for network structures arising from spike-timing dependent plasticity
Baktash Babadi, Larry Abbott, Harvard University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
II-64. Similarity of spontaneous and sensory-evoked activity in cortex does not imply learning
Michael Okun, Pierre Yger, Florian Gerard-Mercier, Matteo Carandini, Kenneth Harris, Imperial College
London . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135

COSYNE 2012

13

Posters II
II-65. Learning from positive and negative rewards in a spiking neural network model of basal ganglia
Jenia Jitsev, Abigail Morrison, Marc Tittgemeyer, Max Planck Institute for Neurological Research . . . . . 136
II-66. On the “Site” and “Source” of Saccadic Countermanding: Reformulations of the Interactive Race
Model
Motonori Yamaguchi, Gordon Logan, Thomas Palmeri, Jeffrey Schall, Vanderbilt University . . . . . . . . 136
II-67. Dimensionality in motor cortex: differences between models and experiment
Jeffrey Seely, Matthew T Kaufman, John Cunningham, Stephen Ryu, Krishna Shenoy, Mark Churchland,
Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
II-68. Infinite-horizon optimal feedback control models for biological systems: application to target jump
Zhai Fangwen, Li Zhaoping, Ning Qian, Tsinghua University . . . . . . . . . . . . . . . . . . . . . . . . . 138
II-69. Theta-phase coding by grid cells in two-dimensional environments
Eric Reifenstein, Andreas VM Herz, Richard Kempter, Susanne Schreiber, Martin Stemmler, Humboldt
Universität zu Berlin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
II-70. Suppressing Actions in the Basal Ganglia
Robert Schmidt, Daniel Leventhal, Jeff Pettibone, Alaina Case, Joshua Berke, University of Michigan

. . 139

II-71. Phase coding of trajectories by grid cells in unconstrained environments
Jason Climer, Michael Hasselmo, Boston University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
II-72. Statistics of junctions in natural images
James Golden, Kedarnath Vilankar, Damon Chandler, David Field, Cornell University . . . . . . . . . . . 140
II-73. V1 and A1 maps: different topographies, a common organizing principle
Hiroki Terashima, Masato Okada, The University of Tokyo . . . . . . . . . . . . . . . . . . . . . . . . . . 140
II-74. Task-dependent feature representations of complex sounds in human auditory cortex
Annika Linke, Rhodri Cusack, The University of Western Ontario . . . . . . . . . . . . . . . . . . . . . . 141
II-75. Predictive Coding with linear threshold neurons
Arjun Bharioke, Dmitri Chklovskii, HHMI, Janelia Farm Research Campus . . . . . . . . . . . . . . . . . 141
II-76. Serotonergic modulation of sensory information processing
Eran Lottem, Guillaume Dugué, Magor Lorincz, Patrícia Correia, Zachary Mainen, Champalimaud Neuroscience Programme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
II-77. Temporal aspect of odor stimuli and odor identity and intensity coding by Olfactory Receptor Neurons
Carlotta Martelli, John R Carlson, Thierry Emonet, Yale University . . . . . . . . . . . . . . . . . . . . . 143
II-78. Adaptive sharpening of tuning in the auditory system of the cricket
Jan Clemens, Florian Rau, K. Jannis Hildebrandt, R. Matthias Hennig, Humboldt Universität zu Berlin . . 143
II-79. Performing noise reduction using realistic spectro-temporal receptive fields as modulation filters
Tyler Lee, R Channing Moore, Frederic Theunissen, University of California, Berkeley . . . . . . . . . . . 144
II-80. Encoding of ultra-sonic vocalizations in the rodent primary auditory cortex
Isaac Carruthers, Ryan Natan, Maria Geffen, University of Pennsylvania . . . . . . . . . . . . . . . . . . 144
II-81. Symmetry in the neural representation of visual motion
Alfred Kaye, James H Marshel, Edward Callaway, Tatyana Sharpee, Salk Institute for Biological Studies . 145
II-82. The omitted stimulus response originates in ON bipolar cells
Nikhil Deshmukh, Frederick Soo, Gregory Schwartz, Michael Berry, Princeton University . . . . . . . . . 145
II-83. Saccade-confounded image statistics explain visual crowding
Bosco Tjan, Anirvan S Nandy, University of Southern California . . . . . . . . . . . . . . . . . . . . . . . 146
II-84. Neurons in macaque area CIP respect the geometric topology of 3D object orientation
Ari Rosenberg, Noah Cowan, Dora Angelaki, Washington University . . . . . . . . . . . . . . . . . . . . 146

14

COSYNE 2012

Posters II
II-85. Reconstruction of the connectome of the fruit fly visual system
Shin-ya Takemura, Shiv Vitaladevuni, Richard Fetter, Zhiyuan Lu, Stephen Plaza, Arjun Bharioke, Lou
Scheffer, Ian Meinertzhagen, Dmitri Chklovskii, HHMI, Janelia Farm Research Campus . . . . . . . . . . 147
II-86. Developmental regulation of sensory processing by spontaneous cortical activity
Matthew Colonnese, The George Washington University . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
II-87. The combined micro-organization of orientation and spatial frequency tuning in primate V1
Ian Nauhaus, Kristina Nielsen, Anita Disney, Edward Callaway, Salk Institute for Biological Studies . . . . 148
II-88. Effects of local orientation on large-scale representations in V1 bias perceived global shape
Melchi Michel, Yuzhi Chen, Wilson S Geisler, Eyal Seidemann, University of Texas at Austin . . . . . . . 149
II-89. Position-specific heterogeneity of orientation pooling in curvature-tuned neurons of macaque area
V4
Anirvan S Nandy, Tatyana Sharpee, John Reynolds, Jude Mitchell, Salk Institute for Biological Studies . . 149
II-90. Inhibition controls the spatiotemporal spread of responses in awake visual cortex
Bilal Haider, Michael Häusser, Matteo Carandini, University College London . . . . . . . . . . . . . . . . 150
II-91. Selectivity and invariance are greater in macaque V2 than V1
Corey M Ziemba, Jeremy Freeman, Tony Movshon, Eero P Simoncelli, New York University . . . . . . . . 150
II-92. Understanding V1 surround modulation with natural stimuli using a principled statistical model
Ruben Coen-Cagli, Adam Kohn, Odelia Schwartz, Albert Einstein College of Medicine . . . . . . . . . . 151
II-93. Thalamic Synchrony and Visual Orientation Information Transmission To Cortex
Sean Kelly, Jianzhong Jin, Yushi Wang, Qi Wang, Michael Black, Jose-Manuel Alonso, Garrett Stanley,
Georgia Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
II-94. Differences in sensitivity to neural timing among cortical areas
Yang Yang, Anthony Zador, Institute of Neuroscience, Shanghai . . . . . . . . . . . . . . . . . . . . . . . 152
II-95. Artificial synchronization across sensory cortical area is sufficient for behavioral discrimination
Hachi Manzur, Joel Alvarez, Cecilia Babul, Pedro Maldonado, National Institue on Aging . . . . . . . . . 153
II-96. Transient activation of distinct striatal pathways mimics changes in the value of actions
Lung-Hao Tai, A. Moses Lee, Antonello Bonci, Linda Wilbrecht, University of California, San Francisco . . 153
II-97. Short-axon Cells Provide Both Excitatory and Inhibitory Drive to the Mitral/Tufted Cells
Arkarup Bandyopadhyay, Fred Marbach, Matthew Koh, Dinu F Albeanu, Cold Spring Harbor Laboratory . 154
II-98. A cell-type-specific population analysis of optogenetically evoked response in primary visual cortex
Ali Mohebi, Jessica A. Cardin, Karim Oweiss, Michigan State University . . . . . . . . . . . . . . . . . . 154
II-99. A Model of I-Wave Generation during Transcranial Magnetic Stimulation (TMS)
Catalin V Rusu, Ulf Ziemann, Jochen Triesch, UBB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155

COSYNE 2012

15

Posters III

Poster Session III

7:30 pm Saturday 27th February

III-1. A model of the effect of visual saliency on ethologically relevant, value-based decisions
R. Blythe Towal, Mili Milosavljevic, Christof Koch, California Institute of Technology . . . . . . . . . . . . 156
III-2. Optimal placement of dynamic range by coordinated populations of retinal ganglion cells
David Kastner, Stephen Baccus, Tatyana Sharpee, Stanford University . . . . . . . . . . . . . . . . . . . 156
III-3. Sparse codes for speech predict spectrotemporal receptive fields in the Inferior Colliculus
Nicole Carlson, Vivienne Ming, Michael DeWeese, University of California, Berkeley . . . . . . . . . . . . 157
III-4. Computing sparse representations using a network of integrate-and-fire neurons
Tao Hu, Alexander Genkin, Dmitri Chklovskii, HHMI, Janelia Farm Research Campus . . . . . . . . . . . 157
III-5. On the role of cortical feedback on invariant odor perception in the mammalian olfactory system
Gonzalo Otazu, Dinu F Albeanu, Cold Spring Harbor Laboratory . . . . . . . . . . . . . . . . . . . . . . 158
III-6. Fast estimation of non-smooth non-stationary receptive fields
Eftychios A Pnevmatikakis, Liam Paninski, Columbia University . . . . . . . . . . . . . . . . . . . . . . . 158
III-7. Improving individual classification learning using a predictive maximum entropy model
Yarden Cohen, Elad Schneidman, Weizmann Institute of Science . . . . . . . . . . . . . . . . . . . . . . 159
III-8. Internal representations of temporal statistics and feedback calibrate sensorimotor interval timing
Luigi Acerbi, Daniel Wolpert, Sethu Vijayakumar, University of Edinburgh . . . . . . . . . . . . . . . . . . 159
III-9. Generalization of uncertainty
Hugo Fernandes, Ian Stevenson, Konrad Kording, Northwestern University . . . . . . . . . . . . . . . . . 160
III-10. Exact inference for time series data on nonstandard state spaces
Carl Smith, Frank Wood, Liam Paninski, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . 161
III-11. Adaptive estimation of nonlinear response functions in V1 with Gaussian processes
Mijung Park, Greg Horwitz, Jonathan W Pillow, University of Texas at Austin . . . . . . . . . . . . . . . . 161
III-12. On the interaction of excitatory and inhibitory synaptic plasticity
Henning Sprekeler, Claudia Clopath, Tim P. Vogels, Humboldt Universität zu Berlin . . . . . . . . . . . . 162
III-13. Dynamics of Gap Junctions Inspired Networks
Merav Stern, Yosef Yarom, Larry Abbott, Hebrew University of Jerusalem . . . . . . . . . . . . . . . . . . 162
III-14. Slow dynamics and high variability in balanced networks with clustered connections
Ashok Litwin-Kumar, Brent Doiron, Carnegie Mellon University . . . . . . . . . . . . . . . . . . . . . . . 163
III-15. Phase precession through intrinsic neural resonance in continuous attractor models of grid cells
Sean Trettel, Ila Fiete, University of Texas at Austin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
III-16. Error-driven learning within the Hippocampus; theta rhythm, and novelty based learning signals
Nicholas Ketz, Srinimisha Morkonda, Randall O’Reilly, University of Colorado, Boulder . . . . . . . . . . 164
III-17. Intrinsic gradient networks: Highly recurrent neural networks with biologically plausible training
Jason Rolfe, Matthew Cook, Yann LeCun, New York University . . . . . . . . . . . . . . . . . . . . . . . 164
III-18. Plasticity in chaotic random recurrent networks leads to complex but non-chaotic neural trajectories
Rodrigo Laje, Dean V Buonomano, University of California, Los Angeles . . . . . . . . . . . . . . . . . . 165
III-19. Reliable and unreliable spike times in sparsely connected networks
Guillaume Lajoie, Kevin Lin, Eric Shea-Brown, University of Washington . . . . . . . . . . . . . . . . . . 165
III-20. Response of a Hodgkin-Huxley model VCN octopus cell to sounds with pitch
Brian Flynn, Laurel Carney, University of Rochester . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
III-21. Ion Channels Overcome the Biophysical Constraints of Neuron Morphology
Corinne Teeter, Victor H Chan, Qualcomm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
III-22. A Sequential Prediction Approach to Measure Time-Varying Causality in Ensemble Neural Recordings
Sanggyun Kim, Todd Coleman, University of California, San Diego . . . . . . . . . . . . . . . . . . . . . 167

16

COSYNE 2012

Posters III
III-23. Memory formation, recall and forgetting in neuronal networks
Christian Tetzlaff, Christoph Kolodziejski, Marc Timme, Florentin Wörgötter, University of Goettingen . . . 167
III-24. On the complementary strengths and weaknesses of spatial vs. hybrid map formation algorithms
Rishabh Jain, Bartlett Mel, University of Southern California . . . . . . . . . . . . . . . . . . . . . . . . . 168
III-25. Active self-organization of disordered arrangements of orientation preference in cortical networks
Juan Florez Weidinger, Wolfgang Keil, Dmitry Tsigankov, Michael Schnabel, Matthias Kaschube, Fred
Wolf, Max Planck Institute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
III-26. Nonnormal amplification in random balanced neuronal networks
Guillaume Hennequin, Tim P Vogels, Wulfram Gerstner, École Polytechnique Fédérale de Lausanne . . . 169
III-27. Non-linear predictive coding and dynamic decorrelation in early sensory systems
Shaul Druckmann, Dmitri Chklovskii, HHMI, Janelia Farm Research Campus . . . . . . . . . . . . . . . 169
III-28. Frequency analysis of short-term memory in nonlinear network models
Ann Kennedy, Haim Sompolinsky, Larry Abbott, Columbia University . . . . . . . . . . . . . . . . . . . . 170
III-29. Local control of non-local information routing in spiking neuronal networks
Christoph Kirst, Marc Timme, Demian Battaglia, Max Planck Institute for Dynamics and Self-Organization 171
III-30. Between-pair spike-field coherence comparison
Kyle Lepage, Mark Kramer, Georgia Gregoriou, Steve Gotts, Robert Desimone, Uri Eden, Boston University171
III-31. Bayesian entropy estimation for infinite neural alphabets
Evan Archer, Il Memming Park, Jonathan W Pillow, University of Texas at Austin . . . . . . . . . . . . . . 172
III-32. Parallel pathways for information processing in the retina: the ON and OFF dichotomy
Julijana Gjorgjieva, Haim Sompolinsky, Markus Meister, Harvard University . . . . . . . . . . . . . . . . . 172
III-33. Fisher and Shannon information in finite neural populations
Stuart Yarrow, Peggy Series, University of Edinburgh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
III-34. Maximally informative stimulus energies in the analysis of neural responses to natural signals
Kanaka Rajan, William Bialek, Princeton University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
III-35. A principle of brain communication based on compressive sampling and sparse coding
Guy Isely, Christopher Hillar, Fritz Sommer, University of California, Berkeley . . . . . . . . . . . . . . . . 174
III-36. Fitting receptive fields in V1 and V2 as linear combinations of nonlinear subunits
Brett Vintch, Andrew D Zaharia, Tony Movshon, Eero P Simoncelli, New York University . . . . . . . . . . 174
III-37. Two-layer synaptic integration in pyramidal neurons
Bardia Behabadi, Bartlett Mel, University of Southern California . . . . . . . . . . . . . . . . . . . . . . . 175
III-38. A Population Approach to Coding and Decoding with Adapting Neurons
Richard Naud, Wulfram Gerstner, École Polytechnique Fédérale de Lausanne . . . . . . . . . . . . . . . 175
III-39. Improving neural control of a simulated arm by decoding intended future movement
Francis R Willett, Aaron Suminski, Andrew Fagg, Nicholas Hatsopoulos, University of Chicago . . . . . . 176
III-40. Long-term Decoding Stability without Retraining for Intracortical Brain Computer Interface
William Bishop, Cynthia A Chestek, Vikash Gilja, Paul Nuyujukian, Stephen Ryu, Krishna Shenoy, Byron
Yu, Carnegie Mellon University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
III-41. An objective approach to learning movement-related features from local field potentials
Kelvin So, Michael Gastpar, Jose Carmena, University of California, Berkeley . . . . . . . . . . . . . . . 177
III-42. Synaptic input correlations and membrane potential decorrelation in spontaneous cortical activity
Michael Graupner, Alex D Reyes, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
III-43. Correlations in Spatially Heterogeneous Neuronal Networks
James Trousdale, Kresimir Josic, University of Houston . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
III-44. Filtering and recurrent connectivity shape higher-order correlations in retinal circuits
Andrea Barreiro, Julijana Gjorgjieva, Fred Rieke, Eric Shea-Brown, Southern Methodist University . . . . 179

COSYNE 2012

17

Posters III
III-45. Neuronal populations model of associative retrieval
Sandro Romani, Itai Pinkoviezsky, Alon Rubin, Misha Tsodyks, Columbia University . . . . . . . . . . . . 179
III-46. The correlation structure induced by fluctuations in attention
Alexander Ecker, Philipp Berens, Andreas Tolias, Matthias Bethge, Max Planck Institute for Biological
Cybernetics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
III-47. Attentional effects in V1 are related to spatial gating but not to allocation of limited resources
Eyal Seidemann, Yuzhi Chen, University of Texas at Austin . . . . . . . . . . . . . . . . . . . . . . . . . 180
III-48. Probabilistic palimpsest memory: multiplicity, binding and coverage in visual short-term memory
Loic Matthey, Paul Bays, Peter Dayan, University College London . . . . . . . . . . . . . . . . . . . . . . 181
III-49. What does information seeking tell us about reinforcement learning?
Ethan Bromberg-Martin, Okihide Hikosaka, National Eye Institute . . . . . . . . . . . . . . . . . . . . . . 181
III-50. Corticostriatal projections mediate auditory decisions
Petr Znamenskiy, Anthony Zador, Cold Spring Harbor Laboratory . . . . . . . . . . . . . . . . . . . . . . 182
III-51. Task set switching: dissecting ideal observer models and their approximation
Jan Drugowitsch, Etienne Koechlin, INSERM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
III-52. Evidence for incidental structured learning and abstraction in cognitive reinforcement learning
Anne Collins, Michael J Frank, Brown University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
III-53. Inactivation of rat frontal and parietal cortex during a temporal integration of evidence task
Chunyu Duan, Jeffrey Erlich, Timothy Hanks, Bingni Brunton, Carlos Brody, HHMI . . . . . . . . . . . . . 184
III-54. Coherent network-wide fluctuations of neural activity in the PFC during behavioral uncertainty
Mattias Karlsson, Dougal Tervo, Alla Y Karpova, HHMI, Janelia Farm Research Campus . . . . . . . . . 184
III-55. Dissecting the Contributions of Sensory and Category Uncertainty in Perceptual Decision-Making
Andre Mendonca, Maria Vicente, Alexandre Pouget, Zachary Mainen, Champalimaud Neuroscience Programme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
III-56. Optimal integration of multisensory event streams in rats and humans
David Raposo, John Sheppard, Paul Schrater, Anne Churchland, Champalimaud Neuroscience Programme185
III-57. Dissociable Influences of D1 and D2-mediated Frontal Eye Field Activity on Target Selection
Alireza Soltani, Behrad Noudoost, Tirin Moore, Stanford University . . . . . . . . . . . . . . . . . . . . . 186
III-58. Joint probability of independent events is consistent with weighted combination of log probabilities
James Tee, Hang Zhang, Laurence Maloney, New York University . . . . . . . . . . . . . . . . . . . . . . 186
III-59. Representation of multiple stimuli in the macaque middle face patch.
Akinori Ebihara, Winrich Freiwald, The Rockefeller University . . . . . . . . . . . . . . . . . . . . . . . . 187
III-60. A neural network model of the primate visuo-motor system
Christopher Kanan, Garrison Cottrell, University of California, San Diego . . . . . . . . . . . . . . . . . . 187
III-61. Short-Term Plasticity Optimizes Synaptic Information Transmission
Ziv Rotman, Panyue Deng, Vitaly Klyachko, Washington University . . . . . . . . . . . . . . . . . . . . . 188
III-62. Stochastic short term depression imposes a frequency-dependent filter on information transmission
Robert Rosenbaum, Jonathan Rubin, Brent Doiron, University of Pittsburgh . . . . . . . . . . . . . . . . 189
III-63. Optimizing online learning capacity in a biologically-inspired memory structure
Xundong Wu, DJ Strouse, Bartlett Mel, University of Southern California . . . . . . . . . . . . . . . . . . 189
III-64. Prediction error signals in ACC are scaled according to rational adjustments of learning
Matthew Nassar, Joshua Gold, University of Pennsylvania . . . . . . . . . . . . . . . . . . . . . . . . . . 190
III-65. A high-performance, robust brain-machine interface without retraining
Paul Nuyujukian, Jonathan Kao, Joline Fan, Sergey Stavisky, Stephen Ryu, Krishna Shenoy, Stanford
University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
III-66. Cerebellar granule cell activity during behavior: dynamics in light of the adaptive filter model
Sherika Sylvester, Kayvon Daie, Melanie Lee, Mark Goldman, Emre Aksay, Cornell University . . . . . . 191

18

COSYNE 2012

Posters III
III-67. Action valuation in multi-effector decision-making
Seth Madlon-Kay, Bijan Pesaran, Nathaniel Daw, New York University

. . . . . . . . . . . . . . . . . . . 191

III-68. Internal metabolic state determines human motor control strategies
Scott V Taylor, Aldo Faisal, Imperial College London . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
III-69. A stable, long-range motor pattern in the songbird brain
Jeffrey Markowitz, Timothy Gardner, Boston University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
III-70. Are grid-cell responses very low-dimensional?
KiJung Yoon, Caswell Barry, Michael A Buice, Neil Burgess, Ila Fiete, University of Texas at Austin . . . . 193
III-71. Scale-dependence of orientation statistics in natural scenes
Tatyana Sharpee, Anirvan S Nandy, John Reynolds, Salk Institute for Biological Studies . . . . . . . . . . 194
III-72. Population decoding algorithms for change detection and discrimination
Nicholas Price, Richard Born, Monash University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
III-73. Adaptive Gating of Information Flow to Cortex
He Zheng, Douglas Ollerenshaw, Bilal Bari, Qi Wang, Garrett Stanley, Georgia Institute of Technology . . 195
III-74. Detection of weak sensory signals by molecular dynamic transformations of interspike interval
seque
William Nesse, Gary Marsat, Andre Longtin, Leonard Maler, University of Utah . . . . . . . . . . . . . . . 195
III-75. Multisensory integration in the rat - behavioral benefits and neural correlates in parietal cortex
Stephanie Gleiss, Michael T Lippert, Kentaroh Takagaki, Nikos K Logothetis, Frank W Ohl, Christoph
Kayser, Max Planck Institute for Biological Cybernetics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
III-76. Multisensory calibration with external feedback is contingent on cue-reliability
Adam Zaidel, Mandy Turner, Dora Angelaki, Washington University . . . . . . . . . . . . . . . . . . . . . 196
III-77. Odors sum linearly in time in the rat olfactory bulb
Priyanka Gupta, Upinder S Bhalla, National Centre for Biological Sciences, India

. . . . . . . . . . . . . 197

III-78. Noradrenergic control of long-term cortical synaptic receptive field plasticity
Ana Raquel Martins, Robert Froemke, University of Coimbra . . . . . . . . . . . . . . . . . . . . . . . . 198
III-79. Real-time changes in single neurons during auditory object recognition learning
Daniel Knudsen, Timothy Gentner, University of California, San Diego . . . . . . . . . . . . . . . . . . . 198
III-80. Encoding of motion onset by retinal ganglion cells
Eric Chen, Joshua Levy, Clark Fisher, Rava da Silveira, Michael Berry, Princeton University . . . . . . . . 199
III-81. A wide-field neuron enhances visual contrast sensitivity in the fly
John Tuthill, Aljoscha Nern, Gerald M Rubin, Michael Reiser, HHMI, Janelia Farm Research Campus . . 199
III-82. Perceptual relevance of neurally-inspired natural image models evaluated via contour discrimination
Holly Gerhard, Matthias Bethge, Max Planck Institute for Biological Cybernetics . . . . . . . . . . . . . . 200
III-83. Continuous Time Infomax Models of Oculomotor Control
Walter Talbott, He Huang, Javier Movellan, University of California, San Diego . . . . . . . . . . . . . . . 200
III-84. Dissection of cortical microcircuits by single-neuron stimulation in vivo
Alex Kwan, Yang Dan, University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
III-85. Selectivity of neurons in area MT to complex motion features
Yuwei Cui, Liu Liu, Farhan A Khawaja, Christopher Pack, Daniel A Butts, University of Maryland . . . . . 201
III-86. Compressive spatial summation: a characteristic of extrastriate computation
Kendrick Kay, Jonathan Winawer, Aviv Mezer, Brian Wandell, Stanford University . . . . . . . . . . . . . 202
III-87. Direction selectivity within large receptive fields in a three-layer visual cortex
Jeff Pobst, David Morton, Ralf Wessel, Washington University . . . . . . . . . . . . . . . . . . . . . . . . 202
III-88. Trial-to-trial variability of MT neurons reveals the nature of their engagement in a motion discrimi
Mark Zarella, Tatiana Pasternak, University of Rochester . . . . . . . . . . . . . . . . . . . . . . . . . . 203

COSYNE 2012

19

Posters III
III-89. Reductions of correlated firing arise from attention-dependent depolarization in a spiking network
Jude Mitchell, John Reynolds, Salk Institute for Biological Studies . . . . . . . . . . . . . . . . . . . . . . 203
III-90. Dynamics of correlated variability in evoked and spontaneous responses of V4 neurons
Matt A Smith, Marc Sommer, University of Pittsburgh . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
III-91. The mesencephalic locomotor region modulates layer-specific activity in V1 independent of locomotion
Cristopher Niell, A. Moses Lee, Linda Wilbrecht, Antonello Bonci, Michael P Stryker, University of Oregon 205
III-92. Probing motion perception with spatiotemporal reverse correlation
Jacob Yates, Lawrence Cormack, Alex Huk, Jonathan W Pillow, University of Texas at Austin . . . . . . . 205
III-93. Dissociating interneuron types in frontal cortex by firing variability and spike timing specificity
Salva Ardid, Martin Vinck, Daniel Kaping, Stefan Everling, Thilo Womelsdorf, York University . . . . . . . 206
III-94. Transient motion analysis reveals differential motion dynamics of synaptic vesicle populations
Ziv Rotman, Amy Peng, Vitaly Klyachko, Washington University . . . . . . . . . . . . . . . . . . . . . . 206
III-95. A mobile imaging system to monitor the cortex in behaving rodents
Joon Hyuk Park, Ahmad Osman, Jelena Platisa, Eugenio Culurciello, Vincent Pieribone, Yale University . 207
III-96. Relating patterns of EEG activity to natural scene categories
Dirk Walther, Bart Larsen, The Ohio State University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
III-97. An open-source system for combining multi-electrode recording with closed-loop feedback
Joshua H Siegle, Jakob Voigts, Stuart Layton, Caleb Kemere, Loren Frank, Christopher Moore, Matthew
Wilson, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
III-98. Fast methods for mapping the full dendritic synaptic connectivity
Ari Pakman, Jonathan Huggins, Liam Paninski, Columbia University . . . . . . . . . . . . . . . . . . . . 209
III-99. Casting light on the interplay between perception and decision making in active sensing
Alex Gomez-Marin, Aljoscha Schulze, Vani Rajendran, Gus K Lott, Eric Trautman, Parvez Ahammad,
Chris Werner, Vivek Jayaraman, Matthieu Louis, Center for Genomic Regulation . . . . . . . . . . . . . . 209

20

COSYNE 2012

Posters III

COSYNE 2012

21

Posters III

The online version of this document includes abstracts for each presentation.
This PDF can be downloaded at: http://cosyne.org/c/index.php?title=Cosyne2012_Program

22

COSYNE 2012

T-1 – T-2

Abstracts
Abstracts for talks appear first, in order of presentation; those for posters next, in order of poster session
and board number. An index of all authors appears at the back.

T-1. Noise correlations and neural coding
Fred Rieke1,2
1 University

RIEKE @ WASHINGTON . EDU

of Washington

2 HHMI

Computation in the nervous system often relies on integrating multiple inputs with different functional properties.
Correlated noise in these input signals can, in principle, strongly affect the reliability of computations based on
them. Such theoretical predictions have rarely been tested experimentally because of a scarcity of preparations
that permit measurement of both covariation of a neuron’s input signals and the effect of manipulating such
covariation on a cell’s output. We have developed an approach to measure covariation of the excitatory and
inhibitory inputs received by individual and nearby retinal ganglion cells and determine how such covariation
affects spike output. I will describe what this approach reveals about the impact of correlated noise on the coding
of motion direction by a defined class of retinal ganglion cells.

T-2. The Neurophysiology of the Unconscious Brain Under General Anesthesia
Emery Brown1,2

ENB @ NEUROSTAT. MIT. EDU

1 Massachusetts

Institute of Technology
2 Harvard University
General anesthesia is a drug-induced, reversible condition comprised of five behavioral states: unconsciousness,
amnesia (loss of memory), analgesia (loss of pain sensation), akinesia (immobility), and hemodynamic stability with control of the stress response. The mechanisms by which anesthetic drugs induce the state of general
anesthesia are considered one of the biggest mysteries of modern medicine. We have been using three experimental paradigms to study general anesthesia-induced loss of consciousness in humans: combined fMRI/EEG
recordings, high-density EEG recordings and intracranial recordings. By using a wide array of signal processing
techniques, these studies are allowing us to establish precise neurophysiological, neuroanatomical and behavioral correlates of unconsciousness under general anesthesia. Combined with our mathematical modeling work
on how anesthetics act on neural circuits to produce the state of general anesthesia we are able to offer specific
hypotheses as to how changes in level of activity in specific circuits lead to the unconscious state. We will discuss
the relation between our findings and two other important altered states of arousal: sleep and coma. Our findings
suggest that the state of general anesthesia is not as mysterious as currently believed.

COSYNE 2012

23

T-3 – T-4

T-3. Electrophysiological Foundations of Human Speech Production
Kristofer Bouchard1
Nima Mesgarani1
Miranda Babiak1
Keith Johnson2
Eddie Chang1
1 University
2 University

KRIS @ PHY. UCSF. EDU
NIMA . MESGARANI @ UCSF. EDU
MIRANDA . BABIAK @ UCSF. EDU
KEITHJOHNSON @ BERKELEY. EDU
CHANGED @ NEUROSURG . UCSF. EDU

of California, San Francisco
of California, Berkeley

No behavior is as unique to humans as the ability to produce spoken language, and few behaviors that every human performs are as complicated to control as speech. Understanding the spatio-temporal organization of cortical
signals that control the articulators of the vocal plant during the production of basic speech elements (consonants
and vowels) is fundamental to our understanding of how the brain produces the complex sequence of movements
that compose spoken language. We recorded neuro-electrical activity from the surface of speech somato-motor
cortex using high-density electrocorticography (ECoG) in neurosurgical patients during the production of a large
and expansive set of American English consonant-vowel (CV) syllables. These recordings revealed that different
syllables are produced by the graded activation of multiple cortical sites in overlapping spatio-temporal patterns.
Across subjects, we found that the functional temporal structure of speech somato-motor cortex is sequenced to
first shape the upper vocal tract, through movement of the lips and tongue, filtering the sound produced at the
larynx and released from the vocal tract by the jaw. Furthermore, the functional modulations of neuro-electrical
activity underlying speech production are somatotopically organized, with both anatomically well isolated as well
as overlapping representations of the speech articulators. Analysis of the spatial patterns of activity (neural
state-space) at both consonant and vowel time point’s reveals a hierarchically clustered structure that is primarily organized by the pattern of articulatory engagements. Finally, the neural state-space exhibits divergent and
convergent dynamics that smoothly reflect the time dependent relationship between neural and articulatory states
for both consonants and vowels. Together, our results lay the foundations of an electro-physiological understanding of speech production in humans. More generally, our results provide insight into how the human neocortex
dynamically controls complex, multi-articulator behaviors, which is crucial to our understanding of basic nervous
system function.

T-4. On the Origins of Motor Noise
Kris Chaisanguanthum
Helen Shen
Philip Sabes

CHAISANG @ PHY. UCSF. EDU
HSHEN @ PHY. UCSF. EDU
SABES @ PHY. UCSF. EDU

University of California, San Francisco
Voluntary movements cannot be repeated to arbitrary precision. We undertook a study to characterize movement
variability and its relation to motor planning activity in the cortex, and found that cortical response variability
manifests in behavior only on slow time scales. Over 26 experimental sessions, Macaque monkeys made a
large number of center-out reaches (with delay) to three targets as multielectrode arrays chronically implanted
in dorsal premotor cortex (PMd) and primary motor cortex (M1) recorded the activity of 37–115 neural units.
We found that population activity in PMd and M1 predicts a significant amount of variability in the speed of
upcoming reach movements (cp. Churchland & al 2006) as well their initial direction. Analysis of the time course
of reach variability shows, however, that 20–40% of the variability in the speed and direction of reach movements
is due to a slowly moving (∼minutes) drift in the mean behavior; behavioral variability can thus be described as
the superposition of this drift and trial-to-trial fluctuations. Analysis of 2011 Major League Baseball data shows
that human subjects (pitchers) exhibit an analogous behavioral drift (in release point). We make the surprising
observation that neural activity in PMd/M1 populations are, at best, weakly related to the trial-to-trial fluctuations
in behavioral variation. Instead, in analogy with behavior, neural responses also exhibit a slow drift in mean

24

COSYNE 2012

T-5 – T-6
response; this drift is persistent, not specifically tied to motor planning and related to/predictive of the drift in
behavior. These results suggest that the central contribution to short time scale motor variability is minimal.
Longer time scale drift is central in origin, and—as suggested by the (comparable) autocorrelation structures of
both behavior and neural activity—may be the result of noise in a continuous online learning process (Cheng &
Sabes 2007).

T-5. Neural dynamics of reaching following incomplete or incorrect planning
Katherine Ames
Stephen Ryu
Krishna Shenoy

KCAMES @ STANFORD. EDU
SEOULMAN @ STANFORD. EDU
SHENOY @ STANFORD. EDU

Stanford University
Reaction times (RT) are faster when subjects prepare an action before moving. During preparation, primary motor
cortex (M1) and dorsal premotor cortex (PMd) neurons display activity specific to the upcoming reach. This has
led to the model that during preparation, motor cortical activity achieves a state (“optimal subspace”) which is
beneficial for generating the planned movement. We asked what happens if neural activity is outside the optimal
subspace when movement initiation is instructed. One model suggests that passing through the optimal subspace
is necessary for movement; the RT deficit for improperly planned trials results from “re-planning” before moving.
Alternatively, the optimal subspace might convey an RT benefit, yet not be obligatory. In this model, movement can
be initiated from outside the optimal subspace, albeit with a temporal penalty. We trained monkey N to perform two
tasks while we recorded simultaneous M1 and PMd activity using two 96 electrode arrays. The first task contained
interleaved blocks of delayed and non-delayed reaches (incomplete plan). The second was an instructed delay
task in which the cued target changed locations in 20% of trials (incorrect plan). We computed low-dimensional
neural trajectories by performing PCA on PSTH’s for each neuron and condition. When a target switch was
followed by a second delay, providing time to re-plan, neural activity changed from one plan to the other, indicating
that re-planning is possible. However, when not given time to re-plan, improperly planned neural trajectories did
not pass through the optimal subspace identified using “correctly planned” conditions. Instead, these neural
trajectories took a parallel but separate path which converged with the correctly planned neural trajectories over
the course of movement. This indicates that passing through the optimal subspace is not obligatory for movement,
providing evidence against a strict “multi-step” model of movement generation.

T-6. Energy-conservation and generalized power-law for curved hand movements
Dongsung Huh1,2
Terrence Sejnowski1
1 Salk

HUH @ SALK . EDU
TERRY @ SALK . EDU

Institute for Biological Studies
of California, San Diego

2 University

In physics, equations of motion can be derived from optimizing the total integrated cost. Noether’s theorem
states that if such Lagrangian formulation is invariant under time-translation, then energy of the system must be
conserved. Recently, it has been shown that human movements can be accurately modeled by the optimality
principle in a similar manner. A crucial observation we made was that these optimal control models share the
same time-translation invariance property, from which we successfully derived a novel conservation principle
for human movements: Defining A as the minimum total cost, the conserved quantity (energy) is the partial
differentiation of A with respect to time, E = −δA/δT . One important contribution of our energy-conservationprinciple (ECP) is that it extends the optimal control models to predict movement durations. For example, ECP
predicts T ∝ Dˆ1/3 for reaching movements (T=duration, D=reaching distance), and T ∝ Rˆ1/3Θ for circle-

COSYNE 2012

25

T-7 – T-8
drawing movements (R=radius, Θ= angular distance), which we confirmed in experiments. In general, timeprofile of movements scales with 1/3 power of movement size. Another prediction regards how angular speed w
fluctuates with curvature k in curved hand movements. Traditionally, the relationship has been incorrectly known to
follow the 2/3 power-law, w(t) ∝ k(t)ˆ2/3. Instead, ECP predicts a family of power-laws with exponents ranging
between 1/3 and 1, depending on how frequently the curvature fluctuates (e.g. spiral movements follows 1/3
power-law - closely related to the scaling-law above), which we confirmed in experiments (fig1). For movements
with multiple frequency components, w(t) depends not only on the local k(t) but also on nearby curvatures (±dt).
This result also allows us to estimate the amount of pre-planning required for generating curved movements.
In summary, ECP not only yields a fresh perspective for understanding human movements, but it also makes new
predictions we have experimentally confirmed.

T-7. Predicting odorant perception and neural activity from odorant structure
Noam Sobel

NOAM . SOBEL @ WEIZMANN . AC. IL

Weizmann Institute of Science
Although it is agreed that physicochemical features of molecules determine their perceived odor, the rules governing this relationship, and its neural underpinnings, remain unknown. A significant obstacle to such understanding
is the high dimensionality of features describing both percepts and molecules. We applied a statistical method
to reduce dimensionality in odor percepts, in odorant physicochemical descriptors, and in odorant-induced neural
activity for a large set of molecules. We found that the primary axis of perception was odor pleasantness, and
critically, that the primary axis of physicochemical properties reflected the primary axis of olfactory perception.
Moreover, the primary axis of perception and the primary axis of structure were both related to the primary axis
of neural activity. These relationships allowed us to predict the pleasantness of novel molecules either by their
physicochemical properties alone, or by their induced neural activity. Olfactory perception is strongly shaped
by experience and learning. However, our findings suggest that olfactory pleasantness is also partially innate,
corresponding to a natural axis of maximal discriminability among biologically relevant molecules.

T-8. Odor-guided decisions in the rat: The origins and uses of uncertainty
Zach Mainen

ZMAINEN @ NEURO. FCHAMPALIMAUD. ORG

Champalimaud Neuroscience Programme
As scientists, we construct knowledge about the world by conducting experiments, formulating hypotheses and
sharing information with one another, such as in this talk. Because our observations, models and communication
are all imperfect, quantification of the reliability of information plays a fundamental role in this process. All animals,
as they seek to understand and negotiate complex environments, face similar problems. Yet we still know little
about the precise roles that uncertainty plays in neural information processing and behavior.
My laboratory has been studying the behavior of rats in a decision task where the value of a left or right choice
depends on the identity of an odor cue. To investigate the origins of the uncertainty crucial for performance in this
task, we have manipulated the cues and other task parameters to find the psychophysical limits of the subjects.
We have also probed how decision uncertainty may itself be quantified and used by the brain. Through this
approach, together with simultaneous recordings from several brain areas and simple formal models, we have
begun to elucidate some of these issues.
At this meeting, I will discuss experiments revealing that uncertainty in olfactory decision-making arises from
multiple sources. I will argue that one form of uncertainty reflects rapid, sensory noise fluctuations while a second
form reflects slower fluctuations in decision criteria driven by trial-to-trial reinforcement learning. Only the fast
fluctuations are subject to integration during stimulus presentation. Thus, the two kinds of uncertainty have

26

COSYNE 2012

T-9 – T-10
different effects on the speed-accuracy relationships of decisions. This helps to explain why accuracy benefits
more from increased sampling time in some decision problems than in others. I will also discuss experiments
showing that confidence in odor-guided decisions is explicitly re-represented in the orbitofrontal cortex and that
this brain area is not necessary for these decisions but is critical for post-decision wagers.

T-9. Temporal segregation of olfactory bulb output
Izumi Fukunaga1
Manuel Berning1
Mihaly Kollo1
Andreas Schaefer2
1 Max
2 Max

IZUMI . FUKUNAGA @ MPIMF - HEIDELBERG . MPG . DE
MANUEL . BERGNING @ MPIMF - HEIDELBERG . MPG . DE
MIHALY. KOLLO @ MPIMF - HEIDELBERG . MPG . DE
ANDREAS . SCHAEFER @ MPIMF - HEIDELBERG . MPG . DE

Planck Institute for Medical Research
Planck Institute, Heidelberg

Neuronal oscillations are observed ubiquitously in the nervous system and are thought to underlie many aspects
of neuronal functions. In the main olfactory bulb of mammals, the activities of excitatory output neurons, mitral
and tufted cells, are tightly linked to the breathing rhythm, which occurs in a theta frequency range. However,
a systematic study of breathing-related temporal coding in identified mitral and tufted cells has been lacking.
Using whole cell patch recordings in vivo in mice anaesthetized with ketamine and xylazine as well as in awake
head-fixed mice, subthreshold membrane potentials and action potentials of mitral and tufted cells with respect
to sniffs were investigated. We find that morphologically identified mitral and tufted cells prefer distinct phases of
sniff cycles, separated by 180 degrees. To probe the source of this phase separation, a combination of GABAA
antagonist and agonist (0.4 mM and 2 mM) was superfused to block phasic GABAA activation while maintaining
network stability. While the tufted cell phase was unperturbed in this condition, the mitral cell phase collapsed
on to the tufted cell phase, indicating that the GABAergic selectively delays mitral cell activation. Furthermore,
we find that when excitatory odours are presented, while tufted cells increase firing rates without significant
change in preferred phase, mitral cell phase gradually advances towards the tufted cell phase. To understand
mechanisms further, we constructed 107 firing rate models of the OB network with connectivity parameters drawn
from uniform distributions. Analysis of models consistent with the experimental observations show that stronger
olfactory sensory connections to tufted cells compared to mitral cells, as well as prominent feed-forward inhibition
on mitral cells, underlie the phase-shift. In conclusion, mitral and tufted cells differentially couple to sniffs, which
results in distinct odour coding. Olfactory bulb thus segregates information into at least two streams of processing.

T-10. Do rats make optimal olfactory decisions under perceptual uncertainty
when rewards are unstable?
Junya Hirokawa
Adam Kepecs

RATCORTEX @ GMAIL . COM
KEPECS @ CSHL . EDU

Cold Spring Harbor Laboratory
Making optimal decisions requires the integration of different sources of information. For instance, past experience with rewards enables animals to make predictions about their quality, size and predictability. When the
main source of uncertainty is due to a perceptual decision process, estimating decision uncertainty provides a
mechanism for predicting reward outcomes. Can rats combine experienced reward value and decision uncertainty
to make optimal choices? And how does the brain dynamically compute and integrate these different estimates
to make predictions? We designed a reward-biased psychometric decision task to dissociate reward estimates
based on decision uncertainty and reward history and recorded neurons in the orbitofrontal cortex (OFC), an area
implicated in computing reward predictions. Rats were trained on a binary odor mixture categorization task. We
varied decision uncertainty by interleaving trials of different odor-mixture ratios and biased reward expectations

COSYNE 2012

27

T-11 – T-12
by changing reward size or probability for correct choices across blocks. The effect of reward bias on the psychometric function was strongest for uncertain odor mixtures and minimal for pure odors, suggesting that rats relied
increasingly on prior knowledge with increasing decision uncertainty. Our simple Bayesian model for combining
decision uncertainty and experienced reward value well predicted both behavioral choices and reaction times.
Next we examined how decision uncertainty and experienced value are combined in OFC neurons. We found a
heterogeneous representation with some neurons signaling either decision uncertainty or experienced value separately, while the majority of neurons showed mixed representations. These results suggest that rat OFC has the
capacity to integrate different types of reward expectations and contribute to optimal choices to maximize reward
acquisition.

T-11. Monte Carlo as mechanism: Sampling and human cognition
Tom Griffiths

TOM GRIFFITHS @ BERKELEY. EDU

University of California, Berkeley
Human behavior is consistent with the predictions made by Bayesian models of cognition across a wide range of
problems. This raises an interesting question: How are people solving these problems, given the computational
challenges posed by Bayesian inference? When we look at the distribution of people’s responses on specific
tasks, we often see that this distribution is similar to the posterior distribution produced by applying Bayes’ rule.
This phenomenon - which we call “probability matching to the posterior” - suggests a possible answer to our
question: That people are approximating Bayesian inference by producing a small number of samples from the
posterior distribution. Recent work in computer science and statistics has resulted in a number of sophisticated
Monte Carlo methods for approximating Bayesian inference, such as importance sampling and particle filters,
which we have explored as candidate explanations for human behavior. Considering the unique constraints of
human cognition has also led us to develop novel algorithms that have surprising properties, such as a sequential
Monte Carlo scheme based on the “win-stay, lose-shift” principle. I will present the results of experiments with
both adults and children that explore the potential of the Monte Carlo principle as an account of human cognition
and consider the implications of these results for computational neuroscience.

T-12. Noise correlations in population codes with finite information
Jeffrey Beck1
Ruben Moreno2
Peter Latham3
Alexandre Pouget1

JEFFBECK @ GATSBY. UCL . AC. UK
RMORENO @ FSJD. ORG
PEL @ GATSBY. UCL . AC. UK
ALEX . POUGET @ GMAIL . COM

1 University

of Rochester
Sant Joan de Deu
3 University College London
2 Foundation

Although theoretical studies have shown that correlations can either increase or decrease information, conventional wisdom has it that the kind of correlations typically observed in the brain (large among neurons with similar
tuning and small among neurons with different tuning) reduce information. Recently, however, Ecker and colleagues suggested that the conventional wisdom is wrong [1]. They showed that whenever tuning curves have
a range of amplitudes, and those amplitudes are not related to the correlational structure, the information in a
network is proportional to the number of neurons. Surprisingly, this is true for just about any correlational structure one can write down, including biologically realistic ones. While this is a profound result, it seems to conflict
with the observation that information can’t scale with the number of neurons, since the number of neurons is fixed
while information is set by noise in the outside world. The resolution, it turns out, is that the amplitudes of the
tuning curves and the correlational structure are necessarily related. This is because the correlations that reduce

28

COSYNE 2012

T-13 – T-14
information are the ones that mimic a shift in the stimulus. This shift adds a very specific component to the covariance matrix, so to show that changes in correlations affect information, it is necessary to show that this particular
component of the covariance matrix changes, not just its overall size. Importantly, the specific component of the
covariance matrix that limits information might be very small compared to the other correlations, to the point of
being difficult to measure experimentally with small numbers of trials. However, one can evaluate their impact on
information by applying the optimal decoder to a cross validation data set, but this requires a large population of
simultaneously recorded neurons.

T-13. Emergence of selectivity in recurrent random networks with balanced
excitation and inhibition
Cengiz Pehlevan1
Haim Sompolinsky2
1 Harvard
2 Hebrew

CENGIZPEHLEVAN @ FAS . HARVARD. EDU
HAIM @ FIZ . HUJI . AC. IL

University
University of Jerusalem

Visual, auditory and somatosensory cortical representations in several species consist of spatial feature maps.
In contrast, experiments in piriform cortex (Stettler and Axel, 2009) and in visual cortex of rodents (Ohki and
Reid, 2007) indicate that in these systems, stimulus preferences are distributed randomly without apparent spatial
organization, although many individual neurons show sharp stimulus selectivity. Previous analysis suggests that
cortical connectivity depends primarily on spatial proximity (Shapley et al 2000) indicating that in rodent olfactory
and visual cortex connectivity might be poorly tuned to feature similarity. This raises the question: Can sharp
stimulus selectivity be maintained in a cortical circuit with random untuned connections? To answer this question,
we consider a recurrent network of excitatory and inhibitory spiking neurons with untuned sparse random connectivity. The network is driven by untuned random projections from an input layer of stimulus selective neurons.
In such architecture, the stimulus modulation of total synaptic input to a neuron is weak compared to the untuned
component. Surprisingly, we find despite its random architecture the network can exhibit high stimulus selectivity.
This is due to the network’s balanced state (van Vreeswijk and Sompolinsky, 1996), in which strong synapses
amplify the variation in synaptic input, and recurrent inhibition cancels the mean. We test this mechanism in a
model of orientation selectivity in layer 2/3 of rodent V1. The network exhibits robust orientation selectivity, similar
to experimentally observed values. We also study a model of olfactory cortical circuit driven by odor activation of
the bulb. The population sparseness and lifetime selectivity are evaluated as a function of network parameters.
We predict that when the strength of the stimulus activation is low (e.g., low contrast or low odor concentration)
the stimulus selectivity decreases due to the increasing dominance of stochasticity at low rates.

T-14. A general, accurate, closed-form rate model derived as an approximation to spiking network dynamics
Evan Schaffer
Srdjan Ostojic
Larry Abbott

ESS 2129@ COLUMBIA . EDU
SRDJAN . OSTOJIC @ GMAIL . COM
LFA 2103@ COLUMBIA . EDU

Columbia University
Firing-rate models provide an attractive approach for studying large neural networks because they can be simulated rapidly and are amenable to mathematical analysis. Traditional firing-rate models assume the dynamics
are governed by a single time constant, which is mathematically convenient but not justified in general, and they
can only describe completely asynchronous activity. This is a serious limitation because transient synchronization
of subgroups of neurons has often been suggested as an important mechanism for generating rapid behavioral
responses. To address this issue without losing the advantages associated with a firing-rate description, we have

COSYNE 2012

29

T-15 – T-16
developed a model derived from an eigenfunction expansion of a Fokker-Planck equation, a common approach
to describe the distribution of membrane potentials for a population of spiking neurons receiving noisy input. We
depart from previous approaches by stressing simplicity along with sufficient accuracy. We find that a closedform expression for the firing rate can be derived by assuming that a single eigenmode dominates the dynamics.
Although the assumption of a single dominant mode is not true in general, it turns out to provide a good approximation across a wide range of parameter space. This approach is equally valid for any Integrate-and-Fire-type
model (Including the Exponential, Quadratic, and Leaky models). The simplicity of this rate model makes it highly
amenable as a tool for understanding spiking networks. Using this approach, we study large randomly-connected
networks of excitatory and inhibitory neurons. We find that the firing rate network gives a surprisingly accurate
approximation to the time-varying activity of a spiking network across a wide range of parameters and well into the
synchronous activity regime. The rate network has a phase diagram that is virtually identical to that of the spiking
network, predicting not just rate instabilities but also the bifurcation line between synchronous and asynchronous
states.

T-15. A New Look at Human Motor Control
Dana Ballard

DANA @ CS . UTEXAS . EDU

UT Austin
Much research has tackled motor control with the tools of classical mathematical optimization theory widely used
in robotic models. Although dynamical systems can be modeled with classical Newtonian equations, for mammalian systems with very high numbers of degrees of freedom, these equations prove prohibitively expensive to
solve except in the case of small subsystems. Nonetheless, humans themselves are an existence proof that some
kind of practical solution must exist, since they have exquisite motor coordination. At the same time, since their
physical properties are so unique, one must be prepared that the human solutions might look very different than
those dictated by the robotics-inspired classical approach. A major point of departure is in the musculoskeletal
design. Anthropomorphic bipedal designs can successfully walk downhill in a completely passive mode, implying
that much of the machinery needed for human movements has been incorporated in the mechanical design itself.
Further, passive muscle synergies augment basic mechanical degrees of freedom. By using activation to direct
set contractions, the muscles can guide the complete motor system through a series of posture changes that are
designed to both respect the basic joint limitations of the mechanical system and at the same time be efficient. The
idea of movement control as a series of postural changes has long been espoused, but has so far resisted a tight
mathematical formulation. Our research shows how to reduce elaborate posture changes into a compact code
that has several advantages. First of all, it provides an overview of how the elaborate computations in abstract
motor control could be parcellated into the brain’s primary subsystems. Secondly, its parametric description could
be used in the extension of learned movements to similar movements with different goals. Thirdly, the sensitivity
of the parameters can allow the differentiation of very subtle variations in movement.

T-16. Tuning to 3-D head-direction in the bat presubiculum
Dori Derdikman1
Arseny Finkelstein2
Jakob Foerster2
Nachum Ulanovsky2
1 Technion,

DERDIK @ TECHNION . AC. IL
ARSENYF @ GMAIL . COM
JAKOBFOERSTER @ GMAIL . COM
NACHUM . ULANOVSKY @ WEIZMANN . AC. IL

Israeli Institute of Technology
Institute of Science

2 Weizmann

Head-direction cells are active whenever the animal’s head points in a specific direction in space, and were
suggested to be a key component of the mammalian navigation system. Surprisingly, head-direction cells in the

30

COSYNE 2012

T-17
rat were reported to be indifferent to movements of the head in the up-down (pitch) axis, suggesting that these
cells in rodents are inherently 2-D spatially-tuned. We asked whether such tuning is a universal property of the
mammalian brain, by recording from flying mammals—Egyptian fruit bats—that may be better adapted to 3-D
behavior. Here, we developed a special 3-D tracking apparatus that allowed monitoring the three head-rotation
angles: yaw, pitch and roll. Cells modulated by head-direction were recorded in the bat presubiculum, while the
bats were actively crawling in an open-field arena, or were passively moved in an upside-down orientation. We
found that these cells were tuned to one or more of the three rotation angles (yaw, pitch, or roll). Furthermore,
in contrast to rats, most neurons retained their head-direction tuning while the bat was held upside-down. Taken
together, these results demonstrate for the first time a 3-D head-direction mechanism in mammals, which may be
part of a broader neuronal network supporting navigation in 3-D space.

T-17. Awake hippocampal sharp-wave ripples support spatial working memory
Shantanu Jadhav
Caleb Kemere
Walter German
Loren Frank

SHANTANU @ PHY. UCSF. EDU
CKEMERE @ PHY. UCSF. EDU
GERMAN @ PHY. UCSF. EDU
LOREN @ PHY. UCSF. EDU

University of California, San Francisco
The hippocampus is critical for the storage and retrieval of spatial memories. In particular, hippocampal damage or
dysfunction leads to deficits in two distinct types of memory: working memory, where representations of specific
past experiences must be retrieved to guide subsequent choices, and reference memory, where choices are
made based on longer term, accumulated memories rather than based on specific experiences. There also
are two dominant neural activity patterns in the hippocampus of awake animals, namely place field activity seen
during exploratory behavior and replay of past experience during sharp-wave ripples (SWRs) seen primarily during
immobility and low speed movement. The relationship between these activity patterns and the spatial memory
functions of the hippocampus is not understood. We specifically disrupted hippocampal neural activity during
awake SWRs as animals learned a hippocampally-dependent spatial memory task to determine how these events
contribute to spatial working and reference memory. We found that transient suppression of hippocampal activity
during awake SWRs caused a marked learning deficit specific to spatial working memory with no discernable
effect on spatial reference memory. SWR interruption did not affect place field activity during behavior, indicating
that place field activity is not sufficient to support working memory processes. Further, reactivation during SWRs in
rest periods after behavior, which has been linked to memory consolidation, also remained intact following awake
SWR interruption. Our results suggest that the awake replay of past experience during SWRs is an essential
component of working memory processes that allow an animal to retrieve specific memories and use them to
guide behavior.

COSYNE 2012

31

T-18

T-18. Bending waves during C. elegans locomotion are driven and organized
by proprioceptive coupling
Quan Wen1
Elizabeth Hulme1
Sway Chen1
Xinyu Liu1
Marc Gershow1
Andrew Leifer1
Victoria Butler2
Chris Fang-Yen1,3
William Schafer4
George Whitesides1
Matthieu Wyart5
Dmitri Chklovskii2
Samuel Aravinthan6

VINEYGEYSER @ GMAIL . COM
EHULME @ GMWGROUP. HARVARD. EDU
SWAYPCHEN @ GMAIL . COM
XLIU @ GMWGROUP. HARVARD. EDU
GERSHOW @ GMAIL . COM
ANDREW. LEIFER @ GMAIL . COM
BUTLERV @ JANELIA . HHMI . ORG
CFANGYEN @ GMAIL . COM
WSCHAFER @ MRC - LMB . CAM . AC. UK
GWHITESIDES @ GMWGROUP. HARVARD. EDU
MW 135@ NYU. EDU
CHKLOVSKIID @ JANELIA . HHMI . ORG
ADTSAMUEL @ GMAIL . COM

1 Harvard

University
Janelia Farm Research Campus
3 University of Pennsylvania
4 University of Cambridge
5 New York University
6 Affiliation not provided
2 HHMI,

Locomotion requires mechanisms that coordinate motor activity throughout an animal’s body. Here, we show
that forward locomotion in C. elegans is both driven and coordinated by a novel form of proprioceptive coupling
within the motor circuit. To characterize this mechanism, we engineered microfluidic devices that enabled us to
control the bending of specific body regions while quantifying movement and neuromuscular activity throughout
the animal. We show that positive stretch-sensitive feedback compels each body region to bend in the same
direction and shortly after bending in the neighboring anterior region. To determine how proprioceptive coupling is
integrated into the neuromuscular network, we performed optogenetics and calcium imaging to directly manipulate
and monitor motor circuit activity of worms in our microfluidic devices. We show that the cholinergic motor neurons
both generate and propagate the proprioceptive signal from anterior to posterior body regions. We also show that
body wall muscles in C. elegans can sustain contraction without synaptic input. Thus, in the C. elegans motor
circuit, motor neuron output is used to trigger changes in bending state, not to maintain bending. We use our
quantitative physiological measurements to build a mathematical model of locomotion. This model shows how
proprioceptive coupling drives and organizes the undulatory gait, and also provides a biophysical explanation for
gait adaptation when the external load on a moving worm is gradually increased.

32

COSYNE 2012

T-19 – T-20

T-19. Activation of Specific Interneurons Improves V1 Feature Selectivity and
Visual Perception
Seung-Hee Lee1
Alex Kwan1
Victoria Phoumthipphavong1
Siyu Zhang1
John Flannery1
Sotiris Masmanidis2
Hiroki Taniguchi3
Josh Huang3
Edward Boyden4
Karl Deisseroth5
Yang Dan1,6

SH LEE @ BERKELEY. EDU
ALEXKWAN @ BERKELEY. EDU
VIC. TORI . A @ LIVE . COM
ZHANG SIYU @ BERKELEY. EDU
FLANNERY @ BERKELEY. EDU
SOTIRIS @ CALTECH . EDU
TANIGUCH @ CSHL . EDU
HUANGJ @ CSHL . EDU
ESB @ MEDIA . MIT. EDU
DEISSERO @ STANFORD. EDU
YDAN @ BERKELEY. EDU

1 University

of California, Berkeley
Institute of Technology
3 Cold Spring Harbor Laboratory
4 Massachusetts Institute of Technology
5 Stanford University
6 HHMI
2 California

Inhibitory interneurons are essential components of the neural circuits underlying a variety of brain functions.
In the neocortex, a large diversity of GABAergic interneurons have been identified based on their morphology,
molecular markers, biophysical properties, and innervation pattern. However, how the spiking activity of each
subtype of interneurons contributes to sensory processing remains largely unknown. Here we show that optogenetic activation of parvalbumin-positive (PV+) interneurons in mouse primary visual cortex (V1) sharpens neuronal feature selectivity and improves perceptual discrimination. Using multichannel recording with silicon probes
and channelrhodopsin 2 (ChR2)-mediated optical activation, we found that elevated spiking of PV+ interneurons
markedly sharpened orientation tuning and enhanced direction selectivity of nearby V1 neurons. These effects
were caused by the activation of inhibitory neurons rather than decreased spiking of excitatory neurons, since
archaerhodopsin-3 (Arch)-mediated optical silencing of calcium/calmodulin-dependent protein kinase IIα-positive
(CaMKIIα+) excitatory neurons caused no significant change in the cortical stimulus selectivity. Moreover, the
sharpening of orientation tuning specifically required inhibition from PV+ interneurons: although activating somatostatin (SOM+) interneurons increased direction selectivity, it caused no sharpening of orientation tuning, and
activating vasointestinal peptide (VIP+) interneurons had no effect on either response property. Notably, optical
activation of PV+ interneurons in awake mice caused a significant improvement in their orientation discrimination,
mirroring the sharpened V1 orientation tuning. Together, these results provide the first demonstration that visual
coding and perception can be improved by elevated spiking of a specific subtype of cortical inhibitory interneurons.

T-20. Entraining gamma oscillations in somatosensory cortex enhances performance in tactile detection
Dominique Pritchett1,2
Joshua H Siegle1
Christopher Moore2

PRITCHED @ MIT. EDU
JSIEGLE @ MIT. EDU
CHRISTOPHER MOORE @ BROWN . EDU

1 Massachusetts
2 Brown

Institute of Technology
University

The utility of gamma oscillations in primary sensory cortices is a contentious issue, largely due to the fact that
experiments have thus far provided only correlative, rather than causal, evidence for their beneficial nature. A
previous study from our lab demonstrated that it is possible to bring gamma-band synchrony under experimental

COSYNE 2012

33

T-21 – T-22
control by varying the temporal offset between an environmental stimulus and an imposed gamma rhythm in
cortex (i.e., varying the phase of a laser stimulus at the cortical surface). Here, we apply this manipulation in
mice trained on a tactile detection task, with the goal of testing whether or not entraining gamma in a primary
sensory area has any effects on perception. PV-Cre mice were injected with AAV-DIO-ChR2-mCherry in SI, had
a head post affixed to the skull, and had electrodes chronically implanted near the injection site to monitor the
efficacy of optogenetic gamma induction. Following a recovery period, mice were trained to lick a reward spout
in response to 40 Hz vibrissal deflections. Once mice became proficient at the task (and viral expression levels
reached a plateau) we added a 40 Hz optogenetic stimulus on approximately half the trials. Performance varied as
a function of laser phase, as indicated by hit rate, reaction time, and d’ (a measure of overall accuracy). On trials
with imposed gamma at the phase shown to enhance spike synchrony most robustly, mice performed significantly
better than on trials without any optogenetic stimulus (baseline). On trials with imposed gamma at the phase
shown to have the greatest detriment to synchrony, mice performed significantly worse than baseline. Enhanced
performance was also observed in a task which included a vibrissa stimulus that mimicked the statistics of natural
vibrissa movements. These results provide causal evidence that gamma-range oscillations can benefit perception
in a mammalian model system.

T-21. Phosphene induction in monkeys using optogenetics
Mehrdad Jazayeri1,2
Zachary Lindbloom-Brown1
Greg Horwitz1
1 University

MJAZ @ U. WASHINGTON . EDU
ZACKLB @ U. WASHINGTON . EDU
GHORWITZ @ U. WASHINGTON . EDU

of Washington

2 HHMI

Optogenetics has had a remarkable impact on our understanding of the causal influences of neural activation on
the behavior of rodents and lower animals. A key area of development is to adapt this technology to non-human
primates for which more sophisticated models of behavior exist. Here we demonstrate the use of optogenetics
to drive a behavioral response in two rhesus monkeys (Macaca mulatta). We expressed channelrhodopsin-2
(ChR2) in a small region of the primary visual cortex (V1), and asked whether ChR2-mediated activation produced
phosphenes at the location of the corresponding receptive fields (RFs). Our behavioral paradigm consisted of two
trial types, “Fix” and “Tar”. On “Fix” trials, monkeys received liquid rewards for maintaining fixation on a central
spot. On “Tar” trials, monkeys received rewards for making a saccade to a visual target that was presented after
fixation spot offset. Unbeknownst to the monkeys, half of the trials in each category were coupled with optical
stimulation (“Op+Fix” and “Op+Tar”). The key finding was that on Op+Fix trials, in which no visual target was
presented, monkeys made saccades toward the RF of neurons at the injection site (top panels). Simultaneous
recording showed that optical stimulation modulated spiking activity near the injection site (bottom panels). To
ensure that the behavioral effect was not due to a nonspecific association between stimulation and saccadic
response, we performed a control experiment in which the visual target was presented in the hemifield opposite
the RF location of the ChR2-mediated activity. In Op+Fix trials of the control task, monkeys continued to orient
toward the location of the RF suggesting that the saccadic responses were induced by phosphene perception. To
the best of our knowledge, this is the first application of optogenetics for evoking an overt behavioral response
from a non-human primate.

T-22. Categories, decisions and parietal cortex
John Assad

JOHN ASSAD @ HMS . HARVARD. EDU

Harvard Medical School
The inferior parietal lobe is involved in the perception of visual space and the control of eye movements. Neurons

34

COSYNE 2012

T-23
in the primate lateral intraparietal area (LIP) have also been implicated in perceptual decision-making. In those
experiments, monkeys typically signal their percept by making saccadic eye movements in specific directions.
We asked whether parietal neurons are involved in decisions that do not have a spatially specific motor read-out.
In our first experiment, we trained monkeys to group directions of motion into two 180◦ -wide "categories". After
training, we found that LIP neurons reflected the learned category boundary, in that individual neurons tended
to respond similarly within direction categories but differently between categories. We examined the generality
of these effects by training animals in a paired-associate task in which the animals learned to group pairs of
arbitrarily chosen static shapes. We found again that LIP neurons reflected the learned pair-associations, in that
individual neurons tended to respond more similarly for associated pairs of shapes than for unassociated pairs of
shapes. In both the direction-categorization task and the shape-paired-associate task we used a delayed matchto-category (-pair) paradigm that dissociated the category (pair) identity from the hand movement the animal used
to signal its report. We also controlled carefully for behavioral artifacts that could have produced the observed
neuronal selectivity. Our results suggest that parietal neurons provide decisional signals that do not fit in a spatialor motor-based framework. These findings challenge the generality of models positing that categorical decisions
are represented in an action- or intention-based framework. Action-based frameworks have been proposed for
other brain representations, such as for the representation of value. However, we also find action-independent
neuronal representations of value in orbitofrontal cortex. We hypothesize that non-action-based representations
are prevalent in the brain and can be revealed by appropriate experimental design.

T-23. The role of disordered representations in neural dynamics
Stefano Fusi

SF 2237@ COLUMBIA . EDU

Columbia University
The response properties of neurons recorded in the prefrontal cortex are very diverse in space (i.e. for different
neurons) and time (i.e. selectivity changes in different epochs). Often, neurons respond to conjunctions of events
and other task related aspects (mixed selectivity). This variability makes the neuronal responses seemingly
disordered and hard to interpret. It is natural to ask whether this variability is just disruptive noise, or whether it
plays a functional role. We addressed this question by analyzing and manipulating the neural activity recorded in
prefrontal cortex during an object sequence memory task(Warden & Miller, 2010). We found that: 1) all relevant
aspects of the task can be decoded with great accuracy on a trial by trial basis from the neuronal population
activity, despite the fact that the information is intermixed and distributed across multiple neurons; 2) even when
the selectivity to a specific aspect of the task is individually removed from all recorded neurons, it is still possible
to decode that specific aspect by harnessing the mixed selectivity components of neural activity; and 3) the
disordered neural representations that have been recorded are significantly more efficient at driving the dynamics
of a simulated neural circuit than representations based on neurons that are purely selective to one aspect of
the task. We show that the main limitation of pure selectivity neurons is caused by the low dimensionality of
their population activity. Typically, for these neurons, the number of effective dimensions representing the different
aspects of the task is much smaller than the number of task conditions. In contrast, the effective dimensionality of
the recorded representations is significantly higher, making the representations more suited to drive the activity of
individual neurons. In conclusion, our study demonstrates that the diversity of the response properties of recorded
neurons does not limit the amount of information stored in the neural representations. Moreover it actually provides
a great computational advantage. This work has been done with M. Rigotti, O. Barak, M.R. Warden, N. Daw, X-J.
Wang and E.K. Miller.

COSYNE 2012

35

T-24 – T-25

T-24. Separable Influences of Reward on Prefrontal Control of Attention and
Target Selection
Alireza Soltani1,2
Bob J Schafer3
Brittany Burrows1
Tirin Moore1
1 Stanford

ASOLTANI @ STANFORD. EDU
BSCHAFER @ MIT. EDU
BRITTANY. BURROWS @ GMAIL . COM
TIRIN @ STANFORD. EDU

University

2 HHMI
3 Massachusetts

Institute of Technology

Understanding the neural mechanisms of value-dependent choice and attention have been two of the most important research objectives in systems neuroscience. However, difficulties in dissociating attentional and rewarddependent mechanisms have greatly limited progress in understanding these two processes and how they relate
to one another. These difficulties have arisen for two reasons: First, attentional deployment is often manipulated
by changing reward contingencies. Second, attentional deployment and value-dependent target selection typically yield a single action (e.g. a saccadic eye movement), so it is difficult to dissociate the two processes. By
combining modeling and experimental approaches, we examined the influence of value on target selection and
attention using saccade metrics in a free-choice task. Specifically, monkeys selected between two visual targets,
each of which was a drifting grating. It has been shown that saccades directed to such targets are displaced in the
direction of visual motion. This “motion induced bias” (MIB) provides an untrained, implicit measure of attentional
deployment as it depends on the features of the target and reveals the extent of processing of those features.
Therefore, on each trial we could independently assess the monkeys’ target selection and attentional deployment.
We also examined how target value interacts with microstimulation of the frontal eye field (FEF) to shape target
selection and spatial attention. We first show that attention is influenced by reward history even when it is not
required to obtain reward, and that reward integration for attention differs from that of target selection. Second, we
show that FEF microstimulation and reward history exert additive effects on attention, while they interact to guide
selection. Third, we show that our experimental observations can be explained by a model in which the efficacy
of microstimulation-driven signals and endogenous reward signals interacts competitively.

T-25. Human single-unit responses in the nucleus accumbens during a financial decision-making task
Shaun Patel1,2
Sameer Sheth1
Matt Mian1
John Gale1
Jason Gerrard1
Benjamin Greenberg3
Darin Dougherty1
Emad Eskandar1

SHAUN . PATEL @ ME . COM
SASHETH @ PARTNERS . ORG
MATT. MIAN @ GMAIL . COM
GALEJ @ CCF. ORG
JGERRARD 00@ GMAIL . COM
BENJAMIN GREENBERG @ BROWN . EDU
DDOUGHERTY @ PARTNERS . ORG
EESKANDAR @ PARTNERS . ORG

1 Massachusetts

General Hospital
University
3 Brown University
2 Boston

Central to flexible human behavior is our ability to bind stimulus value to action and to evaluate expectation to outcome. Despite this, very little is understood on the mechanisms underlying these computations in humans. Here,
we report single-unit responses recorded from the human nucleus accumbens (accumbens) in patients undergoing deep brain stimulation while engaged in a financial decision-making task. The task is modeled as a simplified
version of the classic card game “war”. The subject is dealt a card and asked to make a high or low wager ($5
or $20). Immediately following their choice they are shown their opponent’s card–the player with the highest card

36

COSYNE 2012

T-26 – T-27
wins. We recorded 25 individual neurons from 8 patients. We restricted our analysis to 19 well isolated units based
on stability of the recording, quality of isolation, and minimum number of trials. We found that during the 500 msec
interval following the go-cue, accumbens activity predicted whether the subject would ultimately place a high or
low bet (ROC, a.u.c.=0.62 ; randomization test, p<0.001). This activity occurred on average 2±.81 sec before the
bet was manifested. No other pre-choice interval predicted bet direction. Furthermore, we found that accumbens
activity encoded a reward prediction error signal; accumbens activity selectively increased for unexpected wins
and decreased for unexpected losses during a 500 msec interval in the feedback period (two-tailed t-test, p<.01).
There was no change in activity for expected wins and losses. In conclusion, these findings suggest that the
accumbens encodes two reinforcement learning signals during financial-decision making: (i) binding predicted
stimulus value to action, and (ii) evaluating discrepancies between expectation and outcome. This represents the
first report of single-neuronal responses from the human accumbens during financial decision-making.

T-26. Dynamics of a calcium-based plasticity rule: from single synapses to
networks
Nicolas Brunel

NICOLAS . BRUNEL @ PARISDESCARTES . FR

Université Paris Dèscartes
Calcium is known to play a fundamental role in synaptic plasticity. However, it is still unclear to what extent the
dynamics of calcium concentration in post-synaptic spines alone can account for the phenomenology of plasticity.
In this talk, I will first present a simplified calcium-based synaptic plasticity model, and show that it can reproduce
quantitatively a large amount of experimental data in several preparations (hippocampal cultures, hippocampal
slices and cortical slices). Differences between plasticity outcomes in such preparations are predicted to arise
due to differences in parameters controlling calcium dynamics (such as the extracellular calcium concentration). I
will then present some consequences of this plasticity rule at the network level.

T-27. Long-term modification of cortical synapses improves sensory perception
Ioana Carcea1
Kexin Yuan2
Alison Barker2
Bryan A Seybold2
Ana Raquel Martins3,1
Natalya Zaika1
Hannah Bernstein1
Daniel Polley4
Michael Merzenich2
Christoph Schreiner2
Robert Froemke1

IOANA . CARCEA @ MED. NYU. EDU
KEXIN @ PHY. UCSF. EDU
ALISON . BARKER @ UCSF. EDU
BRYAN . SEYBOLD @ UCSF. EDU
RAQUEL . MARTINS @ MED. NYU. EDU
NATALYA . ZAIKA @ MED. NYU. EDU
HANNAH . BERNSTEIN @ MED. NYU. EDU
DANIEL POLLEY @ MEEI . HARVARD. EDU
MERZ @ PHY. UCSF. EDU
CHRIS @ PHY. UCSF. EDU
ROBERT. FROEMKE @ MED. NYU. EDU

1 New

York University
of California, San Francisco
3 University of Coimbra
4 Harvard University
2 University

Synapses and receptive fields of the cerebral cortex are plastic. However, changes to specific inputs must be
coordinated within neural networks to ensure that excitability and feature selectivity are appropriately configured
for perception of the sensory environment. Here we report that long-lasting positive and negative changes to auditory cortical excitatory synapses were induced by pairing acoustic stimuli with activation of the nucleus basalis

COSYNE 2012

37

T-28
cholinergic neuromodulatory system. We examined synaptic tuning curves for sound level intensity and found
that initially, as previously reported, most neurons showed a monotonically-increasing amount of tone-evoked
excitation as sound level grew louder. However, pairing a quiet, low-intensity tone with nucleus basalis stimulation (’nucleus basalis pairing’) increased these initially weak responses while the strongest responses to louder
tones were depressed. These synaptic modifications were precisely orchestrated across entire receptive fields,
conserving mean excitation while reducing overall variance, and each parameter of cortical synaptic receptive
fields (frequency and intensity) could be modified independently of the other. Computational analysis indicated
that decreased variability should increase detection and recognition of near-threshold or previously imperceptible stimuli, and this was confirmed psychophysically with nucleus basalis pairing in behaving animals. Pairing
in anesthetized animals could lead to behavioral improvements after animals woke up. The effects of pairing
lasted only a few hours unless pairing was performed daily for several (6+ days), after which the effects of pairing endured. Furthermore, pharmacological manipulations indicated that changes to auditory cortex were both
necessary and sufficient for behavioral enhancement. Thus direct modification of specific cortical inputs leads to
wide-scale synaptic changes, which collectively support improved sensory perception and enhanced behavioral
performance.

T-28. Plasticity and stability in motor cortex during learning
Daniel Huber1
Diego Gutnisky2
Simon Peron2
Daniel O’Connor2
Lin Tian2
Loren Looger2
Karel Svoboda2

DANIEL . HUBER @ UNIGE . CH
GUTNISKYD @ JANELIA . HHMI . ORG
PERONS @ JANELIA . HHMI . ORG
OCONNORD @ JANELIA . HHMI . ORG
TIANL @ JANELIA . HHMI . ORG
LOOGERL @ JANELIA . HHMI . ORG
SVOBODAK @ JANELIA . HHMI . ORG

1 University
2 HHMI,

of Geneva
Janelia Farm Research Campus

Animals move their sensors to collect information about the world, and movements in turn are guided by sensory
input. Interactions between movement and sensation are critical for robust motor control and also shape complex
learned behaviors, where sequences of movements are required to achieve a given goal. The motor cortex is
thought to play important roles in learning new motor skills, but the underlying mechanisms at the level of neural circuits are unknown. Superficial layer neurons in the primary motor cortex likely participate in sensorimotor
integration and learning; they receive input from sensory cortex, and excite deep layer neurons, which control
movement. Here we used chronic population imaging with genetically encoded calcium indicators (GCaMP3) to
track the same set of superficial neurons in the motor cortex over weeks, while mice learned to detect objects with
their whiskers and report detection with licking. Neuronal activity correlated with whisker position and whisking
amplitude, with the forces exerted on the whiskers during touch, as well as with licking during reward collection.
The representations of individual neurons changed with learning, but in a restricted manner so that licking neurons
never changed into whisking neurons and vice versa. This indicates that neurons tend to encode default behavioral features, which might be immutable to learning. In expert mice, the representation of behavioral parameters
was stable at the population level, despite continuing changes at the level of individual neurons. Finally, in a subpopulation of neurons the representation of licking triggered by whisker touch emerged with learning, suggesting
that the superficial layers of motor cortex participate in linking task-related sensory inputs and actions.

38

COSYNE 2012

T-29 – T-30

T-29. Nonlinear dendritic processing during active sensing produces an object localization signal
Ning-long Xu1
Mark Harnett1
Stephen Williams2
Daniel Huber3
Daniel O’Connor1
Karel Svoboda1
Jeffrey Magee1

XUN @ JANELIA . HHMI . ORG
HARNETTM @ JANELIA . HHMI . ORG
SRW @ UQ . EDU. AU
DANIEL . HUBER @ UNIGE . CH
OCONNORD @ JANELIA . HHMI . ORG
SVOBODAK @ JANELIA . HHMI . ORG
MAGEEJ @ JANELIA . HHMI . ORG

1 HHMI,

Janelia Farm Research Campus
of Queensland
3 University of Geneva
2 University

Despite decades of research it remains unknown if the powerful processing capabilities of active dendrites are
actually involved in any behaviorally relevant neuronal computations. Here we report that a novel global dendritic
nonlinearity is linked to object localization and an associated motor refinement during an active sensing task.
Layer 5 pyramidal neurons are the primary output of the barrel cortex and they receive functionally distinct inputs
onto separate dendritic regions. Previous studies show that coincident input from these segregated pathways
initiates regenerative dendritic electrical events that shift the action potential output mode to burst firing, representing a powerful dendritic nonlinearity that could mediate computations based on input correlation. However,
the presence and the functional roles of large Ca2+ transients in the distal dendritic tuft in awake behaving animals are yet unknown. Here we designed a whisker-based tactile detection task where mice were trained to
detect the presence of a tactile object (a vertical pole) by active whisking, and performed in vivo two-photon Ca2+
imaging from distal apical dendrites of layer 5 neurons in the barrel cortex using genetically encoded Ca2+ indicator (GCaMP3) (see figure below, left). Large amplitude, global Ca2+ signals were observed throughout the
entire apical tuft dendrites that are tightly linked to active touch in a whisker position-dependent manner during
active sensing (see figure below, right). Dendritic recordings in anesthetized mice and in vitro suggest these
novel global signals are produced by dendritic plateau potentials that are coincident with widespread layer 1 (L1)
synaptic input. Quantitative behavioral assessment also shows that learned refinements in the whisking pattern
were co-evolved with dendritic signals. These data represent the first direct observation of dendritic Ca2+ signals
at subcellular resolution in awake mammalian brain, revealing nonlinear dendritic integration underlying sensory
processing during active sensation.

T-30. Thinking about thinking about thought
Rebecca Saxe

SAXE @ MIT. EDU

Massachusetts Institute of Technology
Perhaps one of the most surprising discoveries of cognitive neuroscience is the existence of brain regions involved
in thinking about thoughts. I will describe neuroimaging evidence to provide clues about the functional responses
of these regions, including their response profiles and selectivity, the developmental mechanism that produced
them, and the representational features that are encoded there. These data open a window on deep and fundamental questions about how the mind goes ‘beyond the data’, inventing abstract concepts and invisible causes
to make sense of the world, especially the social world around us. Nevertheless, the most interesting questions
about these brain regions remain unanswered.

COSYNE 2012

39

T-31 – T-32

T-31. The beads task, information sampling and impulsivity
Bruno Averbeck1
Nicholas Furl2
Atbin Djamshidian3
Andrew Lees3

BRUNO. AVERBECK @ NIH . GOV
NICHOLAS FURL @ YAHOO. COM
ATBIN . DJAMSHIDIAN @ GMAIL . COM
ANDREW. LEES @ ION . UCL . AC. UK

1 National

Institute of Mental Health
of Cambridge
3 University College London
2 University

Decisions are most effective after collecting sufficient evidence to accurately predict rewarding outcomes. We investigated whether healthy participants optimally seek evidence and we characterized the brain areas associated
with their evidence seeking using fMRI. Participants viewed sequences of colored beads drawn from a hidden urn
and attempted to infer the majority bead color in the urn. After participants were shown each bead, they could
choose to infer the majority bead color in the urn, or draw another bead from the urn. We compared the number of
beads they drew against a Bayesian model and found that participants sampled less evidence than optimal. Also,
when faced with urns that had bead splits closer to chance (60/40 vs. 80/20) or potential monetary losses, participants increased their evidence seeking, but by an amount less than that predicted by the ideal observer model.
When participants decided to infer the majority bead color, rather than draw additional beads, we found activity in
insula, striatum, anterior cingulate and parietal cortex. The parietal responses were greater for participants who
sought more evidence on average and for participants who more increased their evidence seeking when draws
came from urns with ratios closer to chance ratios. Parietal cortex and insula were associated with potential monetary loss. Insula responses showed modulation with estimates of the expected gains of urn choices. Our findings
show that participants sought less evidence than predicted by an ideal observer model and their evidence-seeking
behavior may relate to information communicated by insula and parietal cortex. In addition to the fMRI experiment,
we also compared evidence seeking across several patient groups characterized as impulsive. The beads task
proved to be highly sensitive to impulsivity. We found that drug abusers, pathological gamblers, and PD patients
with dopamine induced impulse control disorders all drew significantly less than control groups.

T-32. Shaping neural circuits by early experience
Takao Hensch1,2

TAKAO. HENSCH @ CHILDRENS . HARVARD. EDU

1 Harvard

University
2 Childrens’ Hospital, Boston
Early life experience potently shapes brain function and adult behavior. The biological bases underlying these
windows of plasticity are increasingly being resolved in the developing mouse neocortex. This talk will cover
core concepts of “critical periods” across sensory systems (visual, auditory, cross-modal). Pioneering the use of
a molecular/genetic approach, we revealed that specific GABA circuits orchestrate the functional and structural
rewiring of neural networks during sensory cortical development. Consequently, shifting excitatory-inhibitory (E-I)
circuit balance or lifting a variety of molecular ‘brakes’ enables plasticity even beyond the early critical period.
Understanding these events offers novel biomarkers of maturational milestones and deeper insight into neurodevelopmental disorders and therapeutic strategies in humans.

40

COSYNE 2012

T-33 – T-34

T-33. Patterns of activity initiated by individual photoreceptors in primate
retina
Peter Li1
Greg Field1
Martin Greschner1
Lauren Jepson1
Deborah Gunning2
Keith Mathieson3
Alan Litke2
EJ Chichilnisky1

PETERLI @ SALK . EDU
GFIELD @ SALK . EDU
GRESCHNER @ SALK . EDU
LAHRUBY @ GMAIL . COM
D. GUNNING @ PHYSICS . GLA . AC. UK
KEITH . MATHIESON @ STRATH . AC. UK
ALAN . LITKE @ CERN . CH
EJ @ SALK . EDU

1 Salk

Institute for Biological Studies
of California, Santa Cruz
3 University of Strathclyde
2 University

Purpose: The elementary input to a sensory system is the activation of a single sensory receptor. However, few
direct measurements reveal how this signal diverges to produce patterns of activity in downstream neural circuits,
nor how signals initiated in different receptors interact. We sought to understand how visual signals initiated in individual cone photoreceptors propagate and interact in the circuitry of the primate retina. Approach: Extracellular
recordings were performed simultaneously from hundreds of retinal ganglion cells (RGCs) of identified types in
isolated primate retina. High resolution white noise stimuli and reverse correlation were used to map RGC receptive fields, revealing the functional inputs of complete populations of cone photoreceptors to complete populations
of RGCs of several types simultaneously. Punctate visual stimuli were then used to stimulate single cones in isolation, or two cones simultaneously. Results: Visual stimulation of a single cone produced robust firing in RGCs of
two or more major types simultaneously. Surprisingly, midget (parvocellular-projecting) RGCs responded at least
as strongly to single cone stimulation as did parasol (magnocellular-projecting) RGCs, suggesting that previous
reports of greater contrast sensitivity in parasols may merely reflect the larger number of cones in their receptive
fields. The relative strength of RGC responses to stimulation of two cones individually was not fully consistent with
the predictions of a linear model of light response implicit in the receptive field. This nonlinearity was confirmed
by failures of additivity in response to stimulation of pairs of cones, for both midget and parasol cells.

T-34. Multiple spectral inputs improve motion discrimination in the Drosophila
visual system
Mikko Juusola1
Trevor J Wardill1
Olivier List1
Xiaofeng Li1
Sidhartha Dongre1
Marie E McCulloch1
Chun-Yuan Ting2
Cahir O’Kane3
Shiming Tang4
Chi-Hon Lee2
Roger Hardie3

M . JUUSOLA @ SHEFFIELD. AC. UK
TWARDILL @ MBL . EDU
OLIVIERLIST @ GMAIL . COM
XIAOFENG . LI @ SHEFFIELD. AC. UK
RUINA . PARTUM @ GMAIL . COM
MARIEMCCULLOCH @ HOTMAIL . COM
TINGCH @ MAIL . NIH . GOV
C. OKANE @ GEN . CAM . AC. UK
TANGSHM @ SUN 5. IBP. AC. CN
LEECHIH @ MAIL . NIH . GOV
RCH 14@ CAM . AC. UK

1 University

of Sheffield
Institutes of Health
3 University of Cambridge
4 Beijing University
2 National

From humans to insects, colour and motion information are thought be channelled through separate neural path-

COSYNE 2012

41

T-35
ways for efficient visual processing, but it remains unclear if and how these pathways interact in improving perception of moving coloured stimuli. Such computations are hard to investigate in any species, because the spectral
sensitivities of the colour and motion channels typically overlap extensively. This interference makes it difficult to
activate one pathway without activating the other, as needed for assessing the independence of their individual
parts. To overcome this problem, we used two genetic approaches, complemented by intracellular electrophysiology, 2-photon optical imaging of genetically encoded reporters and behaviour.
Firstly, we generated “UV-flies”, where the blue-green opsin of R1-R6 photoreceptors was genetically replaced
with an ultra-violet opsin, abolishing the spectral sensitivity overlap between the R1-R6 and R8 channels. Thus,
their R8s could be independently excited by long-wavelength light (>460 nm), while R1-R6s should only respond
to UV, allowing us to directly examine whether the early motion and colour channels crosstalk at different processing stages in the fly visual system.
Secondly, by using photoreceptor-specific genetic rescue of a blind transduction mutant we generated flies in
which only one spectral class of photoreceptor was light-sensitive. By repeating this for R1-R6 and each of the
four spectral classes of R7 and R8, we were able to uniquely test the contribution of each spectral class in turn.
Using these approaches we were able to directly and unequivocally demonstrate that colour photoreceptors
(R7/R8) feed directly into the motion pathway at an early stage, and that this significantly improves their optomotor behaviour performance in a flight simulator system. Our results disprove the 40-year old dogma that
motion vision in flies is mediated exclusively by the single spectral class, R1-R6 photoreceptors, changing our
understanding of how motion vision operates in insects.

T-35. Visual attention increases synaptic efficacy in thalamocortical circuits
Farran Briggs1
George R Mangun2
W. Martin Usrey2

FARRAN . BRIGGS @ DARTMOUTH . EDU
MANGUN @ UCDAVIS . EDU
WMUSREY @ UCDAVIS . EDU

1 Dartmouth
2 University

Medical School
of California, Davis

Visual attention modulates neuronal activity across visual cortical areas, however the mechanisms by which attention alters neuronal activity remain elusive. In order to ascertain potential mechanisms of attentional modulation,
we examined whether attention enhances the efficacy of synaptic communication at the earliest stage of cortical
processing—the thalamocortical synapse. Specifically, we asked whether attention changes the probability of
a postsynaptic spiking response in primary visual cortex (V1) given a presynaptic input from the lateral geniculate nucleus of the thalamus (LGN). In two alert monkeys, we first identified pre- and postsynaptic neurons by
electrically stimulating the LGN and recording extracellular postsynaptic responses in layer 4C of V1. We then
measured neuronal responses to drifting, sinusoidal gratings to determine the visual physiology of identified postsynaptic neurons, allowing us to identify neurons as recipients of magnocellular or parvocellular stream inputs. We
measured the efficacy of electrically evoked LGN activity in driving postsynaptic cortical responses while animals
performed an attention task to assess attentional modulation of thalamocortical communication. Animals were
instructed to attend to one of two drifting gratings—one within and one outside the receptive field of the recorded
neuron—and to report the occurrence of a contrast change in the attended grating. Importantly, electrical stimulation was delivered prior to the contrast change (during the peak attentional window); shocks occurred in both trial
conditions (attend toward and attend away); and stimulation levels were set such that less than half of the shocks
evoked postsynaptic spikes. Results of recordings from 61 postsynaptic neurons in V1 demonstrate a significant
enhancement of thalamocortical synaptic efficacy with attention. Both magnocellular- and parvocellular-recipient
neurons in layer 4C demonstrate this attention-mediated increase in synaptic efficacy. Interestingly, the synaptic
efficacy effect is independent of the influence of attention on the overall firing rate of postsynaptic neurons in V1.

42

COSYNE 2012

T-36 – T-37

T-36. Mechanisms and consequences of transforming dense codes to sparse
codes in the auditory system
Sarah Woolley

SW 2277@ COLUMBIA . EDU

Columbia University
Processing communication signals is a crucial, natural function of the brain’s sensory systems. Sensory systems
generate neural representations of external stimuli that lead to perception and guide behavior. A shared feature
among sensory systems is the hierarchical flow of information from periphery to higher cortex through multiple
processing stations. An important part of understanding how sensory processing leads to perception is explaining
the mechanisms whereby neural representations of complex sensory signals such as communication sounds
transform along sensory coding pathways. The auditory system is unique with regard to the large number of
distinct processing stations involved in the coding of sensory stimuli. It is therefore advantageous for studying
hierarchical signal transformations from brainstem to cortex. We study the hierarchical auditory coding of complex
vocal communication sounds in the songbird. I will present our work on the song coding properties of single
neurons in the midbrain, primary forebrain and higher forebrain. We have examined coding of individual songs
presented in auditory scenes and alone. We observe subtle but consistent differences in the dense and nonselective coding of song between the midbrain and primary forebrain. In contrast, we observe a transition from a
dense coding scheme to a sparse and highly selective coding scheme between the primary and higher forebrain,
as little as one synapse away. We present a model for the transformation of song coding from dense to sparse
and selective, and the consequences of dense versus sparse codes for the neural extraction of a target signal
from a complex auditory scene.

T-37. Changes in cortico-striatal connectivity strength during flexible soundaction associations in rats
Santiago Jaramillo1
Petr Znamenskiy1,2
Anthony Zador1
1 Cold

JARA @ CSHL . EDU
ZNAMENSK @ CSHL . EDU
ZADOR @ CSHL . EDU

Spring Harbor Laboratory
School of Biological Sciences

2 Watson

Our ability to adapt to different environments often depends on modifying the actions associated with a given
stimulus. To study the neuronal mechanisms underlying flexible associations between sounds and actions, we
developed a two-alternative choice paradigm for rats in which the meaning of some sounds (the location of reward associated with these sounds) changed several times within a behavioral session. Trained rats reached
asymptotic performance in less than 40 trials after each change.
What brain circuits mediate rapid changes in the behavioral responses associated with a given sensory stimulus?
We found that the activity of neurons from both the auditory cortex and the auditory striatum of behaving rats was
modulated not only by sound identity but also by the animal’s action, with striatal neurons displaying a stronger
dependence on action than cortical neurons. We therefore hypothesized that changes in sound-action association
may be mediated by changes in functional connectivity between the auditory cortex and the striatum.
To test this hypothesis, we developed a method for estimating the strength of connections between these brain
areas in the behaving animal. After expressing channelrhodopsin-2 (ChR2) in auditory cortical neurons, we photostimulated the axons of these neurons that project to the striatum. The release of neurotransmitter from these
axons generates postsynaptic potentials that can be measured extracellularly, and whose magnitude reflects the
strength of the cortico-striatal connections. Using this method, we found changes in connectivity between the auditory cortex and its striatal target that were correlated with changes in sound-action associations. These results
support the hypothesis that the auditory cortico-striatal pathway mediates flexible associations between sounds
and actions.

COSYNE 2012

43

T-38 – T-39

T-38. Emergence of pitch from natural sound statistics in a hierarchical, dualpathway sparse coding model
Vivienne Ming1,2
Engin Bumbacher3

NEURALTHEORY @ SOCOS . ME
ENGIN . BUMBACHER @ GMAIL . COM

1 University

of California, Berkeley
LLC
3 École Polytechnique Fédérale de Lausanne
2 Socos

The neural basis of pitch perception, our subjective sense of the tone of a sound, has been a great ongoing debates in neuroscience. Variants of the two classic theories—spectral Place theory and temporal Timing theory—
continue to drive new experiments and debates (Shamma, 2004). Here we approach the question of pitch by
applying a theoretical model based on the statistics of natural sounds. Motivated by gist research (Oliva and Torralba, 2006), we extended the nonlinear hierarchical generative model developed by Karklin & Lewicki (2003) with
a parallel gist pathway. The basic model encodes higher-order structure in natural sounds capturing variations
in the underlying probability distribution. The secondary pathway provides a fast biasing of the model’s inference
process based on the coarse spectrotemporal structures of sound stimuli on broader timescales. Adapting our
extended model to speech demonstrates that the learned code describes a more detailed and broader range of
statistical regularities that reflect abstract properties of sound such as harmonics and pitch than other standard
models. The spectrotemporal modulation characteristics of the learned code better match the modulation spectrum of speech signals than alternate models, and its higher-level coefficients capture information which not only
effectively cluster related speech signals but also describe smooth transitions over time, encoding the temporal
structure of speech signals. Finally, we find that the model produces pitch-related units which combine temporal
and spectral qualities.

T-39. Sound encoding in the neocortex by combinations of discrete activity
patterns in local neuronal ensembles
Brice Bathellier
Lyubov Ushakova
Simon Rumpel

BRICE . BATHELLIER @ IMP. AC. AT
LYUBOV. USHAKOVA @ IMP. AC. AT
RUMPEL @ IMP. AC. AT

Research Institute of Molecular Pathology
It is believed that patterns of activity are the neuronal correlate of the perception of an external stimulus, e.g.
a sound. Here, we used in vivo two-photon calcium imaging to better understand how sounds are encoded at
the level of local layer 2/3 ensembles in the mouse auditory cortex. We found that activity patterns are highly
constrained into few discrete response modes, i.e. that a wide range of different sounds evoke the same population response pattern. Using online synthesis of sound stimuli to map the transition across response modes with
fine resolution, we observed highly non-linear dynamics suggesting attractor-like competition between response
modes. The combination of sounds that were associated in a specific response mode varied across local populations and was not predictable based on the population’s pure-tone frequency tuning. We used linear classifiers
to test whether different pairs of sound could be discriminated on a single trial basis based on population activity.
Global response patterns constructed by the combination of multiple local patterns largely out-performed local
populations for the number of discriminated sound pairs, indicating that much more information is contained at the
global scale. Interestingly, strong dimensionality-reduction by decomposition of single trial patterns into response
modes did not lead to significant information loss. Furthermore, we found that global activity patterns in the
mouse auditory cortex quantitatively predict discrimination and spontaneous categorization of sounds in behaving mice. We suggest a model of auditory cortex function in which local non-linear dynamics shape a broad basis
set of spontaneous, distinct associations of stimuli that form a representation of sounds available for behavioral
decisions.

44

COSYNE 2012

T-40 – T-41

T-40. A unified neuronal population code fully explains human object recognition
Najib Majaj1
Ha Hong1,2
Ethan Solomon1
James DiCarlo1

MAJAJ @ MIT. EDU
HAHONG @ MIT. EDU
ESOLOMON @ MIT. EDU
DICARLO @ MIT. EDU

1 Massachusetts
2 Harvard

Institute of Technology
University

Our goal is to understand the neuronal mechanisms that underlie human visual object recognition (OR). While
previous work has argued for qualitative links between neuronal responses in the ventral visual stream and human
shape judgements, no study has asked which, if any, neuronal responses are quantitatively sufficient to explain
broad domain human OR performance. The shift from qualitative to quantitative hypotheses requires a framework
to link neuronal responses to behavior (“unified code”). Here we ask: is there a common neuronal basis (e.g., in IT
cortex) and a simple (e.g., linear) transformation that will predict all of human OR performance? We first defined
OR operationally by obtaining human psychophysical measurements using images that explore shape similarity
and identity preserving image variation, resulting in OR benchmarks that span a range of difficulty. Using the same
visual images, we measured neuronal responses in V4 and IT in two monkeys. We implemented 14 unified codes
based on those neuronal data and computed cross-validated neuronal discriminability indices (d’s) to compare
to the human d’s. The dynamic range across those d’s sets a high bar for when a putative code is sufficient to
explain behavior: it is not sufficient for a code to perform well (high d’) or to match one d’. Instead, a sufficient
unified code must also emergently predict the entire pattern of behavior over all tasks. Remarkably, we found
a few unified IT-based codes that meet this high bar. Interestingly, many other IT codes and all V4 codes are
insufficient. While humans outperform computer vision systems on many of our OR tasks, their abilities reliably
depend on the images tested. These dependencies in human performance are fully explained by a simple, unified
reading of monkey ventral stream neurons, a feat unmatched by any computer vision system we tested.

T-41. Linking physiology and perception in V2
Jeremy Freeman1
Corey M Ziemba1
Tony Movshon1
Eero P Simoncelli1,2
1 New

FREEMAN @ CNS . NYU. EDU
ZIEMBA @ CNS . NYU. EDU
MOVSHON @ NYU. EDU
EERO. SIMONCELLI @ NYU. EDU

York University

2 HHMI

Area V2 is the largest extrastriate visual area, but its functions remain enigmatic. We used experimental stimuli
generated from a hierarchical model of natural image structure (Portilla and Simoncelli, 2000) to differentiate the
responses of neurons in V2 from those in V1, and to explore their role in perception. The model has two stages:
the first captures local orientation and spatial frequency content with rectified responses of oriented filters, and
the second computes correlations between these responses at different orientations, frequencies, and positions,
capturing naturalistic features found in visual textures. Starting from original texture images (leaves, fabric, bricks,
etc) we computed model responses at both stages, and generated pairs of novel images that produce the same
average responses at just the first stage, or both stages. Only the latter contain complex naturalistic features.
We measured responses of V1 and V2 neurons in anesthetized macaque to these pairs of synthetic images.
In V1, the images within each pair yielded similar single-unit responses, suggesting that they are approximately
matched with respect to neural sensitivities in V1. In contrast, firing rates in V2 were reliably higher for the second
(naturalistic) image in each pair (64% of single units in V2, compared to 10% in V1). fMRI responses to these
image pairs also reliably differentiate V2 from V1 in humans. To relate these differential responses to perception,
we asked human observers to discriminate between images drawn from the two ensembles. Strikingly, the texture

COSYNE 2012

45

T-42 – T-43
categories for which perceptual discrimination is best are those that exhibit the largest differential responses, both
in macaque V2 neurons and human fMRI. We conclude that neuronal responses in V2, but not V1, are sensitive to
the joint statistics of local structure found in natural images, and may reflect an emerging specialization for these
features in the extrastriate visual pathway.

T-42. Visual Coding in the primary visual cortex is enhanced during active
navigation
Aman Saleem1
Asli Ayaz1
Kenneth Harris2
Kate Jeffery1
Matteo Carandini1

AMAN . SALEEM @ UCL . AC. UK
ASLI @ CARANIDNILAB . NET
KENNETH . HARRIS @ IMPERIAL . AC. UK
K . JEFFERY @ UCL . AC. UK
MATTEO @ CARANDINILAB . NET

1 University
2 Imperial

College London
College London

The sequence of visual stimuli that an animal encounters in nature is largely determined by the animal’s navigation through the environment. Yet, responses of visual cortex are typically studied in stationary animals, using
pre-defined visual stimuli independent of behavior. Does the visual cortex respond similarly to these stimuli as to
stimuli generated by the animal’s navigation? We recorded from primary visual cortex (V1) of mice that navigated
a virtual environment. Head-fixed mice walked on an air-suspended ball, whose movements were captured to
update the visual scene. The environment was a 2.5 m long virtual track with grating and plaid textures on the
floors, ceilings and walls. We recorded from 203 V1 neurons in 5 mice using multisite probes (NeuroNexus). Mice
traversed the track 20 times, running 88% of the time (average speed 15 cm/s). We compared V1 responses during active navigation of the virtual reality environment with responses to a playback of the same stimuli, unrelated
to the mouse’s current movements (open-loop). We asked to what degree the following three factors could predict
(cross-validated) responses: visual scene (determined by position in the room), visual velocity, and running velocity. During active navigation the latter two are equal. Prediction of responses by both visual scene and velocity
was enhanced during active navigation compared to the open-loop condition. During active navigation, responses
could be better predicted based on velocity than on visual scene. In the open-loop condition, responses were predicted best by running velocity and least by visual velocity. Responses did not appear to be coding for a simple
mismatch between running and visual velocities, but rather depended on both running speed and visual speed.
These observations suggest that primary visual cortex contains mechanisms that are specialized to process the
information that arises from active navigation.

T-43. Linear and non-linear receptive fields for optimal disparity estimation in
natural stereo-images
Johannes Burge
Wilson S Geisler

JBURGE @ MAIL . CPS . UTEXAS . EDU
GEISLER @ PSY. UTEXAS . EDU

University of Texas at Austin
Many animals make use of the disparities between the images in the two eyes to judge distance, depth, and
shape. Despite vast neurophysiological, psychophysical, and computational literatures on disparity processing
there is no formal theory for how disparities should be estimated (i.e. how to solve the correspondence problem). The absence of such a theory makes it difficult to interpret response properties of binocular neurons and
behavioral performance in disparity estimation and discrimination tasks. Here, we present a formal theory of optimal disparity estimation in natural scenes. Using a set of natural scenes, a model of the human vision system’s
optics and photoreceptors, and Bayesian statistical methods, we determine the binocular receptive fields (filters)

46

COSYNE 2012

I-1 – I-2
that maximize accuracy in disparity estimation. These linear filters bear strong similarities to the widely reported
receptive fields of binocular simple cells in primate and cat. The theory also specifies how the filter responses
should be combined to obtain nonlinear filters that encode the posterior probabilities of particular disparities. The
combination rules are biologically plausible, and the tuning characteristics of the resulting nonlinear filters are
similar to those of binocular complex cells in primate visual cortex. Optimal estimation performance (based on
appropriate weighted summation of the nonlinear filter responses) parallels human psychophysical performance.
Additionally, the optimal estimation performance shows modest but systematic improvements over current computational accounts of disparity estimation. Thus, on the basis of an analysis of the information available at the
level of the photoreceptors, we improve upon existing computational methods, and provide a normative account
of a range of established neurophysiological and psychophysical findings.

I-1. Kinematic limitations to texture discrimination by whiskers
Andreas AC Thomik
Aldo Faisal

ANDREAS . THOMIK 10@ IMPERIAL . AC. UK
ALDO. FAISAL @ IMPERIAL . AC. UK

Imperial College London
Rodents discriminate texture with a sensitivity which rivals that of human touch by moving their whiskers across
surfaces, despite the sparsity of contact points. The mechanisms underlying this perceptual feat are not well
understood, and its reliability has not been thoroughly investigated. Therefore, we studied the physical limits set
by the mechanical properties of the whisker to discriminate textures. To this end we use high-speed videography
to track the vibrations of different whiskers while sandpaper of different grades is moved across them under
naturalistic conditions. Whisker motion is analysed with respect to high-speed events which have been suggested
previously to be prominent features of texture-induced vibrations. A physically realistic model of the whisker,
combining both its mechanical properties and features from the stimulus, is subsequently used to model the
response at the base of the whisker. Based on an ideal observer model, we can predict the rodent psychophysical
response curve for the texture discrimination tasks. This curve reflects the sensory properties of whiskers and
their ability to distinguish textures. In the absence of further sensory processing, this curve sets an upper limit
to rodents’ behavioural performance. We can use our approach to directly compare our whisker biomechanical
data to published behavioural experiments (Morita et al. 2011). Our results are based on measurements from 6
different whiskers from mice (C57BL/6) and show that the raw whisker response cannot account for the observed
performance, suggesting active motor-control by the mouse to improve sensory acuity. Control experiments on
rat whiskers are currently under way to avoid trans-species comparison. This is to our knowledge the first study
thoroughly investigating the quality of whiskers as texture sensors and provides a baseline against which the
behavioural acuity of rodents can be compared in order to e.g. assess the effectiveness of neural computations
in the barrel cortex.

I-2. Fast encoding model estimation via expected log-likelihoods
Alexandro Ramirez
Liam Paninski

ADR 2110@ GMAIL . COM
LIAM @ STAT. COLUMBIA . EDU

Columbia University
Receptive fields are critical to our understanding of neuronal tuning properties and are traditionally measured
using the spike-triggered average (STA). Recent work has shown that the STA is a special case of a family of
estimators derived from the “expected log-likelihood” of a Poisson model (Park and Pillow, 2011; Paninski, 2003).
We generalize these results to the broad class of neuronal response models known as generalized linear models
(GLM), and develop fast iterative methods for including spike-history and interneuronal interaction effects. The
basic idea is that the GLM log-likelihood may be written as a large sum over all observed stimuli. This sum

COSYNE 2012

47

I-3 – I-4
can often be well approximated by an expectation. This expectation, in turn, can often be computed analytically,
drastically reducing the computational complexity of GLM estimation. For example, in the Poisson setting, the
expected log-likelihood can be expressed as a simple quadratic function in some cases, leading to a surprisingly
simple linear optimal estimator. We discuss a number of applications of this basic idea. First, we show that
MAP estimation of the GLM parameters can be sped up by orders of magnitude under some simple conditions
on the priors and likelihoods involved. Second, we perform a risk analysis, using both analytic and numerical
methods, and show that the “expected log-likelihood” estimators come with a minimal cost in accuracy (and in
some cases even improved accuracy) compared to standard MAP estimates. Third, we find that these methods
can significantly decrease the computation time of marginal likelihood calculations for model selection and of
Markov chain Monte Carlo methods for sampling from the posterior parameter distribution. We use the multi-unit,
primate retinal responses to stimuli with naturalistic correlation to validate our findings.

I-3. Unconstrained Gaussian mixture models: the best models for natural
image statistics?
Daniel Zoran
Yair Weiss

DANIEZ @ CS . HUJI . AC. IL
YWEISS @ CS . HUJI . AC. IL

Hebrew University of Jerusalem
Modeling the statistics of natural image patches has been of great interest in recent years. Such models may
shed light on many of the problems encountered in vision, neuroscience and image processing. While many of
these models differ in their motivation, learning, and statistical structure they have much in common. In a recent
work we have observed that a simple Gaussian Mixture Model (GMM) learned from natural image patches is able
to outperform several current models of natural images both in log likelihood and image restoration performance.
This is indeed a surprising result! These results motivate several questions: How does the GMM model compare
to cutting edge models of natural image patches? What makes the GMM model such a good performer? What
kind of constraints or properties in the other models hinders their performance? We train an unconstrained GMM
with zero means and full covariance matrices on a set of natural image patches. We learn both 8x8 and 16x16
models, with a varying number of mixture components, ranging from 1 to several hundreds. Likelihood increases
with the number of components, but seems to converge at a couple of hundreds. We show that the resulting
model (with ∼200 components) is able to outperform current models in terms of likelihood. The resulting model
has zero means and full rank covariances which capture interesting properties of natural images such as textures,
oriented textures, texture boundaries and edges. We suggest that this model can be related to structured sparse
coding and to mixtures of elliptical contour distributions.

I-4. Biophysically accurate inhibitory interneuron properties in a sparse coding network
Mengchen Zhu1,2
Bruno Olshausen3
Chris Rozell1

MCZHU @ GATECH . EDU
BAOLSHAUSEN @ BERKELEY. EDU
CROZELL @ GATECH . EDU

1 Georgia

Institute of Technology
University
3 University of California, Berkeley
2 Emory

While theoretical models based on the efficient coding of natural scene statistics can account for some aspects
of biological sensory systems (e.g., receptive field structure, non-classical response properties), these models
are rarely expressed in terms of biophysically realistic mechanisms. In particular, these types of models often
implement excitation and inhibition in the same model neuron, and the unique properties of inhibitory interneu-

48

COSYNE 2012

I-5
rons are often ignored. In reality, inhibitory interneurons exhibit a variety of baffling properties: there are both
orientation tuned and untuned inhibitory interneurons in cat and rodent visual cortex (Lee and Reid 2011), while
the cumulative inhibitory conductance in the excitatory cells is reported to be tuned (Anderson et al. 2000).
Furthermore, there are many fewer inhibitory cells than excitatory cells (Meyer et al. 2011) despite the overall
inhibition and excitation being balanced and co-tuned (Alitto and Dan 2010). In this study, we aim to bridge the
gap between the complex biophysical reality and the abstract theoretical model of sparse coding. Starting with a
network that implements sparse coding (based on Rozell et al. 2008), we show utilizing inhibitory sub-populations
in the implementation results in the following observations: (1) The excitatory to inhibitory cell ratio may reflect the
overcompleteness of the representation and the low rank structure of the receptive field matrix. These structures
can be exploited by a low rank plus sparse decomposition (Lin et al. 2010) of the network recurrence matrix;
(2) Both orientation tuned and untuned inhibitory interneurons emerge as a result of such decomposition; (3)
Balanced and co-tuned E/I inputs to principal excitatory cells can be explained by the co-alignment of the ON and
OFF regions in their feedforward receptive field.

I-5. Sparse coding model of binocular receptive field development reproduces
changes in abnormal rearing
Jonathan Hunt1
Peter Dayan2
Geoffrey Goodhill3

JJH @42 QUARKS . COM
DAYAN @ GATSBY. UCL . AC. UK
G . GOODHILL @ UQ . EDU. AU

1 Queensland

Brain Institute
College London
3 University of Queensland
2 University

Some of the best evidence for the importance of sensory input in the development of receptive fields in the visual
system comes from the effect of abnormal rearing conditions. For example, stripe-reared (Stryker et al., 1978),
strabismic (Hubel & Wiesel, 1965) or partially monocularly reared animals (Mitchell et al., 2006, 2009) all develop
markedly different receptive fields to those of conventionally reared subjects. By contrast, most modelling of the
unsupervised development of receptive fields has characterised normal development. Here, we examine the capacity of one of the more powerful approaches to unsupervised development, sparse coding (Olshausen & Field,
1996), to account for the effects of abnormal visual input under 6 different rearing conditions. A number of abnormal rearing conditions involve differing manipulations to each eye, however, most existing sparse coding models
of receptive field development have considered only monocular receptive fields for simplicity. We extend this work
to consider binocular receptive fields. In addition to allowing comparison with abnormal rearing conditions, this
exposes some important changes in representation due to binocular receptive fields. In particular, asymmetries
in inter-ocular correlation between orientations leads to orientation-specific receptive field properties. We find
that sparse coding models retrodict the qualitative changes in receptive fields found in abnormally reared animals
for all six modified rearing conditions considered. This reinforces the validity of sparsity as a principle of early
sensory coding and provides evidence that V1 receptive fields are, at least in part, optimised during development.
Additionally, in agreement with predictions of Li and Atick (1994), we find that vertically oriented neurons develop
with decreased binocularity compared to horizontal orientations.

COSYNE 2012

49

I-6 – I-7

I-6. Analysing fixations using latent Gaussian fields
Simon Barthelmé1,2
Hans Trukenbrod3
Ralf Engbert3
Felix Wichmann4

SIMON . BARTHELME @ BCCN - BERLIN . DE
HANS . TRUKENBROD @ UNI - POTSDAM . DE
ENGBERT @ UNI - POTSDAM . DE
FELIX . WICHMANN @ UNI - TUEBINGEN . DE

1 Technische

Universität Berlin
Center for Computational Neuroscience Berlin
3 University of Potsdam
4 University of Tübingen
2 Bernstein

Although eye movements are often described as arising from one of the simplest decision mechanisms, they
sometimes reflect fairly sophisticated behaviour and are not trivial to predict (Schutz et al., 2011). One important
aspect of eye movement sequences is naturally fixation locations—i.e, where people choose to look. Several
factors are often at work, because where people look will depend on what information they need, on what the
stimulus is, but also on things less directly relevant to the analysis: for example, the common bias for central
locations (Tatler and Vincent, 2009). To understand the strategies at work, one must be able to somehow separate
these different factors. We formulate a framework based on techniques borrowed from Functional Data Analysis
(Ramsay and Silverman, 2005) and Spatial Statistics (Moller et al., 1998). We use Latent Gaussian Fields to
describe conditional fixation distributions, adapting the Logistic Gaussian Process of Lenk (1988). To introduce
some regularity we assume that fixation distributions do not vary completely freely but are functions of some
known variables, based on the stimulus or on trial history. The use of log-additive decompositions lets one
separate out the various factors at work. We show that our framework is extremely useful for the analysis of
saliency in natural images. It is customary to analyse the role of low-level saliency by focusing on the properties
of regions that are empirically salient, i.e. regions that subjects often look at. However, we know that people tend
to fixate the same locations regardless of what the image is, and make small saccades rather than large ones.
The implication is that not all fixations signal the same level of intrinsic saliency. Using a dataset collected by
Kienzle et al. (2009), we illustrate how intrinsic saliency can be inferred using our framework.

I-7. Semantic organization of a neural population codebook and accurate decoding using a neural thesaurus
Elad Ganmor1
Ronen Segev2
Elad Schneidman1
1 Weizmann

ELADGM @ GMAIL . COM
RONENSGV @ BGU. AC. IL
ELAD. SCHNEIDMAN @ WEIZMANN . AC. IL

Institute of Science
University of the Negev

2 Ben-Gurion

Noise in neural circuits, from synaptic unreliability to variable responses of neurons to repeated stimuli, determines
how neural populations encode information. The noisy nature of the code implies that to decode neural population
activity patterns we must understand which activity patterns are similar and which are different—in terms of
the information they carry. This is especially important for decoding responses of large populations since the
number of potential activity patterns is exponentially large, and most patterns are rarely seen. To characterize the
“semantic” organization of the codebook of large neural populations, we learned a “thesaurus” for the responses
of a neural population. We defined the similarity of population activity patterns (words) Ri and Rj, as the similarity
of the distributions over stimuli that may elicit them, P(S|Ri) and P(S|Rj). These distributions were estimated using
Bayes’ rule from the encoding dictionaries, P(R|S), which were inferred from repeated presentations of a large
set of stimuli. We learned the similarity between code-words of large groups of retinal ganglion cells, presented
repeatedly with artificial and naturalistic movies. Because of strong “noise correlations” in large populations,
and to overcome sampling issues, we used maximum entropy models for P(R|S). We found that the population
codebook is organized in clear clusters of words with similar meanings. Importantly, the codebook organization

50

COSYNE 2012

I-8 – I-9
is significantly different than the one given by the hamming distance between words. We further found that if we
were to replace each code-word with the identity of the cluster it belongs to, this would still retain most of the
information about the stimulus. The neural thesaurus further allowed us to accurately decode novel stimuli from
population responses we did not observe before, based on their similarity to other, known, responses.

I-8. Internal model estimation for feedback control in brain-computer interfaces
Matthew Golub
Steven Chase
Byron Yu

MGOLUB @ CMU. EDU
SCHASE @ CMU. EDU
BYRONYU @ CMU. EDU

Carnegie Mellon University
The motor system effortlessly plans and executes sophisticated movements despite sensory feedback delays and
non-stationary motor plant dynamics. Behavioral studies suggest that internal models of the plant are central to
motor control, but neurophysiological evidence of such models has been limited. We developed a probabilistic
framework for extracting a subject’s internal forward model of the motor plant from recorded neural activity. We
applied our framework to brain-computer interface (BCI) experiments, where a rhesus monkey acquired visual
targets with a BCI cursor. Cursor velocity was linearly decoded from spike counts recorded with a 96-electrode
Utah array implanted in motor cortex. Using only the recorded neural activity, visual feedback, and target locations,
we extracted the subject’s internal forward model and its predictions–the subject’s internal estimates of the cursor
position at each 30ms decoder time step. Inferred internal state estimates reveal that the subject often believed the
BCI cursor was moving straight to the target, even when the actual cursor did not. Across 20 BCI experiments,
these models explain roughly 50% of angular errors between cursor movement direction and cursor-to-target
direction. BCI is an attractive paradigm for investigating motor control because it provides experimenters the
opportunity to monitor activity of all neurons that drive the prosthetic plant, to define plant dynamics as linear and
low-dimensional, to observe all relevant sensory feedback, and to design task goals. While we are also interested
in developing assistive BCI systems, our immediate goal is developing a statistical framework for basic scientific
studies of the neural basis of motor control and learning. By enabling quantitative identification of internal models
and comparison of identified models at multiple time points, we expect to gain insight into the neural mechanisms
that drive internal model formation and adaptation.

I-9. Circuit- and systems-level contributions to successful memory retrieval
in the hippocampus
Cristina Savin1
Peter Dayan2
Mate Lengyel1
1 University
2 University

CS 664@ CAM . AC. UK
DAYAN @ GATSBY. UCL . AC. UK
M . LENGYEL @ ENG . CAM . AC. UK

of Cambridge
College London

Two critical problems plague neural implementations of recurrent auto-associative memory networks, such as
area CA3 of the hippocampus. First, as stressed by Fusi et al (2005), synapses only have limited dynamic
ranges. Memory thus has a palimpsest character, with traces degrading as new patterns are stored. Second,
synapses sharing pre- or post-synaptic partners are significantly correlated (Song et al., 2005). This oft-ignored
fact severely complicates recall, as the information stored in a synapse can only be interpreted correctly in light
of the strengths of other synapses to the same neuron. Here, we suggest systems- and circuit-level solutions to
these problems in the context of the hippocampus. First, since traces degrade over time, pattern age needs to
be considered for successful recall. Age is a form of unfamiliarity, a quantity that is known to be reflected in the

COSYNE 2012

51

I-10 – I-11
activity of a subpopulation of perirhinal neurons. We show that a dual system, combining the hippocampus (for
recollection) and perirhinal cortex (for familiarity) can provide an efficient solution for this problem (Savin et al.,
2011). Second, at the circuit level, (approximately) optimal retrieval dynamics predict a close link between CA3
synaptic plasticity and homeostatic mechanisms reported in this region (Zhang & Linden, 2003), and stabilization
from feedback inhibition. Homeostatic mechanisms also compensate for the inter-synaptic correlations. Overall,
our results provide an unifying view of various aspects of hippocampal circuitry, offering high-level solutions for
synapse-level concerns.

I-10. How plastic is the “slow speeds prior”?
Grigorios Sotiropoulos1
Aaron Seitz2
Peggy Series1
1 University
2 University

G . SOTIROPOULOS @ SMS . ED. AC. UK
ASEITZ @ UCR . EDU
PSERIES @ INF. ED. AC. UK

of Edinburgh
of California, Riverside

Perception is commonly viewed as a form of Bayesian inference, where noisy or ambiguous sensory evidence
(“likelihood”) is combined with expectations (“prior”) about the world. A well-known example of such prior is that
our visual system expects that objects are static or move slowly rather than quickly. This “slow speed” prior was
postulated because it could elegantly explain a number of perceptual biases. Interestingly, those biases affect not
only the perception of speed but also the direction of motion. For example, the direction of a line whose endpoints
are hidden (as in the “aperture problem”) or poorly visible is more often perceived as being perpendicular to the
line than it really is—an illusion consistent with expecting that the line moves more slowly than it really does.
Is this prior rigid or plastic? We asked two groups of subjects to report the direction of a field of parallel lines
that moved either perpendicularly or obliquely to their orientation, as we varied stimulus contrast and duration.
One group was exposed, during a “training” phase, to high speeds (8 deg/s), the other to lower speeds (4 deg/s)
and the experiment was conducted over 5 sessions on consecutive days. Consistent with previous studies,
initial perception of motion direction was biased towards perpendicular judgments at low contrast for both groups.
However, for the high-speed group, the initial bias gradually diminished until the illusion reversed and direction
was most often perceived as being oblique. This suggests that systematic exposure to high-speed stimuli can
change the prior towards favouring higher speeds. Our results can be accounted for by the Bayesian model of
Weiss et al (2002), when extended to allow the speed prior to have a nonzero mean. In the best-fit model, the
mean increases approximately linearly across experimental sessions.

I-11. Hamiltonian Monte Carlo sampling and oscillatory activity in V1
Gergo Orbán1
Laurence Aitchison2
Mate Lengyel1
1 University
2 University

GO 223@ CAM . AC. UK
LAURENCE . AITCHISON @ GMAIL . COM
M . LENGYEL @ ENG . CAM . AC. UK

of Cambridge
College London

Bayesian computations have been implicated in a range of cognitive phenomena but their neural underpinning
remains unknown. In order for the brain to implement Bayesian inference it needs to represent probability distributions over features of the environment. One way to achieve such a representation is to use a temporal sequence of
activities in a network of neurons to represent stochastic samples from the distributions (Fiser et al, 2010). While
this approach can account for a wide range of stimulus-dependent changes in neural response variability and
co-variability in the visual cortex (Orban & Lengyel, 2011), it has been incomplete at least in two aspects. First,
it accounted only for variability across trials and over long time scales (>50ms), and second it left the key imple-

52

COSYNE 2012

I-12 – I-13
mentational level question unanswered: what kind of neural circuit dynamics may be able to implement sampling?
Here, we investigate a form of neural dynamics that can generate samples from probability distributions that are
appropriate for V1 (Schwartz & Simoncelli, 2001). These dynamics amount to a deterministic approximation to
a class of stochastic sampling schemes, Hamiltonian Monte Carlo (Neal, 1996), and are readily implementable
by standard E-I networks (Wilson & Cohen, 1972). We demonstrate that these deterministic dynamics can effectively approximate stochastic sampling on time scales relevant to neural computations and give a functional
interpretation to inhibitory interneurons as representing useful “auxiliary” variables to speed up the convergence
(in distribution) of the sampler. The model accounts for several short-time scale physiological phenomena, including the time scale of decorrelation of membrane potential traces and the increased dominance of high-frequency
oscillations during evoked compared to spontaneous activity. Our results also suggest that particular forms of
neural synchrony may simply be signatures of sampling from special cases of noise correlations and modeled
within the same framework.

I-12. Sparse and expansive representations in network models of associative
memory
Baktash Babadi1
Haim Sompolinsky2
1 Harvard
2 Hebrew

BBABADI @ FAS . HARVARD. EDU
HAIM @ FIZ . HUJI . AC. IL

University
University of Jerusalem

Several brain areas exhibit expansive transformation of signals in which the downstream population is much
larger than the number of incoming axons. Often, these transformations result in a neural overcomplete and
sparse representation. Examples are visual representations in V1, granule cell layer in cerebellum, and odor
coding in piriform cortex. Dimensionality expansion enhances the memory capacity and facilitates classification
of activity patterns by downstream readouts. ICA theories (Olshausen and Field, 1996; Bell and Sejnowski,
1997) and Compressed Sensing theories (Bruckstein et al., 2009; Coulter et al., 2010) have highlighted the
advantage of sparse representations of inherently sparse signals. However, the computational advantage of
spareness in cases where the represented signals are dense is unclear. Here we address this issue by analytical
and numerical investigations of a feedforward associative memory network containing an intermediate layer with
sparse neuronal activity (Baum et al., 1988; Kanerva, 1988). Weights are learnt by versions of Hebb rules. We
ask: (1) is sparseness advantageous even when the memory representations at the input and output layer are
dense? (2) how does the functionality of spare representation depend on the pattern of synaptic projections from
the input layer? To answer these questions, we evaluate the role of dimensionality expansion and sparseness
in memory capacity and pattern completion. Our theory shows that capacity is increased with increasing size of
hidden layer. Pattern completion or error correction in the memory recall are controlled by the degree of sparsity
(fraction of active neurons) in the intermediate layer, with best error correction achieved at limit of low sparsity.
Importantly, sparse coding is not beneficial if synaptic projections are random. Incorporating Hebbian recurrent
connections is studied. Our theory delineates the respective computational benefits of dimensionality expansion
and sparse coding in neural memory systems. Implications for odor coding in piriform cortex are discussed.

I-13. A computational role for noise in the cortex
David Barrett
Peter Latham

BARRETT @ GATSBY. UCL . AC. UK
PEL @ GATSBY. UCL . AC. UK

University College London
Spike trains in the cortex are noisy, irregular and asynchronous (Shadlen and Newsome, 1998). Although the
dynamical mechanisms responsible for generating this noise are known, what is not known is why it’s there. We

COSYNE 2012

53

I-14
show here that it makes at least one computation—orientation selectivity—robust. Dynamic balancing of strong
excitatory and inhibitory synaptic input has been identified as the predominant source of spike train noise (van
Vreeswijk and Sompolinsky, 1996). This balancing is dynamically robust, shielding cortical networks from pathological network dynamics in which neurons never spike or spike incessantly. Accurate tracking of cortical input
and temporal differentiation have been proposed as computations that exploit balanced network dynamics (van
Vreeswijk and Sompolinsky, 1996; Barrett and Latham, 2011). However, noise is an undesirable property in those
computations, and would seem to be an undesirable property in many others. We investigate the computational
role of cortical noise by quantifying the ability of a balanced network to perform orientation selectivity - a simple
computation in which the orientation of a visual stimulus is estimated from network activity. We do this analytically by calculating linear Fisher Information (Barrett and Latham, 2010). We find that information (averaged over
stimulus contrast) is largest when a network is balanced. This result is surprising because we would expect noise
associated with balanced networks to reduce information. Indeed, it does, but across only a narrow range of
contrasts. This exemplifies a performance-stability trade-off: the cost of high performance is a lower operating
range. The trade-off arises because while dynamic balance adds noise, it also maintains the network in a highly
informative state. It thus provides computational stability, just as it provides dynamic stability. Furthermore, we
find, at least in our model, that the contrast invariance of orientation tuned cells is a signature of this computational
stability.

I-14. Quadratic networks for invariant perceptual discrimination
Yoram Burak1
SueYeon Chung1
Haim Sompolinsky2
1 Harvard
2 Hebrew

YBURAK @ FAS . HARVARD. EDU
SCHUNG @ FAS . HARVARD. EDU
HAIM @ FIZ . HUJI . AC. IL

University
University of Jerusalem

Many perceptual tasks require fine discrimination between similar stimuli in the presence of nuisance, or hidden,
parameters. An example is a 2AFC between an oriented bar presented at an angle θ, and a similar bar rotated
clockwise or anti-clockwise (at an angle θ ± δθ). Importantly, the angle θ varies from trial to trial, so that the
discrimination must be invariant to θ. It has long been realized that a single layer neural network, which linearly
weights incoming spikes from sensory neurons, inevitably fails for some values of θ. Thus, single layer networks
are unsuitable for discrimination that involves invariance to a hidden variable. Previous work has shown that
training a two-layer perceptron network resulted in suboptimal performance even in the limit of a large hidden
layer. Thus, it remained unknown what is the minimal neural architecture required for performing such a task in
a Bayes-optimal manner. We consider a decoder that generates decisions by thresholding a quadratic function
of input spike counts. We prove that (1) surprisingly, with suitable choice of parameters, the quadratic decoder
performs optimally on the orientation task; (2) the result extends readily to an arbitrary discrimination task between
similar stimuli, with invariance to one or more hidden variables, and; (3) Bayes-optimality is achieved if the number
of possible independent values of the hidden variable is not greater than the number of sensory neurons. In
addition, we show that a two-layer feed-forward perceptron network can compute the quadratic based decision to
arbitrary precision, and close to optimal performance can be achieved with remarkably small number of hidden
units. Taken together, these results provide constructive proof that a simple neural architecture can perform
complex nonlinear invariant discrimination tasks in a Bayes-optimal manner. We consider several applications,
including motion detection in a patch of a natural visual scene.

54

COSYNE 2012

I-15 – I-16

I-15. Significance of graph theoretic measures in predicting neuronal network
activity
Tuomo Mäki-Marttunen
Jugoslava Acimovic
Keijo Ruohonen
Marja-Leena Linne

TUOMO. MAKI - MARTTUNEN @ TUT. FI
JUGOSLAVA . ACIMOVIC @ TUT. FI
KEIJO. RUOHONEN @ TUT. FI
MARJA - LEENA . LINNE @ TUT. FI

Tampere University of Technology
One of the most prominent patterns of activity observed in developing cortical neuronal networks in vitro is
network-wide spontaneous bursting (Wagenaar et al. 2005). In this work, we study computationally the spontaneous emergence of bursts and the effect of network structure on burst shape and frequency. Recent computational structure-function approaches show the effect of, e.g., second-order connections (Zhao et al. 2011) and
degree distribution widths (Roxin 2011) on activity patterns. We aim to study a wider set of graph-theoretical measures using networks with identical in-degree distributions. We apply a biophysically plausible point-neuron model
of a cortical cell (Golomb et al. 2006). The model network consists of a small (N=100) number of neurons, both
excitatory pyramidal neurons and inhibitory interneurons. A model of short-term depression (Golomb and Amitai
1997) is used for glutamatergic synapses. The activity simulation is run over a wide set of classes of network
structure. To quantify the structure of the network, we consider graph theoretical measures such as clustering
coefficient, geodesic path length, node-betweenness and occurrence of different motifs. We restrict to unweighted
bidirectional graph representation, hence the synaptic weights between the neurons are uniform. We study the
significance of different graph theoretic measures using a prediction framework: How well can a bursting property,
such as burst duration or frequency, be estimated using various measures of structure as attributes? We show
that the prediction of bursting properties is improved by taking one or more of the aforementioned measures as
prediction attributes. It is best improved when the prediction is based on the clustering coefficient or occurrence of
the most highly connected motifs. We confirm the results using a noise-driven LIF model with short-term depression (Tsodyks et al. 2000). We conclude that the significance of measures of clustering is prominent compared
to other measures of structure.

I-16. Sequences and the emergence of continuous attractor networks
Alan Veliz-Cuba
Carina Curto
Vladimir Itskov

AVELIZ - CUBA 2@ UNL . EDU
CCURTO 2@ MATH . UNL . EDU
VITSKOV 2@ MATH . UNL . EDU

University of Nebraska, Lincoln
It has been long hypothesized that both spatial navigation in hippocampo-entorhinal circuits and parametric working memory in various cortices are the product of a continuous attractor structure in the underlying recurrent
networks. Continuous attractor networks are recurrent networks whose patterns of synaptic efficacies depend on
the distances between neurons in some “feature space.” In such networks, neurons preferentially excite others
having similar coding properties, such as overlapping receptive fields. The resulting “bump” attractors are subpopulations of neurons that respond to similar stimuli. While this model explains many experimentally observed
features of neural activity, there is little direct evidence for or against such a synaptic organization. Moreover, a
plausible mechanism for the creation and maintenance of continuous attractor networks is not well-understood.
In contrast, robust sequences of neuronal activation have been experimentally observed in hippocampus and
several neocortical areas. We suggest a simple mechanism by which robust sequences of neuronal activation,
such as those observed during hippocampal sharp waves, can lead to the formation of continuous attractor networks. We have found that, in the presence of a natural Hebbian learning rule, a pair of short temporal sequences
leads to a pattern of synaptic weights that is characteristic of continuous attractor networks. Moreover, the resulting network exhibits 2-dimensional “bump” attractor dynamics. This behavior also emerges for networks that
are formed from many sequences, with bump attractors evident in 2-dimensional slices of a higher-dimensional

COSYNE 2012

55

I-17 – I-18
organization of neurons. Remarkably, the resulting bump attractor dynamics reinforce the established pattern of
synaptic weights. We propose that robust sequences of neural activity and continuous attractor networks are not
only compatible, but may be a part of the same mechanism as they mutually reinforce each other.

I-17. Short-Term Memory Capacity in Recurrent Networks via Compressed
Sensing
Adam Charles
Han Lun Yap
Chris Rozell

ACHARLES 6@ GATECH . EDU
YAPHANLUN @ GATECH . EDU
CROZELL @ GATECH . EDU

Georgia Institute of Technology
Characterizing the memory capacity of neural circuits is vital to understanding the performance of tasks such as
working memory. In particular, critical questions recently raised in the literature include the effects of network size,
connectivity pattern and natural signal statistics on the ability of a neural circuit to encode temporal memories in
transient dynamics. Recently, results from compressive sensing (CS) have indicated that the memory capacity of
a linear neural circuit could exceed the number of neurons when the temporal signals are sparse (i.e., having few
non-zeros) [White et al., 2004, Ganguli and Sompolinsky, 2010]. In this work we perform a full CS analysis of the
memory capacity of a neural circuit for general inputs that have sparse coefficients in a known basis (a model for
many natural signals [Olshausen and Field, 1996]). Using a model similar to [Ganguli and Sompolinsky, 2010],
our results 1) provide signal recovery guarantees for the exact linear network dynamics with no approximations;
2) provide guarantees for every temporal sequence sparse in any basis (not average performance), where the
characteristics of the basis have well-defined effects; and 3) quantify the dependence of the memory capacity on
properties of the neural connectivity pattern. Our analyses (based on the restricted isometry property [Candes,
2006]) demonstrate that the memory capacity can far exceed the number of neurons, and illustrate the magnitude
of this excess in two distinct cases: 1) the perfect recovery of finite-length signals that are within the capacity of
the circuit, and 2) the recovery error for signals that are beyond the capacity of the circuit (possibly infinitely long),
with an exponential forgetting factor in the circuit. These results point to the existence of an optimal signal length
(for a given network) that minimizes the joint recovery error due to un-recalled signal and recall errors.

I-18. Norepinephrine, neural gain, and “first one wins” network dynamics
Eran Eldar1
Angela Radulescu1,2
Yael Niv1
Jonathan Cohen1
1 Princeton
2 Columbia

ELDAR . ERAN @ GMAIL . COM
ANGELAR @ PRINCETON . EDU
YAEL @ PRINCETON . EDU
JDC @ PRINCETON . EDU

University
University

Norepinephrine is thought to increase neural gain (i.e. responsivity to input). We use neural network simulations
to show that high neural gain causes network output to reflect only the earliest-processed aspect of the input, and
prevent a slower convergence process that integrates multiple aspects of the input. For instance, consider the
stimulus “C[]T”. The global resemblance of the stimulus to the word CAT favors perception of the middle letter as
an A, whereas the actual shape of the middle letter favors perception of an H. With low neural gain the network
integrates these two conflicting aspects and outputs A and H with similar probabilities. In contrast, with high gain
the output mainly reflects the shape of the middle letter, which is processed more quickly than the global pattern,
and thus the network “perceives” an H. This effect can be inverted if prior attention is directed to the word CAT, in
which case the word representation is active first, and thus, high neural gain biases the network towards the letter
that completes the word (A). We present experimental evidence that supports our theory. We designed 52 letter-

56

COSYNE 2012

I-19 – I-20
morphs (such as []) and placed each one in a letter string so that it may complete a word depending on the letter
that is perceived. Subjects were asked to report which letter they saw while we recorded their pupillary dilation,
an inverse measure of norepinephrine activity. In accordance with the simulation results, mean pupil response
was positively correlated across subjects with perception of the letter that completes the word. Furthermore,
when prior attention was directed to the word through priming (e.g. by showing a semantically related word such
as DOG immediately before C[]T), we found the inverted result. Namely, mean pupillary dilation was negatively
correlated with word completion.

I-19. Recurrent vs. feedforward networks: differences in neural code topology
Vladimir Itskov
Carina Curto

VITSKOV 2@ MATH . UNL . EDU
CCURTO 2@ MATH . UNL . EDU

University of Nebraska, Lincoln
Neural networks in various brain areas exhibit both recurrent and feedforward organization. It has often been
observed that many of the functions attributed to recurrent networks can in fact also be accomplished by networks
with strictly feedforward architecture. This raises a natural question: what functions of recurrent networks can not
be accomplished by feedforward ones? We address this question by examining the coding properties of networks.
For a given network, the (combinatorial) neural code consists of all subsets of neurons that can stably co-fire in
response to an external input. Given a neural code (i.e., a set of neural activity patterns), there are natural
topological invariants that can be computed from the code’s combinatorial structure. It has previously been shown
that these invariants match topological features of the represented stimulus space if the neurons have convex
receptive fields (Curto & Itskov, 2008). These invariants thus capture meaningful properties of the stimuli being
represented by the network. Remarkably, we find that neural codes of recurrent networks are capable of realizing
any topological type (homotopy type), whereas two-layer feedforward networks can only generate neural codes
with highly restricted topology. In particular, neural codes of excitatory two-layer feedforward networks correspond
to stimulus spaces that are topologically trivial (contractible), with all topological invariants vanishing. We conclude
that recurrent networks may be necessary for representing stimulus spaces that do not have these highly restricted
topologies.

I-20. Diffusion of nodal sodium channels can restore function in multiple sclerosis
Ali Neishabouri
Aldo Faisal

M . NEISHABOURI 10@ IMPERIAL . AC. UK
ALDO. FAISAL @ IMPERIAL . AC. UK

Imperial College London
Long-range communication in the CNS and PNS is mediated by myelinated axons. We are interested in the
engineering design principles of this wiring. In multiple sclerosis these axons demyelinate, which increases the
attenuation of action potentials (APs) between Nodes of Ranvier, leading to propagation failure. In healthy myelinated axons, thousands of Nav1.6 channels cluster at the nodes of Ranvier were they regenerated APs. In a
similar manner, almost all K+ ion channels are mechanically attached to the nodal or paranodal regions of the
axon, to stabilise the resting potential. However, during demyelination, K+ channels lose their attachment, diffusing freely along the membrane of the demyelinated region, whereas Nav1.6 channel remain constrained to the
nodes of Ranvier. Once the K+ channels diffuse away, the membrane potential cannot be sufficiently stabilised
by leak conductances and Na+ channel noise produces a persistent subtrehsold depolarisation (consistent data
(Waxman (2006)) - which increases resting metabolic cost of these axons. We predict that if Nav1.6 channels
were free to diffuse along the demyelinated axon, their positive feedback could help decrease the effects of de-

COSYNE 2012

57

I-21 – I-22
myelination and re-establish AP propagation. To test this hypothesis, we simulate a stochastic model, based on
experimental data from mammalian PNS. The healthy axon models reliably conducts APs. We simulate multiple
sclerosis by increasing gradually the capacitance and leak conductance of one internode. The axon increasingly
fails to conduct action potentials, reproducing the predicted pathology. Finally, we allow Na+ channels to diffuse
from one node along the demyelinated internode. The axon is once again able to propagate action potentials,
suggesting pharmacological disruption of Na+ channel anchors as a possible drug target. We hypothesise that
this effect relies on the stochasticity of Na+ channels that allows the axon to fire despite the very low channel
density when diffused across the membrane.

I-21. Perceptual grouping and figure-ground segregation arising from shortterm plasticity in a spiking network
Jason Fleischer
Alexandar Kozarev

FLEISCHER @ NSI . EDU
KOZAREV @ NSI . EDU

The Neurosciences Institute
The segmentation of visual scenes is a fundamental process of early vision, but the underlying neural mechanisms
are still unclear. Both theory and neurophysiology data point to the importance of temporal correlations in neural
activity. Here we extend previous work on a mean firing rate model of figure-ground segregation (Sporns et.
al., PNAS 88:129-33), and develop an equivalent spiking network that displays similar behaviors. This network
has dense local excitatory connections between motion-responsive neuronal units arranged in a retinotopic array.
Synapses in the network are modified by both long-term spike-timing dependent plasticity and by a model of
short-term changes in synaptic efficacy. The network is presented with a visual stimulus of waffle-like texture
elements, where some of these elements move coherently together and in anti-phase to the rest. In humans, this
stimulus produces the perception of a diamond shaped figure oscillating against a textured background. In our
model, temporal correlations between spike trains of neurons whose receptive fields cover figure elements enable
the grouping together of those elements into a perceptual whole. Short-term synaptic plasticity is shown to be
critical for the development of such temporal correlations.

I-22. The multi-class tempotron: a neuron model for processing of sensory
streams
Robert Gütig

GUETIG @ EM . MPG . DE

Max Planck Institute of Experimental Medicine
Neurons operate in continuous time. To understand how neurons transform high-dimensional streams of input
spikes into continuous sequences of output spikes is a central challenge of neuroscience. Recently, spike-based
computational studies of neuronal systems function have advanced through the tempotron family of supervised
learning rules. However, because of its binary decision rule (spike vs.∼no spike) the applicability of the tempotron
to neuronal processing has remained of limited scope. Here we introduce a new principle that generalizes the
tempotron from a binary to a multi-class decision rule, allowing for any number of output spikes to be set as
the desired response. We analytically compute the resulting gradient descent learning rule for a leaky integrate
and fire neuron with reset. To test the multi-class tempotron we mimic streams of sensory features by randomly
concatenating segments of frozen Poisson spike trains drawn with replacement from a fixed pool. The tempotron
can quickly learn to report the presence of a specific target feature within such sequences. Note that this is
accomplished by only providing the number of occurrences of the target feature in each training trial. No temporal
information is given. In an even more difficult task, we use a weighted linear sum of the frequencies of multiple
target features as desired output. Astonishingly, the neuron readily performs a regression and starts signaling
each of the targets with the appropriate number of spikes. Using these principles, we have successfully trained

58

COSYNE 2012

I-23 – I-24
neurons to report with high fidelity spoken digits within connected speech. Also here no segmentation of the
training data was required. The multi-class tempotron is an important generalization of the original model and
strongly boosts its applicability to spiking models of neuronal systems including sensory processing streams,
spike count modulated tuning curves and multi-layer networks.

I-23. Biologically plausible learning of sparse-coding dictionary in a neural
network
Ziqiang Wei1,2
Tao Hu1
Dmitri Chklovskii1
1 HHMI,
2 Johns

WEIZ @ JANELIA . HHMI . ORG
HUT @ JANELIA . HHMI . ORG
CHKLOVSKIID @ JANELIA . HHMI . ORG

Janelia Farm Research Campus
Hopkins University

Many natural signals can be represented as linear combinations of a few feature vectors chosen from an overcomplete dictionary (Olshausen & Field, 1996, 1997). Such sparse representation strategy is likely used in the
brain because neuronal activity is often sparse (DeWeese & Zador, 2006, Lewicki, 2002). Moreover, training
a sparse-coding network on natural images by updating synaptic weights using a local Hebbian-like results in
feature vectors similar to receptive fields of V1 neurons (Olshausen & Field, 1997). However, existing models of
dictionary learning (Olshausen & Field, 1997; Mairal et al., 2010) activate a learning rule only after the network
activity converges to a sparse representation. Such learning paradigm seems biologically implausible because it
would be difficult for synapses to know when the transient period of activity ends and a sparse representation is
found. This becomes particularly problematic for sensory systems, which face continuously varying stimuli rather
than static stimulus presentations. Here, we considered a biologically realistic learning paradigm where synaptic
update proceeds simultaneously with the transient neural activity. Specifically, we modeled dictionary learning
in the reciprocal neural network by alternating activity updates with the local Hebbian-like update of synaptic
weights, which was derived from stochastic gradient descent. Activity updates combine the gradient step and
linear-thresholding step reminiscent of biological neuron response function (Yin et al., 2008), which is needed
because of the non-differentiability of the L1 norm. Simulating the network activity in response to prewhitened
natural image patches, we find feature vectors similar to receptive fields of simple cells in V1. Therefore, a
sparse-coding dictionary can be learned in a reciprocal neural network, in a biologically plausible manner with a
Hebbian-like learning rule operating continuously in time. Our result support sparse representations as a model
of neural computation in the brain.

I-24. Embodied Exploration
Daniel Little
Fritz Sommer

DYLITTLE @ BERKELEY. EDU
FSOMMER @ BERKELEY. EDU

University of California, Berkeley
Computational modeling of exploration has largely focused on its role in the acquisition of external rewards. In
contrast, the field of Psychology, through studies of human and animal behavior, has emphasized that exploration
is largely driven towards learning as a primary objective and is not secondary to external reinforcement. We have
developed an information theoretic model of learning-driven exploration in Controllable Markov Chains (CMCs).
CMCs are discrete, stochastic systems in which state-transitions depend both on the current state and on a control
variable set by an active agent. Through Bayesian inference we demonstrate here: 1) How an agent can best
model the inherent dynamics of its world from previously collected data alone and 2) How an agent can predict the
amount of information a future observation will carry towards improving its internal model. The ability to predict
such future information gain allows an agent to guide its exploration of the world yielding efficient learning. An

COSYNE 2012

59

I-25 – I-26
innate tendency in humans and animals to seek out information may have been selected for during evolution by
the generalized utility such information offers the organism towards accomplishing arbitrary tasks. Accordingly, in
CMCs, we show that efficient explorers are better able to use their learned world models to accomplish a range
of goal-directed tasks. While our framework is derived for discrete systems, we demonstrate that they can be
applied to modeling continuos systems such as manipulation of a weighted pendulum. Revealing how action
strategies shape the flow of information over time provides a new dimension to the study of active learning. It has
applications in the design of autonomously learning robots and brings insight to an array of human behaviors. We
conclude by offering preliminary results extending our work to the problem of predicting human gaze attention.

I-25. Critical exponents derived for neuronal avalanches in alert non-human
primate
Shan Yu1
Hongdian Yang2
Dietmar Plenz1
1 National
2 National

YUSHAN . MAIL @ GMAIL . COM
HONGDIAN @ MAIL . NIH . GOV
PLENZD @ MAIL . NIH . GOV

Institute of Mental Health
Institutes of Health

The theory of criticality provides important insights into the organization of complex systems. Experimental and
theoretical advances in the study of neuronal avalanches (Beggs and Plenz, 2003; Petermann et al., 2009) suggest that the cortex operates at criticality. These conclusions are based on power laws that govern avalanche
dynamics and optimization principles found experimentally in line with predictions for critical dynamics (Shew et
al, 2009, 2011). However, a clear proof of criticality includes the demonstration of typical critical behaviors, i.e.
divergence of specific heat, C, and susceptibility, chi, the phase transition of an order parameter, M, and the
characterization of critical exponents, based on which critical dynamics can be categorized into different universality classes. Here, we examined critical behaviors and derived critical exponents for the first time for neuronal
avalanches. Neuronal avalanche dynamics were identified based on ongoing local field potential activity in premotor cortex from two alert macaque monkeys during rest. The probability of avalanche patterns was analyzed
to identify the critical thermodynamic temperature, Tc, under which the system reaches criticality. To this end,
we used the single histogram method (Ferrenberg and Swendsen, 1988; Tkacik et al., 2009) to infer the system’s behavior at various temperatures. We found that the temperature T0, under which the actual recordings
were taken, was very close to Tc. Near T0, C and chi diverged, i.e. exhibited peaks with unbounded increase
in amplitudes when system size increased, and M went through a phase transition. Using finite size scaling, we
computed several critical exponents, including alpha, beta and gamma, which were consistent with Rushbrooke’s
law: α + 2β + γ = 2. Our results provide strong evidence that neuronal avalanches are a signature of cortical dynamics at criticality, and potential explanation for recent experimental findings that the brain is extremely sensitive
to small external perturbations (Houweling and Brecht, 2008).

I-26. Functional Connectivity of the Neural Integrator in Larval Zebrafish
Kayvon Daie1
Mark Goldman2
Emre Aksay1
1 Cornell

KPD 7@ CORNELL . EDU
MSGOLDMAN @ UCDAVIS . EDU
EMA 2004@ MED. CORNELL . EDU

University
of California, Davis

2 University

The oculomotor integrator generates eye position signals by integrating, in the sense of calculus, velocity inputs
from the saccadic, optokinetic and vestibular systems. Most models propose that integration results from network interactions, the pattern of which remains unknown. Here we use two-photon calcium imaging to measure

60

COSYNE 2012

I-27
the dynamics of integrator neuron activity and infer functional network connectivity. We first find that following
saccades, the network’s population activity (as assessed by the singular value decomposition) contains three
significant modes, which can be explained by a connectivity matrix containing as few as three non-zero eigenvalues. We next compared this activity pattern to that following a constant-velocity optokinetic ramp, and found
that the activity patterns generated during these two behaviors occupied significantly different volumes of state
space. Specifically, following saccades, the most persistent cells were located at the most medial, ventral, and
caudal positions, whereas following optokinetic stimulation, the most persistent cells were located at the most medial, ventral, and rostral positions. In addition, cells with larger spontaneous responses following saccades were
in more rostral positions whereas cells with larger optokinetic responses were located in more caudal positions.
Network modeling shows that this reversal of gradients in the rostro-caudal direction is consistent with 1) a pattern
of input where the most rostral cells receive stronger saccadic inputs and the most caudal cells receive stronger
optokinetic inputs, and 2) a pattern of functional connectivity within the integrator network dominated by localized
connections arranged in a symmetric, chain-like organization. Together, these results demonstrate how a single
network can use its multiple modes of activity to differentially process inputs arriving along separate pathways.

I-27. Dynamic grouping in neuronal networks by inhibition induced neuronal
excitability transitions
Christoph Kirst1,2
Julian Ammer3
Felix Felmy3
Andreas VM Herz3
Martin Stemmler3

CKIRST @ NLD. DS . MPG . DE
AMMER @ BIOLOGIE . UNI - MUENCHEN . DE
FELMY @ ZI . BIOLOGIE . UNI - MUENCHEN . DE
HERZ @ BIOLOGIE . UNI - MUENCHEN . DE
STEMMLER @ BIO. LMU. DE

1 Max

Planck Institute for Dynamics and Self-Organization
Center for Computational Neuroscience Göttingen
3 Ludwig-Maximilians-Universität München
2 Bernstein

Neurons fall into different neuronal excitability types [1,2]: While type I neurons integrate and support arbitrarily long inter-spike-intervals, type II neurons show resonance and start firing with a non-zero frequency. Here
we show that across a large number of conductance-based neuron models (including Wang-Buszaki, ConnorStevens) an increase in leak conductance induces a transition from type I to type II neuronal excitability. We prove
that the bifurcation structure of this transition is organized by a degenerate Bogdanov-Takens-cusp bifurcation
point of co-dimension 3 [3] which implies a switch from type I to type II for the spiking dynamics, a transition
from integration to resonance near spike threshold, as well as a region of bistability of resting and regular spiking
dynamics. We confirm these predictions experimentally for different neurons using dynamic patch clamp recordings to artificially change the leak conductance. Interestingly, the neuronal excitability type can also be switched
dynamically via activation of shunting synapses, which we mimicked experimentally by bath application of GABA.
These results imply that inhibitory cells can dynamically control the neuronal excitability type of postsynaptic
neurons and as a consequence their synchronization properties. In particular, we show both analytically and
numerically that inhibition can separately synchronize several coexisting sub-populations of excitatory neurons.
Moreover, the maximal amount of synchrony in the network can be efficiently regulated by dynamically forcing
the neurons into the region of bistability. In conclusion, inhibition-induced dynamic neuronal excitability switching
provides a mechanism for flexible and activity controlled dynamic formation of synchronized neuronal cell assemblies. [1] A. L. Hodgkin, J. Physiol. 107, 165 (1948). [2] E. M. Izhikevic, Dynamical Systems in Neuroscience, MIT
Press (2007). [3] F. Dumortier, R. Roussarie, J. Sotomayor, and H. Zoladek. Bifurcations of Planar Vector Fields,
Springer (1991).

COSYNE 2012

61

I-28 – I-29

I-28. Phase coherence of field potentials facilitates prediction of single-trial
outcome in a memory task
Beth Lopour
Abtine Tavassoli
Itzhak Fried
Dario Ringach

BETHLOPOUR @ UCLA . EDU
ATAVASSOLI @ MEDNET. UCLA . EDU
IFRIED @ MEDNET. UCLA . EDU
DARIO @ UCLA . EDU

University of California, Los Angeles
The reset of neuronal oscillations has been associated with both short-term memory recall (Rizzuto et al. 2003)
and error-related signals (Cavanagh et al. 2009). We investigated this phenomenon using human local-field
potential (LFP) data measured during a classic “memory” matching card game. We found that the LFP response
differed during correct (matching) and incorrect (non-matching) trials. In many cases, this difference was robust
enough to classify new single trials as either correct or incorrect. Moreover, using wavelet analysis, we found that
the delta-band phase rivaled the full LFP signal as a classifier. This appears to be possible due to an increase
in inter-trial phase coherence after the presentation of the stimulus. While this increase was present during both
correct and incorrect trials, we found that the incorrect trials reached maximum coherence later than the correct
trials. An analysis of the full wavelet transform results revealed increases in coherence at frequencies up to 8
Hz and increases in power above 30 Hz, although the specific responses varied by brain region and for correct
and incorrect trials. Sometimes the increase in coherence appeared to coincide with an increase in power; this
may indicate that the increase in coherence is due to an evoked response, rather than phase resetting. To test
this, we calculated the correlation between power and coherence over all frequencies during the presentation
of the stimulus. We found no significant correlation in the hippocampal delta band or in the delta band of the
amygdala during correct trials. This implies that, in these cases, coherence at low frequencies is not due to an
evoked oscillation and may be indicative of phase resetting. In general, our study elucidates the importance of
low-frequency phase for single trial decoding and examines the possibility of phase resetting as an important
mechanism in mesiotemporal lobe function.

I-29. Two modes of phase-amplitude coupling in human cortical electrical dynamics under general anesthesia
Eran Mukamel1
Kin Foon Kevin Wong2
Eric T Pierce2
P. Grace Harrell2
John L Walsh2
Emery Brown3,4
Patrick Purdon2

ERAN @ POST. HARVARD. EDU
WONG @ NMR . MGH . HARVARD. EDU
ETPIERCE @ PARTNERS . ORG
PHARRELL @ PARTNERS . ORG
JWALSH @ PARTNERS . ORG
ENB @ NEUROSTAT. MIT. EDU
PATRICKP @ NMR . MGH . HARVARD. EDU

1 University

of California, San Diego
General Hospital
3 Massachusetts Institute of Technology
4 Harvard Medical School
2 Massachusetts

Understanding the role of rhythmic oscillations in the organization of cortical electrical dynamics is a fundamental
aim in systems neuroscience. Rhythmic activity across widely separated frequency bands appears closely coordinated during active behavior and in reduced states of consciousness such as sleep or general anesthesia (GA),
yet no unified framework for such coordination has emerged. To address this we induced GA using propofol under experimentally controlled conditions in normal human volunteers, while continuously recording cortical activity
via 64-channel electroencephelogram (EEG) and monitoring instantaneous level of arousal via a behavioral task.
Following loss of consciousness we observed robust phase-amplitude coupling between two low-frequency bands
with prominent power spectral peaks (slow oscillation, 0.1–2 Hz; alpha band, 8—14 Hz). This cross-frequency

62

COSYNE 2012

I-30
coupling was organized in two distinct modes with opposite phase-amplitude relationships. During profound unconsciousness at high propofol concentration, alpha-band power was concentrated in the surface-positive phase
of scalp EEG slow oscillation. This in-phase coupling pattern was evident both during epochs of burst-suppression
(BS), in which electrical silence alternates with bursts of broadband electrical activity, and in non-BS epochs. A
second, anti-phase coupling mode occurred at lower drug concentrations near the threshold for loss of consciousness. This mode had an opposite phase-amplitude relationship in which alpha-band power is greatest at
the surface-negative phase of the scalp EEG. The abrupt transition between in- and anti-phase coupling was
not correlated with the major behavioral and EEG power spectral changes observed at loss of consciousness.
Our findings suggest that unconsciousness in GA is not a unitary state of the cortical network; rather, cortical
dynamics may be organized in multiple distinct modes depending on drug concentration. The results suggest
that phase-amplitude coupling could be a useful supplement to power spectral measures in clinical EEG-based
monitoring of brain state in GA.

I-30. Retinal adaptation and invariance to changes in higher-order stimulus
statistics
Gasper Tkacik1
Anandamohan Ghosh2
Elad Schneidman3
Ronen Segev4

GTKACIK @ IST. AC. AT
ANANDAMOHAN @ GMAIL . COM
ELAD. SCHNEIDMAN @ WEIZMANN . AC. IL
RONENSGV @ BGU. AC. IL

1 IST

Austria
Institute of Science Education
3 Weizmann Institute of Science
4 Ben-Gurion University of the Negev
2 Indian

Adaptation in the retina optimizes the encoding of natural light signals into sequences of spikes sent to the
brain. However, adaptation also entails computational costs: adaptive code is intrinsically ambiguous, because
output symbols cannot be trivially mapped back to the stimuli without the knowledge of the adaptive state of
the encoding neuron. It is thus important to learn which statistical changes in the input do, or don’t, invoke
adaptive responses, and ask about the reasons for potential limits to adaptation. To address the issue of retinal
adaptation to statistical properties of the light intensity distribution, we first analyze theoretically the adaptation
process that would maximize information transfer from the retina to the brain, and find how noisy spiking retinal
ganglion cells should change their encoding properties when the stimulus distribution changes. Then we compare
the theoretical result to the properties of the salamander retina by measuring experimentally the retinal ganglion
cell responses to controlled changes in the second (contrast), third (skew) and fourth moments (kurtosis) of light
intensity distributions of spatially uniform temporally independent stimuli. We choose the skew and kurtosis of our
stimuli to cover the range observed in natural scenes. We quantify adaptation in the ganglion cells by studying
two-dimensional linear-nonlinear models that can capture well the retinal encoding properties across all stimuli.
We find that the retinal ganglion cells adapt to contrast, but show remarkably invariant behavior to changes in
higher-order statistics. This invariance is demonstrated both by insignificant changes in the inferred linear filters
across the skewed / kurtotic stimuli for each cell, as well as the ability of non-adapting models to predict well the
firing rate and generalize across stimulus classes. We further show why the neural code can maintain a high
information rate despite changes in skew and kurtosis without dynamic adaptation.

COSYNE 2012

63

I-31 – I-32

I-31. Inhibition in mouse IC affects the rate code but not the timing code when
processing vocalizations
Alexander Dimitrov
Graham Cummins
Zachary Mayko
Christine Portfors

ALEX . DIMITROV @ VANCOUVER . WSU. EDU
GRAHAMIANCUMMINS @ GMAIL . COM
ZMAYKO @ WSU. EDU
PORTFORS @ VANCOUVER . WSU. EDU

Washington State University, Vancouver
Inhibition is well known to shape responses to sensory stimuli. In the auditory system, it shapes frequency response curves and responses to complex stimuli. Here, we examined how inhibition affects response to vocalizations in mouse inferior colliculus (IC). We studied two cases in awake mice: normal auditory processing (control)
and auditory processing after pharmacological blocks of inhibition (test, application of bicuculline and strychnine
to block GABA A and glycine receptors). As previously reported, we observed that the overall spike rate tended
to increase when inhibition was blocked. More surprising to us was the finding that the temporal precision of the
neural representation was essentially unchanged after blocking inhibition. To explain these findings, we hypothesized a mechanism analogous to the iceberg effect in vision: there is an underlying continuous driving potential,
and inhibition controls the effective threshold for spike generation (or scale of the driving potential). Specifically,
we distributed the sample of recorded cells into three main classes, depending on the way they responded to test
vocalizations: 1) Cells that responded only when inhibition was blocked; 2) Cells that were very selective for a few
specific stimuli; and 3) Cells that responded distinctly to a variety of stimuli. All classes responded with increased
rate when inhibition was blocked. Class 1 was not analyzed any further. Class 2 did not change its selectivity
substantially, so most of the statistical and information-theoretic methods we used did not register any difference
between conditions apart from rate changes. Most of the subsequent analysis refers to Class 3, in which rate
changes but not significant other coding changes were found.

I-32. Hierarchical generalized linear models of dendritic integration and somatic membrane potential
DJ Strouse1,2
Mate Lengyel1
1 University
2 University

DANIELJSTROUSE @ GMAIL . COM
M . LENGYEL @ ENG . CAM . AC. UK

of Cambridge
of Southern California

Accumulating evidence suggests that dendritic trees play a crucial role in single-neuron information processing.
While dendritic integration is currently studied experimentally and using compartmental or cable-theoretic models,
there exists no simple, analytically tractable, and canonical mathematical model for dendritic processing. Poirazi
et al. (2003) suggested that the thin dendrites of pyramidal neurons may be viewed as a “two-layer neural network”
in which a weighted sum of the synaptic inputs to each dendrite is passed through a dendrite-specific sigmoidal
nonlinearity before being globally summed to yield the somatic membrane potential. However, their approach
focused only on static inputs and output and a particular subclass of dendrites of a particular subclass of neurons.
We developed a data-driven model of dendritic integration by building hierarchies of generalized linear models
(GLMs), which have previously been successfully applied to modelling the stimulus-dependent spiking behavior
of sensory neurons (Pillow et al., 2008). Our hierarchical GLM model generalizes previous work by (1) representing the dependence of a cell’s somatic membrane potential on arbitrary spatiotemporal inputs to its dendrites,
rather than only static inputs, and (2) flexibly inferring the appropriate hierarchy of GLMs from experimental data,
rather than assuming a priori the number of layers and particular identification of synapses with subunits. We
demonstrate the success of a maximum likelihood fitting procedure on synthetic data from single-dendrite and
two-layer networks. In subsequent work, we validate our fitting procedure on synthetic data from GLM hierarchies
of more than two layers. We also explore the success of our framework in modeling synthetic data from compartmental models and data from glutamate-uncaging experiments produced by the labs of Michael Häusser, Judit

64

COSYNE 2012

I-33 – I-34
Makara, and Szabolcs Káli. In doing so, our method allows us to assess how “functional” morphology, that is the
GLM hierarchy inferred from electrophysiological data, relates to anatomical morphology.

I-33. Structure-preserving model reduction of the passive and quasi-active
neuron
Kathryn Hedrick
Steve Cox

KW 5@ RICE . EDU
COX @ RICE . EDU

Rice University
We use a Krylov subspace projection method to construct a reduced model of the passive and quasi-active
neuron which accurately captures the membrane potential at the spike initiation zone (SIZ) using a system of
dramatically reduced dimension (3-5%). Unlike previous neuronal reduction techniques (Kellems et al. 2009),
the reduced model retains the RC and RLC circuit structure of the passive and quasi-active neurons, providing
a biophysical interpretation for the input/output map of the reduced neuron. Furthermore, the model retains
the spatial specificity of inputs within the dendritic tree, rendering it to be a useful tool for examining dendritic
computations both numerically and conceptually. The reduced model is robust to instability and resonance and
captures the neuron’s resonant frequency. Inputs into each reduced compartment are filtered by the columns of
the reducer matrix, whose null space determines the set of inputs causing the SIZ to be blind to an input at any
given dendritic location. The same reduced model is effective for injected and synaptic inputs into the passive
neuron, and we compare the latter case, which has nonlinearities through sublinear summation, to a related
bilinear reduction technique (Phillips 2003). The reduced model is accurate for the general dendritic tree, and we
present two examples of the reduction of morphologically accurate neurons: the lobula giant movement detector
(LGMD) and the CA1 pyramidal neuron. Each dendritic location in the LGMD corresponds to a location on the
locust eye, and reducing a neuron in which space is well understood while retaining the neuronal structure may
help to elucidate the dendritic computations performed in the LGMD. The CA1 neuron receives inputs locally on
the dendrite through the Schaffer collaterals and distally through the perforant path, and the reduced model can
be used to examine the role of dendrites in integrating these two distinct inputs.

I-34. Using dimensionality reduction to explore muscle synergies and torque
encoding during insect flight
Simon Sponberg
Tom Daniel
Adrienne Fairhall

BERGS @ U. WASHINGTON . EDU
DANIELT @ U. WASHINGTON . EDU
FAIRHALL @ U. WASHINGTON . EDU

University of Washington
A central challenge in understanding motor control, particularly during complex rhythmic behaviors like flight, is
decoding the specific features of body dynamics controlled by a changing motor command. We recently discovered that the main downstroke muscles powering flight in the hawkmoth, Manduca sexta, are modulated by
optomotor feedback to produce turns in response to left-right visual motion. This is surprising because these muscles receive only a single action potential for each wingstroke. The animal controls torque in part through precise
timing modulation of these spikes. Here we investigate how to identify the control authority encoded in changes
in peripheral motor signals. The goals of our study are 1) to extract, with as few dimensions as possible, the
features of torque controlled by spike timing and 2) to test whether the timing of each of the left and right muscle’s
spike have independent contributions to controlling torque or function as a single control channel. We constructed
spike triggered ensembles (STEs) of yaw (turning) torques and extracted the dominant features using singular
value decomposition (SVD) of the STEs and a new approach applying partial least squares (PLS–Krishnan, A.
et al., 2011) to extract features that best capture task-relevant variation. The PLS analyses capture the feedback

COSYNE 2012

65

I-35 – I-36
controlled components of torque in only one or two features, while the general SVD analyses typically required
five or more. Torque features are comparable to the within-stroke torque measured previously from a dynamicallyscaled model of a flying insect (Fry, S. et al., 2003). In most individuals, the timing difference between the muscles
alone best explains the torque features, indicating that these two muscles work together in a functional synergy
for control. Finally, including the left-right timing difference from the previous wingstroke significantly improves the
models’ explanatory power indicating significant history dependence.

I-35. Retinal metric: a stimulus distance measure derived from population
neural responses
Gasper Tkacik1
Einat Granot-Atedgi2
Ronen Segev3
Elad Schneidman2
1 IST

GTKACIK @ IST. AC. AT
EINAT. GRANOT @ WEIZMANN . AC. IL
RONENSGV @ BGU. AC. IL
ELAD. SCHNEIDMAN @ WEIZMANN . AC. IL

Austria

2 Weizmann

Institute of Science
University of the Negev

3 Ben-Gurion

Retinal ganglion cells transmit information about the organism’s visual environment to the brain using a population
neural code that consists of combinations of spikes and silences. If this code is to efficiently represent the
environment, different stimuli should reliably elicit different population responses. It has been shown, however,
that this is not true for single neurons, which produce distinguishable responses only when the stimuli differ
along a small number of directions in the stimulus space, but not otherwise. It is thus unclear whether a neural
population uses the diversity of neural sensitivities to distinguishably represent all possible stimuli, or whether it
is inherently able to discriminate much better between some pairs of stimuli than between others. We recorded
simultaneously from 100 neurons in the salamander retina to measure the population responses to repeated
presentations of temporally random and spatially uniform stimuli. From this data we built the stimulus-dependent
maximum entropy model for the population and used it to construct a corresponding “retinal distance” function
between arbitrary pairs of stimuli. In contrast to previously used choices for the distance measure in the space of
stimuli, the retinal distance tells us precisely how distinguishable, given the noise in population neural responses,
a pair of stimulus clips is to the retina, thus placing bounds on any downstream decoding layer. We find that the
retinal distance strongly deviates from Euclidean, and indeed from any distance with a static metric. For a given
stimulus trace, we are able to create ensembles of similar stimuli that are distinguishable from the given one to
within a prescribed level of discriminability of the response, and we identify which snippets are well constrained
by the retinal response and which are not. This has important consequences for decoding and understanding of
the neural code in the retina.

I-36. Bayesian spike-triggered covariance and the elliptical LNP model
Il Memming Park
Jonathan W Pillow

MEMMING @ AUSTIN . UTEXAS . EDU
PILLOW @ MAIL . UTEXAS . EDU

University of Texas at Austin
An important problem in computational neuroscience is to estimate the (typically low-dimensional) space of stimulus features that affect a sensory neuron’s response. One popular method uses the first two moments of the
spike-triggered stimulus distribution: the spike-triggered average (STA) and the spike-triggered covariance (STC).
However, these estimators have not previously been formulated within an explicit probabilistic encoding model.
Here we provide that formulation, clarifying the conditions under which STA/STC estimates are optimal, and
pointing to several useful model-based generalizations. Specifically, we show that the STA and STC provide an

66

COSYNE 2012

I-37 – I-38
asymptotic maximum-likelihood estimate when the nonlinearity of a Linear-Nonlinear-Poisson (LNP) neuron takes
the form of an exponentiated quadratic and the stimuli are Gaussian. We can therefore connect the STA and
STC to the likelihood of an explicit probabilistic encoding model, making it straightforward to: (1) consider the
same likelihood when stimuli are non-Gaussian, resulting in consistent and efficient feature space estimation with
non-Gaussian stimuli; and (2) introduce priors and perform Bayesian estimation of neural feature spaces. We
illustrate these extensions using a smoothing prior on filters, which substantially improves performance on multidimensional feature spaces. We also describe an empirical Bayes method for automatically selecting the feature
space dimensionality (i.e., the number of filters), which avoids nested hypothesis testing or cross-validation. Finally, we extend the model to accommodate an arbitrary elliptically-symmetric nonlinear response function, resulting in a more powerful and flexible model for low-dimensional neural responses. We validate these methods
using neural data recorded extracellularly from macaque primary visual cortex.

I-37. Using a doubly-stochastic model to analyze neuronal activity in the visual cortex
Robbe Goris1
Eero P Simoncelli1,2
Tony Movshon1
1 New

ROBBE . GORIS @ NYU. EDU
EERO. SIMONCELLI @ NYU. EDU
MOVSHON @ NYU. EDU

York University

2 HHMI

Sensory neurons encode information probabilistically: repeated stimulus presentations elicit variable firing. This
variability is often described using a cascade model, in which spikes arise from a Poisson process whose rate is
a deterministic function of the stimulus. However, it has recently been shown that time-dependent rate variability
is a wide-spread phenomenon in cortex (Churchland et al 2010, 2011). Consequently, the Poisson noise model
commonly underestimates the variability of visual cortical recordings, which can lead to systematic errors in
inferring neuronal characteristics. We measured responses to a variety of stimuli in visual cortex in anesthetized
and alert monkeys. The fluctuations in neural responsiveness that typically occur over the timescale of these
experiments are significantly greater than predicted by a Poisson model—estimated Fano factors as high as
10 occur in the acute preparation. We propose a doubly stochastic model, in which the stimulus-driven firing
rate is modulated according to a stimulus-independent gamma-distributed random variable. This fluctuating rate
generates spikes according to a Poisson process. Fitting the resulting mixture of Poisson processes to neural data
reveals that the model is statistically superior for all neurons, and therefore provides an improved framework for
analyzing neuronal tuning. The framework offers two further advantages over existing methods. First, it provides
a natural means of estimating and tracking fluctuations in responsiveness (state changes) that occur during the
course of an experiment. Second, it offers an efficient and accurate estimate of the upper bound on discrimination
performance that can be supported by each neuron. Application of this new method can substantially improve the
analysis of neuronal data, both in fitting explicit models and in assessing the limits of neuronal performance.

I-38. A feedback error learning approach to online-adaptive decoding for dynamic prosthetic arm control
Matthew Bodenhamer1
Francis R Willett2
Aaron Suminski2
Nicholas Hatsopoulos2
Andrew Fagg3
1 School

MBODENHAMER @ GMAIL . COM
FWILLETT @ UCHICAGO. EDU
ASUMINSKI @ UCHICAGO. EDU
NICHO @ UCHICAGO. EDU
FAGG @ CS . OU. EDU

of Computer Science
of Chicago

2 University

COSYNE 2012

67

I-39
3 University

of Oklahoma

Dynamic neural decoders for prosthesis control will allow the user to anticipate the necessary task forces and
to command the prosthesis accordingly. Because the user is faced with many different dynamic contexts in the
course of her day, it is important that the neural decoder be capable of adapting to the new contexts without
requiring an explicit recalibration session. In this work, we present an online-adaptive, hybrid neural decoder that
is capable of improving task performance when placed in novel dynamic scenarios. The hybrid neural decoder
estimates “intended” arm positions/velocities and joint torques from recent MI activity Suminksi et al. (2011).
The decoded torque is interpreted as a feed-forward control signal, while the decoded position and velocity are
interpreted as references signals for a proportional-derivative (PD) feedback controller. The arm is commanded
with a weighted contribution from each of the feedback and feed-forward components. We extend this work by
adapting the torque decoder as it is being used to command an arm in a random target reaching task. Inspired by
the Feedback Error Learning model of Kawato and Gomi (1992), we interpret position errors (differences between
actual and decoded position) as 1) an error that must be corrected immediately through the PD controller and 2) an
estimate of recent errors made by the torque decoder. We use this latter error signal to drive a quasi-supervised
learning process that alters the parameters of the torque decoder. Through a simulation study, we show that the
adaptive decoder improves performance over a fixed decoder under two novel scenarios: 1) the corruption of
the original torque decoder parameters with zero-mean Gaussian noise, and 2) the introduction of a curl field. In
both scenarios, the adaptive decoder outperforms the fixed decoder over time in both relative time-to-target and
relative path length.

I-39. Inferring eye position and saccade direction from populations of LIP
neurons
Arnulf Graf
Richard Andersen

GRAF @ VIS . CALTECH . EDU
ANDERSEN @ VIS . CALTECH . EDU

California Institute of Technology
A central problem in systems neuroscience is how multiple signals jointly encoding the state of the world are
inferred from responses of neuronal populations. It has been shown that the eye position and saccade direction
signals are jointly encoded by single neurons in the lateral intraparietal cortex (LIP) of macaques. However it is
still unclear how these signals are represented at the population level. Using Bayesian inference, we predicted
eye position and saccade direction from the population activity of LIP neurons, and found that both signals were
combined more accurately than predicted by the product of their error distributions (“optimal combination”). This
suggests a neuronal population code that reliably transmits information by combining signals, and falls comfortably
within the framework of gain fields. We also found that a small subset of neurons yielded decoding accuracies
similar to entire populations, and that different neuronal subsets were recruited for eye position and saccade
direction predictions. This shows a sparse and selective neuronal code. The final eye position could be decoded
well before the movement, presumably by combining the current eye position and the eye movement plan. The
accuracy of the eye position and saccade direction decoding signals both peaked after the saccade, suggesting a
strong memory signal representing past states and actions respectively. Such retrospective signal can be used for
learning or monitoring. The onset of the decoding accuracy differed for the two signals. For the saccade direction
signal, this onset was 100 ms before the saccade, hinting at corollary discharge signals. For the eye position,
the onset was 100 ms after the saccade, suggesting either slow corollary discharge or proprioception. Taken
together, our findings show that cortical computations can untangle jointly encoded signals by inferring them with
distinct temporal signatures and from distinct neuronal population subsets.

68

COSYNE 2012

I-40 – I-41

I-40. Adaptation to spectro-temporal correlation in the primary auditory cortex
Ryan Natan
Isaac Carruthers
Maria Geffen

RNATAN @ MAIL . MED. UPENN . EDU
CIS @ SAS . UPENN . EDU
MGEFFEN @ MAIL . MED. UPENN . EDU

University of Pennsylvania
Sounds stemming from the same object—be it a human voice, an animal vocalization, or a running brook—contain
characteristic correlations in their structure across spectro-temporal channels. The temporal modulation across
spectral channels, denoted here by the correlation coefficient (r), forms an important cue for sound encoding. In
this study, we examined neuronal mechanisms in the primary auditory cortex, which serve to encode r. Using
chronically implanted multi-tetrode microdrives, we recorded responses of neurons in the primary auditory cortex
(A1) of awake rodents to acoustic stimuli with varying r between 0 and .97. The mean firing rate of neurons
driven by the stimulus increased with r. We also quantified changes in neuronal spectro-temporal receptive field
(STRF). The temporal width of the excitatory lobe of the STRF increased with r. This is consistent with the efficient
coding hypothesis, by which neurons adjust the dynamic range of their responses to match the stimulus. We next
quantified the timecourse of adaptation to r. We presented a stimulus, consisting of a sequence of 2 second
long segments that alternated between r of 0 and 0.9, and recorded neuronal responses to 300 repeats of this
sequence. Firing rates of neurons exhibited adaptation to the transition from high to low r over 400ms. This
adaptation was differential between neurons preferring high r and those preferring low r. Changes in firing rate
were accompanied by changes in temporal and spectral width of the STRF. This study demonstrates a novel form
of adaptation to the temporal statistics of complex sounds, suggesting that A1 neurons modulate their response
properties to match the temporal dynamics of incoming signals.

I-41. Decision-making and attention in a sampling-based neural representation
Ralf M Haefner
Pietro Berkes
Jozsef Fiser

RALF. HAEFNER @ GMAIL . COM
BERKES @ BRANDEIS . EDU
FISER @ BRANDEIS . EDU

Brandeis University
According to the sampling hypothesis, the activity of sensory cortex can be interpreted as drawing samples
from the probability distribution over features that it implicitly represents. Perceptual inference is performed by
assuming that the samples are drawn from an internal model that the brain has built of the external world (Fiser
et al 2010). We explore the implications of this hypothesis in the context of a perceptual decision-making task
and present three findings: (1) Because the simple generative model for typical experimental stimuli does not
match the rich internal model of the brain, the psychophysical performance is below what is theoretically possible
based on the sensory neurons’ responses. This can explain why previous studies have found that surprisingly
few sensory neurons are required to match the performance of the animal, and why traditional decoding models
need to invoke ad-hoc “decision noise” (Shadlen et al 1996) when pooling the responses of all relevant sensory
neurons. (2) We show that in the sampling framework typical 2AFC tasks induce higher correlations between
neuron pairs supporting the same choice, than between those contributing to different choices - as has previously
been observed empirically (Cohen & Newsome 2008). (3) We demonstrate that, given the limited number of
samples in a trial and a reward structure that is strongly concentrated on particular parts of the sampling space,
expected reward is maximized by sampling from a probability distribution other than the veridical posterior (for a
related, but parametric, idea see Lacoste-Julien et al 2011). Based on these findings we propose that the brain
actively adapts the posterior distribution to account for (1) and (3), and that this adaptation is closely related to
the cognitive concept of attention. Using this interpretation of attention, we replicate existing neurophysiological
findings and make new predictions.

COSYNE 2012

69

I-42 – I-43

I-42. Attention, Information, Normalization and Correlations
Vikranth Bejjanki
Jeffrey Beck
Alexandre Pouget

VRAO @ BCS . ROCHESTER . EDU
JEFFBECK @ GATSBY. UCL . AC. UK
ALEX . POUGET @ GMAIL . COM

University of Rochester
Enhanced visual attention leads to improved behavioral performance in a range of perceptual tasks. Previous
neural models have argued that the observed improvement in performance is due to a combination of gain modulation at the single-cell level and divisive normalization at the population level. However, these prior models were
based on the assumption that response variability is independent across neurons in the population, both in the
presence and absence of attention. This assumption is problematic because neuronal variability is correlated in
vivo and these noise correlations are known to change when attention is engaged. Indeed, a recent study found
that noise correlations were significantly decreased when attention was engaged during an orientation discrimination task, and that this decrease in noise correlations accounted for the majority of attention-related improvements
in behavior. Here we investigate the properties of a biologically plausible neural network model of visual attention in the context of an orientation discrimination task. Building on our previous work on analytically quantifying
Fisher information in realistic networks of spiking neurons, we analyze the link between attention-related changes
in network connectivity, noise correlations and Fisher information. In such networks, there is no guarantee that
an increase in gain implemented via a mechanism such as divisive normalization will increase information. We
found however that an increase in the strength of feedforward connections between the LGN and V1 can increase
Fisher information, thus improving behavioral performance. Moreover, this change also decreases pairwise noise
correlations in V1 and V4 in a manner consistent with experimental findings. This work is the first neural theory of attention that can simultaneously account for the pattern of correlations found in cortex, the behavioral
improvement observed with attention, and the gain modulation induced by divisive normalization.

I-43. Short-term memory with balanced excitation and inhibition based on
derivative feedback control
Sukbin Lim
Mark Goldman

SBLIM @ UCDAVIS . EDU
MSGOLDMAN @ UCDAVIS . EDU

University of California, Davis
Persistent patterns of neural activity that last long after the offset of a stimulus are thought to be the neural
substrate for short-term memory. Because the observed decay of persistent activity in memory circuits is much
slower than the typical decay time constants associated with synaptic or intrinsic neuronal dynamics, it has been
suggested that network interactions must be used to prolong the duration of persistent activity. Most often, these
network interactions have been assumed to mediate positive feedback between neurons that supports a longlasting reverberation of activity. However, most positive feedback models do not naturally fit the architecture of
working memory-storing structures in neocortex that have been suggested to exhibit a close balance between excitation and inhibition. Furthermore, these models are highly non-robust against commonly studied perturbations
in network connectivity. Here, we suggest that synaptic interactions in networks with balanced excitation and inhibition can provide a negative derivative feedback that detects drifts in activity and sends a corrective feedback to
the system. We construct a network with excitatory and inhibitory populations in which a derivative-like feedback
arises naturally when excitation has strength equal to inhibition but has slower kinetics. We show that our networks maintain a continuum of stable firing rates even in the presence of intrinsic input-output nonlinearity. These
memory networks operating in this balance regime are robust against many commonly studied perturbations to
synaptic weights that grossly disrupt the performance of persistent activity circuits based on positive feedback.
Specifically, in response to uniform increases in synaptic excitation, synaptic inhibition, or intrinsic neuronal gains,
there is minimal decay or instability in persistent firing. Furthermore, we show that spiking network models implementing derivative feedback generate persistent firing with Poisson-like statistics. This work suggests a new

70

COSYNE 2012

I-44 – I-45
paradigm for short-term memory storage based upon a balanced network with cortical-like architecture.

I-44. Selective Allocation of attention is crucial in setting a capacity limit in
visual short-term memory
Antonio Lara
Jonathan Wallis

HOMERO @ BERKELEY. EDU
WALLIS @ BERKELEY. EDU

University of California, Berkeley
Temporary storage of information in visual short-term memory (VSTM) is a key component of many complex cognitive abilities. However, VSTM it is highly limited in capacity. We used a multiple-item color change detection
task to probe the limits of VSTM in macaque monkeys. Consistent with previous reports, we found that as the
number of items in the memory increased, subjects’ performance significantly decreased. We measured the precision of memory representations by parametrically varying the similarity between sample and test colors. For all
array-sizes, subjects were less likely to detect the change in trials with similar sample and test colors, compared
to trials with highly discriminable colors. We modeled the probability of subjects reporting a change as cumulative
Gaussian function and used this to estimate the precision of VSTM representations. We found that as the number
of items increased, the precision of representations, significantly decreased. Additionally, we found that shifting
attention to one of the items increased that item’s precision at the expense of the remaining items. Determining
the precise neuronal mechanisms of how selective allocation of attention gives rise to variable precision representations is crucial in understanding the fundamental nature of the capacity limit in VSTM. To determine the neuronal
basis of the decrease in precision, we recorded single unit activity from the lateral prefrontal cortex and focused
on the effect that adding a second item into VSTM had on neuronal representations. We found that neuronal
tuning broadened with the addition of a second item to memory. However, when attention was focused on one of
the items in the array, neuronal tuning sharpened to a degree comparable to the tuning for a single item. These
results point to a significant involvement of attention in controlling the fidelity of the contents of VSTM.

I-45. Dynamic networks in frontal cortex support the cognitive flexibility to
switch between rules
Timothy J Buschman1
Eric Denovellis2
Cinira Diogo1
Daniel Bullock2
Earl Miller1

BUSCHMAN @ MIT. EDU
EDENOVELLIS @ GMAIL . COM
CINIRAA . DIOGO @ GMAIL . COM
DANB @ CNS . BU. EDU
EKMILLER @ MIT. EDU

1 Massachusetts
2 Boston

Institute of Technology
University

Complex, goal-directed behavior depends on the ability to flexibly switch between “rules” that define the correct
behavior in a given context. Neuropsychological and human imaging studies suggest the frontal cortex is key to
this flexibility. To understand its neural mechanisms, we recorded from two frontal regions, dorsolateral prefrontal
cortex (PFC) and anterior cingulate cortex (ACC), while monkeys switched between two rules. PFC neurons
played a leading role in rule-switching, encoding the cued rule and switch before ACC neurons. Different subnetworks within PFC supported each rule, synchronizing at beta frequencies (19-40 Hz) during task execution.
The sub-network supporting the more-dominant rule also showed preparatory alpha synchronization (6-16 Hz)
during the competing rule, suggesting a suppressive mechanism. These results support a primary role for PFC in
cognitive flexibility with different rules supported by different synchronous PFC sub-networks.

COSYNE 2012

71

I-46 – I-47

I-46. A winner-take-all mechanism of decision rule discrimination by the supplementary eye field
Supriya Ray
Stephen Heinen

SUPRIYA @ SKI . ORG
HEINEN @ SKI . ORG

The Smith-Kettlewell Eye Research Institute
In the absence of an explicit instruction, rules learned from experiences can guide a decision of whether to initiate
or withhold a movement. How the brain encodes a rule that connects a set of sensory inputs to a motor output
is not well understood. Previously we identified two complementary populations of neurons in the supplementary
eye field (SEF) of macaque monkeys that differentially modulate the firing rate during categorization of a motion
direction in a rule-based decision making task that required the animals to either track a moving target using a
smooth pursuit eye movement or inhibit the movement to maintain fixation. Whether the SEF neurons increase
activity to select the agonist rule that guides a categorical decision, or decrease activity to deselect the antagonist
rule remained unclear. Here we show that the SEF neurons predominantly deselect the antagonist rule prior
to the selection of the agonist rule, and do so at a faster rate when the motion direction invokes the antagonist
rule more certainly. Mutual inhibition between a pair of integrators, one that instantiates movement initiation and
the other that instantiates movement inhibition, account for the predictive deselection of the antagonist rule. The
results suggest that an inhibitory interaction between two complementary populations of neurons is a prerequisite
for discrimination of categorical decision rules by the SEF.

I-47. How prior probability influences decision making: A unifying probabilistic model
Yanping Huang
Abram Friesen
Rajesh Rao

HUANGYP @ CS . WASHINGTON . EDU
AFRIESEN @ CS . WASHINGTON . EDU
RAO @ CS . WASHINGTON . EDU

University of Washington
How does the brain incorporate prior probabilities of stimuli into decision making? Two apparently contradictory
models have been suggested: (1) a model that adds an offset to a decision variable, implying a static effect of
the prior (Carpenter et al., 1995, Dorris & Munoz, 1998, Gold et al., 2008), and (2) a model that dynamically
weights the influence of the prior on the decision variable (Hanks et al., 2011). The additive offset model can
successfully predict behavioral data when discriminating between two low contrast stimuli but is inconsistent
with recent data from a random dots motion discrimination task where the prior probability of motion direction
is manipulated (Hanks et al., 2011). The drift diffusion model has been popular as a model for the random
dots task but to fully account for the experimental data, one is forced to invoke ad-hoc assumptions such as an
urgency signal and a time-varying influence function for the prior. We present a normative framework for decision
making that incorporates prior knowledge in a principled manner. The additive offset model and the dynamically
weighted prior model both emerge naturally when decision making is viewed within the framework of partially
observable Markov decision processes (POMDPs). We show that the ad-hoc assumptions utilized in the drift
diffusion model are unnecessary: the urgency signal emerges naturally in the form of a collapsing deadline and
the time-varying effects of the prior on decision arise directly as a consequence of reward maximization. By fitting
model parameters to the psychometric function in the neutral prior probability condition, we are able to predict
both the psychometric function and reaction times for a new prior probability for motion direction. The predictions
of our normative model are consistent with experimental data reported by Hanks et al. (2011).

72

COSYNE 2012

I-48 – I-49

I-48. Semi-Markov models of the molecular psychophysics of brain stimulation reward
Ritwik K. Niyogi1
Yannick-Andre Breton2
Kent Conover2
Rebecca Solomon2
Peter Shizgal2
Peter Dayan1
1 University
2 Concordia

RITWIK . NIYOGI @ GATSBY. UCL . AC. UK
YANNICK . BRETON @ GMAIL . COM
KENT. CONOVER @ CONCORDIA . CA
RB SOLOMON @ HOTMAIL . COM
PETER . SHIZGAL @ CONCORDIA . CA
DAYAN @ GATSBY. UCL . AC. UK

College London
University

Brain stimulation reward (BSR) has long been recognized as offering the best platform for quantitative psychophysical analyses of the nature of preference and choice. Particular illumination has been cast on effects
of the value of the reward (i.e., the strength of the stimulation in the medial forebrain bundle) and the work requirements (i.e., the price, or the length of time a lever must be pressed) to get those rewards. However, theoretical
accounts of these studies have been restricted in two ways: by being algorithmic rather than computational, and
thus not offering a normative account, and by offering a molar rather than a molecular characterization of the
subjects’ choices, assessing overall rates of pressing rather than the detailed temporal topography of choice. We
extend a semi-Markov model of appetitive vigour (Niv et al, 2007) to address both lacunae. In the model, subjects can receive rewards from BSR or from leisure, and make stochastic, approximately optimizing choices of
both whether to work or to rest, and for how long to do so. In an average reward setting, they pay an automatic
opportunity cost for the time they allocate. We show that when the benefit of leisure increases non-linearly with
leisure duration, it is possible to model in a qualitative manner many molecular features of choices. In particular,
when the value of work exceeds that of leisure then the subjects will pre-commit to working for the whole price;
furthermore, the distribution of leisure bouts post-reinforcement is roughly exponential with short means for high
payoffs, gammas with means possibly exceeding the trial duration for low payoffs, and bimodal with short and
long modes for medium payoffs.

I-49. Neuronal activity in anterior cingulate cortex predicts susceptibility to
distraction
R Ebitz
Michael Platt

REBITZ @ GMAIL . COM
PLATT @ NEURO. DUKE . EDU

Duke University
The oft-cited “conflict monitoring” model of dorsal anterior cingulate cortex (dACC) function predicts increased
neuronal activity in the presence of competing response options that predicts success in adjusting behavior. This
model is based on research conducted in human subjects however, and no study of single-unit dACC function
in macaques has observed conflict-like signals. The present study utilized a social distraction task, which adds
potent, variable-onset images of faces to a simple visually guided saccade task, to generate a behavioral model
of endogenous control in the presence of distraction. A significant proportion of neurons within dACC signaled the
presence of distractors by increasing firing rate. Further, increased firing in response to the distractors predicted
slower reaction times. Finally, pre-task firing rates in an overlapping population of cells predicted distractibility
on subsequent trials, indexed by saccade deflections away from ideal trajectories, independent of recent reward,
error, or distraction history. This study provides the first evidence for conflict signals in the macaque and the
first single-unit characterization of conflict and endogenous control in the dACC. It remains unclear whether the
distractor responses in these cells represent response conflict per se or a potentially evolutionarily older system
for endogenous control.

COSYNE 2012

73

I-50 – I-51

I-50. Cortico-basal ganglia computations in controlled decision making: an
extended diffusion model analysis
Thomas Wiecki
Michael J Frank

THOMAS WIECKI @ BROWN . EDU
MICHAEL FRANK @ BROWN . EDU

Brown University
The antisaccade task has widely been used to research cognitive functions such as response inhibition and executive control. While the Prefrontal Cortex (PFC) is generally associated with cognitive functions, more recently,
a prominent role of the Basal Ganglia (BG) in these processes has been established. We have recently linked
detailed dynamics within a neural circuit model of PFC-BG interactions to higher level computations summarized
by an extension of the Ratcliff drift-diffusion model (DDM). The model consists of a two-staged drift-process: in
the prepotent phase, the drift-process evolves towards responding to the salient but erroneous stimulus direction;
after a delay (i.e. time of cognitive control; tcc), the cognitive control mechanism reverses the drift-process to
exert controlled responding. This basic pattern gives rise to fast errors and slow correct responses, as observed
empirically in the antisaccade task. This model provides two crucial advantages over traditional analysis that
uses the mean reaction time (RT) and accuracy: (i) the model takes the complete RT distribution of correct and
erroneous pro and antisaccade trials into account; and (ii) if fitted to data, it allows inference of cognitive latent
variables underlying response inhibition and executive control, such as strength of bottom-up saliency (i.e. prepotent drift-rate), speed of cognitive control (i.e. tcc and cognitive control drift-rate) and response threshold – all of
which have been linked to distinct cortico-basla ganglia mechanisms. We present the feasibility of this method by
fitting the model to antisaccade data from a large healthy population and compare the results to parameter fits of
a patient population. Finally, we discuss other areas of application for our model such as the flanker and Simon
task.

I-51. The importance of being slow: Extreme-value theory of cognitive representations.
Alberto Bernacchia
Xiao-Jing Wang

ALBERTO. BERNACCHIA @ YALE . EDU
XJWANG @ YALE . EDU

Yale University
The activity of neural circuits depends on a multitude of biological processes, and its dynamics is characterized by
a spectrum of timescales spanning a few orders of magnitude. Whereas fast processes are suitable for reflexive
responses, much slower timescales are needed for cognitive functions. It is currently unknown how neural dynamics adjusts to perform “slow computation”, such as memory and decision-making. How do we make decisions
based on the evidence accumulated for seconds or hours? To address this issue, we develop and test a new hypothesis based on the mathematical concept of extreme value and the operation of maximization. We assume
that neural dynamics can be described by a given set of timescales, determined by diverse biological processes
affecting neural activity. Any given process impacts some neural populations and not others; therefore the dynamics of each population is characterized by a different subset of timescales. When engaged in slow cognitive tasks,
in order to slow down the dynamics, each population becomes dominated by the longest (maximum) timescale in
its own subset. As a consequence, the maximum timescales sampled across different neural populations follow a
specific, universal distribution, the Extreme-value distribution. We tested this prediction by analyzing neural activity in the cortex of primates performing a matching pennies task, where long integration timescales are required
to generate unpredictable decisions. We found that the timescales of neural responses follow the Extreme-value
distribution across neurons. In addition, the timescales of behavioral responses also follow the Extreme-value
distribution, albeit on a different temporal scale. These results point to a new interpretation of the dynamics of
neural activity and the behavior observed in “slow” cognitive tasks. Extreme-value theory may serve to generate
and test new hypotheses on the neural representations and mechanisms underlying cognitive functions.

74

COSYNE 2012

I-52 – I-53

I-52. Executive control and arbitration in reinforcement learning
Anya Skatova1
Seth Madlon-Kay2
Nathaniel Daw3

ANYA . SKATOVA @ GMAIL . COM
SETH . MADLONKAY @ NYU. EDU
NDD 204@ NYU. EDU

1 University

of Nottingham
York University
3 New York University Center for Neural Science
2 New

There is considerable evidence that decisions are influenced by separate competing processes. Recent work
suggests these influences can be formalized computationally as different algorithms for reinforcement learning
(RL): ’model free’ strategies for repeating reinforced actions (associated with the striatal dopamine system), which
are generally computationally simple and easily implemented, and more computationally demanding deliberative
’model-based’ planning methods. However, little is known about how the brain solves the problem of arbitrating
between multiple decision systems. We investigated whether the expression of difficult model-based RL over
simpler model-free RL might involve executive control mechanisms studied in other areas of human cognitive
neuroscience. To address this question, we compared the performance of human subjects (N=48) on an RL
task and a standard test of executive control. The RL task was a sequentially structured MDP that permits
distinguishing model-based and model free contributions to learning by their different consequences for trialby-trial adjustments in action preferences (Daw et al., Neuron 2011). We tested whether the degree of these
influences, assessed by regressing recent rewards on choices, was related to individual differences in executive
control, measured by the reaction time penalty for suppressing an incongruent color-naming response in a Stroop
task. As expected, evidence for both model-free and model-based influences on choices was observed at the
group level in the RL task (mixed logit regression, ps<.01), but with individual variability in their degree. Comparing
tasks, a lower Stroop penalty (better executive control) was associated with a greater influence of model-based RL
(ps<.05). This result was robust to attempts to rule out less specific sources of covariation, such as motivation.
These results suggest that the executive control system contributes to model-based RL or its expression, perhaps
by permitting deliberative planning to be undertaken even in the face of preponent, model-free influences.

I-53. Natural grouping of neural responses reveals spatially segregated clusters in prearcuate cortex
Roozbeh Kiani
Christopher Cueva
John Reppas
William Newsome

ROOZBEH @ STANFORD. EDU
CCUEVA @ GMAIL . COM
JOHN @ MONKEYBIZ . STANFORD. EDU
BNEWSOME @ STANFORD. EDU

Stanford University
A fundamental challenge in studying the frontal lobe is to parcellate this cortex into “natural” functional modules
despite the absence of topographic maps, which have proven so helpful in more primary areas. Here we show
that unsupervised clustering algorithms, applied to large-scale recordings from prearcuate gyrus, reveal spatially
segregated sub-networks that remain stable across contexts and have different physiological signatures. We
recorded from 96-channel microelectrode arrays while two monkeys discriminated the direction of motion in a
dynamic random dot display and reported the perceived motion direction by a saccadic eye movement. Looking
for natural groupings of neurons based on the similarity of their response variation across the entire recording
session, we discovered that the recorded area consists of at least two spatially segregated regions: one closer to
the arcuate sulcus whose responses are tightly correlated with choice and reaction time, and another more medial
and rostral that is less predictive of choice and reaction time. Our finding suggests that area 8r is not homogenous,
but divides naturally into two smaller sub-networks. Importantly, these sub-networks are easily detectable during
all trial epochs including the inter-trial interval, and surprisingly, are defined better by “common noise” within the
areas than by task-evoked responses. The “noise” signal that drives the spatial clustering is temporally broadband

COSYNE 2012

75

I-54 – I-55
over at least three orders of magnitude (.01-17 Hz). Our results demonstrate a powerful new tool for parcellating
the cerebral cortex into sub-networks by objective classification of simultaneously recorded electrophysiological
activity. Our parcellation process works well on ‘spontaneous’ neural activity, and thus bears strong resemblance
to the identification of “resting state” networks in fMRI data sets. In the future, it will be important to determine
whether the spontaneous “noise” activity we are recording is in fact the neural basis of the fMRI resting state
signal.

I-54. Neural threshold for patch leaving decisions in posterior cingulate cortex
David Barack1
Benjamin Hayden2
John Pearson1
Michael Platt1
1 Duke

DLB 28@ DUKE . EDU
BENHAYDEN @ GMAIL . COM
PEARSON @ NEURO. DUKE . EDU
PLATT @ NEURO. DUKE . EDU

University
of Rochester

2 University

Deciding when to leave a depleting resource and move on to a new one is a ubiquitous problem faced by foraging
animals and web-searching humans alike. The Marginal Value Theorem (MVT; Charnov 1976) of behavioral
ecology dictates that the optimal time to leave a patch occurs when the instantaneous resource intake rate falls
below the average for the environment. Animals as diverse as bees, fish, birds, monkeys, and humans behave in
accordance with the MVT, suggesting convergent evolution of decision mechanisms implementing the algorithm
for solving the patch-leaving problem. In a prior study, we showed that the primate brain implements this algorithm
via a rise-to-threshold process (Hayden et al., 2011). Specifically, neurons in anterior cingulate cortex (ACC)
respond phasically each time monkeys choose to stay in a patch, these responses grow with increasing time
in a patch, and monkeys abandon the patch when neuronal responses reach a fixed threshold. When travel
times between patches are increased, the firing rate threshold for leaving increases and monkeys stay longer in
each patch, consistent with the MVT. Here we test the idea that the posterior cingulate cortex (CGp) encodes
the threshold for patch-leaving. ACC and CGp are strongly and reciprocally connected. Further, firing rates of
CGp neurons vary tonically across multiple decisions, and slow variations in neuronal activity therein predict both
exploration behavior and distractability. We recorded from 28 cells in CGp in monkeys performing our foraging
task. We found that CGp neurons respond to each decision to stay in a patch and these responses grow as
monkeys continue to forage in the patch, just like neurons in ACC (Figure 1). Consistent with our hypothesis, we
found that mean firing rates of CGp neurons varied with travel time between patches (Figure 2). Such signals may
set the threshold for leaving a patch.

I-55. Trial-by-trial perceptual learning during odor category decisions: Value,
uncertainty and dopamine
Thiago Gouvea
Gil Costa
Eric DeWitt
Zachary Mainen

THIAGO. GOUVEA @ NEURO. FCHAMPALIMAUD. ORG
GIL . COSTA @ NEURO. FCHAMPALIMAUD. ORG
ERIC. DEWITT @ NEURO. FCHAMPALIMAUD. ORG
ZMAINEN @ NEURO. FCHAMPALIMAUD. ORG

Champalimaud Neuroscience Programme
Choices in a sensory discrimination task need not be influenced by reward history after asymptotic performance
has been reached. However, even after extensive training in a binary odor mixture categorization task, rats continue to show a form of learning that depends on the history of rewards received on recent trials. The magnitude of
this ongoing learning is proportional to the difficulty or uncertainty associated with the stimulus—the distance from

76

COSYNE 2012

I-56
the 50/50 mixture categorization boundary—on the preceding trial. We hypothesize that this learning is the result
of dopamine (DA) dependent reinforcement learning (RL). To test this hypothesis, we first changed ratio of rewards
for correct choices to test sensitivity to reward. When we manipulated the reward ratio in blocks, subjects learned
to bias their choices towards the larger reward, consistent with an RL-like learning process and implying that
choice functions depend not only on stimuli but on choice values. We then manipulated the DA learning pathway
using the DA D2-like agonist quinpirole. Systemic quinpirole injection (i.p., 0.1 mg/kg) sessions were compared
with alternating saline injection sessions. Quinpirole produced a systematic increase in trial-by-trial learning effect independent of prior stimulus difficulty, consistent with the involvement of DA. Average performance and odor
sampling times were not statistically different from control sessions. We are now performing localized pharmacological manipulations to test the involvement of the olfactory tubercle, a ventral striatal DA target downstream
of the primary olfactory areas, in this learning process. We hypothesize that on-going RL in this task reflects a
source of irreducible uncertainty in the sensory representation that causes continuing learning near the category
boundary and that constant increases in DA cause positive reward prediction errors that are attributed primarily
to the prior choice. We suggest an extension of the standard RL model to account for these effects.

I-56. Generic and stimulus-dependent value signals are encoded in human
ventromedial prefrontal cortex
Daniel McNamee
Antonio Rangel
John O’Doherty

DMCNAMEE @ CALTECH . EDU
RANGEL @ HSS . CALTECH . EDU
JDOHERTY @ CALTECH . EDU

California Institute of Technology
In order to facilitate decisions to be rendered between different kinds of goals it has been suggested that the brain
may encode the value of diverse goal-stimuli within a common currency (Berns and Montague, 2002). However,
the neural computations underlying the implementation of a common currency are currently not well understood.
In a previous study (Chib et al., 2009), we showed that a region of vmPFC in humans had overlapping correlations
with the value of three different classes of economic goods: sums of money, food items, and trinkets (books,
DVDs). However the nature of the encoding of the goal-value signal remains unaddressed: goal-values could
be encoded in a generic manner indifferent as to goal identity, or alternatively, goal-values could be represented
in a category specific manner, whereby distinct yet spatially intermingled patterns depict the value of different
classes of good. To discriminate these possibilities we used multivariate pattern analysis techniques on fMRI data
acquired from human participants performing a paradigm similar to that used by Chib et al. 2009. Our results
support the existence of both a generic distributed goal-value code on the medial wall where activity patterns
differentiate high vs. low values irrespective of the good category, as well as for a category-specific goal-value
coding mechanism on the medial orbital surface, in which different distributed patterns of voxels encode for the
value of each class of good but do not generalize to the others. To the best of our knowledge, this is the first
evidence for signals in the human brain that encode both the value and identity of a stimulus in a combined
representation. Such a signal could form the basis of an input into a decision comparator for the purpose of
rendering a decision over specific goals on the basis of their respective values.

COSYNE 2012

77

I-57 – I-58

I-57. Spatial heterogeneity in visual perception; a new conceptual framework
for translation invariance.
Arash Afraz1,2
Maryam Vaziri-Pashkam2
Edith Reshef1
James DiCarlo1
Patrick Cavanagh3

AFRAZ @ MIT. EDU
MVAZIRI . P @ GMAIL . COM
ERESHEF @ MIT. EDU
DICARLO @ MIT. EDU
PATRICK . CAVANAGH @ PARISDESCARTES . FR

1 Massachusetts

Institute of Technology
University
3 Universitet Paris Descartes
2 Harvard

The identity of an object is a fixed property independent of where it appears, and an effective visual system should
capture this invariance. However, we have reported that the perceived gender of a face is strongly biased toward
male or female at different locations in the visual field (Afraz et al, CB, 2010). The spatial pattern of these biases
was distinctive and stable for each individual. Identical neutral faces looked different when they were presented
simultaneously at locations maximally biased to opposite genders. A similar effect was observed for perceived
age of faces. We measured the magnitude of this perceptual heterogeneity for four other visual judgments:
perceived aspect ratio, orientation discrimination, spatial-frequency discrimination, and color discrimination. The
effect was sizeable for the aspect ratio task but substantially smaller for the other three tasks. We also evaluated
perceptual heterogeneity for facial gender and orientation tasks at different spatial scales. Strong heterogeneity
was observed even for the orientation task when tested at small scales. In a new set of experiments, we show
that it is possible to induce new gender-biased locations in the visual field following ∼1 hour of exposure to a face
that orbited the fixation point continuously while morphing back and forth from female to male. Based on these
results and previous work, we formulate a new conceptual framework for the problem of translation invariance. In
the new framework, the traditional view that translation invariance is achieved through the large receptive fields of
neurons in the higher brain areas is challenged. Instead, we suggest that translation tolerant perception depends
on adequate sampling of the visual signal by neurons that are calibrated together (at a temporal scale as short as
one hour) based on visual cues such as spatio-temporal continuity of the stimulus.

I-58. Visual Object Classification is Consistent with Bayesian Generative Representations
Feryal Mehraban Pour Behbahani
Aldo Faisal

FERYAL . MEHRABANPOUR 10@ IMPERIAL . AC. UK
ALDO. FAISAL @ IMPERIAL . AC. UK

Imperial College London
The ability to learn and distinguish categories is essential for human behavior, and the underlying neural computations are actively investigated (Freedman, 2011). Taking a normative view, we can relate categorisation to
the distinction between generative and discriminative classification in machine learning. Generative approaches
solve the categorization problem by building a probabilistic model of how each category was formed and infer
then category labels. In contrast, the discriminative approach learns a direct mapping between input and label.
Recent work (Hsu and Griffiths, 2010) shows human classification is consistent with discriminative and generative classification depending on conditions. We hypothesize that humans employ generative mechanisms for
classification, when not encouraged otherwise. To test this we exploit a counterintuitive prediction for generative
classification, namely how the discrimination boundary between two classes shifts if one category’s distribution is
revealed to be broader during learning. We trained N=17 subjects to distinguish two classes, A and B in two tasks
(two Persian-characters, armadillo-horse stick-drawings). The classes in each task were parameterized by two
scalars; objects for each class are drawn from Gaussian parameter distributions, with equal variance and different
means (class “prototypes”). Next, subjects classify unlabelled examples drawn between the classes, so we can
infer their discrimination boundary. This process is then repeated but includes training data for class A, which lie

78

COSYNE 2012

I-59 – I-60
far away from B. Counter-intuitively, generative classification predicts a shift of the discrimination boundary closer
to B. Conversely, discriminative classifiers will show either no shift of the boundary or a shift of the boundary away
from class B. Our results show that categorization in both tasks is consistent with generative and not discriminative
classifiers, as classification boundaries shifted towards B for both tasks in all subjects. Our experiments provide
an ideal framework for neurophysiological and imaging investigations of the underlying neural mechanisms.

I-59. Event timing in associative learning: From biochemical reaction dynamics to behavioral observations
Ayse Yarali1
Johannes Nehrkorn2,1
Hiromu Tanimoto1
Andreas VM Herz2
1 Max

YARALI @ NEURO. MPG . DE
NEHRKORN @ BIO. LMU. DE
HIROMUT @ NEURO. MPG . DE
HERZ @ BIOLOGIE . UNI - MUENCHEN . DE

Planck Institute for Neurobiology
München

2 Ludwig-Maximilians-Universität

Associative learning requires a neural coincidence detector that modifies a behavioral output according to the relative timing of the relevant external events. Spike timing-dependent plasticity (STDP) has been proposed to serve
this purpose at the millisecond time scale. Olfactory conditioning experiments in fruit flies show the same bimodal
characteristics as STDP – but on a time scale that is three orders of magnitude larger (Fig A). Attempts to explain
this phenomenon through classical STDP (Drew and Abbott, PNAS 2006) are not compatible with electrophysiological characteristics of the fruit fly olfactory system. We here present an alternative approach that is based on
biochemical as well as behavioral data: Fruit flies trained with an odor that precedes electric shock subsequently
avoid this odor (punishment learning); if, on the other hand, shock precedes odor, the odor is approached later on
(relief learning). In the model, an odor-induced Ca(2+) signal and a shock-induced dopaminergic signal converge
in the Kenyon cells during learning, synergistically activating a Ca(2+)-calmodulin-sensitive adenylate cyclase
(AC), which triggers the synaptic plasticity underlying the conditioned odor avoidance (Fig B). AC thus acts as a
molecular coincidence detector between shock and odor. Using a mathematical model of the signaling cascade
we can quantitatively explore this biochemical property of AC and demonstrate that it indeed can generate the
effect of event timing on associative learning. Systematic parameter variations and theoretical analysis shows
that the model framework is robust with respect to parameter changes and model variants. Beyond its applicability for classical olfactory conditioning in fruit flies, the AC coincidence detector may thus also serve as a basic
computational motif in various other associative learning systems.

I-60. Bridging the gap: A third time-scale between plasticity and homeostasis?
Friedemann Zenke1
Henning Sprekeler2
Tim P Vogels1
Wulfram Gerstner1
1 École

FRIEDEMANN . ZENKE @ EPFL . CH
H . SPREKELER @ BIOLOGIE . HU - BERLIN . DE
TIM . VOGELS @ EPFL . CH
WULFRAM . GERSTNER @ EPFL . CH

Polytechnique Fédérale de Lausanne
Universität zu Berlin

2 Humboldt

Recurrent cortical circuits are plastic and their activity dynamics are stable. Most network models fail to incorporate both of these features because the mechanisms for potentiation and depression are intrinsically unstable
and lead either to run-away potentiation and epilepsy-like network states, or silent networks with no activity at all.
To ensure stable, “healthy” dynamics, potentiation and depression have to be tightly balanced. Experimentally,
a large variety of homeostatic mechanisms that control this balance have been characterized, and consequently

COSYNE 2012

79

I-61 – I-62
deployed in theoretical models. Still, there are only few examples of plastic recurrent network models that exhibit
stable long term behavior. Why? We answer this question, using a mean field approach, where we limit our analysis to BCM type learning rules. We show that stability is achievable, but comes with strict temporal requirements.
For plausible network and plasticity parameters we find that the stabilizing mechanism has to act on a time-scale
of seconds to minutes rather than hours as most homeostatic mechanisms indicate. We verify these findings in
simulations of spiking neural networks subject to a triplet-STDP learning rule. Here the homeostatic mechanism
takes the form of a sliding threshold that regulates the ratio between potentiation and depression. Similarly to
the analytical results, the circumstances in which spontaneous background activity can be stable in the presence
of plasticity require threshold sliding on the time scale of minutes. Our results show that plasticity control has
to be constantly adjusted on a time-scale of minutes, whereas the typical time-scale for homeostasis is in the
order of hours. These findings suggest that there should be a third time-scale between the one of plasticity and
homeostasis that preserves network stability. We discuss these findings in regard to recent experimental findings.

I-61. The incorporation of new information into prefrontal cortical activity after learning new tasks
Ethan Meyers1
Xue-Lian Qi2
Christos Constantinidis2

EMEYERS @ MIT. EDU
XQI @ WFUBMC. EDU
CCONSTAN @ WAKEHEALTH . EDU

1 Massachusetts
2 Wake

Institute of Technology
Forest University

Humans and nonhuman primates have the ability to learn complex new tasks. This ability requires new information to be integrated into neural systems that already support other behaviors. To study how task learning
changes neural representations, we analyzed single unit recordings from the prefrontal cortex (PFC), a brain region important for task acquisition and working memory, before and after monkeys learned to perform two new
behavioral tasks. A population decoding analysis of the PFC firing rate activity revealed that there was a large
increase in task-relevant information, and smaller changes in stimulus-related information, after training. This
new information was contained in dynamic patterns of neural activity, with many individual neurons containing
the new task-relevant information for only a short period of time in each trial in the midst of other large firing rate
modulations. Additionally, examining data from dorsal and ventral PFC separately revealed that stimulus information could only be decoded with high accuracy from dorsal PFC, while task-relevant information was distributed
throughout both areas. These findings help reconcile the controversy about whether PFC is innately specialized
to process particular types of information, or whether its responses are completely determined by task demands,
by showing there is both regional specialization within PFC that was present before training, as well as more
widespread task-relevant information that is a direct result of learning new tasks. The results also show that new
information is incorporated into PFC through the emergence of a small population of highly selective neurons
that overlay new signals on top of patterns of activity that contain information about previously encoded variables,
which gives new insight into how information is coded in neural activity.

I-62. Sequence Learning in Primary Visual Cortex
Jeff Gavornik1,2
Mark Bear2,1

GAVORNIK @ MIT. EDU
MBEAR @ MIT. EDU

1 HHMI
2 Massachusetts

Institute of Technology

Stimulus-specific response potentiation (SRP, Frenkel et al. 2006, Cooke and Bear 2010) is a robust form of
experience driven plasticity characterized in the mouse primary visual cortex by a dramatic increase in the mag-

80

COSYNE 2012

I-63 – I-64
nitude of visually evoked potentials (VEPs) following repeated exposures to sinusoidal gratings. This response
potentiation is highly specific for grating orientation, spatial frequency and contrast, is NMDA dependent, occludes
and is occluded by thalamocortical LTP and is rapidly reversed by local cortical infusion of a peptide that inhibits
PKMζ. Sharing attributes and mechanisms with classical LTP, SRP seems to serve a form of perceptual learning
that allows neurons in the visual cortex to recognize familiar spatial visual stimulus patterns. Here we present evidence that the visual cortex can also learn to recognize and predict familiar sequences of spatial stimulus patterns
presented with specific temporal organizations. Similar in some respects to SRP and LTP, our data indicate that
expression of this learning is based on divergent mechanisms and suggest an unanticipated role for intracortical
circuits. While previous work implicates hippocampal involvement in the acquisition of temporal sequences, this is
the first clear evidence showing that sequence learning can occur in a primary sensory region and may provide a
new and easily accessible system for mechanistic investigations into the coding and recall of sequentially ordered
memory.

I-63. BMI learning results in highly precise cell-specific coherence in corticostriatal networks
Aaron C Koralek
Jose Carmena

AKORALEK @ BERKELEY. EDU
CARMENA @ EECS . BERKELEY. EDU

University of California, Berkeley
For any given cognitive task, our nervous system must coordinate the activity of large ensembles of individual
neurons across distant cortical sites. However, the ways in which distinct neuronal ensembles can coordinate their
activity and communicate effectively remains a key problem in neuroscience. Recent theories have proposed that
coherent oscillations across neural ensembles could serve as the substrate for selective coordination between
cells and the development of functional cell assemblies. Here we show that coherence develops between primary
motor cortex and the dorsal striatum over the course of learning in a novel neuroprosthetic task. Rats learned
to volitionally modulate activity in primary motor cortex while receiving auditory feedback about neuronal activity
levels to guide performance. Strong coherent interactions developed between the two regions in low frequency
bands as learning progressed. Importantly, this coherence was specific to task-relevant cells, despite them being
intermingled with non-task-relevant cells. The phase offset of this coherence closely matches estimates of the
corticostriatal conduction delay measured using intracortical microstimulation, suggesting that the developing
interactions are timed with high precision between the two regions. In addition, spikes from primary motor cortex
were followed by a consistent phase in the dorsal striatum, suggesting that network feedback reinforces this
synchrony. Together, these results suggest that highly precise coherence develops during learning in task-relevant
neuronal populations, and further, that entrainment of oscillatory activity serves to synchronize large-scale brain
networks and facilitate communication across neuronal populations.

I-64. Dendritic processing underlying temporal integration
Melanie Lee1
Kayvon Daie1
Sherika Sylvester1
Dimitry Fisher2
Mark Goldman2
Emre Aksay1
1 Cornell

MEL 2010@ MED. CORNELL . EDU
KPD 7@ CORNELL . EDU
SJS 2006@ MED. CORNELL . EDU
DFISHER @ UCDAVIS . EDU
MSGOLDMAN @ UCDAVIS . EDU
EMA 2004@ MED. CORNELL . EDU

University
of California, Davis

2 University

In neural integrators, transient input signals are mathematically accumulated into sustained neuronal activity. Nu-

COSYNE 2012

81

I-65
merous models of temporal integration have been proposed. Some depend purely on recurrent synaptic feedback
within a population of neurons. Others instead rely on regenerative currents within the dendritic compartments
of individual neurons that may yield propagating wavefronts of elevated calcium or stationary plateaus resulting
from voltage bistability. Here we aimed to discriminate between these proposed mechanisms by imaging the
dendritic activity of oculomotor integrator neurons during behavior. We loaded integrator neurons of the larval
zebrafish with Oregon green BAPTA-1 AM through bolus injection into the caudal hindbrain. Recordings of the
soma showed the expected optokinetic, saccadic, and fixation sensitivity; recordings in the neuropil showed a
mixture of these signals with spatial heterogeneity at the few microns scale. We then targeted selected somata
using a micropipette containing Alexa 594 dextran, electroporated, and visualized the dendritic morphology of the
targeted neuron while imaging calcium dynamics. During optokinetic tracking we found along numerous dendritic
branches of a given cell distinct hotspots of activity that were 1−5µm long and separated by at least 2µm. Activity
in the hotspots were generally correlated with somatic activity, but also exhibited some heterogeneity. These data,
although early, do not seem consistent with models proposing propagating waves of elevated calcium. To explore
possible mechanisms underlying these observed response patterns, we have developed a conductance-based
network of spiking neurons with dendritic subunits containing voltage-dependent currents capable of generating
hotspots. The network integrates its inputs and exhibits heterogeneous dendritic outputs consistent with experimental observations. Overall, these experiments and modeling provide a rare case in which the role of individual
dendritic subunits can be directly connected to network function and animal behavior.

I-65. Loom sensitive neurons link computation to action in the Drosophila
visual system
Saskia de Vries
Thomas Clandinin

SASKIAD @ STANFORD. EDU
TRC @ STANFORD. EDU

Stanford University
Many animals extract specific cues from rich visual scenes to guide appropriate behaviors, such as navigation
and collision avoidance. The complexity of these visual signals requires neural circuits to link particular patterns of
motion to specific behavioral responses. In flies, the lobula complex, the third neuropil of the optic lobe, is thought
to underlie such motion processing. This complex comprises two ganglia, the lobula and the lobula plate, and
contains a diverse array of cell types. While the visual response properties of many of these cells remain unknown,
electrophysiological studies of specific subsets of lobular neurons, including lobula plate tangential cells and small
target motion detector neurons, have described cells that become tuned to specific patterns of movement through
the integration of local motion cues. However, in no case has the activities of identified neurons in the lobula
complex that are tuned to a particular pattern of motion been demonstrated to be critical to trigger the specific
behavioral response appropriate to that signal. Here we examine both the visual responses and behavioral role
of a group of genetically identified neurons in the Drosophila lobula complex, the Foma-1 neurons. We found that
these cells are tuned to detect looming stimuli, visual motion signals generated by an object on a direct collision
course with the fly. Such stimuli elicit escape behaviors from flies, allowing them to avoid imminent collisions.
Using genetic tools to silence and activate these neurons, we demonstrate that these cells are necessary for
normal responses to looming stimuli, and that their activation is sufficient to trigger an escape response, even
in the absence of visual input. Thus, these neurons serve as a nexus that integrates specific motion cues and
triggers the escape response in this sensorimotor pathway.

82

COSYNE 2012

I-66 – I-67

I-66. Identifying the neural initiation of a movement
Biljana Petreska1
Matthew T Kaufman2
Mark Churchland3
Stephen Ryu2
Krishna Shenoy2
Maneesh Sahani1

BILJANA . PETRESKA @ GMAIL . COM
MATT 235@ STANFORD. EDU
MC 3502@ COLUMBIA . EDU
SEOULMAN @ STANFORD. EDU
SHENOY @ STANFORD. EDU
MANEESH @ GATSBY. UCL . AC. UK

1 University

College London
University
3 Columbia University
2 Stanford

The preparation of voluntary movements takes time (Rosenbaum, 1980), suggesting that such actions depend
on sequentially readying and then executing a motor plan. When these two phases are separated by a delay,
neurons in the motor cortex often modulate their firing during both, indicating that the cortical circuits involved in
the two computations overlap. Thus, to start the movement, the cortex must switch from a preparatory mode to
an executory one. Is the moment of this switch evident in the neural firing? Reasoning that the switch should
manifest as a change in computational dynamics, we fit a “hidden switching linear dynamical system” model
(Petreska et al., NIPS, 2011) to multi-neuron data from macaque dorsal premotor (PMd) and primary motor (M1)
cortices, recorded as the animal performed delayed reaches. Modelling the spike trains alone—without reference
to external events—we could indeed see a trial-specific change in dynamics in both areas that appeared to
anticipate initiation of arm movement. The change in M1 tended to follow that in PMd (mean shift = 36±0.13
ms), and its estimated timing on each trial was more tightly correlated with the beginning of movement (crossvalidated r2 = 0.70±0.03) than was the estimated change in PMd (cross-validated r2 = 0.51±0.06). It has been
suggested that movements are triggered when population firing reaches a threshold. If so, then applying a similar
threshold to the recorded population might better predict movement onset. In fact, no matter how we estimated or
thresholded firing rates, this approach could explain no more than 0.37±0.02 of the reaction-time variance. Thus,
we conclude that the neural initiation of movement is better identified with a change in estimated circuit dynamics
than with the crossing of a firing-rate threshold.

I-67. How does pacemaking in the globus pallidus affect striatal microcircuits?
Arpiar Saunders
Kevin Beier
Bernardo Sabatini

ASAUNDER @ FAS . HARVARD. EDU
BEIER @ FAS . HARVARD. EDU
BSABATINI @ HMS . HARVARD. EDU

Harvard Medical School
The basal ganglia (BG) are a collection of highly interconnected forebrain nuclei that network the cortex, thalamus
and brainstem and are critically involved in action-selection and reward-based learning. The circuit structures
underlying this functionality are poorly understood, but likely involve coordinated activity in cell assemblies across
the BG. The globus pallidus (GP) is a hub of this network, containing GABAergic projection neurons that innervate
all of the BG. Most GP neurons are spontaneously active yet their firing is desynchronized, providing independent
inhibitory pacemaking throughout the BG. Here we use transgenic mice, optogenetics, and viral synaptic tracing
to define the physiological properties and anatomical topography from a single class of genetically defined GP
neurons. We focus on projection from the GP to the striatum, the major input nucleus of the BG. Recordings
from acute brain slices demonstrate that GP neurons expressing the calcium binding protein parvalbumin are
spontaneously active around gamma frequency and selectively innervate two types of striatal interneurons with
different synaptic properties. Thus while inhibitory projections from the striatum heavily innervate the GP and are
sufficient to pause GP pacemakers, these pacemakers indirectly affect striatal output by differential modulation
of two types of local interneurons. Are these synaptic loops between the GP and striatum “closed” or “open”?

COSYNE 2012

83

I-68 – I-69
In other words, is the function of these projections more akin to feedback or horizontal inhibition? We are using
retrograde viral tracing from the two classes of GP-connected striatal interneurons to determine the topography
of these synaptic loops.

I-68. Spike time-dependent plasticity can organize a recurrent network to generate grid cell responses
John Widloski
Ila Fiete

WIDLOSKI @ PHYSICS . UTEXAS . EDU
ILAFIETE @ MAIL . CLM . UTEXAS . EDU

University of Texas at Austin
There are many competing models of possible mechanisms underlying grid cell activity, but none explain how
the complex networks assumed in such models could arise through development and plasticity. We present
a biologically plausible model for the formation of the grid cell network with periodic spatial neural responses.
Further, the network develops the ability to path integrate. Our model uses an asymmetric spike time-dependent
plasticity (STDP) rule applied to an initially unstructured network of spiking neurons that receive different speedmodulated head-direction inputs and also receive randomly assigned place cell-like inputs. As the animal explores
its spatial environment, the STDP rule causes neurons firing at short time-lags – i.e. neurons receiving similar
place inputs – to become recurrently connected. This produces a center-surround structure in the weights, if,
for visualization, the neurons are rearranged topographically according to their place inputs. At the same time,
the STDP window enhances connectivity between neurons receiving similar direction inputs. This enhancement
induces slight asymmetries in the network weights based on the direction tuning of the cells. The end product
– an asymmetrically shifted center-surround weight structure – produces grid-like activity patterns in the neural
population. The asymmetries drive movements of the population activity pattern in proportion to animal velocity,
which enables path integration. The mature network displays the low- dimensional continuous attractor (CA)
dynamics of models (Fuhs & Touretzky, 2006; Burak & Fiete, 2009) that successfully predict many features of
the grid cell response. The simplicity and plausibility of this developmental model should lay to rest critiques
about the complexity of wiring assumed in CA models.The model explains why the mature network need not be
topographic, and generates predictions about inputs to and maturation of responses in the grid cell network during
development (Wills et al., 2010; Langston et al. 2010).

I-69. Which edge probabilities reveal long-range horizontal connections in
visual cortex?
Matthew Lawlor
Steven Zucker

MATTHEW. LAWLOR @ YALE . EDU
STEVEN . ZUCKER @ YALE . EDU

Yale University
Anatomically the long-range horizontal connections in superficial V1 appear highly complex but densely clustered.
This clustering, plus the cortical organization revealed by optical imaging, implies an order to their structure.
Under the assumption that they are involved in boundary inference, we seek to understand this order functionally.
Informally, the logic is that edge pairs, triples, ..., and so on will “wire” together provided they occur together
in natural images. The question is: what is the smallest order of edge statistics that predicts the known longrange horizontal connection statistics. There exists a literature on pairwise statistics, but this only agrees in the
mean with existing population data; different models build on this, such as the association field, make incorrect
predictions about the variance. We extend edge statistics to triples in a large natural image corpus. Because
the underlying probability distribution is high-dimensional (for our experiments, e.g. 20 pixels x 20 pixels x 10
orientations = 4,000 dimensions) we develop a non-linear dimensionality reduction technique to reveal where
the probability mass concentrates. In brief, we find that third-order statistics are necessary to predict the mean

84

COSYNE 2012

I-70 – I-71
and variance of excitatory long-range horizontal connections, but inhibitory connections are more uniform. This
asymmetry agrees with physiology. Contingency table tests confirm statistical validity.

I-70. What kinds of local motion signals are present in naturalistic movies?
Eyal Nitzany
Jonathan Victor

EIN 3@ CORNELL . EDU
JDVICTO @ MED. CORNELL . EDU

Cornell University
Extraction of motion from visual input plays an important role in many visual tasks, such as separation of figure
from ground and navigation through space. Motion analysis begins with the extraction of local motion signals.
Several kinds of local motion signals have been distinguished based on mathematical and computational considerations (for example, Fourier and non-Fourier), but little is known about their prevalence in the real world. Here
we address this by examining the strength of each kind of local motion signal in naturalistic movies. To approach
this question, we first note that all local motion signals are characterized by correlation in a slanted spatiotemporal region. Specific different kinds of local motion signals correspond to specific region shapes and orders of
correlation. Fourier motion, for example, corresponds to pairwise correlation in two checks, arranged on a slant in
space-time. Non-Fourier motion, as identified by Chubb and Sperling (1988), corresponds to motion of a simple
feature, such as an edge or a luminance change, independent of the sign of that feature. This is equivalent to
a fourth-order correlation in a spatiotemporal parallelogram. A third kind of local motion signal, “glider motion”
(Hu et al., 2010) corresponds to third-order correlation within a spatiotemporal triangle. Thus, the prevalence of
local motion signals in natural scenes can be estimated by determining the extent to which these correlations are
present in spacetime patches of binarized movies. We apply this technique to several Hollywood movies. The
results show that all three kinds of motion signals are present. The proportions are relatively constant across
movies, but from scene to scene, different kinds of motion signals predominate. This suggests that the different
kinds of motion signals are non-redundant, and raises the possibility that the different kinds of motion signals
occur in distinct contexts.

I-71. Sparse coding neurons encode individual vocalizations in complex auditory scenes
David Schneider
Sarah Woolley

DMS 2159@ COLUMBIA . EDU
SW 2277@ COLUMBIA . EDU

Columbia University
Many animal species communicate with one another vocally, often in a distracting acoustic background. Although
psychophysical experiments and everyday experience suggest that the brain filters background sounds to focus
on a single vocalization (“the cocktail party effect”), the neural correlates of this ability remain unclear. Here,
we experimentally characterize a putative neural mechanism for extracting important signals from complex sensory scenes. We trained songbirds to behaviorally discriminate among vocalizations presented in a distracting
acoustic background and we recorded from individual neurons at multiple stages of the auditory pathway. We
found that the neural representation of vocalizations transformed from a continuous and redundant code in early
auditory areas into a sparse and distributed code in later regions of the auditory system. In contrast to continuous firing neurons, sparse coding neurons were better at extracting individual vocalizations from a distracting
background, commensurate with psychophysical findings. Sparse coding neurons extracted vocalizations from
auditory scenes at higher levels of background sound; retained a noise-invariant spike-train pattern in the presence of low background sound; and abruptly stopped firing when the background reached a critical threshold.
Using electrophysiology, pharmacological manipulations and simulations, we characterized a simple neural circuit
of fast excitation and delayed and recurrent inhibition that transforms a continuous input into a sparse output.

COSYNE 2012

85

I-72 – I-73
This same circuit also shuts down during auditory scenes with high levels of background sound, consistent with
physiology.

I-72. A normative theory of Weber’s law
Jeffrey Beck1
Ingmar Kanitscheider2
Alexandre Pouget1
1 University
2 University

JEFFBECK @ GATSBY. UCL . AC. UK
IKANITSCHEIDER @ BCS . ROCHESTER . EDU
ALEX . POUGET @ GMAIL . COM

of Rochester
of Geneva

A diverse array of studies have shown that discrimination thresholds for sensory variables are often proportional
the stimulus magnitude, a phenomenon known as Weber’s law. Typical explanations invoke a finely tuned combination of a nonlinear neural representation and noise in neural responses. For instance, one such explanation
assumes that neural responses are sensitive to the logarithm of the sensory variable (or the ratio of sensory variables) corrupted by noise with fixed variance. We propose a purely computational explanation for Weber’s Law
which does not require an appeal to internal representation or noise. Rather, it arises purely from the statistical
nature of the problem faced by the brain. For example, imagine having to estimate the number of items in a
scene. If we treat the items as blobs of activity in feature maps, the total sum of the activity provides a numerosity
estimate. If the variability within the feature map is independent, the variance of the estimate scales with the
mean numerosity, in violation of Webers’ law which predicts that the variance scale with the square of the mean.
However, the independence assumption is problematic because activity in such maps is likely to be scaled by
global parameters which vary from trial to trial such as the overall luminosity in the image or the level of attention.
The presence of such global scaling parameters correlates neural activity in such a way that the variance of the
total sum scales with the square of the mean, thus yielding Weber’s law. We argue that this simple intuition can
be generalized to more complex situations by showing that optimal inference in scale mixture models such as the
Gaussian Scale or Gamma-Gamma mixture models precisely replicate Weber’s Law when the variance of the
scale parameter is large enough.

I-73. Efficient coding of visual motion signals in the smooth pursuit system
Leslie Osborne
Patrick Stinson

OSBORNE @ UCHICAGO. EDU
PWS @ UCHICAGO. EDU

University of Chicago
Performance in sensorimotor behaviors guides our understanding of many of the key computational functions of
the brain: the representation of sensory information, the translation of sensory signals to commands for movement, and the production of behavior. Eye movement behaviors have become a valuable testing ground for
theories of neural computation because the neural circuitry has been well characterized and eye movements can
be tightly coupled to cortical activity. Here we show that smooth pursuit eye movements, and the cortical sensory
signals that mediate them, demonstrate the hallmarks of efficient coding. Barlow proposed that neurons adapt
their sensitivity as stimulus conditions change in order to maintain efficient coding of sensory inputs. Evidence for
efficient coding of temporal fluctuations in visual contrast has been observed in the retina, thalamus, and visual
cortex. We asked whether adaptation to stimulus variance generalizes to higher cortical areas whose neurons
respond to features of visual signals that do not drive adaptation in the periphery and whether such adaptation
impacts performance of visually-driven behavior. Specifically, we studied the impact of adaptation to dynamic
fluctuations in motion direction on smooth pursuit. We recorded eye movements of monkeys pursuing moving
targets with an added stochastic perturbation. We found that the amplitude of the linear filter that relates eye to
target movement rescaled in proportion to target motion variance, consistent with the efficient coding hypothesis.

86

COSYNE 2012

I-74 – I-75
Steps in target motion variance create a transient decrease in the information capacity of pursuit behavior (either
a low to high variance transition or vice versa). To test whether the adaptation to motion variance arises in the
visual system, we recorded single units in the middle temporal cortical area (MT). The linear component of MT
neuron responses (e.g. spike-triggered average stimulus) also rescales with target motion variance consistent
with the effect on pursuit.

I-74. Stereotyped and diverse computations in third order olfactory circuits
Mehmet Fisek
Rachel Wilson

FISEK @ FAS . HARVARD. EDU
RACHEL WILSON @ HMS . HARVARD. EDU

Harvard Medical School
Summary: Understanding central odor representations is of fundamental importance to neuroscience. However,
basic principles of higher order olfactory processing remain poorly understood. In vertebrates and invertebrates,
secondary olfactory neurons project to multiple distinct third order areas. It has been proposed as a basic principle
that these anatomically distinct areas are functionally specialized such that they fall into two broad categories:
those that perform primarily associative computations and underlie olfactory learning capabilities, and those that
perform stereotyped computations and mediate innate olfactory behaviors. While the “associative” areas have
received considerable attention, the “stereotyped” areas have received less scrutiny. In fact, there exists little
physiological evidence for this sort of stereotypy in any animal. Here we describe sensory representations in
the lateral protocerebrum, a putative “stereotyped” area in the olfactory system Drosophila melanogaster. We
describe two different computations that are performed by anatomically distinct and genetically identifiable types
of third order neurons that reside in this area. Using in vivo whole cell electrophysiology combined with genetic
labeling, we demonstrate that indeed, the computations performed by both of these neuron types are stereotyped
across individual flies. Moreover, using dual whole-cell recordings, we show that the feedforward connectivity
that underlies these computations is also stereotyped across individuals. In particular, each of these neuron
types receives direct input from specific second-order neurons residing in identified olfactory glomeruli. Having
identified presynaptic partners for our third order neurons, we describe the transformation of odor representations
as they travel into this higher brain region. We compare and contrast connectivity and transformations for these
two neuron types and show that while one type appears to simply sum direct excitatory inputs from multiple
glomeruli, the other combines direct excitation with strong indirect inhibition. We propose a functional role for
each computation and describe our preliminary work on testing our proposals.

I-75. Sensation of a “noisy” whisker vibration in rats: Psychometric and neurometric analysis
Arash Fassihi
Athena Akrami
Vahid Esmaeili
Mathew E Diamond

FASSIHI @ SISSA . IT
AKRAMI @ SISSA . IT
VAHID. ESMAEILI @ SISSA . IT
DIAMOND @ SISSA . IT

SISSA
We have devised a delayed comparison task for rats; they discriminate between two stimuli delivered to their
whiskers. The rats learn to position their snout in a nose hole such that their head is stationary and their right-side
whiskers are firmly in contact with a plate that delivers motion along the anterior-posterior axis. Each stimulus
is a sequence of position values drawn from a truncated Gaussian distribution with standard deviation denoted
as S. The first stimulus is called “base” (defined by S1) and the second stimulus “comparison” (defined by S2).
In the standard protocol, the duration of the base and comparison stimuli both are 400ms. By considering trials
with S1 fixed while S2 spans a range of values, we construct psychometric curves. Performance is above 75%

COSYNE 2012

87

I-76 – I-77
correct when the Standard Deviation Index (STDI) defined as the absolute value of |S1-S2|/(S1+S2))—is larger
than 0.1. It has been debated how sensory systems accumulate stimulus information over time. To investigate
this, we varied the duration of the comparison stimulus: 200, 400, or 600ms. The rats’ performance improved for
longer comparison stimuli, suggesting that for stimuli with a probabilistic structure, evidence can be accumulated
over time. A preliminary analysis of neuronal activity from anesthetized rats provides support for a simple model
where (i) both firing rate and spike count are correlated with the stimulus position standard deviation S; and (ii)
differences in both firing rate and spike count could be decoded to allow comparison of the base and comparison
stimuli in 400ms condition. The change in performance as a function of variable stimulus duration suggests that
firing rate is a better candidate than spike count as the decoded feature, in contrast to previous studies.

I-76. Neural processing of social signals in the medial amygdala
Joseph F Bergan1,2
Yoram Ben-Shaul3
Catherine Dulac1,2
1 Harvard

JOEBERGAN @ GMAIL . COM
YORAMB @ EKMD. HUJI . AC. IL
DULAC @ FAS . HARVARD. EDU

University

2 HHMI
3 Hebrew

University of Jerusalem

All animals must recognize conspecifics to carry out social interactions that ensure individual survival and the
transmission of genes to subsequent generations. In many species, signals essential for guiding social interactions are communicated by the emission and detection of specific chemical cues. Rodents rely heavily on the
vomeronasal system (VNS) to guide innate behaviors including: predator avoidance, territorial defense, mating,
and parenting. An important goal is determining how sensory signals transduced by the vomeronasal organ
(VNO) are transformed to elicit adaptive behaviors. Recent studies have revealed the molecular logic within the
sensory epithelium of the VNO by which different vomeronasal receptors extract biological information: including
sex, species, and physiological status, about animals in the immediate environment. This information is processed by subsequent nuclei of the VNS including the accessory olfactory bulb (AOB) and the medial amygdala
(MeA). At present, little is known about how sensory information is processed within this serial network or how
this information is used to guide adaptive behaviors. We recorded the electrophysiological activity of single units
using a novel anesthetized preparation in order to reveal the sensory representation in the MeA. These experiments demonstrate a set of emergent properties not present in the VNO or AOB including: the topographical
segregation of sensory responses into distinct subnuclei of the MeA, a sharpened representation of the sensory
stimulus space, and the emergence of sexual dimorphic responses to reproductively relevant cues. We believe
that these changes reflect an important stage in the translation of sensory cues into adaptive behavior. Indeed,
these sexually dimorphic responses are not present in juvenile animals or in mice with perturbed sex-steroid signaling. Taken together, these results offer new insights regarding how the VNS processes social cues and provide
a new avenue to investigate how differences in neural circuitry lead to individual differences in behavior.

I-77. Summary statistics in auditory perception
Josh McDermott1
Eero P Simoncelli1,2
1 New

JHM @ CNS . NYU. EDU
EERO. SIMONCELLI @ NYU. EDU

York University

2 HHMI

Sensory signals are transduced at high resolution, but their structure must be stored in a more compact format.
We hypothesize that time-averaged sound statistics form a perceptual code used by the auditory system to summarize the acoustic structure of natural sounds. To explore this hypothesis, we generated synthetic sound stimuli

88

COSYNE 2012

I-78 – I-79
using a recently developed model of auditory texture representation (McDermott & Simoncelli, 2011). The synthesis procedure shapes samples of noise to have the same summary statistics as a target sound (despite having a
completely different acoustic waveform), matching the moments and pair-wise correlations of simulated cochlear
channel envelopes and their modulation bands. We conducted experiments with excerpts of five-second signals
synthesized to have the same statistics as a real-world sound texture (rain, fire, etc.). The summary statistics of
different excerpts were variable when the excerpts were short, but converged to the statistics of the full-length
signal as the excerpt length increased. We first presented listeners with excerpts from two signals with distinct
statistics, asking them to judge whether the excerpts arose from the same sound source. Performance improved
with excerpt duration, presumably because the summary statistics that differentiate different sound types are more
accurate for longer excerpts. We then presented excerpts of two different exemplars synthesized from the same
summary statistics, and asked listeners to judge whether these were identical. Performance in this task was
good for short excerpts but paradoxically declined with duration, even though the longer sounds contain more
information to support discrimination. The results can be explained by supposing that listeners represent sounds
with time-averaged statistics, such that discrimination of two sounds becomes difficult as their summary statistics
converge to the same values. Such statistical representations produce good categorical discrimination, but limit
the ability to discern fine-grained temporal detail.

I-78. The neural representation of behaviorally relevant acoustical sequences
Justin Kiggins
Timothy Gentner

JKIGGINS @ UCSD. EDU
TGENTNER @ UCSD. EDU

University of California, San Diego
Sensory events unfold in time, and the relative positioning of events is particularly important in speech and language perception. Vocal production in songbirds, a model for human speech production and perception, shows
temporal structure at multiple hierarchical levels, including sequencing of ethologically relevant auditory objects
(motifs), and European starlings are sensitive to this structure when using song to identify other starlings. Responses in mammalian auditory cortex can be modulated by earlier acoustical events, but it is unclear whether
these long timescale (800+ ms) response dependencies reflect learned statistical properties of the environment.
To explore the neural representations of learned sequences, we trained starlings on an operant conditioning task
in which the only relevant information available to solve the task were transitions between motifs. We show that
starlings can discriminate between strings of motifs based solely upon transition statistics, independent of motif
identities. Subject performance covaried with trial difficulty. In the face of ambiguity, performance is consistent with accumulating evidence from individual transitions during the presentation of a string and differentially
weighing learned transition cues. Importantly, this training protocol establishes behaviorally meaningful transitions (associated with either reward or no reward, or are non-diagnostic) which can then be used to explore the
neural representation of behaviorally meaningful sequences, independent of representations of individual motifs.
After birds reached criterion performance, we recorded extracellular single units and local field potentials from a
region in the starling forebrain analogous to auditory cortex. We find that approximately half of recorded units in
NCM show sensitivity to motif transitions (i.e., responses during the second motif in a pair are modulated by both
the first and second motif), indicating that sequence information is represented in the spiking response. Further
analyses explore the extent to which behaviorally meaningful transitions are represented by spiking and local field
potential activity.

I-79. Feedback from retinal ganglion cells to the inner retina
Hiroki Asari
Markus Meister

ASARI @ FAS . HARVARD. EDU
MEISTER @ FAS . HARVARD. EDU

Harvard University

COSYNE 2012

89

I-80 – I-81
Retinal ganglion cells are thought to be strictly postsynaptic within the retina. They carry visual signals from the
eye to the brain, but do not make chemical synapses onto other retinal neurons. Nevertheless, they do form
gap junctions with other ganglion cells and with amacrine cells in the inner retina. This provides a possible route
for ganglion cell signals to feed back into retinal circuitry via electrical connections. To investigate this issue, we
electrically stimulated the optic nerve emerging from an isolated salamander retina. Simultaneously we presented
visual stimuli and recorded the firing of many ganglion cells using a multi-electrode array. Here we report that optic
nerve shock has pronounced effects on the activity of retinal ganglion cells. In absence of a visual stimulus, the
optic nerve shock produced immediate antidromic spikes (at a delay of ∼1 ms) followed by a period of enhanced
firing (10s of ms) and a suppression of firing on longer time scales (100s of ms). However, the proportion and
time course of the two effects varied greatly among ganglion cells. These modulations were largely eliminated
by blocking gap junctions pharmacologically, suggesting that the feedback pathways from ganglion cells involve
electrical synapses. By pairing the nerve shock with visual stimulation, we encountered a rich set of changes in
the light response. Both the gain and the kinetics were affected, in either direction, and at various delays from
the optic nerve shock. Thus it appears that ganglion cells are actively involved in visual computations rather
than merely collecting postsynaptic signals from the inner retina. Several feedback pathways seem to contribute,
accounting for the diversity of affected response patterns. Taken together, these results suggest that the principal
neurons of the retina do participate in a recurrent circuit, much as has been demonstrated elsewhere in the brain.

I-80. Evidence for a class of dLGN neurons with extra-strong classical surrounds in the awake rat
Balaji Sriram
Pamela Reinagel

BSRIRAM @ UCSD. EDU
PREINAGEL @ UCSD. EDU

University of California, San Diego
Classical center-surround antagonism in the early visual system is thought to serve important functions such as
enhancing edge detection and increasing sparseness. The resulting image representation depends upon the
relative size and strength of the center and surround of each neuron. Surround strength has been measured in
the retina, dorsal Lateral Geniculate (dLGN), and primary visual cortex (V1), mostly in anesthetized preparations.
Response properties in the dLGN differ in awake and anesthetized conditions, however (Alitto et al, 2011). We
revisit the center-surround architecture of dLGN neurons in the un-anesthetized rat. We report the spatial frequency tuning responses of N=68 neurons and fit these tuning curves to a classic difference-of-Gaussians (DOG)
model of the spatial receptive field. We find that some dLGN neurons in the awake rat have weak surrounds,
but the majority have well balanced center and surround weights. Surprisingly, a substantial fraction of neurons
have a stronger surround than center. Within the space of DOG models, strong surrounds were necessary and
sufficient to explain the bimodal tuning curves we observed in spatial frequency tuning data of these cells. These
results indicate that prior measurements of the classical surround obtained from measurement in anesthetized
preparations may be an underestimate.

I-81. Probing mechanisms of contrast adaptation in retina with modeling of
synaptic currents and spikes
Daniel A Butts1
Yanbin V. Wang2
Jonathan B Demb2

DAB @ UMD. EDU
YANBIN . WANG @ YALE . EDU
JONATHAN . DEMB @ YALE . EDU

1 University
2 Yale

of Maryland
University

Nonlinear processing in retinal ganglion cells has been implicated in a variety of response properties important

90

COSYNE 2012

I-82
for natural vision, including adaptation to stimulus contrast and the generation of temporal precision. Here, we
use nonlinear modeling applied to in vitro recordings from mouse retinal ganglion cells (RGCs) to understand
how nonlinear response properties are derived from RGC synaptic input and computations within each neuron.
RGCs were recorded both in cell-attached mode (spikes) and whole-cell voltage-clamp mode (excitatory synaptic
currents), and later classified using dendritic stratification. We focus on off-alpha RGCs, which strongly adapt
to contrast, and additionally have transient synaptic currents not explained by standard linear-nonlinear cascade
modeling. A more complex nonlinear model applied to both synaptic currents and spikes reveals that both the
effects of adaptation and precision can be largely explained by a nonlinear “suppression” that is delayed relative
to the neuron’s excitatory input. Furthermore, this suppression can be predicted by a simple two-parameter
synaptic depression model acting on the modeled excitatory inputs. Such a description of contrast adaptation is
an alternative to gain control models based on feedback from contrast sensors in the retina, and works as follows.
Synaptic depression generates transient responses at high contrast, but has a much smaller effect at low contrast,
when there is much less depletion of synaptic resources. The resulting contrast-dependent modulation of synaptic
depression contributes to familiar changes in processing at high contrast: lower gain and faster temporal filtering.
Furthermore, modeling describes how these effects of adaptation are accentuated through interaction with spike
generation. The resulting nonlinear processing in the retina appears to be amplified in downstream areas and
fundamentally shapes the information used by the rest of the visual system.

I-82. Decorrelation of retinal response to natural scenes by fixational eye
movements
Irina Yonit Segal
Michael Gedalin
Ronen Segev

IRINA . YONIT @ GMAIL . COM
GEDALIN @ BGU. AC. IL
RONENSGV @ BGU. AC. IL

Ben-Gurion University of the Negev
Fixational eye movements are critical for vision since without them the retina adapts fast to a stationary image
and the entire visual perception fades away in a matter of seconds. Still, the connection between fixational eye
movements and retinal encoding is not fully understood. To address this issue, it was suggested theoretically
that fixational eye movements are required to reduce the spatial correlations which are typical for natural scenes.
By analyzing a model of retinal ganglion cells, which are the only cells that send axons from the retina to the
brain, it was shown that the spatial correlation between ganglion cells is significantly reduced when fixational eye
movements are present. These reduced correlations can yield a compact representation of the visual information
when it is sent to the brain. The goal of our study was to put this theoretical prediction under experimental test.
Using a multi electrode array, we measured the response of the tiger salamander retina to movies which simulated
two types of stimuli: fixational eye movements over a natural scene and flash followed by static view of a natural
scene. Then we calculated the cross-correlation in the response of the ganglion cells as function of receptive
fields distance. We found that when static natural images are projected, strong spatial correlations are present in
the neural response due to correlation in the natural scene. However, in the presence of fixational eye movements,
the level of correlation in the neural response drops much faster as function of distance which results in effective
decorrelation of the channels streaming information to the brain. This observation confirms the prediction that
fixational eye movement act to reduce the correlations in retinal response and provides better understanding of
the contribution of fixational eye movements to the information processing by the retina.

COSYNE 2012

91

I-83 – I-84

I-83. The role of nonlinear phase processing in photoreceptors
Uwe Friederich
S.A. Billings
Mikko Juusola
Daniel Coca

U. FRIEDERICH @ SHEFFIELD. AC. UK
S . BILLINGS @ SHEFFIELD. AC. UK
M . JUUSOLA @ SHEFFIELD. AC. UK
D. COCA @ SHEFFIELD. AC. UK

University of Sheffield
Fly photoreceptors transform visual information into graded voltage responses and shape the first neural image
in the retina. It is well known that photoreceptor responses to naturalistic stimuli are nonlinear (van Hateren
1997) whereas Gaussian white noise (GWN) stimuli linearize the response (Juusola 1994). Using a nonlinear
photoreceptor model with integrated gain control, which accurately predicts Drosophila photoreceptor responses
over a wide range of light intensities, we demonstrate that nonlinear phase processing plays a crucial role in
linearizing photoreceptor responses to GWN stimuli. By exploiting the recently developed concept of Nonlinear
Output Frequency Response Functions (Lang 2007) we were able to decompose the photoreceptor responses
into linear, second- and higher-order responses, allowing us to characterize their relative contribution to the overall
photoreceptor response in time as well as in the frequency domain. We show how the magnitude and phase of
the nonlinear generalized frequency response functions are tuned such that GWN stimuli do not elicit a nonlinear
response. In contrast, the nonlinear component of the response is stronger to stimuli patterns which exhibit a
high degree of phase coherence. We find striking evidence that the photoreceptor’s nonlinear responses correlate highly with a local phase congruency measure, which is widely used for edge detection in image processing.
Finally, we propose a simple circuit by which local phase coherence information encoded by the nonlinear component of the photoreceptor response can be extracted from the overall photoreceptor response.

I-84. Humans integrate motion information using noise adaptive filters
Justin Ales
Anthony Norcia

JUSTIN . ALES @ GMAIL . COM
AMNORCIA @ STANFORD. EDU

Stanford University
Visual processing of motion requires mechanisms that can calculate the temporal derivative of location. If information is perfectly noise-free it is possible to use an instantaneous estimate of the derivative. But the natural
environment presents us with stimuli that have a wide range of signal-to-noise ratios (SNR). In the presence of
noise an instantaneous derivative measure becomes unreliable and it is necessary to smooth over time in order
to generate an accurate estimate. However, in a causal system, increasing integration time comes at the cost of
increasing the lag between the input to the system and its output. Therefore, an optimal system would dynamically
modulate its integration time in response to changes in the SNR of the incoming information. Motion perception
has classically been modeled as a set of fixed spatio-temporal filters applied by neurons in early visual cortex. We
propose a variant of this motion-energy model that uses filters that adapt their spatio-temporal integration as a
function of the SNR of the stimulus. We tested the adaptive filter model of motion processing in humans by fitting
our model to both psychophysical and EEG data. The high temporal resolution of EEG allowed us to measure
how quickly neural responses followed changes in motion coherence. We found that neural responses were faster
for stimuli with a high SNR, consistent with an adaptive system. In the psychophysical task we measured participants’ thresholds for detecting sinusoidal oscillations of motion direction (“wiggles”). We found that participants’
sensitivity to high oscillation rates increased as stimulus SNR increased. Both the EEG and the psychophysical
results demonstrate that neurons in early visual areas change their integration time in response to the statistical
reliability of the visual stimulus.

92

COSYNE 2012

I-85 – I-86

I-85. Adaptive sampling of visual stimuli in cortical neurons
Jens Kremkow
Jianzhong Jin
Yushi Wang
Reza Lashgari
Stanley Jose Komban
Jose-Manuel Alonso

JENS @ KREMKOW. DE
JJIN @ MAIL . SUNYOPT. EDU
YUSHIWANG @ SUNYOPT. EDU
RLASHGARI @ SUNYOPT. EDU
JKOMBAN @ SUNYOPT. EDU
JALONSO @ SUNYOPT. EDU

State University of New York
Neurons in the visual system have been characterized for decades using predefined sequences of stimuli that
sample a limited portion of sensory space. Ideally, it should be possible to adaptively modify the stimuli for each
individual neuron to explore larger portions of stimulus space in a “closed-loop” manner (i.e. the stimuli are modified based on the response of the neuron). We have tested different approaches of adaptive stimulus sampling
on responses of single neurons in the visual cortex (area V1) in cat. At this time, one of the most successful approaches is a Particle Swarm Optimization (PSO)[1] algorithm that optimizes sequences of spatiotemporal noise
pixels. The PSO is a population based technique in which particles move around in a search-space to locate the
optimum. Social interactions among particles (and the history of individual particles) are then used to update the
particles positions. In our case, a particle represented one sequence of spatiotemporal pixels and the searchspace was the entity of the luminance values of all pixels. The PSO algorithm was able to optimize stimulus
sequences to drive strong and reliable responses in both linear and nonlinear cortical neurons. Preliminary results suggest that the optimization is remarkably restricted in cortical space (i.e. neighboring V1 neurons that were
just a few hundred microns away from the optimized neuron did not show robust increases in firing rate during the
optimization process). Taken together, adaptive sampling provides a powerful approach to optimize white noise
stimuli for single V1 neurons with linear and nonlinear receptive fields. Our preliminary data suggests that this
optimization can be remarkably restricted in cortical space, providing new insights into the amount of redundancy
in the visual cortex. Support: NIH Grant EY02067901 and EY05253 [1] Kennedy J, Eberhart R (1995) Particle
swarm optimization. Neural Networks 4:1942-1948

I-86. Presaccadic modulation of visual responses in area V4 measured simultaneously across cortical layers
Nicholas Steinmetz
Tirin Moore

NICK . STEINMETZ @ GMAIL . COM
TIRIN @ STANFORD. EDU

Stanford University
Visual and saccade preparation-related representations converge in extrastriate cortex, yet little is known about
how these signals are integrated within neocortical microcircuits. Specifically, how does the representation of
top-down visual and bottom-up saccadic information differ between cortical layers? To address this issue, we
recorded simultaneously from all cortical layers of macaque area V4 during a task involving saccades to receptive
field stimuli. The behavioral paradigm was designed to isolate in time the onset of saccadic and visual signals in
V4. The electrode arrays consisted of single shafts with 16 recording sites each spaced 150µm apart (Plexon,
Inc.). We recorded spiking activity and local field potentials from each of the 16 sites simultaneously. In order to
record from different layers in the same cortical “column,” we inserted the electrode arrays into V4 perpendicularly
to the cortical surface, confirmed by measured RF alignment. To determine the laminar positions of the recording
sites, we used current source density analysis, which allowed identification of four distinct functional layers. Similar
to previous studies, we find that the visual responses of V4 neurons are significantly enhanced by the preparation
of saccades to RF stimuli. In addition, this novel recording approach allows us to examine the laminar pattern
of presaccadic modulation including differences in the magnitude and timing of these modulations across all
layers within the cortical column. These data comprise a fundamentally novel view of the interaction between
feedforward and feedback visual information in the neocortex of awake, behaving animals.

COSYNE 2012

93

I-87 – I-88

I-87. Predicting functional connectivity in primary visual cortex
David Schulz
Maneesh Sahani
Matteo Carandini

DAVID @ CARANDINILAB . NET
MANEESH @ GATSBY. UCL . AC. UK
MATTEO @ CARANDINILAB . NET

University College London
The responses of neurons in primary visual cortex to repeated presentations of the same stimulus are highly
variable. Much of this variability is shared across neurons. The structure of shared variability is called functional
connectivity which is usually measured by the correlation coefficient, or noise correlation. The strength and origin
of noise correlations are hotly debated. We asked how these and other factors work together in determining
noise correlations, and established their relative importance. Further, we asked whether simple and complex cells
differ in their functional connectivity. Since noise correlations can depend on firing rate purely due to a threshold
non-linearity, we had to separate the contributions of firing rate and linearity of spatial summation as measured
by the F1/F0 ratio. We did so in two ways: first, by regressing noise correlations onto a comprehensive set of
potential factors, thereby taking interactions into account. Second, we used the Dichotomized Gaussian Model
(DGM) which removes firing rate effects caused by rectification. We analyzed spike trains 31,412 pairs. The
resulting regression model is a compact description of the structure of noise correlations and explains ∼60%
of the variance (cross-validated). We find that noise correlations grow with firing rate, which is their strongest
determinant. The next strongest determinants are proximity in cortex and similarity of orientation tuning. In
addition, we find a clear effect of the pair’s F1/F0 ratio: complex cells are more noise correlated than simple cells.
The DGM strongly reduces the effect of firing rate modulation. It confirms all other findings. We conclude that
firing rates are most predictive for noise correlations, and that their effect is largely due to simple rectification. We
also find that complex cells are more noise correlated and generally more variable in their response, probably
because they participate more in the network activity.

I-88. Visual cortex learns novel representations under anesthesia
Andreea Lazar1
Wolf Singer1
Danko Nikolic2
1 Max
2 Max

ANDREEA . LAZAR @ BRAIN . MPG . DE
WOLF. SINGER @ BRAIN . MPG . DE
DANKO. NIKOLIC @ GOOGLEMAIL . COM

Planck Institute for Brain Research
Planck Institute, Frankfurt

The activity of neural populations has the tendency to adapt to the statistics of the environment [Berkes et al.,
2011, Sotiropoulos et al., 2011]. Here we explore the adaptation of neuronal responses to visual stimuli in cat
area 17 under anesthesia. Repeated exposure to a set of images (26 letters and 8 digits) changed the response
properties of the neural population (63 units) such that stimulus classification steadily improved over time. We
applied linear and non-linear classification methods (see [Nikolic et al., 2009]) to investigate the presence of
stimulus-related information in the spiking activity of neuronal ensembles across several hours-long recording
sessions (1700 trials). The classification performance of letter identity based on short integration intervals (20ms)
of evoked responses reached up to 25% correct (chance level = 3%). Most importantly, the performance improved
gradually with the amount of stimulus exposure and was not accounted for by fluctuations in spike rate. Next,
we increased the difficulty of the task by performing a classification which relied on the detailed representation
of stimulus features. A 12x12 array of readout neurons was trained to reconstruct the original stimulus based
on the population activity within a single 20 ms window. The readouts were able to generalize across stimuli
and could reconstruct letters omitted from the training set. Most interestingly, however, during later trials in the
experiment, the spiking patterns of the spontaneous population activity began replaying the activity evoked by
previously shown stimuli. The results suggest that efficient visual discrimination of familiar stimuli is achieved
partly through separation of neuronal representations already at the level of the primary visual cortex. Also,
these newly learned representations seem to be reactivated spontaneously. Given that the experiments were

94

COSYNE 2012

I-89 – I-90
performed under anesthesia, this process is likely to involve local mechanisms independent of attention and
conscious control.

I-89. Co-variability of spontaneous synaptic excitation and inhibition in visual
cortex
Andrew Y Tan
Nicholas Priebe

ATYY @ ALUM . MIT. EDU
NICHOLAS @ MAIL . UTEXAS . EDU

University of Texas at Austin
Spontaneous activity may be a signature of neural circuitry, is modulated by behavioral state, and affects stimulus encoding and synaptic plasticity. Here we estimate the co-variability of spontaneous synaptic excitation and
inhibition in the pentobarbital-anesthetized rat visual cortex. We measured synaptic currents at various membrane potentials with in vivo whole cell voltage clamp recordings. Assuming an isopotential neuron with linear
current-voltage relation and ergodic spontaneous activity yields a quadratic relation between synaptic current
variance and membrane potential whose coefficients are excitatory and inhibitory conductance covariances. Fits
of the observed synaptic current variances to the quadratic relation provide 90% confidence lower limits for the
cross-correlation coefficient, averaged across the population, of 0.2±0.1 (std). The upper limits for the crosscorrelation coefficients are sensitive to the assumed reversal potential and thus only weakly constrained. We
estimate stronger upper limits to the cross-correlation coefficients by further assuming that the synaptic currents
are filtered Poisson processes. This assumption is consistent with the observed synaptic currents consisting
of distinct events whose inter-event intervals were approximately exponentially distributed. Across neurons the
excitatory Poisson rate of a neuron was 2.7 times greater than its inhibitory Poisson rate. We estimate the maximum cross-correlation given a greater excitatory than inhibitory rate via a model in which the inhibitory rate is
entirely due to a filtered Poisson process that is also a source of the excitatory rate, with the remainder of the
excitatory rate provided by an additional independent filtered Poisson process. The maximum cross-correlation
coefficient is unity when excitatory and inhibitory rates are equal, and decreases as a power law function of the
excitatory to inhibitory rate ratio. An excitatory to inhibitory rate ratio of 2.7 corresponds to a maximum crosscorrelation coefficient of 0.6. The greater excitatory rate suggests that inhibitory neurons spike less frequently or
more synchronously.

I-90. Model-based analysis of 3D surface representations in human visual
cortex
Andrew Welchman
Hiroshi Ban

A . E . WELCHMAN @ BHAM . AC. UK
H . BAN @ BHAM . AC. UK

University of Birmingham
Neurons sensitive to binocular disparity are widespread in visual, temporal and parietal cortices. Human fMRI
suggests that dorsal visual areas are particularly engaged by stereoscopic stimuli, but what do these areas encode? We test responses to an important 3D property, surface slant. We set out to distinguish representations of
slant from lower-level stimulus covariates such as local disparities or edges. We measured fMRI responses while
participants (n=8) viewed random dot stereograms depicting slanted planes (±7.5 to 52.5 deg in 15 deg steps)
in a blocked design. We developed a cross-correlation voxel similarity measure to evaluate fMRI responses in
regions of interest in the visual cortex. Specifically, we correlated the vector of voxel responses evoked by a given
slant with those evoked by other slants, testing for similarity/dissimilarity between slants. We further compared
responses in control conditions that addressed changes in (i) overall disparity (disparity control) and (ii) changes
in projection size (spatial control) that result from rotating surfaces in depth. To evaluate the computations performed in different visual areas, we developed a model that comprised five detector mechanisms: (1) filters tuned

COSYNE 2012

95

I-91 – I-92
to disparity; (2) disparity edge-detectors; (3) disparity gradient detectors; (4) units tuned to retinotopic extent; and
(5) units responding to size. We then used model responses to different slants as regressors for the empirical
cross-correlation matrices. We show that fMRI responses to slanted surfaces are gradually transformed in the
dorsal visual pathway, from representations of edges in early (V1, V2) cortex to responses reflecting disparity gradients in V3A. Further, V3A responses are highly similar when the same slant is depicted, irrespective of changes
in edge locations or disparity range. Our results suggest that surface slant is encoded in intermediate portions of
the dorsal visual hierarchy, suggesting a potential locus for neuronal computations that underlie slant perception.

I-91. Inferred functional circuitry in a microcolumn of cat visual cortex
Urs Koster1,2
Christopher Hillar1
Bruno Olshausen1
Charles Gray2

URS @ BERKELEY. EDU
CHILLAR @ MSRI . ORG
BAOLSHAUSEN @ BERKELEY. EDU
CMGRAY @ CNS . MONTANA . EDU

1 University
2 Montana

of California, Berkeley
State University

We attempt to elucidate the circuit structure of a single cortical microcolumn by simultaneously recording units
from all layers of cat visual cortex and fitting probabilistic models to this data. While much research has gone
into the anatomical basis of connectivity between laminae, we focus on functional circuit structure inferred from
activity evoked by visual stimulation. We analyze how circuits are dynamically remapped in response to grating
and natural movie stimuli by fitting Ising models to pairwise correlations between units. In particular, we focus
on the question of whether gamma oscillations in response to preferred stimuli are accompanied by changes in
the functional connectivity. Recordings were made from anesthetized cat area 17 using 32-channel laminar silicon probes, simultaneously recording from 33 single units with spikes binned at 10ms. Because Ising models of
this size are computationally challenging to estimate by maximum likelihood, we use Minimum Probability Flow
learning, which is a novel estimation method that circumvents the need to compute the computationally expensive partition function. To alleviate the problem of overfitting in high-dimensional models with limited data, we
applied L1-regularization to the parameters. The result of this model fitting procedure is a coupling matrix that
describes all pairwise interactions between units. The coupling terms of this matrix reveal excitatory interactions
between layers II/III and V/VI. Cross-correlation analysis is consistent with elevated gamma activity sharpening
this coupling. Inhibitory interactions are more specific and can be tracked to putatively identified inhibitory interneurons. Additionally, there are global patterns of excitatory connections within layers II/III and V/VI as well
as inhibition within layer V. This result is in agreement with the anatomically defined canonical microcircuit with
strong excitatory connections from layer III to layer V as well as feedback connections back onto layer III.

I-92. Precise decoding of dynamical motion from a large retinal population
Olivier Marre1
Gasper Tkacik2
Michael Berry1

OLIVIER . MARRE @ GMAIL . COM
GTKACIK @ IST. AC. AT
BERRY @ PRINCETON . EDU

1 Princeton
2 IST

University
Austria

Retinal ganglion cells perform several non-linear computations on moving objects. However, it is unclear how
the representation of moving objects is distributed across the entire population of ganglion cells. To address this
issue, we recorded a large (100-200) population of ganglion cells with a large, dense array, while displaying a
bar animated with random, Brownian motion. The position of the bar could be reconstructed from retinal activity
with very high precision using a linear decoder taking a large population of cells as input. Reconstruction using a

96

COSYNE 2012

I-93
second order decoder did not lead to a significant improvement in quality. We estimated the decoding error as a
function of both bar position and trajectory frequency. We found the minimal error to be below the average spacing
between pairs of cones for a broad range of frequencies, indicating that the decoding performance was in a regime
of hyperacuity. This high precision was made possible thanks to the large number of cells recorded: performance
increased with the number of cells used for decoding, slowly reaching a plateau of high correlation between
real and estimated trajectories (r=0.9) towards 70 cells. We estimated the mutual information between real and
estimated stimulus trajectories for subsets of cells with different sizes. For up to 10 cells, the mutual information
estimated this way could be predicted by summing the mutual information obtained for each cell individually,
indicating there was little redundancy between them. However, for larger groups, a significant and increasing
redundancy was found. These large-scale recordings showed that the cells did not carry the information about a
moving object independently. Rather, they conveyed redundant information, and accumulating a large number of
these redundant cells was necessary to decode precisely the stimulus trajectory.

I-93. Dealing with sequential dependencies in psychophysical data
Ingo Fründ1,2
Felix Wichmann3
Jakob Macke4

INGO. FRUEND @ TU - BERLIN . DE
FELIX . WICHMANN @ UNI - TUEBINGEN . DE
JAKOB @ GATSBY. UCL . AC. UK

1 Bernstein

Center for Computational Neuroscience Berlin
Universität Berlin
3 University of Tübingen
4 University College London
2 Technische

Psychophysical experiments are the standard approach for quantifying functional abilities and properties of sensory systems, and for linking observed behaviour to the underlying neural mechanisms. In most psychological
experiments, human observers or animals respond to multiple trials that are presented in a sequence, and it is
commonly assumed that these responses are independent of responses on previous trials, as well as of stimuli presented on previous trials. There are, however, multiple reasons to question the ubiquitous assumption of
“independent and identically distributed trials”. In addition, it has been reported that inter-trial dependencies are
pronounced in behaving animals (Busse et al, 2011, J Neurosci). These observations raise two central questions:
First, how strong are sequential dependencies in psychophysical experiments? Second, what are statistical methods that would allow us to detect these dependencies, and to deal with them appropriately? Here, we present a
statistical modelling framework that allows for quantification of sequential dependencies, and for investigating their
effect on psychometric functions estimated from data. In particular, we extend a commonly used model for psychometric functions by including additional regressors that model the effect of experimental history on observed
responses. We apply our model to both simulated data and multiple real psychophysical data-sets of experienced
human observers. We show that our model successfully detects trial by trial dependencies if they are present
and allows for a statistical assessment of the significance of these dependencies. We find that, in our datasets,
the majority of human observers displays statistically significant history dependencies. In addition, we show how
accounting for history dependencies can lead to changes in the estimated slopes of psychometric functions. As
sequential dependencies are presumably stronger in inexperienced observers or behaving animals, we expect
that methods like the ones presented here will become important tools for modelling psychophysical data.

COSYNE 2012

97

I-94 – I-95

I-94. Diverse Network Representations of Risky Decision-Making in mPFC
Shirin Hadizadeh1
Nina C. Di Pietro1
James M Hyman1
Stan B Floresco1
Eldon Emberly2
Jeremy K. Seamans1

SHIRINHADIZADEH @ GMAIL . COM
NDIPIETRO @ GMAIL . COM
HYMAN . JM @ GMAIL . COM
FLORESCO @ PSYCH . UBC. CA
EEMBERLY @ SFU. CA
SEAMANS @ INTERCHANGE . UBC. CA

1 University
2 Simon

of British Columbia
Fraser University

The medial prefrontal cortex (mPFC) is critical for decision making involving cost/benefit evaluations. Recent studies in rodents suggest that decisions involving risky outcomes engage the mPFC to integrate information about
changing reward probabilities in order to update value representations. However, the neural mechanisms underlying this form of decision-making are largely unknown. To investigate this issue we used a task that consisted of
4 discrete blocks of trials in which a response on safe lever always delivered one food pellet, whereas a response
on risky lever delivered four food pellets with decreasing probability (100, 50, 25, 12.5%) as blocks of trials progressed. Rats trained on the task were then implanted with 16 drivable tetrodes bilaterally into the mPFC. From
the unit recordings we generated population instantaneous firing rate (iFR) vectors and using various statistical
methods we evaluated the distinction between clusters of points in the N-dimensional space (where N=number
of recorded units). Individual mPFC neurons displayed behaviorally correlated discharge patterns in response to
a number of distinct aspects of the task, including large/risky vs. small/safe lever presses and reward epochs
across trial blocks. At the population level we reliably observed distinct network activity states when animals were
making risky decisions under the different probabilities of risk. These network states displayed adaptive encoding
in that they became more distinct from one another as the relative difference in risk increased. This analysis
reveals that mPFC networks are not encoding pure representations of risky lever reward probability levels but
rather are forming relative representations of probability that indicate the size of the changes in probability over
multiple blocks. While the mPFC tracks diverse aspects of behavior, in this case PFC ensembles employed an
adaptive and relative encoding scheme for rewards that could guide behavior in situations with changing reward
probabilities.

I-95. How criticality of visuo-motor control behaviour depends on task objective.
Klaus Pawelzik
Felix Patzelt

PAWELZIK @ NEURO. UNI - BREMEN . DE
FELIX @ NEURO. UNI - BREMEN . DE

University of Bremen
When humans perform closed-loop control tasks like standing upright or balancing a stick, their behavior exhibits
spatiotemporal scaling suggesting operation close to a critical point. Prominently, control errors during balancing
are power-law distributed. A possible explanation is self-tuning of the control system to a critical point due to the
annihilation of local information by adaptive predictive control. A simple model based on this principle was shown
to reproduce many experimental findings. This model makes several predictions, including that the underlying
neuronal control system continues to be adaptive even when the controlled system itself is stationary. This
seemingly irrational behavior is caused by the controller trying to eliminate local random trends in its interaction
with the controlled system over short time scales close to its reaction time. We performed virtual balancing
experiments to test the theoretical predictions. Most notably, we found the subjects’ behavior to be dramatically
dependent on the utilized reward function. The existing model was found to be an excellent description for subjects
trying to minimize mean squared controller-target distances. Given a different reward function, subjects displayed
other solutions to the balancing problem that have not been reported before. Some of these solutions which do
not exhibit power-law fluctuations can be explained by simple modifications to the existing model. Our results

98

COSYNE 2012

I-96 – I-97
provide important constraints for possible neuronal implementations of visuo-motor control.

I-96. Maximum entropy models of social behavior reveal high-order interactions in groups of mice
Oren Forkosh
Yair Shemesh
Yehezkel Sztainberg
Alon Chen
Elad Schneidman

OREN . FORKOSH @ WEIZMANN . AC. IL
YAIR . SHEMESH @ WEIZMANN . AC. IL
HEZI . SZTAINBERG @ WEIZMANN . AC. IL
ALON . CHEN @ WEIZMANN . AC. IL
ELAD. SCHNEIDMAN @ WEIZMANN . AC. IL

Weizmann Institute of Science
Social behavior is often quantified from measurements of how an animal interacts with one other animal of its own
kind, and not in the context of a group. However, the behavior of groups as a whole, and of individuals within a
group, may arise from complex network of dependencies among them. We studied small groups of freely behaving
mice in an ethological relevant environment, whose behavior was automatically tracked for several days with high
temporal and spatial resolution. To uncover the underlying social interaction network of groups of animals, we
built mathematical models of the joint activity patterns of the members of the group. We found that the location
of a mouse in the arena, was significantly correlated and informative of the location of the other mice. Moreover,
when predicting the location of a mouse the information that was gained from the locations of the other mice was
synergetic when they were viewed as a group. We used a maximum entropy framework to model the distribution
of mouse configurations, where the state of each animal was given as one of 10 locations in the arena. The
maximum entropy model in this case is a generalized Potts model. We found that a pairwise maximum entropy
model could account for less then 60% of all correlations among the mice; the third order model, explain 90% of
the correlations or more. We then used the interaction terms in the generalized Potts model to construct a social
interaction map between the mice. Finally, in spite of the variability in the interactions when we compared groups
of mice with environmental or genetic variations we found that groups with similar backgrounds clustered together
in terms of their distribution of states.

I-97. A retinotopic systems identification method reveals parallel visual streams
in the fruit fly
Jacob Aptekar1,2
Jessica Fox2,1
Patrick Shoemaker3
Mark Frye2,1

JAPTEKAR @ GMAIL . COM
JESSFOX @ UCLA . EDU
PAT. SHOEMAKER @ TANNER . COM
FRYE @ UCLA . EDU

1 HHMI
2 University
3 Tanner

of California, Los Angeles
Research

Figure tracking—the acquisition and pursuit of objects superimposed on a background—is a requirement of all
high performance visual systems. One example of this behavior is the frontal fixation of vertical edges by flies in a
rigid tether. While it is known that flies process panoramic motion cues along an Elementary Motion (EM) stream
that computes coherent motion by correlating luminance measurements on the retina in space and time, we
demonstrate that while figures may produce EM cues, flies employ a second, EM-independent figure motion (FM)
subsystem to saccade toward and smoothly track visual figures. We show that (i) the two systems extract different
information from a single time varying visual input; (ii) their characteristics vary differently across the visual field;
(iii) the FM system superimposes saccadic turns upon smooth pursuit; (iv) the two systems in combination are
necessary and sufficient to predict the full range of figure tracking behavioral capabilities, including those that

COSYNE 2012

99

I-98 – II-1
generate no EM cues. The technical advance that enables this analysis is a non-stationary white noise based
systems identification tool known as a SpatioTemporal Action Field (STAF). In the same way that a SpatioTemporal
Receptive Field (STRF) is a piecewise linearization of the dynamics of a single cell’s membrane potential to stimuli
confined to discrete locations within its receptive field, the STAF assigns a linear filter to each retinotopic position
describing the flies’ steering effort perturbations in EM or FM optic flow. This analysis and the conclusions
drawn by it expand the classical model that fly visual tracking is based solely on signals encoded by elementary
motion detectors. In addition, the ability to perform system identification in a retinotopic manner constitutes a
computational phenotype through which mutants with abnormal neural circuitry can be compared.

I-98. Odor detection vs. mixture categorization: crucial differences in behavioral and learning strategies
Maria Vicente1
Andre Mendonca1
Alexandre Pouget2
Zachary Mainen1

MARIA . VICENTE @ NEURO. FCHAMPALIMAUD. ORG
ANDRE . MENDONCA @ NEURO. FCHAMPALIMAUD. ORG
ALEX . POUGET @ GMAIL . COM
ZMAINEN @ NEURO. FCHAMPALIMAUD. ORG

1 Champalimaud
2 University

Neuroscience Programme
of Rochester

In the visual system, multiple streams of information are processed in parallel by specialized channels to solve
different problems. In olfaction, the problem of detecting the presence of a low concentration of an odor and the
problem of distinguishing closely related odors make very different demands, but specialized neural processing
strategies have yet to be identified. In order to address this issue, we studied odor detection and categorization
in parallel in the same animals, using tasks that are identical except for the relevant stimulus parameters. Here
we show that while most aspects of the behavior remain identical between tasks, two critical differences can be
identified. First, we found that, whereas odor sampling times increased substantially when odor concentrations
approached detection threshold, they remained almost constant as odor mixtures became closer to the category
boundary. This was true even for categorization at threshold concentrations. We hypothesize that in odor detection, accuracy is limited by stimulus uncertainty, whereas in odor mixture categorization accuracy is limited by
variability in the mapping of the stimulus to the response, which must be learnt by the subject on a trial-by trial
basis. Given this hypothesis, we investigated whether ongoing learning has a different influence on the choice
strategy of the animals in detection and categorization. In both tasks there was a clear trial-by-trial updating of
the animal’s choice function. However, whereas in mixture categorization choice bias increased with difficulty of
the previous trial as well as the outcome, in odor detection this bias was dependent only on choice side and outcome. Thus, a detailed comparison of odor categorization and detection revealed differences in strategies both
at the level of behavior (odor sampling time) and learning (trial-by-trial updating). These observations suggest
differences at the neurophysiological level that can be tested by simultaneous recordings.

T-44.
II-1. Efficient coding of natural images and movies with populations of noisy
nonlinear neurons
Yan Karklin1,2
Eero P. Simoncelli1,2
1 New

YAN . KARKLIN @ NYU. EDU
EERO. SIMONCELLI @ NYU. EDU

York University

2 HHMI

100

COSYNE 2012

II-2
Efficient coding provides a powerful principle for explaining early sensory processing. Most attempts to test this
principle have been limited to linear, noiseless models, and when applied to natural images, have yielded localized
oriented filters (e.g., Bell and Sejnowski, 1995). Although this is generally consistent with cortical representations,
it fails to account for basic properties of early vision, such as the receptive field organization, temporal dynamics, and nonlinear behaviors in retinal ganglion cells (RGCs). Here we show that an efficient coding model that
incorporates ingredients critical to biological computation – input and output noise, nonlinear response functions,
and a metabolic cost on the firing rate – can predict several basic properties of retinal processing. Specifically, we
develop numerical methods for simultaneously optimizing linear filters and response nonlinearities of a population
of model neurons so as to maximize information transmission in the presence of noise and metabolic costs. We
place no restrictions on the form of the linear filters, and assume only that the nonlinearities are monotonically
increasing. In the case of vanishing noise, our method reduces to a generalized version of independent component analysis; training on natural image patches produces localized oriented filters and smooth nonlinearities.
When the model includes biologically realistic levels of noise, the predicted filters are center-surround and the
nonlinearities are rectifying, consistent with properties of RGCs. The model yields two populations of neurons,
with On- and Off-center responses, which independently tile the visual space. As observed in the primate retina,
Off-center neurons are more numerous and have filters with smaller spatial extent. Applied to natural movies, the
model yields filters that are approximately space-time separable, with a center-surround spatial profile, a biphasic
temporal profile, and a surround response that is slightly delayed relative to the center, consistent with retinal
processing.

II-2. Optimally adapting heuristics: humans quickly abandon the constant
bearing angle strategy
Constantin Rothkopf1
Paul Schrater2
1 Frankfurt

ROTHKOPF @ FIAS . UNI - FRANKFURT. DE
SCHRATER @ UMN . EDU

Institute for Advanced Studies
of Minnesota

2 University

Animals ranging from dragonflies through teleost fish to humans all intercept moving targets using the same
strategy of adjusting their speed so as to hold the angle pointing towards their target constant over time. This
constant-bearing-angle strategy has been suggested as a fundamental visuomotor heuristic and as an instance
of Darwininan intelligence that overcomes the need for complex and expensive computations involving multiple
sources of uncertainty. We consider the task of intercepting a moving ball for which many previous studies have
shown that humans use this constant bearing angle strategy. Here we manipulated the observation function
in a virtual reality setup so as to change the uncertainty of the balls position parametrically. Specifically, the
contrast of the ball changes as a function of the heading angle towards the ball along the subject’s momentary
trajectory. Subjects adjusted their interception strategy within an average of 26 trials and were consistently able
to catch these balls. To gain insight into the adopted new interception strategy, we setup two approximate optimal
control models, which know the observation function. In one case, an iterated signal dependent linear-quadraticGaussian controller was modified to handle non-linear observation models. The second approach utilizes a
Monte Carlo sampling of smooth trajectories of increasing complexity in a low dimensional parameter space.
These analyses show that the ideal actor modifies its trajectories by executing controls that increase information
gain, and that these changes mirror human behavior. Thus, we provide evidence that humans quickly abandon
the constant bearing angle strategy in favor of more informative action sequences, if this allows catching moving
targets more reliably. The constant-bearing-angle-strategy is not an invariant heuristic of Darwinian intelligence
as humans employ near-optimal information seeking actions that violate the constant bearing angle strategy, but
produce less uncertainty in the interception.

COSYNE 2012

101

II-3 – II-4

II-3. The simultaneous silence of neurons explains higher-order interactions
in ensemble spiking activity
Hideaki Shimazaki1
Kolia Sadeghi2
Yuji Ikegaya3
Taro Toyoizumi1

SHIMAZAKI @ BRAIN . RIKEN . JP
KOLIA @ STAT. COLUMBIA . EDU
IKEGAYA @ MOL . F. U - TOKYO. AC. JP
TARO. TOYOIZUMI @ BRAIN . RIKEN . JP

1 RIKEN

Brain Science Institute
University
3 Tokyo University
2 Columbia

Sparseness is a major characteristic of the spiking activity of neuronal populations [1,2]. Neurons are often simultaneously inactive with a probability greater than expected if neurons fired independently [1]. A complete
description of ensemble neuronal activities, however, remains a challenging problem. While higher-order interactions (HOI) are typically introduced to capture the sparse activities [1,2], inclusion of combinatorial HOIs into a
probability model is problematic [2]. This is because the model can easily over-fit the data even in small networks.
An unnormalized model was proposed in [2] aiming at capturing sparse HOIs in a large network, but this approach
makes statistical interpretations obscure. In the present work, we suggest adding to the classical log-linear model
(LLM) a single HOI parameter that introduces a specific global interaction favoring sparse spiking activities. This
parameter describes the extent to which all neurons are simultaneously inactive more/less frequently than expected from their firing rates and pairwise correlations (if the parameter is added to a pairwise LLM). We applied
these models to parallel spike data of subsets of 4-10 neurons (out of 138) reconstructed from calcium imaging
of a hippocampal CA3 pyramidal cell layer. Compared to independent and pairwise LLMs, adding a single additional parameter dramatically improved the goodness-of-fit. The fitted sparsity values were positive, indicating
that neurons tended to be inactive together. This means that the addition of the single HOI parameter successfully captured positive pairwise and negative triple-wise interactions observed in many CA3 neurons. This work
suggests that seemingly complex HOIs can be explained by the simultaneous inactivation of many neurons. [1]
Ohiorhenuan et al. Nature(466), 617-621, 2010. [2] Ganmor et al. PNAS(108), 9679-9684, 2011.

II-4. Neural representations that are good for both generalization and discrimination
Omri Barak
Mattia Rigotti
Stefano Fusi

OMRI . BARAK @ GMAIL . COM
MR 2666@ COLUMBIA . EDU
SF 2237@ COLUMBIA . EDU

Columbia University
How do we tell whether a neural representation is good or bad? The answer depends on the input statistics, the
task at hand and the readout. Previous work, mainly in early sensory areas, focused on the amount of information
about a stimulus contained in the neural representation. Here we took a different perspective and evaluated a
neural representation by considering the dynamics of a generic cortical circuit. This approach led us to test the
classification performance of a linear readout from a population of input neurons that encode a few different noisy
sources of information. Our analysis revealed that input neurons have to respond to mixtures of the information
sources in order to enable classification. One efficient way to achieve these response properties is to mix different
sources of information with randomly connected neurons (RCNs). Under the assumption that the output neuron
reads out the RCNs, we derived a formula for the classification performance of noisy inputs (generalization error).
The performance depends on the “discrimination factor”, expressing how much the population activity changes
when only one of the information sources is altered; and on the “generalization factor”, expressing the change
when none of the sources are altered, but different noisy versions are presented. Specifically we explored the
effect of the population coding level on the tradeoff between these factors, and show that a coding level of about
0.1 is optimal for many different cases. The advantage of optimal coding is greater for higher levels of noise

102

COSYNE 2012

II-5 – II-6
in the inputs. Our results provide a possible explanation for the abundance of mixed selectivity found in neural
recordings, and for the coding level observed in many areas. Furthermore, we provide a prescription to measure
components of the generalization-discrimination tradeoff from neural data. Funding: DARPA SyNAPSE. Gatsby,
Kavli and Sloan-Swartz Foundations.

II-5. Semi-supervised learning of high-level representations of natural video
sequences.
Steven P Brumby1
Michael I Ham1
Garrett T Kenyon1,2
1 Los

BRUMBY @ LANL . GOV
MIKEH @ LANL . GOV
GARKENYON @ GMAIL . COM

Alamos National Laboratory
Mexico Consortium

2 New

Hierarchical models of visual cortex based on the HMAX/Neocognitron model have reported state-of-the-art accuracy on object classification tasks using natural still imagery. More recent work has started to explore the
role of sparse representations in these models, using feature dictionaries learned from the data. This work has
mostly focused on whole image classification, applied to standard datasets such as CalTech256, which provides
∼100 human-labeled example images for each of 256 image object categories, with a total dataset size ∼1 Gpixel.
Much larger image and video datasets are now becoming available, which raise questions about how to learn from
mostly unsupervised data streams where any given frame of video may contains hundreds of objects drawn from
thousands of object categories (involving object detection as well as classification), presented at data rates of several Gpixels/min for high definition video. We describe new work exploring semi-supervised methods for learning
representations of video sequences of natural scenes. Our methods are based on learning over-complete feature
dictionaries that capture texture, color and motion, with local pooling to provide invariant representations of image
patches while allowing reconstruction of the original image. Our model then uses this unlabeled representation
to learn the natural background classes in these video sequences. We describe how to add supervised learning
of additional foreground categories that may represent a relatively rare but behaviorally important component of
the external environment. This type of multiple-category clustering algorithm can be learned using fast, online
methods, and supports high-level description of the scene and learning of high-level relationships between foreground and background object categories, which can further enhance object recognition rates. We demonstrate
our models using high performance implementations of our algorithms on public domain video datasets.

II-6. Implicit representation of high-dimensional stimulus distributions
Xaq Pitkow

ZSP 2101@ COLUMBIA . EDU

University of Rochester
Our sensory stimuli are ambiguous, and behavioral experiments show that we often use probabilistic information
in our judgments and decisions. Recent work in neural probabilistic codes posit that marginal probabilities are
encoded variously in individual neural activities or distributed across neural populations. In both these cases it
remains unclear how our brains could represent the exponentially high-dimensional joint probability distributions of
complex stimuli. Here we show that sparse posterior distributions can be represented implicitly using surprisingly
little information, scaling nearly linearly with the dimensionality of the stimulus. In typical conditions, a sensory
scene is ambiguous but this ambiguity is limited to a tiny fraction of all possible stimuli. This is for two reasons:
first, our prior expectations over stimuli is highly restrictive, and second, our sensory evidence is usually consistent
with only a small subset of stimuli with any appreciable prior probability. Under normal conditions this makes the
posterior distributions very sparse. Recent results in the field of compressive sensing prove that sparse images
can be represented accurately with surprisingly few measurements. We adapt these results to compressing

COSYNE 2012

103

II-7 – II-8
not images but rather probability distributions over images, specifically binary images in this study. We find
that with sufficiently limited ambiguity, the most probable configurations in the complete posterior distribution are
implicitly but accurately encoded in a small number of local marginal probabilities. These results suggest that
the brain may not need to explicitly represent any unwieldy, high-dimensional probabilities in order to capture the
relevant probabilistic information. This information is also captured effectively by random marginals, the marginal
distributions of random combinations of stimulus inputs. We discuss how random marginals provide a novel
interpretation of the complex feature conjunctions observed in receptive fields of high-level neurons.

II-7. Fano factor constancy and scale-invariant sampling in recurrent networks with probabilistic synapses
Ruben Moreno

RMORENO @ FSJD. ORG

Foundation Sant Joan de Deu
Neuronal activity in cortex is variable both spontaneously and during stimulation. This variability has the remarkable property that the Fano factor of the neuron’s spike counts (variance/mean) is constant over broad ranges
of their firing rates, a property that we call “Fano factor constancy”. Realistic neuronal networks models in the
balanced regime do not lead to Fano factor constancy over broad ranges of firing rate unless their parameters
are fine-tuned, posing a problem. What mechanism could lead to Fano factor constancy? One well-documented
source of variability in cortex is synaptic noise. However, whether synaptic noise can lead to the observed properties of neuronal variability is currently unknown. Here we show that realistic neuronal networks of spiking neurons
endowed with probabilistic synaptic release naturally generate constant Fano factors over several orders of magnitude in the neurons’ firing rate. The mechanism does not require fine tuning of the network parameters, but
rather it arises naturally from the amplification of the synaptic noise and the neuronal spiking threshold. Probabilistic synaptic release naturally leads to information loss. Then, what could be the role of this form of variability?
We show that synaptic noise allows sampling the states of the network and, furthermore, that the Fano factor
constancy property implies that the state-sampling dynamics obeys a scale-invariant rule: if the input drive to the
network is scaled by some factor, the probability of visiting the states of the network remains constant. Therefore,
synaptic noise might not only be the responsible for the type of variability found in cortex, but it also might endow
cortical neurons with crucial computational properties to solve inference problems with ambiguous information.

II-8. On the precision of sensory encoding in visual search
Helga Mazyar
Ronald Van Den Berg
Wei Ji Ma

HE . MAZYAR @ GMAIL . COM
RVDBERG @ CPU. BCM . EDU
WJMA @ BCM . EDU

Baylor College of Medicine
The neural encoding of visual stimuli is typically noisy. In many visual tasks, a stimulus is not presented in isolation
but is part of a set. We ask whether set size affects the encoding precision (inverse variance) of a stimulus, and
whether encoding precision is identical among individual items within a set. The existing literature is inconsistent
about the effect of set size on precision, even for simple stimuli. Many visual search studies report that precision is
constant with set size, but visual short-term memory studies typically report that it decreases. This has led to the
suggestion that memory might be a key factor (Palmer, 1993). In the search studies, however, non-target items
were all identical to each other (homogeneous distractors), whereas the memory studies used heterogeneous
stimuli. Therefore, an alternative hypothesis is that the decrease in precision is caused by stimulus heterogeneity
(Wilken and Ma, 2004). To disentangle both factors, we asked subjects to detect a target among heterogeneous
distractors in a task without or with a memory component. In the pre-cue condition, a target orientation was
presented before viewing a search display. In the post-cue condition, the order was reversed, requiring subjects

104

COSYNE 2012

II-9 – II-10
to memorize the entire search display. Using an ideal-observer model, we found that noise increases with set size
in both conditions, suggesting that distractor heterogeneity is the critical factor. The data also allow us to examine
whether encoding precision at a given set size is constant or variable. Traditionally, encoding precision is thought
to be constant given the stimuli and experimental conditions. We tested a new model in which precision is itself a
random variable, fluctuating across trials and items. We found that precision is indeed variable. Together, these
results offer a new perspective on the precision of sensory encoding in visual search.

II-9. Neural implementation of Bayesian inference using efficient population
codes
Deep Ganguli1
Eero P Simoncelli1,2
1 New

DGANGULI @ CNS . NYU. EDU
EERO. SIMONCELLI @ NYU. EDU

York University

2 HHMI

Experimental evidence suggests that human judgments of many perceptual attributes are consistent with Bayesian
inference, in which noisy sensory measurements are combined with prior knowledge to obtain estimates. How
does the brain represent and utilize prior probabilities to achieve this computation? Recent work has shown that
a population vector decoder can approximate a Bayes Least Squares Estimator (BLSE), if one assumes a neural
population with tuning curves proportional to the likelihood, and preferred stimuli sampled from the sensory prior
(Shi & Griffiths 2009; Fischer & Pena 2011). Here, we examine and derive more precise conditions under which
this can hold. We assume sensory variables are encoded with a heterogeneous neural population optimized for
transmission of sensory information, subject to limitations on the number of neurons (N) and the total average
spike rate (R). This encoder implicitly represents the sensory prior in the distribution of preferred stimuli, and is
consistent with experimental data for a variety of sensory modalities and attributes (Ganguli & Simoncelli 2010).
Given this encoder, we derive a novel decoder to approximate the BLSE. Similar to the population vector, it computes weighted averages of the preferred stimuli. However, the firing rates are not used directly as weights, but
are first convolved with a linear filter then exponentiated. The decoder is neurally plausible, and requires knowledge only of the preferred stimuli and a fixed filter, and not the prior or tuning curves (Jazayeri & Movshon 2009).
Simulations demonstrate that it outperforms the standard population vector, and converges to the true BLSE as
N increases. In a low signal-to-noise regime, the decoder outperforms a BLSE operating on a resource-matched
homogeneous population. We conclude that in a regime where resources are limited, neural representations
optimized for transmitting information enable neurally plausible decoding that can utilize implicit prior information
to perform Bayesian inference.

II-10. Change detection as probabilistic inference under variable resources
Shaiyan O Keshvari1,2
Ronald Van Den Berg2
Wei Ji Ma2

SHAIYAN @ MIT. EDU
RVDBERG @ CPU. BCM . EDU
WJMA @ CPU. BCM . EDU

1 Massachusetts
2 Baylor

Institute of Technology
College of Medicine

Visual short-term memory (VSTM) is generally believed to have a fixed, discrete capacity of about four items.
This belief is largely derived from experiments in which observers detect whether a change occurred between
two successive displays containing multiple items. Here, we introduce a different conceptualization of change
detection, namely as a form of optimal probabilistic inference under limited resources. We assume that the internal
representation of each item is noisy, with a level of precision that is determined by the amount of resource allocated
to the item. A previous idea (Shaw 1980) has been that resource is spread equally over all items (fixed-precision,

COSYNE 2012

105

II-11 – II-12
or FP, model). We make the novel proposal that the amount of resource—and therefore precision—varies from
item to item and trial to trial, possibly due to attentional fluctuations (variable-precision, or VP, model). On each
trial, the model observer uses the internal representations of the stimuli to compute the probability that a change
occurred. We conducted a set of change detection experiments in which we varied magnitude of change, number
of items, and stimulus reliability. Besides testing the discrete-capacity, FP, and VP models, we considered variants
of the FP and VP models in which the observer performs suboptimal probabilistic inference, due to incomplete
knowledge about item-to-item precision and/or by using an alternative rule for integrating evidence across items.
We found that among the 16 resulting models, human behavior was best described by the one in which precision
is variable, observers have complete knowledge of variations in precision, and use the optimal integration rule.
Our results not only provide strong evidence that VSTM resource is continuous and variable rather than discrete
and fixed, but also suggest that neural populations encode precision for each item on each trial, and use this
information in subsequent computation.

II-11. Change localization: a new paradigm for visual short-term memory
Hongsup Shin
Ronald Van Den Berg
Wei Ji Ma

HSHIN @ CNS . BCM . EDU
RVDBERG @ CPU. BCM . EDU
WJMA @ BCM . EDU

Baylor College of Medicine
To survive, animals must not only detect changes between visual scenes but also localize them. Despite its
ecological importance, change localization remains underexplored. Here, we use change localization as a novel
paradigm to compare models of visual short-term memory (VSTM). Subjects sequentially viewed two displays
containing N ellipses each, separated by a 1-second blank. Exactly one ellipse changed its orientation between
the displays, and subjects reported the location of this change. This is an N-alternative forced-choice task.
We found significant effects of set size and change magnitude on proportion correct (see figure). Established
discrete-capacity models claim that VSTM consists of a small number of discrete chunks of resource. Here,
we instead introduce a set of continuous-resource models based on the assumption that each item is encoded
in a noisy way. The precision of encoding could either be independent of N (“flat”) or inversely proportional to
N (“1/N”). In addition, precision could be constant for given N (fixed precision, FP), or vary across items and
trials (variable precision, VP). In these models, to make a decision, observers compute the posterior distribution
over the location of the change based on the noisy internal representations of the items. The FP models have
one free parameter, all others two. We found that the 1/N VP continuous-resource model describes human
observers’ behavior very well (R2=0.99), and outperforms both the discrete-capacity models and the alternative
continuous-resource models. This indicates that precision is variable across items and trials, and on average
inversely proportional to N. Variability in encoding precision may be explained by top-down mechanisms such as
attentional modulation. Our findings suggest that VSTM is limited by a continuous and variable resource, and
establish change localization as a suitable paradigm for further study of these limitations.

II-12. Phase precession in a network model of entorhinal cortex stellate cells
without external pacemaker
Kay Thurley
Franziska Hellmundt
Christian Leibold

THURLEY @ BIO. LMU. DE
FRANZISKA . HELLMUNDT @ GMX . DE
LEIBOLD @ ZI . BIOLOGIE . UNI - MUENCHEN . DE

Ludwig-Maximilians-Universität München
Stellate cells in layer II of the medial entorhinal cortex exhibit place-specific firing in-vivo. Their firing fields are
arranged on a spatial hexagonal lattice, generally referred to as grid. Grid-field activity is accompanied by os-

106

COSYNE 2012

II-13
cillations of the local field potential (LFP) in the theta band (≈10 Hz). The theta phase of single spikes thereby
decreases with the distance traveled in the field, a phenomenon called phase precession. Stellate cells have
been characterized as type II oscillators with a subthreshold resonance in the theta range. As such they are
considered to be pacemakers. It is unclear how spiking of such putative pacemaker neurons would be able to
precess in phase relative to a self-generated oscillation. Based on a recent model of phase precession in the
hippocampus (Geisler et al. 2010 Proc. Natl. Acad. Sci, U.S.A. 107: 7957), we developed a theory on how this
paradox can be resolved. For the hippocampus model, the core idea is that the limited spatial extent of the firing
fields ensures that the sum of all synaptic currents, i.e., the LFP, is a little slower than the firing frequency of the
individual cells, hence leading to phase precession. Here, we show that extending this idea to grid fields is not
straightforward since the periodicity of the grid fields generally destroys the phase coordination. Moreover, the
original model strongly relies on a compression parameter with entirely unclear mechanistic origin. Our simulations show that the type II property of stellate cells is instrumental in synchronizing small cell groups and thereby
accounts for theta oscillations and phase precession. Direct excitatory coupling between the stellate cells, indirect
inhibitory coupling via a gamma-oscillating network of interneurons, or both could mediate phase coordination.
The compression index follows as a natural consequence of the self organization of firing phases.

II-13. Neurally plausible reinforcement learning of memory representations in
delayed-response tasks
Jaldert Rombouts1
Sander Bohte1
Pieter Roelfsema2

J. O. ROMBOUTS @ CWI . NL
SBOHTE @ CWI . NL
P. ROELFSEMA @ NIN . KNAW. NL

1 Centrum
2 The

Wiskunde & Informatica, Amsterdam
Netherlands Institute for Neuroscience

A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for
later use. Neurons in association cortex play an important role in this process: during learning these neurons
become tuned to relevant features and represent the information that is required later as a persistent elevation of
their activity. It is however not well known how these neurons acquire their task-relevant tuning. Here we present
a biologically plausible neural network model based on reinforcement learning that explains how neurons learn
to represent task-relevant information in delayed response tasks. Reinforcement Learning (RL) provides a formal
framework for learning action sequences with delayed rewards. Our learning scheme implements a Temporal
Difference (TD) learning algorithm, SARSA(λ). The neural network model consists of a layer of sensory neurons,
a second layer of association neurons and finally a layer of motor neurons. The association layer contains memory
units that have persistent activity allowing the network to solve the state aliasing problem in delayed response
tasks. Each motor neuron tries to predict the Q/Action-Value of its associated action, and a stochastic WinnerTakes-All competition biased by the predicted values determines the action that is executed. Motor neurons
have feedback connections to the association layer, and an interaction of feedforward and feedback activation
from the winning motor neuron lays down synaptic tags on those synapses that were involved in the decision.
Synaptic plasticity is determined by the product of tag strength and a globally released neuromodulator that
reflects the TD error. We can show that on average the updates are equal to a variant of the Error-Backpropagation
algorithm. The model can explain tuning of neurons as observed in area LIP (lateral intraparietal cortex) in (1)
saccade/antisaccade tasks; (2) categorization tasks; and (3) probabilistic classification tasks.

COSYNE 2012

107

II-14 – II-15

II-14. Modeling and analysis of rhythm generation mechanisms in excitatory
neural networks
Yaroslav Molkov1
Patrick Jasinski2
Natalia Shevtsova2
Jeffrey Smith3
Ilya Rybak2

YMOLKOV @ IUPUI . EDU
PEJ 23@ DREXEL . EDU
NATALIA . SHEVTSOVA @ DREXELMED. EDU
JSMITH @ HELIX . NIH . GOV
RYBAK @ DREXEL . EDU

1 Indiana

University
University
3 NINDS NIH
2 Drexel

The mechanisms generating neural rhythmic activity that persist after blockade of synaptic inhibition remain poorly
understood. Experimental studies in thick slices from neonatal mice containing the pre-Botzinger Complex (preBotC) identified two types of pacemakers and proposed two intrinsic neuronal bursting mechanisms that may contribute to rhythm generation: one based on the persistent sodium current (INaP), and the other involving calcium
(ICa) and calcium-activated, nonspecific cationic (ICAN) currents. Only the INaP-dependent bursting mechanism
has been found in the pre-BotC within thinner slices from neonatal rats. Both these mechanisms were also suggested to contribute to the generation of rhythmic activity in the isolated spinal cord. However, an involvement
and relative roles of these mechanisms in the operation of rhythmogenic excitatory networks within the brain stem
respiratory and spinal cord locomotor central patter generators are still under debate. Studies of the effects of
pharmacological blockers of INaP and/or ICAN on the network busting activity and its characteristics have shown
inconsistent results. In this theoretical/modeling study we have investigated rhythmogenic mechanisms in a population of excitatory neurons with INaP, ICa and ICAN conductances randomly distributed within the population.
We incorporated in the model and investigated the possible roles of Na+/K+ pump, IP3-dependent intracellular
calcium release, and mutually excitatory synaptic interactions within the population in generation of population
busting activity. We have demonstrated that such population can operate in several regimes of oscillatory bursting activity, which can be dependent on INaP and/or ICAN, or independent of both. The particular oscillatory
regime critically depends on general neuronal excitability and the number of neurons involved, which may vary
with the size of the slices studied experimentally. This may provide explanations for the different rhythmogenic
mechanisms inferred to operate under various experimental conditions.

II-15. Fluctuations in attractor networks
Michael A Buice
Ila Fiete

MABUICE @ MAIL . CLM . UTEXAS . EDU
ILAFIETE @ MAIL . CLM . UTEXAS . EDU

University of Texas at Austin
Noise correlations, which can provide information about functional connectivity between neurons and significantly
alter estimates of the information carried in neural populations, are of burgeoning interest in neuroscience with
the advent of simultaneous in-vivo recordings from multiple neurons. However, the theory of fluctuations in neural
networks – essential for interpreting such measurements – has not kept pace. Rate-based models provide only
an approximation to the mean firing rate, ignoring fluctuations. Going beyond this mean field is often handled
in an ad hoc manner. We present a theory of fluctuations in general, non-linear recurrent neural networks of
Poisson neurons with rates given by their time-varying synaptic inputs. The theory uses a path integral approach,
enabling a systematic and analytic evaluation of fluctuation effects beyond mean field. For concreteness and
comparison with existing work, we consider (continuous) attractor networks. The path integral mean-field theory
gives the standard rate-based result for a given attractor network, while allowing us to compute noise correlations
as a function of network connectivity, the effects of such fluctuations on the mean rates of neurons, the loss
of persistence of states on the attractor through diffusion, and the effects of input correlations on the rates and
correlations of network neurons. We illustrate these results in a simple 1-dimensional ring-attractor network

108

COSYNE 2012

II-16 – II-17
that underlies models of visual orientation tuning. Finally, our results are instrumental for accurately estimating
the Fisher Information in population codes, because they specify the actual correlations that will be present in
network models of population tuning. We show that, surprisingly, although neural correlations grow vanishingly
small in increasingly large networks, their contribution to Fisher Information remains finite. We also show that this
effect cannot exist in the corresponding Mutual Information, and thus contribute to growing evidence that Fisher
Information is a flawed proxy for information.

II-16. A neural system for motor planning and control
Greg Wayne
Larry Abbott

GDW 2104@ COLUMBIA . EDU
LFA 2103@ COLUMBIA . EDU

Columbia University
Two ideas [1] have resurfaced in motor control that, if validated experimentally, should strongly constrain neural network modeling efforts. The first [2] proposes that the “pre-movement” state of motor cortex codes “perimovement” neural activity and behavior – but not any particular task-level variable like the end-point of a reach.
The second idea [3] states that the motor system is not a static controller but an optimization machine that generates motor programs online for new task contexts. We demonstrate a novel neural system that synthesizes these
two ideas. For the first idea, we design a perceptual working memory (PWM), related to Pollack’s RAAM [4]. It
takes in a stream of sensory measurements s(t) and produces at each time point a code C(t) that compresses
the history s(0), ..., s(t). The PWM is recursive in that it works by jointly compressing s(t) and the previous code
C(t-dt) to make C(t). At time t, C(t) provides an enriched representation of the state of the body. C(t+T) for some
future time t+T, represents a proposed future trajectory that the body can undergo. Thus, it predictively codes
the neural activity that will occur during movement. To implement online optimization, a planning circuit samples
possible codes C(t+T). A controller receives C(t) and C(t+T) and propagates motor outputs to a forward model
to produce predicted sensory measurements, which are compressed by the PWM. The controller must generate
motor commands that subsequently produce a code similar to C(t+T). The planner must find a code C(t+T) to
generate a desirable trajectory of the body. Importantly, the planner imposes a cost function on the controller
without itself knowing how to optimize it.

II-17. Structure of stimulus induced correlations in random networks with distance dependent connectivity
Alejandro Fernandez Bujan
Ad Aertsen
Arvind Kumar

ALEJANDRO. BUJAN @ BCF. UNI - FREIBURG . DE
AD. AERTSEN @ BIOLOGIE . UNI - FREIBURG . DE
ARVIND. KUMAR @ BIOLOGIE . UNI - FREIBURG . DE

Bernstein Center Freiburg
Correlations among neural spike trains are believed to play an important role in information processing and learning [Averbeck et al. 2006]. The origin of correlations could be attributed to common inputs, which may arise
due to stimulus properties and/or the structure of the recurrent connectivity. Recently, it has become possible to
calculate the structure of correlation in the ongoing activity of a network from its recurrent connectivity [Pernice et
al. 2011]. However, it is not clear how the structure of input correlations interacts with the network connectivity to
shape the stimulus-induced correlation. We addressed this question using numerical simulations of random and
spatially structured networks of spiking neurons. Specifically we considered ring networks with nearest-neighbor
connectivity, which could be transformed in to random network by rewiring a fraction of neurons to make distance
independent connections. Surprisingly, we found that input correlation can both correlate and de-correlated the
network response. In random networks the input correlations always resulted in a net increase in the output correlations. However, in networks with nearest-neighbor couplings input correlations either increased or decreased

COSYNE 2012

109

II-18 – II-19
the output correlations depending on the ratio of local versus random connections. For a given network, below a
threshold ρtran input correlations were either ineffective or de-correlated the output. Importantly the parameter
ρtran increased with the ratio of distance-dependent and random connections in the network. Network correlations are tightly linked to the coding capacity of a network. Our works suggests that networks with some degree of
distance dependent connectivity are capable of altering their coding capacity according to the stimulus properties.

II-18. Perturbative memory encoding in recurrent networks
Carina Curto1
Anda Degeratu2
Vladimir Itskov1

CCURTO 2@ MATH . UNL . EDU
DEGERATU @ AEI . MPG . DE
VITSKOV 2@ MATH . UNL . EDU

1 University
2 Max

of Nebraska, Lincoln
Planck Institute for Gravitational Physics

Various mechanisms for encoding memories in recurrent networks have been studied in the past few decades,
most notably since the 1980s (Hopfield 1982, 1984). Given a prescribed list of memory patterns, however, it
is still an unsolved problem how to arrange the connections between neurons in a recurrent network such that
these – and only these – memories are encoded via stable fixed points of the network dynamics. The difficulty
with standard attractor neural network approaches lies in the emergence of “spurious” states, corresponding to
unintended memories outside the prescribed list. This phenomenon is especially apparent when the list contains
overlapping (or non-orthogonal) memory patterns, although this case is prevalent in experimentally observed
patterns of neural activity. Here we take a novel approach to memory encoding, where memories are encoded
as perturbations of a “background” synaptic weight organization. In this framework, the connectivity matrix W can
be understood as the sum of two components, W=J+A, where J corresponds to a fixed background of synaptic
weights, and A is a matrix of perturbations of these weights in the service of memory encoding. Whereas the
matrix J is generated by a simple neural field-type connectivity rule, with connection strengths depending only on
the arrangement of neurons in a “feature space,” the matrix A allows memories to be encoded from a prescribed
list. Perhaps surprisingly, we find that addressing the problem of encoding overlapping memory patterns in the
presence of certain backgrounds helps, rather than hinders, finding appropriate connectivity matrices. These
backgrounds are special in that they allow great flexibility to encode, or unencode, new memories. Interestingly,
we find unexpected connections to discrete geometry that enable greater control in designing networks with
desired sets of stable fixed points.

II-19. Efficient horizontal and vertical information processing in neural networks
Yen-Nan Lin
Yu-Chi Huang
Yi-Hsuan Lee
Chung-Chuan Lo

FO 60213@ GMAIL . COM
AIAPPLEG @ GMAIL . COM
TY 7 U 2001@ GMAIL . COM
CCLO @ MX . NTHU. EDU. TW

National Tsing Hua University
Neural networks are characterized by their ability of processing large amount of input in parallel. Typical analyses of the network architecture focus on the cluster coefficient (local communication) and shortest path length
(long-range signal propagation). The approach treats every nodes (neurons) equally and do not consider any particular direction of functional information flow in the networks. However, the approach may not fully characterize
the architecture of neural networks as they are distinct from other types of networks in at least two ways: 1) a
neural network has a specific direction of information flow: signals enter the network from a specific set of input
neurons, processed by local neurons and leave the network from output neurons. 2) The neural pathways via

110

COSYNE 2012

II-20
multiple synaptic connections may be functionally more important than the direct synaptic connections (shortest
pathways). To address the issues, we propose a novel analysis which characterizes information flows in a neural network. Specifically, we measures two quantities: 1) processing speed: how quickly all vertical information
pathways are established between input and output nodes and 2) information sharing: how far information entering a given input node travels horizontally to different output nodes. We analyzed C. Elegans neural networks,
protocerebral bridge network in Drosophila, and, as comparison, artificially generated small-world and random
networks. We found that while the small-world networks are faster in the processing speed and the random networks are superior in the information sharing, C. Elegans network and the protocerebral bridge network perform
well in both measures. The result suggests that neural networks are very efficient in vertical (from input neurons
to output neurons) as well as in horizontal (between input-output channels) information processing.

II-20. Scale-invariant effective connectivity in spontaneously active monkey
V1 cortical networks
Iñigo Romero Arandia1
Jan Drugowitsch2,3
Adam Kohn4
Alexandre Pouget5
Ruben Moreno6
1 University

IRARANDIA @ GMAIL . COM
JDRUGO @ GMAIL . COM
ADAM . KOHN @ EINSTEIN . YU. EDU
ALEX . POUGET @ GMAIL . COM
RMORENO @ FSJD. ORG

of Barcelona

2 INSERM
3 École

Normale Supérieure
Einstein College of Medicine
5 University of Rochester
6 Foundation Sant Joan de Deu
4 Albert

Spontaneous activity in cortical areas is structured, potentially revealing neuronal connectivity. A major problem is
that correlated activity between neurons might indicate connectivity between them, presence of common inputs, or
both. Statistical models that try to disentangle the effects of these two correlation sources are typically intractable,
and yet developing tractable ones is crucial to our understanding of how microcircuits are connected. We study
the correlations generated in monkey V1 networks measured by multielectrode arrays (100 electrodes) during
spontaneous activity. The spike train cross-correlograms (CCG) show a range of timescales up to a few hundred
milliseconds and have asymmetric exponential tails. These long timescales are induced by bursts of activity
whose rate and duration increase slowly with trial number. We show that stochastic linear rate (SLR) models
fit with surprising level of accuracy the CCGs of the V1 data. The estimates of the connectivity matrix can be
efficiently obtained from these fits. Crucially, SLR models are robust to the presence of common inputs and
provide very reliable estimates of connectivity even in the presence of unobserved common inputs in networks
of integrate-and-fire neurons. We show analytically that in SLR models the problems of estimating connectivity
among units and their common inputs decouple, and therefore it allows solving for the connectivity matrix without
knowledge of the common inputs in the network. We find that, unlike retina, the effective connectivity strength in
V1 is roughly constant as a function of distance in the range 0.4-3mm, despite the fact that the correlations show a
marked variation with distance in the same range. Therefore, we provide evidence for distance-invariant effective
interactions between cortical neurons, suggesting that the dynamics of neurons and their connectivity interact in
such a way that the effective connectivity lies close to a critical point with scale-invariant properties.

COSYNE 2012

111

II-21 – II-22

II-21. Synaptic consolidation: from synapses to behavioral modeling
Lorric Ziegler
Wulfram Gerstner

LORRIC. ZIEGLER @ EPFL . CH
WULFRAM . GERSTNER @ EPFL . CH

École Polytechnique Fédérale de Lausanne
Hippocampal plasticity is widely believed to underlie episodic memory formation. In this area, synaptic plasticity
consists of several phases spanning different time scales, from milliseconds to hours. Experiments have shown
that long-term potentiation or depression (LTP/D) of synaptic weights is complemented by a maintenance mechanism which depends on several intrinsic factors but also on an external novelty signal (Frey & Morris, 98). This
maintenance signal is non local and leads to associative effects between different plastic events occurring within
one single neuron. A behavioral analogue has been developed, in which fear Long-Term Memory (LTM) of rodents
is shown to obey similar rules (Moncada & Viola, 07). We propose a model network bridging the two concepts,
LTP/D and LTM. The model consists of three layers of integrate-and-fire units representing hippocampal place
cells, lateral amygdala neurons coding for emotional memory and a decision unit inspired from (Wang, 02). Initial
storage and longer temporal evolution of spatial or emotional memory is implement via an updated version of the
Tag-Trigger-Consolidation (TagTriC) model by (Ziegler, Clopath et al, 08), developed to reproduce induction and
maintenance of LTP/D in CA1. Simulation results show that the functional properties of TagTriC transpose to this
behavioral paradigm. Moreover, the model was able to explain a lack of associative effects observed when two
events occurred within a certain time window. Responsible for this interference is a resetting of the plastic modifications induced within the hippocampal layer when active consecutively in different contexts. This interaction
unfolds naturally from our modified TagTriC consistent with tag resetting in vitro (Sajikumar & Frey, 04). This work
proposes a first step towards integration of different levels, functional plasticity and behavior, of a general theory
of memory. It supports the tagging and capture hypothesis as a basic building block for episodic memory.

II-22. An adaptive spiking neural network for decision making in partially observable environments
Mattia Rigotti1
Daniel Ben Dayan Rubin1
Nathaniel Daw2
Stefano Fusi1

MR 2666@ COLUMBIA . EDU
DANIELBDRUBIN @ GMAIL . COM
NDD 204@ NYU. EDU
SF 2237@ COLUMBIA . EDU

1 Columbia
2 New

University
York University

Predicting future events is essential for deciding between alternative courses of action, some of which may lead
to rewarding outcomes, others which may result in loss of resources. Reinforcement Learning (RL) offers a theoretical framework for formalizing the problem of predicting the result of interactions with the environment and
adapting behavior so as to choose an optimal course of action. RL’s popularity in neuroscience is at least partly
due to its success in modeling neural data as exemplified by the well-known interpretation of dopamine neurons
activity as a temporal-difference reward-prediction error. Considerable effort has recently been devoted to extending the RL formalism to the case of only partially observable environments, a situation which is commonly
abstractly formalized as a Partially Observable Markov Decision Process (POMDP). In an attempt to bridge the
abstract algorithmic RL formalism with a biophysically plausible neural network implementation, we study a spiking network model endowed with representations of internal states and their values, in addition to observable
features of the external environment. Our model defines a probabilistic policy which corresponds to a set of
transition probabilities between internal states conditional on an observed external stimulus. We show how this
architecture is formally equivalent to a finite-state controller (FSC), a finite policy parametrization which has been
shown to efficiently approximate an optimal POMDP policy under many circumstances. This structure has several
advantages. FSCs induce finite-state Markov chains in a POMDP, and thus simplify the problem of computing
policy values. FSCs deal with a finite set of discrete states, as opposed to the high-dimensional belief-space of

112

COSYNE 2012

II-23 – II-24
exact POMDP methods. From an implementational point of view, the discrete nature of FSC states allow us to
represent them as attractors of the neural dynamics of a spiking network. Fundings: DARPA SyNAPSE; Gatsby,
Kavli, Sloan-Swartz and Swiss National Science Foundations

II-23. Learning precisely timed spiking responses
Raoul-Martin Memmesheimer1
Ran Rubin2
Haim Sompolinsky2

R . MEMMESHEIMER @ SCIENCE . RU. NL
RAN . RUBIN @ MAIL . HUJI . AC. IL
HAIM @ FIZ . HUJI . AC. IL

1 Radboud
2 Hebrew

University Nijmegen
University of Jerusalem

Experiments have revealed precisely timed patterns of spikes in several neuronal systems, raising the possibility
that these temporal signals are used by the brain to encode and transmit sensory information. It is thus important
to understand the capability of neural circuits to learn to produce stimulus specific temporally precise spikes.
Learning to spike at given times is challenging since the spike threshold and the ensuing reset introduce strongly
nonlinear dependence of the voltage on the value of the synaptic weights. We develop error-based supervised
learning paradigms that force the neuron to spike only near the desired times, during learning, allowing the
learning of precise spike times using Perceptron-like rules. For feed-forward networks, our model enables leaky
integrate-and-fire neurons to learn any set of spike patterns that is realizable by a LIF neuron, with arbitrarily high
precision in finite time. For the first time the capacity of generating multiple desired spiking patterns by a LIF
neuron is calculated. Importantly, we show that this capacity obeys a scaling relation between the maximum total
output spike count and the number of spikes in a single neuronal integration time. Our novel learning paradigms
can be used for training recurrent networks to generate multiple stimulus specific periodic and aperiodic spike
sequences. Interestingly, when the trained patterns consist of multiple spike pattern cycles, learning leads to
the creation of stable periodic spike patterns, without enforcing stability constraints. The learning paradigms can
be directly employed for learning timed responses to stimuli, such as in classical conditioning, as well as for
learning to detect the occurrence of precisely timed stimulus features. In recurrent networks, our learning can be
applied for temporal sequence generation, for associative recall of temporal sequences, and for transferring spike
sequences between networks, e.g. from a short-term to a long-term memory storage.

II-24. Synaptic scaling generically stabilizes circuit connectivity
Christian Tetzlaff1
Christoph Kolodziejski2
Marc Timme2
Florentin Wörgötter1

TETZLAFF @ PHYSIK 3. GWDG . DE
KOLO @ NLD. DS . MPG . DE
TIMME @ NLD. DS . MPG . DE
WORGOTT @ PHYSIK 3. GWDG . DE

1 University
2 Max

of Goettingen
Planck Institute for Dynamics and Self-Organization

Conventional plasticity (e.g. Hebbian [Hebb, 1949] or Spike-Timing Dependent Plasticity [STDP; Bi and Poo,
1998]) does in general not stabilize synaptic connection strength (or weights). Most of the existing plasticity
rules use stability mechanisms that rely on biologically unrealistic assumptions (e.g. Oja rule [Oja, 1982], weight
normalization [Gerstner and Kistler, 2002], sliding threshold [Bienenstock et al., 1982] or simply hard boundaries
[Song et al., 2000]). This led us to systematically analyze the stability properties of synaptic scaling [Turrigiano et
al, 1998; Turrigiano and Nelson, 2004] in combination with plasticity rules. Synaptic scaling uses the difference
between a neuron’s output activity and a desired target activity to scale its weight and this takes place on a
larger time scale compared to conventional plasticity. We developed an analytical method [Tetzlaff et al., 2011] to
understand the weight dynamics resulting from synaptic scaling and conventional synaptic plasticity, independent

COSYNE 2012

113

II-25 – II-26
of the exact form of the plasticity rule (e.g. Hebbian or STDP). This method reveals that synaptic scaling offers
robust weight stabilization if the scaling depends also on the weight itself. Then synaptic scaling suffices to
stabilize weights and no additional (presumably non-biological) stabilization mechanism (see above) is needed.
Furthermore, weights converge towards a diverse set of stable values that are determined (proportional) by their
presynaptic activity. Thus, synaptic scaling in combination with, for example, Hebbian plasticity can build up
memory traces and lead to cell assembly formation [Hebb, 1949] even in random recurrent networks [Tetzlaff et
al., 2011]. Our analytical method enables us to estimate the size and the duration of such assemblies depending
on input strength and plasticity parameters. Weight dependent synaptic scaling is, thus, a biologically realistic
candidate for both stabilizing circuit connectivity and generating memory traces in neuronal networks.

II-25. Regularisation reveals smooth dynamics of shared variability in neural
population activity
Lars Buesing
Jakob Macke
Maneesh Sahani

LARS @ GATSBY. UCL . AC. UK
JAKOB @ GATSBY. UCL . AC. UK
MANEESH @ GATSBY. UCL . AC. UK

University College London
Experimental advances have made it possible to simultaneously record the spiking activity of dozens or even hundreds of cortical neurons, and have thus crystallised the importance of understanding the statistical structure of
population activity. Latent factor models, including Gaussian-process factor analysis and hidden linear dynamical
system (LDS) models, are particularly well suited to capture shared variance in some cortical recordings (Macke
et al., NIPS 2011). How important is the dynamical structure of such models? Parametric statistical models may
be evaluated on their ability to generalise. If the model’s structure accords well with the true structure of the data,
then parameter values estimated from one set of data should provide respectable predictions for another set.
Generalisation is often helped by a regulariser - a term that biases parameters towards values favoured a priori.
A common approach to regularisation penalises large parameter values: applied to the LDS dynamics matrix this
favours short correlation timescales in the latent space and hence also in the observation space. We propose
an alternative regularisation scheme for LDSs which penalises deviations from constancy. This corresponds to a
prior on dynamics favouring longer timescales. We show that this approach yields a better statistical model for
neural data from primate motor cortex using likelihood on test data, as well as a previously established crossprediction measure. This finding, that holds for a wide range of training set sizes and latent dimensions, suggests
that shared variability in the neural population is indeed best described by a smooth process in time. Furthermore,
our fitting method is guaranteed to yield stable systems, ruling out biologically-implausible high firing rates and
variances when predicting neural activity with LDSs. Appropriate regularisation of this sort may help overcome
the difficulties posed by limited data to both scientific and prosthetic applications.

II-26. Identifying endogenous rhythmic spatio-temporal patterns in micro-electrode
array recordings
Michel Besserve1,2
Fanis Panagiotaropoulos2
Britni Crocker2
Vishal Kapoor2
Andreas Tolias3
Stefano Panzeri4
Nikos K Logothetis1
1 Max
2 Max

114

MICHEL . BESSERVE @ TUEBINGEN . MPG . DE
THEOFANIS . PANAGIOTAROPOULOS @ TUEBINGEN . MPG . DE
BRITNI . CROCKER @ TUEBINGEN . MPG . DE
VISHAL . KAPOOR @ TUEBINGEN . MPG . DE
ATOLIAS @ CNS . BCM . EDU
STEFANO. PANZERI @ IIT. IT
NIKOS . LOGOTHETIS @ TUEBINGEN . MPG . DE

Planck Institute for Biological Cybernetics
Planck Institute for Intelligent Systems

COSYNE 2012

II-27
3 Baylor
4 Italian

College of Medicine
Institute of Technology

Microelectrode arrays are a privileged recording modality to study neural processes with a very fine spatial and
temporal resolution. They capture the activity of small populations and permit assessment of synergistic interactions between cells. Patterns of rhythmic ongoing activity are of particular interest because they reflect the
intrinsic dynamics of neural populations and the way such dynamics may optimize the processing of incoming
information. In this study, we identify the various coherent spatio-temporal patterns of rhythmic activity occurring
across time using a two steps approach. First, signals were bandpass filtered in a relevant frequency band and
subsequently Hilbert-transformed. Second, the complex patterns of activity occurring across time were clustered
using a graph cut algorithm based on a phase shift invariant similarity measure. This invariance is a key-property
of our approach to isolate wave propagation phenomena. We apply our method to Local Field Potentials recorded
in the inferior convexity of the Prefrontal Cortex (icPFC) in two anesthetized macaques using a multi electrode array. We found a dominant travelling wave pattern in the beta band (15-25Hz), propagating along the ventral-dorsal
plane, emerging and vanishing across time both in the absence of visual stimulation (spontaneous activity) and
during binocular stimulation with movie clips. By computing mutual information, we showed that the amplitude of
this wave actually carries sensory information during the presentation of several movies. Altogether, our analysis
provides evidence for travelling wave phenomena reflecting the distributed computation in icPFC, which is known
to be involved in higher order sensory processing. More generally, our approach enables the unsupervised analysis of the complex spatio-temporal neural dynamics in ongoing signals, providing key information to understand
cooperative mechanisms in spatially distributed neural populations.

II-27. Embracing disorder: making sense of complex population codes
Omri Barak1
David Sussillo2
Misha Tsodyks3
Ranulfo Romo4
Larry Abbott1

OMRI . BARAK @ GMAIL . COM
SUSSILLO @ STANFORD. EDU
MISHA @ WEIZMANN . AC. IL
RROMO @ IFC. UNAM . MX
LFA 2103@ COLUMBIA . EDU

1 Columbia

University
University
3 Weizmann Institute of Science
4 IFC, UNAM
2 Stanford

Cognitive tasks require the joint activity of a large population of neurons. Hence there is no a priori reason to find
single neurons with easily interpretable activity profiles. Yet when we record from single neurons, we look for and
bias our models by precisely these neurons. Here, we use data from the delayed vibrotactile discrimination task
from the Romo laboratory to highlight the “not easily interpretable” neurons[1,2]. We then train a randomly connected recurrent neural network to perform the same task [3]. Even though this network has an inherent disorder
in its design, we are able to tease apart the underlying dynamical principles that enable the model to perform the
task. Finally, we discuss potential caveats of this framework such as generalization and reset—and propose ways
to overcome them. Our work stresses the need to consider all recorded neurons when forming models of network
dynamics. Research supported by the Swartz, Gatsby, Mathers and Kavli Foundations. [1] Barak O, Tsodyks M,
Romo R. Neuronal population coding of parametric working memory. Journal of Neuroscience 2010;30(28):9424.
[2] Brody CD, Hernandez A, Zainos A, Romo R. Timing and neural encoding of somatosensory parametric working memory in macaque prefrontal cortex. Cerebral cortex 2003;13(11):1196-1207 [3] Sussillo D, Abbott LF.
Generating coherent patterns of activity from chaotic neural networks. Neuron 2009 Aug;63(4):544-557.

COSYNE 2012

115

II-28 – II-29

II-28. The Coherence of Brain and Environment, not Input Statistics, Determines Neural Correlations
Christopher Buckley
Taro Toyoizumi

CHRISBUCKLEY @ BRAIN . RIKEN . JP
TARO. TOYOIZUMI @ BRAIN . RIKEN . JP

RIKEN Brain Science Institute
Coherent behavior arises from the dynamic interaction of the brain with the environment across multiple timescales.
Sensory/motor feedback via the environment has been show to be crucial to sensory perception. For example,
new born infants can sense the disengagement of feedback with their mothers from a very early age (Murray
et. Al, Social perception in infants, 1985), and sensory responses are significantly different in the actively and
passively sensing conditions (Neil and Stryker, 2010 : Harris and Theile, 2011). Here, we conduct a theoretical
investigation into the implications of this feedback loop for the characterization of neural dynamics. We construct a
minimal stochastic dynamical model that is consistent with the observed phenomenology of the rat barrel cortex,
where sensitivity and intra-neural correlations is greater in the quiet attentive state than the whisking state (Poulet
and Petersen, 2008). We hypothesize that the quiet attentive state corresponds to an open loop condition (without
environmental feedback) and the whisking state corresponds to a closed loop condition (with the environmental
feedback). We demonstrate that the reduction of neural correlations from the quiet attentive to whisking states
can robustly arise from the stabilization of a system, originally close to a dynamical instability, by environmental
feedback. Importantly, even if the input stimuli in the open loop condition is identical to the input in the closed
loop condition this is insufficient to elicit a change in brain state and the coherence of the system to the input
also remains small. This suggests that the feedback interaction, but not the input statistics, is critical for the reduction of neural correlation. We discuss the implications of this result for active perception and more broadly for
experimental work attempting to characterize the operation of neuronal populations in an open loop condition.

II-29. Sparse gamma rhythms arising via clustering in adapting neuronal networks
Zachary Kilpatrick
Bard Ermentrout

ZPKILPAT @ PITT. EDU
BARD @ PITT. EDU

University of Pittsburgh
Gamma rhythms (30-100 Hz) are an extensively studied synchronous brain state responsible for a number of
sensory, memory, and motor processes. Experimental evidence suggests that fast-spiking interneurons are responsible for carrying the high frequency components of the rhythm, while regular-spiking pyramidal neurons fire
sparsely. We propose that a combination of spike frequency adaptation and global inhibition may be responsible
for this behavior. Excitatory neurons form several clusters that fire every few cycles of the fast oscillation. Irregularity in their interspike intervals results from the combination of minor heterogeneities in network architecture
and external noise that triggers cycle skipping. This is first shown in a detailed biophysical network model and
then analyzed thoroughly in an idealized model. We exploit the fact that the timescale of adaptation is much
slower than that of the other variables. Using both singular perturbation theory and a weak coupling analysis, we
predict the relationship between the number of clusters arising spontaneously in the network as it relates to the
adaptation time constant. Both approaches identify the same power law scaling of cluster number to adaptation
time constant, which is corroborated in numerical simulations of the full system. Thus, we develop several testable
predictions regarding the formation and characteristics of gamma rhythms with sparsely firing excitatory neurons.

116

COSYNE 2012

II-30 – II-31

II-30. Spatial properties of the hippocampal theta rhythm in the hippocampus
Gautam Agarwal1,2
Gyorgy Buzsaki3
Fritz Sommer2

GAGARWAL @ BERKELEY. EDU
BUZSAKI @ AXON . RUTGERS . EDU
FSOMMER @ BERKELEY. EDU

1 Redwood

Center for Theoretical Neuroscience
of California, Berkeley
3 New York University
2 University

The theta rhythm is a prominent hippocampal oscillation that may serve as a reference for decoding the activity of
neurons known as place cells. One salient property of this rhythm is that it is spatially inhomogeneous in phase
and amplitude. However, current models of hippocampal physiology and function are largely based on the idea of
a single, global theta oscillation. By characterizing the spatial variations, we may better understand the generative
process underlying the theta rhythm(s), as well as the functional consequences of spatial variation upon the
neural code. We conduct our study using multi-electrode array (MEA) recordings of activity in a cross-section of
region CA1 in freely-moving rats. The spatial variations in the LFP are found largely within a two-dimensional,
complex subspace; the 1st component captures a phase gradient across space (i.e. a traveling wave), while the
2nd component represents the magnitude of this gradient (i.e. wave velocity). These two components evolve
in an interdependent manner: their relative phase drifts over time, and their amplitudes are anti-correlated over
several seconds. Furthermore, they correlate with neural activity: during ripples, which are characterized by
a transient increase in spiking activity across many neurons, the 2nd component exhibits a brief increase in
power. These ripple events account for only some of the variation in the 2nd component. We study the additional
variation by examining the joint evolution of the spatial LFP with behavioral variables (velocity and acceleration)
as well as population spiking activity. Finally, we assess the functional utility of spatial structure by comparing a
phase precession model based on the classical, single-site LFP, to one that incorporates fluctuations in the phase
gradient.

II-31. Human cortical neurons form functionally isolated networks during propofolinduced unconsciousness
Laura D Lewis1
Veronica Weiner1
Eran Mukamel2
Jacob Donoghue3
Emad Eskandar3
Joseph Madsen4
William S Anderson5
Leigh Hochberg6
Sydney Cash3
Emery Brown1,7
Patrick Purdon8

LDLEWIS @ MIT. EDU
VSW @ MIT. EDU
ERAN @ POST. HARVARD. EDU
JACOB . A . DONOGHUE @ GMAIL . COM
EESKANDAR @ PARTNERS . ORG
JOSEPH . MADSEN @ CHILDRENS . HARVARD. EDU
WANDERS 5@ JHMI . EDU
LHOCHBERG @ PARTNERS . ORG
SCASH @ PARTNERS . ORG
ENB @ NEUROSTAT. MIT. EDU
PATRICKP @ NMR . MGH . HARVARD. EDU

1 Massachusetts

Institute of Technology
of California, San Diego
3 Massachusetts General Hospital
4 Children’s Hospital Boston
5 Johns Hopkins University
6 Institute for Brain Science
7 Harvard Medical School
8 Harvard University
2 University

General anesthesia is associated with striking patterns in the electroencephalogram (EEG), such as a large

COSYNE 2012

117

II-32
amplitude slow (<1 Hz) oscillation and an increase in gamma (25-40 Hz) power. The neurophysiology underlying
these effects is poorly understood. Although the molecular-level effects of many general anesthetic drugs are
well studied, how these effects in single cells modulate electrical activity in larger-scale networks to produce
unconsciousness is unclear. To address this question, we recorded single units, local field potentials (LFP), and
intracranial EEG from three patients throughout a clinical induction of general anesthesia with the drug propofol.
We found that propofol-induced unconsciousness is not due to widespread neuronal silencing, but instead is
associated with specific changes in the structure of neuronal spiking. Although propofol enhances inhibitory
signaling via GABA-A synaptic connections, a large subset of units (54%) continue to spike at high rates after
loss of consciousness (LOC). However, within seconds of LOC, action potentials become entrained to the phase
of a local slow oscillation in the LFP. Spikes occur in short bursts of activity locked to the trough of the slow
oscillation, during which significant local network structure is preserved. At the peaks of the slow oscillation,
activity is periodically silenced, interrupting local processing. In addition, we find that the slow oscillation is
asynchronous across distant cortical regions, potentially impairing long-range communication. We conclude that
propofol-induced loss of consciousness is associated with slow neuronal rhythms that isolate cortical networks
both temporally and spatially, preventing sustained local information processing as well as coordinated global
network interactions.

II-32. Information theoretic limits on performance in short-term memory tasks
O. Ozan Koyluoglu
Ila Fiete

OZAN @ MAIL . UTEXAS . EDU
ILAFIETE @ MAIL . CLM . UTEXAS . EDU

University of Texas at Austin
Short-term memory is limited. Early notions, that only a fixed number of items can be stored in memory, have been
amended by recent experiments showing that memory systems can flexibly allocate memory resources across
fewer or more items with a corresponding trade-off in recall accuracy. However, it remains unclear what factors limit
working memory, and how it is best modeled to include parameters like item number, storage interval duration,
and item complexity. Here, we build a normative, or information-theoretically ideal, model for the short-term
memory of a set of analog variables. We treat all inputs presented to the memory system on an equal footing, as
information. Noisy neural dynamics, that degrade memory over the storage interval, are interpreted as passing the
information through a noisy channel. We first show why this mapping is appropriate. Next, we derive fundamental
limitations on short-term memory performance using information theoretic results on optimal communication over
noisy channels. We show how the resulting performance – which depends on optimal encoding and decoding
of the stored information for the given channel noise – scales with item number, storage time, and the dynamic
range of the inputs. We apply these results to a visual working memory task, and compare predictions for memory
performance with psychophysical findings. The functional dependence of recall on the parameters listed above is
well-predicted by the normative model, suggesting that the brain performs very good information encoding before
the storage period, and decoding after. As we show, these results stand in clear contrast to the predictions made
if the brain were directly storing the uncoded variables in continuous attractor networks matched to the dimension
and coding range of the variables.

118

COSYNE 2012

II-33 – II-34

II-33. Information processing changes during development in primary auditory cortex
Badr F Albanna1,2
Michele Insanally3
Hania Kover1
Heesoo Kim1
Shaowen Bao1
Michael DeWeese1

BADR ALBANNA @ BERKELEY. EDU
MNI 1@ NYU. EDU
HANIAKOEVER @ BERKELEY. EDU
HEESOO @ BERKELEY. EDU
SBAO @ BERKELEY. EDU
DEWEESE @ BERKELEY. EDU

1 University

of California, Berkeley
Center for Theoretical Neuroscience
3 New York University
2 Redwood

Sensory neurons encode information about environmental stimuli. Recent work in information theory has developed novel measures for quantifying exactly how much information neurons carry about a particular stimulus – in
particular, the specific information (SI) and stimulus specific information (SSI). Although these tools have previously been used to analyze responses from adult auditory cortex (Montgomery & Wehr, 2010), how the developing
brain encodes information about the sensory environment remains largely unknown. To address this question, we
apply several information theoretic approaches to sound-evoked multiunit spiking responses from the developing
rat primary auditory cortex. We find that 1) both the mutual information and SSI peak from postnatal days 16 to
18 coincident with an increase in receptive field bandwidth tuning, 2) the maximal SSI corresponds to the characteristic frequency (frequency with the highest firing rate), and 3) that this correspondence degrades with age.
These findings provide insight into the information processing capabilities of the developing auditory cortex.

II-34. The neural mechanisms involved in finding specific objects and switching between targets
Marino Pagan
Luke Urban
Margot Wohl
Nicole Rust

MPAGAN @ MAIL . MED. UPENN . EDU
LSURBAN @ GMAIL . COM
WOHLMP @ SAS . UPENN . EDU
NRUST @ PSYCH . UPENN . EDU

University of Pennsylvania
Finding a specific object requires neural mechanisms that can integrate visual information (i.e. what you are
looking “at”) with task-specific information (i.e. what you are looking “for”). To investigate the neural mechanisms
by which the brain determines whether a currently viewed scene contains a sought target, we recorded the responses of neurons in inferotemporal cortex (IT) and perirhinal cortex (PRH) while monkeys performed an object
search task. Each trial began with the presentation of a cued target object, followed by the sequential presentation of a random number of distractor objects, and then a target match; monkeys were rewarded for indicating the
presentation of the target. We found that PRH neurons had higher average target/distractor discriminability (d’)
than neurons in IT. However, the specific response properties underlying this difference were difficult to discern
because neurons in both IT and PRH were heterogeneously modulated by mixtures of visual, working memory,
and target/distractor information. We thus developed a procedure to deconstruct each neuron’s responses into a
weighted sum of intuitive basis functions. A neuron’s target/distractor discriminability could then be determined
by a function that is: 1) proportional to the projection of a neuron’s responses along a dimension that captures
the average target/distractor modulation, 2) inversely proportional to the combined projection of a neuron’s responses along dimensions that capture all other types of modulation (i.e. visual, working memory and residual),
and 3) inversely proportional to a neuron’s variability across trials. This analysis revealed that increased PRH
target/distractor discriminability resulted from increased target/distractor modulation rather than more trivial differences between the two populations. These results suggest the existence of PRH computations that are designed
to detect the presence of a target, invariant of target identity.

COSYNE 2012

119

II-35 – II-36

II-35. Second order dimensionality reduction using minimum and maximum
mutual information models
Ryan Rowekamp1
Jeffrey Fitzgerald1
Lawrence Sincich2
Tatyana Sharpee1
1 Salk

RROWEKAMP @ SALK . EDU
JFITZGERALD @ PHYSICS . UCSD. EDU
SINCICH @ UAB . EDU
SHARPEE @ SALK . EDU

Institute for Biological Studies
of Alabama at Birmingham

2 University

Conventional methods used to characterize multidimensional neural feature selectivity, such as spike-triggered
covariance (STC) or maximally informative dimensions (MID), are limited to Gaussian stimuli or are only able to
identify a small number of features due to the curse of dimensionality. To overcome these issues, we propose two
new dimensionality reduction methods that use minimum and maximum information models. These methods are
information theoretic extensions of STC that can be used with non-Gaussian stimulus distributions to
nd relevant linear subspaces of arbitrary dimensionality. We compare these new methods to the conventional
methods in two ways: with biologically-inspired simulated neurons responding to natural images and with recordings from macaque retinal and thalamic cells responding to naturalistic time-varying stimuli. With non-Gaussian
stimuli, the minimum and maximum information methods significantly outperform STC in all cases, whereas MID
performs best in the regime of low dimensional feature spaces. These new techniques should improve the characterization of neural features in higher areas of the brain where the application of currently available approaches
is restricted.

II-36. Optimal neural tuning for arbitrary stimulus priors
Zhuo Wang
Kevin Shi
Alan Stocker
Daniel Lee

WANGZHUO @ SAS . UPENN . EDU
KSHI @ SAS . UPENN . EDU
ASTOCKER @ SAS . UPENN . EDU
DDLEE @ SEAS . UPENN . EDU

University of Pennsylvania
What controls and determines the characteristic shape of a neuron’s tuning curve? Previous work has suggested
that neural tuning curves are the result of an optimal representation of stimulus information, and thus e.g. should
change with varying stimulus presentation times (Bethge, et al 2002). Here, we address the question how such
optimal neural tuning should depend on the stimulus distribution. We first consider the case in which a neuron’s
firing rates are Poisson distributed with mean firing rate determined by its tuning curve h(s), with max/min constraints. The expected squared reconstruction error of the stimulus magnitude s is bounded by Fisher Information,
via the Cramer-Rao bound. For any arbitrary stimulus distribution, this error is minimal when the Euler-Lagrange
equation is satisfied, leading to the analytical solution for the neuron’s optimal tuning curve. The result reduces
to the well-known quadratic form of the optimal tuning curve in the special case of a uniform prior distribution
(Brunel & Nadal 1998). We have generalized our analytic derivation to account for other types of noise, such as
e.g. stimulus dependent, additive Gaussian noise. Furthermore, our analysis can be extended to optimize other
information theoretic quantities such as mutual information. Numerical simulations successfully validated our theoretical results. In addition, we analyzed electro-physiological recordings of the spiking activity of the blowfly H1
neurons, with visual motion stimulus (de Ruyter, et al 1997). By fitting the the observed spiking activity to optimal
tuning curve according to our model, we were able to derive the stimulus distribution for which the blowfly neuron
is optimally tuned. Our results suggest that stimulus speed is approximately Gaussian distributed with mean 0
deg/sec and standard deviation 8 deg/sec. This prediction is in agreement with previous studies that have found
a prior for slow stimulus speed (Stocker & Simoncelli 2006).

120

COSYNE 2012

II-37 – II-38

II-37. A generative Model for Adaptation in Primary Visual Cortex Neurons
derived from Movie Statistics
Michoel Snow
Ruben Coen-Cagli
Odelia Schwartz

MICHOEL . SNOW @ MED. EINSTEIN . YU. EDU
RUBEN . COENCAGLI @ EINSTEIN . YU. EDU
ODELIA . SCHWARTZ @ EINSTEIN . YU. EDU

Albert Einstein College of Medicine
Adaptation is a phenomenon found in many neural systems whereby the response of a neuron is affected (e.g.,
reduced) by stimulus history. We focus on primary visual cortex (V1), for which there is considerable experimental
data, yet the computational principles underlying adaptation effects are incompletely understood. We hypothesize that the brain is “adapted” to the statistical properties of natural movies over time. Based on the Gaussian
Scale Mixture (GSM; Wainwright and Simocelli, 1999) generative model, we developed a computational model
that captures temporal statistical dependencies between oriented filters. Analogous to our previous work on spatial context (Coen-Cagli, Dayan, Schwartz, 2009), we considered a mixture model that combines two extreme
cases: one where the input stimuli seen in the recent past are statistically coordinated with the current input, and
one where they are independent. The parameters of the model, including the prior probability over the mixture
components, are learned from an ensemble of natural movies. Using Bayesian inference we computed model
responses to novel stimuli. Akin to divisive normalization (e.g., Heeger 1996), in response to a relatively constant
visual stimulus, the current (test) input is suppressed, in a divisive fashion, by the past (adapter) inputs; conversely,
with a rapidly changing signal, the model deems the inputs independent and no suppression results. Our model
replicated response suppression of the test stimuli to an optimally oriented adapter, which has previously been
ascribed to divisive normalization. Additionally, when we assumed that the probability of the adapter normalizing
the test depended on the orientation difference between the two conditions, rather than on their absolute orientations, we accounted for repulsion of tuning curves, an effect not explained by standard divisive normalization. The
model can be used to make predictions about adaptation to more natural (slow and fast varying) visual input.

II-38. Identifying dendritic processing in Drosophila OSNs
Aurel Lazar
Yevgeniy B Slutskiy

AUREL @ EE . COLUMBIA . EDU
YS 2146@ COLUMBIA . EDU

Columbia University
In sensory neurophysiology, both the input and the output of neural circuits are typically avail- able for reverseengineering a sensory system. Since parameters of the underlying biophysical circuits are usually unknown,
algorithms are devised for identifying parameters of input/output-equivalent models. The widely used identification algorithms (i) assume that spikes are generated according to an inhomogeneous Poisson process; (ii) rely
on stimuli having a particular distribution and/or being white; and (iii) require the evaluation of the PSTH through
a time-consuming repetition of complex experiments [1]. In [2] and [3] we investigated the identification of parameters in a wide class of neural circuits consisting of linear dendritic processing filters in cascade with spiking
neuron models, including conductance-based models such as the Hodgkin-Huxley neuron. The novel methodology assumes that input stimuli are bandlimited, or smooth. In many sensory modalities, including olfaction, this
is a natural assumption since in practice it is very difficult to deliver stimuli that are purely white and/or have a
particular distribution. Furthermore, since the model parameters are computed directly from spike times, the proposed methodology circumvents the burden of repeating the same experiment. Finally, this methodology bridges
the gap between identification using artificial and naturalistic stimuli and provides key insights into the relationship
between the identified kernel and the space of input stimuli. Here we tested our identification methodology on
the input/output data that we recorded from olfactory sensory neurons of Drosophila melanogaster. We modeled
the transduction of an odor- ant concentration into an ionic current using a linear filter. Spike generation was described by an integrate-and-fire neuron with random thresholds. Upon cross-validation, the identified [Filter]- [Ideal
IAF/RT] model consistently outperformed the LNP model derived from the same data set using STA. Furthermore,

COSYNE 2012

121

II-39 – II-40
the model was constructed from a single experimental trial.

II-39. Support Vector Machines in Spiking Neurons with Non-Linear Dendrites
Ran Rubin1
Raoul-Martin Memmesheimer2
Haim Sompolinsky1
1 Hebrew

RAN . RUBIN @ MAIL . HUJI . AC. IL
R . MEMMESHEIMER @ SCIENCE . RU. NL
HAIM @ FIZ . HUJI . AC. IL

University of Jerusalem
University Nijmegen

2 Radboud

The nonlinearities of synaptic integration in the dendritic trees of many neuron types are well established experimentally. However, their role in neural information processing is still an open challenge. According to one proposal
(Poirazi and Mel, 2001; Polsky et al., 2004) the thin dendritic branches endow pyramidal cells with a functional
‘two-layer’ Perceptron architecture. However, it is unclear if this architecture can be utilized for nonlinear computation in the temporal domain. Here we propose that dendritic nonlinearity allows neurons to emulate Temporal
Support Vector Machines (T-SVM) with nonlinear kernels. The T-SVM is a system that generates desired precisely
timed output spikes in response to input spike trains. Many interesting tasks involve complex, nonlinear relations
between input and output spike times, which cannot be emulated by linear summation of inputs. In the T-SVM,
the postsynaptic voltage is a weighted sum of contributions, each of which (analogous to a support vector) sums
nonlinearly a subset of synaptic inputs. The weighted contributions of the SVs to the total voltage represent the SV
coefficients. We develop a learning algorithm that yields the maximum-margin solution, in which the postsynaptic
voltage has maximal deflection from the threshold. Such a solution enjoys robustness to noise and enhanced
generalization abilities. Importantly, despite the fact that each time point in the example patterns can contribute a
SV, we show that in the optimal solution only a small set of discrete time points contribute. We apply the T-SVM
to several nonlinear problems, such as the spatio-temporal XOR problem and interval estimation tasks. We show
that T-SVM systems can be realized by a leaky integrate-and-fire neuron in which inputs arriving on the same
dendritic branch are summed nonlinearly. Thus, dendritic nonlinearities may allow neurons to implement complex
mappings between input spike trains and precisely timed responses.

II-40. Combinatorial neural codes from a mathematical coding theory perspective
Katherine Morrison
Vladimir Itskov
Zachary Roth
Judy Walker
Carina Curto

S - KMORRI 11@ MATH . UNL . EDU
VITSKOV 2@ MATH . UNL . EDU
S - ZROTH 1@ MATH . UNL . EDU
JWALKER 7@ MATH . UNL . EDU
CCURTO 2@ MATH . UNL . EDU

University of Nebraska, Lincoln
Shannon’s seminal work in the 1940s gave rise to two distinct, though interrelated, areas of research: information
theory and mathematical coding theory. While information theory has had a strong influence on theoretical neuroscience, ideas from mathematical coding theory have received considerably less attention. In particular, the idea
of a code functioning primarily to enable accurate and efficient error correction is often overlooked in neural coding
theory, though there have been some recent exceptions (Hopfield 2008, Sreenivasan & Fiete, 2011). We take
a new look at neural coding from the mathematical coding theory perspective, focusing on combinatorial codes
derived from neurons with idealized receptive fields. These codes can be easily thought of as binary codes, with
1s and 0s denoting neurons that are “on” or “off” in response to a given stimulus. Although it has been recently
argued that the entorhinal grid cell code may be quite good for error correction (Sreenivasan & Fiete, 2011),
we show that more typical receptive field codes (RF codes) perform quite poorly as compared to random codes

122

COSYNE 2012

II-41 – II-42
with matching size, length, and sparsity. The error-correcting performance of RF codes “catches up,” however,
when a small tolerance to error, in terms of a metric inherited from the stimulus space, is introduced. The error
tolerance reflects the fact that perception of parametric stimuli is often inexact. We suggest that a decrease in
error-correcting capability may be a necessary compromise for neural codes that reflect relationships between
stimuli via relationships between codewords.

II-41. A concurrent brain-machine interface for enhanced motor function
Maryam Shanechi1,2
Rollin Hu2
Marissa Powers3
Gregory W Wornell1
Emery Brown1,2
Ziv Williams2,3

SHANECHI @ MIT. EDU
RHU 1@ PARTNERS . ORG
MARISSA . POWERS @ TRINCOLL . EDU
GWW @ MIT. EDU
ENB @ NEUROSTAT. MIT. EDU
ZWILLIAMS @ PARTNERS . ORG

1 Massachusetts

Institute of Technology
Medical School
3 Massachusetts General Hospital
2 Harvard

Brain-machine interfaces (BMIs) largely focus on restoring original motor function. However, a more compelling
aim of such research is the development of BMIs that can surpass original motor function by also considering the
higher-level goal of the task. Since typical tasks consist of sequential movements, a BMI that can concurrently
decode the complete sequence before execution may be able to reformulate and perform such motor plans more
effectively. Here, we demonstrate that such concurrent decoding is possible as a result of a neural partitioning mechanism during working memory. Using population-wide modeling, we discover two functionally distinct
neural subpopulations in the primate premotor cortex that allow two planned targets of a sequential movement
to be concurrently held in working memory without degradation. Such surprising stability occurs because one
subpopulation always encodes only currently held target information and the other always encodes only newly
added target information, irrespective of target locations. We also find that surprisingly small subpopulations are
sufficient for reliable decoding. Based on these findings, we develop a BMI that can simultaneously decode the
full motor sequence in advance and then accurately execute it as desired.

II-42. Global Synchronous Spontaneous Activity in Xenopus Optic Tectum
Kazuo Imaizumi
Hamilton Farris

KAZUO IMAIZUMI @ MSN . COM
HFARRI @ LSUHSC. EDU

Louisiana State University
Patterned spontaneous activity is often critical to the development of neural circuits underlying topographic organization. Thus, studying spontaneous activity patterns and their source can elucidate how the functional features of neural circuits emerge. Although the optic tectum of Xenopus tadpoles is a model system for studying
developmental plasticity of topographic organization, its spontaneous activity has been relatively overlooked in
studies of circuit development. We studied spatio-temporal patterns of spontaneous activity in the tectum using
in vivo two-photon calcium imaging technique combined with a bulk loading cell-permeate tracer, Oregon Green
BAPTA1-AM. Our goals in the study are to define the developmental characteristics of spontaneous activity patterns in the tectum and to determine their source. Using a long imaging time window (6-8 min) with a slow frame
speed (0.6 frames/s), we found globally synchronized spontaneous activity, which was confirmed by the additional
imaging sessions with faster frame speeds (10-100 frames/s) in a subset of the preparations. We also found that
spontaneous activity patterns depended on developmental stage and not visual experience. To understand the
source of the patterned activity, tectal input was systematically removed by enucleating a contralateral eye (visual

COSYNE 2012

123

II-43 – II-44
system) and/or cutting the connection with the hindbrain (e.g., auditory, somatosensory, and lateral-line systems).
Whereas removing visual or mechanosensory input alone had little effect on the patterned spontaneous activity,
removing both of them drastically decreased event frequency and magnitude. The result is novel because it not
only suggests that both visual and mechanosensory inputs are necessary, but also that a non-linear combination of visual and mechanosensory inputs generates the patterned spontaneous activity. This study is, to our
knowledge, the first to show globally synchronized spontaneous activity by non-linear input combination in the
developing brain.

II-43. The Distinct Behavior of Membrane Potential and Spike Train Statistics
Robert Rosenbaum1
Kresimir Josic2
1 University
2 University

ROBERTR @ PITT. EDU
JOSIC @ MATH . UH . EDU

of Pittsburgh
of Houston

Understanding information processing in neuronal networks, requires a precise description of population responses. Theoretical approaches typically focus on spiking activity. However, spikes provide a sparse representation of a cell’s activity. Membrane potentials are continuously modulated by a cell’s input, and cells connected
by gap junctions respond to changes in each other’s membrane potentials. Popular recording techniques, such
as those of voltage sensitive dyes and local field potentials, capture a mixture of subthreshold and spiking activity.
Theoretical approaches to describe the statistical structure of membrane potential responses have not been fully
developed. We present theoretical tools to examine how the statistics of inputs to neurons determine the marginal
and joint statistics of their membrane potentials. This allows us to study how membrane potential and spiking
statistics are related. Counter to intuition, current coded signals are reliably reflected by membrane potentials and
firing rates in distinct regimes: Firing rates are sensitive to modulations of a cell’s input current when excitation is
strong and firing rates are high. In contrast, the mean membrane potential is most sensitive to such modulations
when excitation is weak and firing rates low. We find that when two uncoupled cells receive correlated inputs, their
spiking correlations and membrane potential correlations are reflect their inputs in distinct regimes. Hence, models that capture the spiking output of cells in a network may provide an incomplete picture of population activity.
These findings illuminate some of the fundamental filtering properties of neurons and have significant implications
for the interpretation of experimental recordings. We illustrate their impact by considering the correlation between
signals representing the pooled activity of cell populations, such as those obtained using voltage sensitive dyes.
We show that pooled signals can exhibit a decrease in correlations while spiking correlations between pairs of
cells increase.

II-44. The spatiotemporal structure of learned and recalled information in
whole frontal cortical networks
Ziv Williams1,2
Robert Haslinger3
Rollin Hu1

ZWILLIAMS @ PARTNERS . ORG
ROB . HASLINGER @ GMAIL . COM
RHU 1@ PARTNERS . ORG

1 Harvard

Medical School
General Hospital
3 Massachusetts Institute of Technology
2 Massachusetts

A central tenet of memory processing suggests that information is likely stored by neocortical networks through
the precise pattern of their neural activations. What these patterns are and how they stably represent associations
between learning and recall remains poorly understood. Specifically, are associations encoded by the patterns
of firing rates across neurons, the patterns of interactions (cross-correlations) between neurons, or some com-

124

COSYNE 2012

II-45 – II-46
bination of the two? To address this question, we performed multiple-neuronal recordings across three frontal
cortical areas while macaque monkeys performed a cued-recall task. We then determined how informative the
specific population-wide patterns of firing rates, temporal structure and interactions between neurons were about
associations as they were being learned and recalled. We used a combined encoding/decoding approach in
which the collective patterns of firing rates, temporal structure and interactions were first characterized using a L1
regularized, Generalized Linear network model and then performed a maximum a posteriori decoding on an independent test data in order to determine how informative the modeled patterns were of the different associations.
We find that even though the mean level of activity (population firing rate) did not change between associations
or between learning and recall, the patterns of both the firing rates and interactions did. We specifically find that
the same pattern of firing activities observed during learning was largely ‘reactivated’ during recall, whereas the
pattern of interactions was largely unique. Moreover, information provided by the pattern of interactions was not
distinct from the firing pattern across the same population of cells, but provided significant, new information about
the different recalled associations. The observed firing rate pattern completion and unique interaction structure
may provide a stable, content-addressable framework for storing associative information in memory.

II-45. Inhibition of return in natural vision revealed by non-parametric analysis
of gaze
Paul Bays
Masud Husain

P. BAYS @ UCL . AC. UK
M . HUSAIN @ UCL . AC. UK

University College London
Active exploration of the visual world depends on sequential shifts of gaze that bring prioritized regions of a
scene into central vision. The efficiency of this system is commonly attributed to a mechanism of “inhibition
of return” (IOR) that discourages re-examination of previously-visited locations. Such a process is fundamental
to computational models of attentional selection and paralleled by neurophysiological observations of inhibition
of target-related activity in visuomotor areas. However, studies examining eye movements in naturalistic visual
scenes appear to contradict the hypothesis that IOR promotes exploration. Instead, these reports reveal a surprisingly strong tendency to shift gaze back to the previously fixated location—suggesting that refixations might
even be facilitated under natural conditions. Here we resolve this apparent contradiction, based on a nonparametric conditional density analysis of gaze patterns recorded during both free-viewing and search of naturalistic
scenes. We show that the observed frequency of return saccades is in fact substantially less than predicted for
a memoryless system—demonstrating that refixation is actively inhibited under natural viewing conditions. This
study has important implications for computational models of selection based on a salience or priority map, which
have until now lacked quantitative estimates of the strength and specificity of return inhibition in natural vision.
Furthermore, our analysis reveals that memory for gaze history significantly influences the way in which natural
scenes are explored, contrary to accounts that suggest visual search has no memory.

II-46. Evidence for Attention-dependent inactivation of Sodium Channels
Emily Anderson
Jude Mitchell
John Reynolds

EBANDERS @ SALK . EDU
JUDE @ SALK . EDU
REYNOLDS @ SALK . EDU

Salk Institute for Biological Studies
The shape of the action potentials emitted by a neuron can vary, depending on the gating of the neuron’s Na+
and K+ channels, which become inactivated by depolarization. These changes are more pronounced among
broad spiking neurons, by virtue of the slower kinetics of the K+ and Na+ channels expressed by these neurons.
Consistent with this, we find that broad spiking neurons Area V4 of the awake macaque exhibit reductions in

COSYNE 2012

125

II-47 – II-48
action potential height following spiking activity. These changes in action potential height provide a moment-tomoment “window” into a neuron’s internal state, allowing us to see how it changes when attention is directed into
the neuron’s receptive field. We hypothesized that if attentional feedback signals depolarize neurons, this would
result in changes in action potential height. Consistent with this, we find significant reductions in action potential
height with attention, which are more pronounced among broad- than narrow-spiking neurons. A portion of this
reduction in height results from attention-dependent increases in firing rate, but after controlling for this, we find
an additional significant reduction in action potential height with attention. In addition to reducing action potential
height, depolarizing currents have also been found to reduce burst firing among pyramidal neurons. We reasoned
that if attention leads to an increase in the depolarization of V4 neurons, this might reduce their tendency to fire
action potentials in bursts. Consistent with this, we find a significant reduction in burstiness with attention among
broad spiking neurons. Together, these results provide evidence for attention-dependent depolarization of cortical
neurons, and provide constraints on conductance-based models of attentional state.

II-47. Cognitive efficiency explains intelligence effects on risk sensitivity and
temporal discounting
Nisheeth Srivastava
Paul Schrater

NISHEETHS @ GMAIL . COM
SCHRATER @ UMN . EDU

University of Minnesota
Qualitative and quantitative similarities between risk sensitivity and temporal discounting have led researchers
(Hayden, 2007) to suggest a common underlying mechanism. (Burks, 2009) have recently showed that both
risk sensitivity and intertemporal discounting appear to be correlated with general intelligence of human subjects.
In particular, subjects with greater intelligence demonstrate lower discount rates and greater risk neutrality than
subjects with lower measures of general intelligence. This observation has led them to postulate a high-level
model for the emergence of these biases as a process of detecting a true utility signal that must be separated from
noise via perceptual processes, with greater intelligence being associated with lesser noise. Instead of assuming
that intelligence somehow improves perceptual signal quality, we show that a choice model that attempts to
elicit subject preferences by minimizing the cost of memory recall provides a mechanistic explanation for the
observations recorded in (Burks, 2009) and (Platt, 2007) while making additional testable predictions. Our choice
model considers preference construction to be a process of recalling past beliefs about the goodness of options
into working memory from long-term memory with minimal possible cognitive effort. Cognitive effort, in turn,
is defined in terms of information-theoretic surprise of past experiences with respect to expectations such that
highly surprising and highly unsurprising experiences become easier to recall. Our model replicates the positive
correlation between general intelligence and lower risk-seeking for low probability high gain options observed in
Burks et al. and further predicts that this behavior can be understood as resulting from a flattening of the sigmoidal
probability weighting curve defined in prospect theory. Consequently, we further predict that greater intelligence
should be positively correlated with lower risk aversion to options with low probability of high loss. Lastly, we also
show a correlation between intelligence and intertemporal preference consistency.

II-48. Modeling maladaptive decision-making in a rat version of the Iowa Gambling Task
Vincent Valton1
Alain Marchand2
Francoise Dellu-Hagedorn2
Peggy Series1
1 University
2 Université

126

VINCENT. VALTON @ GMAIL . COM
ALAIN . MARCHAND @ U - BORDEAUX 1. FR
FRANCOISE . DELLU @ U - BORDEAUX 2. FR
PSERIES @ INF. ED. AC. UK

of Edinburgh
de Bordeaux

COSYNE 2012

II-49 – II-50

Deficits in decision-making have been repeatedly observed in various psychiatric disorders (e.g. ADHD, Mania,
OCD) and are often assessed using the Iowa Gambling Task (IGT). The IGT represents a realistic decisionmaking task where subjects have to choose between targets associated with rewards and penalties of varying
likelihood and amplitude. Previous studies have shown that a third of healthy subjects perform poorly in the IGT,
as observed in psychiatric patients [1]. Recently, the IGT was adapted for rodents (the Rat Gambling Task, RGT).
As in human studies, a third of healthy rats were found to exhibit poor decision-making [2]. These rats were
then run on a battery of tests to extract measures of impulsivity, reward sensitivity, behavioral inflexibility and riskseeking. Poor decision-makers were always characterized by high scores for a combination of these behavioral
traits. We modified the TD-learning algorithm to model learning and decision-making in the RGT and include
reward sensitivity, inflexibility and risk-seeking. This novel model was then used to assess: (1) how the behavioral
traits influence learning (2) Whether they can they explain different performances in healthy subjects. The model
was able to account for the performances of good and poor decision-makers. The model was fitted to individual rat
performances to describe their levels of reward sensitivity, inflexibility and risk-seeking. The parameters correlated
significantly with the scores obtained from experiments assessing these behavioral traits. This suggests that the
mathematical description of the traits is valid. This work supports the hypothesis that a combination of high scores
for reward sensitivity, inflexibility and risk-seeking affects the rats’ learning by altering reward prediction and their
ability to reverse their initial estimations. Biased perception and representation of the environment lead to aberrant
decisions according to the real outcome of the task but optimal according to the rat’s internal model.

II-49. A mechanism for value-guided choice based on the excitation-inhibition
balance in prefrontal cortex
Gerhard Jocham
Laurence Hunt
Jamie Near
Tim Behrens

GJOCHAM @ FMRIB . OX . AC. UK
LHUNT @ FMRIB . OX . AC. UK
JNEAR @ FMRIB . OX . AC. UK
BEHRENS @ FMRIB . OX . AC. UK

University of Oxford
There have been few mechanistic explanations of fMRI signals observed during cognitive tasks. Here we use
a mechanistic biophysical model to predict both behavioural performance and the dynamics of an fMRI value
comparison signal in ventromedial prefrontal cortex (vmPFC). The predictions depend on the levels of excitation
relative to inhibition in the vmPFC, which we estimate using MR spectroscopy. These data provide evidence for a
neural competition mechanism in vmPFC supporting value-guided choice.

II-50. An optimal control perspective of the competition hypothesis
Vasileios Christopoulos1
Paul Schrater2
1 California
2 University

VCHRISTO @ CALTECH . EDU
SCHRATER @ UMN . EDU

Institute of Technology
of Minnesota

A soccer player moves the ball down the field, looking for an open teammate or a chance at the goal. To be
effective, the player should maintain this set of possible goals, waiting to see which will become the best option.
The soccer player faces an example of a common control problem, where we must initiate an action plan, even
when the final goal is partially or completely unknown. To handle both goal ambiguity and changes in goals while
acting, the sensorimotor system must be able to generate flexible and partially prepared plans. Recent experimental findings support this hypothesis, suggesting the brain generates several concurrent policies associated
with alternative goals. These policies compete against each other, while perceptual information is used to bias

COSYNE 2012

127

II-51 – II-52
this competition, until a single policy is followed. Despite the experimental evidence, little is known about the
optimality of such policy competition. In the current study, we propose an extended optimal control framework
to model human/animal behavior in the presence of multiple potential goals and show that goal competition is a
natural by-product of handling goal uncertainty. We show that the optimal strategy in the presence of goal ambiguity can be expressed as a weighted mixture of multiple control policies, each of which produces a sequence of
actions associated with particular goals. One of the novelties of this framework is that the weight factor includes
both the effort cost and the benefits to achieve each goal, as well as the prior knowledge associated with the
goals, producing near-optimal behavior.

II-51. Decision making and working memory in a parietal-prefrontal loop model
John Murray
Xiao-Jing Wang

JOHN . MURRAY @ YALE . EDU
XJWANG @ YALE . EDU

Yale University
Working memory and decision-making involve a distributed interacting network of brain areas, with the parietal
and prefrontal cortices (PPC and PFC) at the core. However, the differential roles of these areas and the nature of their interactions are poorly understood. To examine these issues, we model both cognitive functions in
a loop circuit model of interacting modules PPC and PFC. Within each module, excitatory populations selective
for choice options compete through mutual inhibition. Populations send long-range projections between modules
onto excitatory and inhibitory cells. Stimulus input enters into PPC, reflecting the dorsal visual pathway. We identify conditions where both areas display persistent activity during working memory, and intervening distractors are
represented in PPC but filtered in PFC. These dynamics are observed experimentally and functionally desirable:
PPC encodes saliency and PFC ensures robustness according to behavioral demands. Feedback from PFC to
PPC can flexibly gate whether each stimulus is filtered or maintained in working memory. We propose the concept of pathway-specific excitation-inhibition balance for long-range projections. In this regime, only differences
in activity are propagated and integrated downstream, after the local computation is completed. Balance thereby
enables gating and serial computation within the distributed network. The same circuit model is applicable to
interactions between functionally distinct cell types, ‘target selection’ cells in PPC and ‘response’ cells in PFC. It
provides a mechanistic explanation of the experimental observation that reaction time correlates with the separation time of target selection cells and the onset time of response cells. We explore when the two modules receive
different, potentially conflicting, inputs, relevant to multisensory and reward-biased decision-making. Integration
of these inputs depends on the relative strengths of local and long-range connections. With strong local and
weak long-range connections, the network can generate ‘conflict states’. We examine conflict dynamics and how
conflict may be resolved.

II-52. Control allows confidence learning
Jacqueline Fulvio1
C Shawn Green2
Paul Schrater1
1 University
2 University

JMFULVIO @ UMN . EDU
CSGREEN 2@ WISC. EDU
SCHRATER @ UMN . EDU

of Minnesota
of Wisconsin

Risky decisions invariably involve prediction—given a set of current data, make an estimate of a future state. In
such cases, it is important to know not only the estimate, but also whether that estimate is reliable. Are we likely
to succeed right now? Should we defer the decision and if so, what will be the reliability of a future estimate?
How do you learn these quantities from data? Here we investigated the hypothesis that the ability to evaluate
performance reliability depends on the ability to do credit assignment on errors (motor error, trajectory prediction,

128

COSYNE 2012

II-53 – II-54
etc.). Observers were divided into two groups. Both groups watched a “swarm” of dots moving via a particular
dynamics model toward a target. The “no control” group had to determine which half of the target the swarm
would impact. The “control” group had to ensure target impact and was allowed to adjust the swarm’s position
during flight to do so, providing additional information useful for credit assignment. At any point in the trajectory
they could “lock in” the path. For both groups, the earlier a response was locked in, the more points would
be earned if it was correct. Observers’ actual extrapolation uncertainty was measured in interleaved blocks of
trials where they extrapolated the swarm’s target location from a set trajectory. This allowed direct calculation
of the “optimal” response time given observers’ actual extrapolation ability and uncertainty. Observers in the
“control” group improved their decision-making behavior by tracking their performance reliability. By contrast,
the “no control” group appeared unable to utilize this information. Instead, they heavily relied upon previous
trial outcomes and current trial conditions, rather than their individual abilities more generally. These results
demonstrate the importance of control in learning and appropriate confidence in risky decision-making.

II-53. A rodent model for studying mechanisms of behavioral response variability
Dougal Tervo
Mayank Kabra
Kristin Branson
Alla Y Karpova

GOWANTERVO @ GMAIL . COM
KABRAM @ JANELIA . HHMI . ORG
BARNSONK @ JANELIA . HHMI . ORG
KARPOVAA @ JANELIA . HHMI . ORG

HHMI, Janelia Farm Research Campus
Behavioral variability is essential for exploration and successful competition in social settings. How and to what
degree animals are able to generate variable behavior remains a central question in neuroscience. To examine
this question we determined if rats were able to successfully compete against a series of artificial agents designed
to punish progressively smaller deviations from the optimal strategy of selecting an action according to a memoryless coin toss. On each trial, the rat and the competitor selected one of two reward ports and the reward was
delivered if both made the same selection. Three competitive agents were used. Competitor 1 implemented
an algorithm that based its prediction on the detection of biases in animals’ choice patterns using the data from
the entire session (Lee et al, 2004). Competitor 2 also used an extended history but adapted more rapidly.
Competitor 3 used a large set of more local features to make its prediction (Freund and Schapire, 1997). We find
that in the absence of a strongly adaptive environment, rodents generate highly structured behavioral sequences.
When faced with progressively tougher competitors, however, they rapidly generate more variable behavioral
sequences. To examine strategies that rats used to successfully compete against each agent, we assessed their
ability to detect and learn a specific rewarding sequence while competing against each of the algorithms. Limited
exposure to the adaptive environment that only punished more global patterns (Competitor 2) left rats sensitive
to the local statistics of the environment and more able to detect and generate preferentially rewarded sequential
choices. Having competed against the more robust Competitor 3, however, rats appeared to abandon a memorybased strategy and were unable to find rewarded patterns efficiently. Their flexibility in adapting memory-based
and memory-less behavioral strategies makes rodents an ideal model to study neural mechanisms of response
variability.

II-54. Adaptive reinforcement learning in dynamic environments
Chaohui Guo1
Peter Bossaerts2
Kerstin Preuschoff3
1 University
2 California

CHAOHUI . GUO @ ECON . UZH . CH
PBS @ RIOJA . CALTECH . EDU
KERSTIN . PREUSCHOFF @ EPFL . CH

of Zürich
Institute of Technology

COSYNE 2012

129

II-55
3 École

Polytechnique Fédérale de Lausanne

In reinforcement learning (RL), the learning rate is a fundamental parameter that determines how past prediction
errors affect future predictions. While in modelling the learning rate is often kept constant, it has been shown that
organisms adapt their learning rate to the statistics of the environment. We have previously proposed an RL algorithm that adapts the learning rate by minimizing the overall prediction risk through Q-learning (i.e., by maximizing
the prediction precision). This implicitly incorporates additional information about underlying processes and thus
accelerates learning. Here, we test and further develop this model by studying how the learning rate in humans
adapts in a changing environment using functional magnetic resonance imaging (fMRI). Twenty healthy subjects
participated in an fMRI study. Each participant viewed a series of samples drawn from a normal distribution and
was asked to make a series of predictions about the statistics of the distribution. Known to the participants, the
statistics of the distribution were subject to change throughout the experiment, resulting in stable and volatile
periods. Reinforcement learning models (with and without adaptive learning rates) were fitted to the behavioral
data. The resulting parameter estimates reflect the subjects’ beliefs about the true state of the world and were
entered as regressors in the imaging analysis. We find regions involved in processing uncertainty such as the
anterior cingulate cortex (ACC), anterior insula and caudate to reflect the (objective and subjective) statistics of
the environment as well as individual learning rates. (This study was funded by NCCR Affective Sciences and the
Neurochoice project of SystemsX. We also gratefully acknowledge support from the research priority program at
the University of Zurich “Foundations of Human Social Behavior” and NCCR FINRISK.)

II-55. Scientists are suboptimal in judging scientific data
Ronald Van Den Berg1
Jeffrey Beck2
Wei Ji Ma1
1 Baylor

RVDBERG @ CPU. BCM . EDU
JEFFBECK @ GATSBY. UCL . AC. UK
WJMA @ BCM . EDU

College of Medicine
of Rochester

2 University

It is well-established that the human brain is close to optimal in solving relatively simple tasks, such as combining
two perceptual cues (Knill and Richards, 1996), but uses suboptimal heuristics when dealing with probabilities at
a more cognitive level (Tversky & Kahneman, 1974). It has been argued that deviations from optimality tend to
be smaller when the statistics of the task are familiar (Griffiths and Tenenbaum, 2006) or when probabilities are
communicated in implicit rather than explicit form (Trommershauser, Maloney, Landy, 2008). We tested scientists
on a task that satisfies both criteria, yet we found suboptimal behavior. Subjects were presented with noisy data
points generated by one of two possible linear models. The points were presented in a single scatter plot (1-plot
displays) or distributed across two scatter plots (2-plot displays). Subjects reported which of the two models was
most likely to have generated the data points and rated their confidence. The optimal observer would base his
model choice on the sign of the log posterior ratio for both options (which we call the decision variable, d) and his
confidence on its magnitude. When conditioned on d, we found that human performance was largely independent
of the number of scatter plots, but the confidence ratings were significantly lower in the 2-plot condition. This
suggests that subjects were unable to optimally combine evidence across two scatter plots. Human responses
are accurately described by a model in which the points in the scatter plots are represented in a noisy manner
and d is logistically mapped to confidence rating. When faced with relatively abstract probabilistic information, the
brain is apparently unable to utilize the circuitry that serves it so well in cue combination.

130

COSYNE 2012

II-56 – II-57

II-56. A hippocampal-cortical network underlies model-based planning in humans
Aaron Bornstein
Nathaniel Daw
Thomas Geib

AARONB @ NYU. EDU
NDD 204@ NYU. EDU
TAG 299@ NYU. EDU

New York University
How do we deliberate about the best action to take, without direct experience of the possible outcomes? The
instrumental decisions of humans and animals can be broadly classified into two types: habitual responses and
goal-directed plans. These are distinguished by the represen- tations they develop: action-reward contingencies
estimated via direct experience in the former, and incidentally acquired stimulus-stimulus associations, used to develop flexible plans, in the latter. Computational models of reinforcement learning (RL) describe the development
of these two types of representations, termed “model-free” or “model-based”, respectively. Model-free algorithms
have been applied to extraordinary success in describing the neural architecture of habitual decisions, in large
part because the action-reward representation offers a strong match to biological signals. However, the search for
similar correlates of goal-directed decisions have been frustrated by the wide range of possible representations
that might be learned by model- based algorithms. We approach this problem by engaging human subjects in a
task in which the available associative information is restricted to a specific form, and comparing the fit of models that describe response behavior driven by this information. We successfully describe goal-directed response
and choice behavior using predictions of a model-based algorithm, and, using fMRI, identify hippocampus and
category-selective visual cortical regions as key loci of learned associative representations that support this behavior. Building on this observation, we explore neural activity during the course of decision making. In particular,
we examine how human subjects deliberate about goal-directed plans, and provide support for a model architecture where these plans are constructed by sampling from available stimulus-stimulus associations, reinstating
them from a cache of hippocampally-mediated episodic memories.

II-57. Push-pull neural architecture naturally arises from optimal sensory stimulus detection
Tomoki Tsuchida
Angela J Yu

TTSUCHIDA @ UCSD. EDU
AJYU @ UCSD. EDU

University of California, San Diego
Push-pull neural architecture is a recurring property of sensory processing organization across modalities and
species. Examples include V1 simple cells (Hubel and Wiesel, J. Physiol., 1962), retinal ganglion cells (Meister
et al, J. Neurosci., 2008), LGN neurons (Hirsch et al, Nat. Neurosci., 2005), vestibular neurons (Jones et al, J.
Neurophys., 1984), and motion-detecting H1 neuron in flies (Ögmen and Gagné, Neural Networks, 1990). In this
work, we attempt to answer the question of why the push-pull architecture prevails in the brain by considering a
normative model of sensory decision- making. Sensory systems are often tasked with detecting discrete changes
in sensory states based on a noisy stream of inputs. Efficient detection of these state changes requires negotiating
a trade-off between detection speed and accuracy. Previously, we showed that the Bayes-optimal control policy
for detecting a change in a single stream of noisy inputs requires accumulating Bayesian evidence up to a fixed
threshold, and that the optimal procedure naturally gives rise to integrate-and-fire neuronal dynamics, with the gain
and firing threshold parameters determined by stimulus statistics and task constraints (Yu, NIPS, 2007). Here,
we extend the framework to show that when the task is to detect the onset of one of multiple possible stimulus
states, the optimal policy can be realized with a push-pull feed-forward neural architecture. For example, in order
to detect a change from a no-motion state to a state with motion in one of two possible directions, the theoretically
optimal neural detector must positively integrate the evidence in the preferred direction, and negatively integrate
the evidence in the anti-preferred direction, until the cumulative evidence for the preferred direction exceeds
a threshold. We also examine how the optimal detection policy and the implicated push-pull neural dynamics

COSYNE 2012

131

II-58 – II-59
should change as a function of task goals and constraints.

II-58. Concurrent integration and gating of sensory information with orthogonal mixed representations
Valerio Mante1,2
David Sussillo1
Krishna Shenoy1
William Newsome1
1 Stanford

VALERIO @ MONKEYBIZ . STANFORD. EDU
SUSSILLO @ STANFORD. EDU
SHENOY @ STANFORD. EDU
BNEWSOME @ STANFORD. EDU

University

2 HHMI

Computations in neural circuits are inherently flexible, allowing humans and animals to respond to sensory stimuli with actions that are appropriate in a given context. Fundamental to this flexibility is the ability to integrate
only context-relevant sensory information while ignoring irrelevant, distracting information. We studied the neural
mechanisms underlying such context-dependent gating in monkeys performing two different sensory discriminations on the same set of visual stimuli. A contextual cue instructed the monkeys to report either the direction of
motion or the color of a noisy visual stimulus with a saccade to one of two targets. During this task, we recorded
neural responses from the frontal eye fields (FEF). We found that the gating of relevant sensory signals, and
their integration towards a choice, can be understood as two aspects of a single dynamical process reflected in
FEF population responses. Using linear regression, we identified a multitude of task-related signals represented
simultaneously in the responses of FEF neurons, including the direction of motion and the color of the stimulus,
the context, and the developing choice. While these different signals are mixed at the level of single neurons,
they can be separated at the level of the population by projecting the neuronal activity onto the corresponding
regression vectors. To understand better the nature of the mixed signals in FEF, we trained a recurrent network
model to integrate only one of two noisy input streams. We found that the network created two context-dependent,
approximate line attractors to integrate the relevant sensory inputs. Surprisingly, both the relevant and irrelevant
inputs drive the network activity along directions in state space that are almost orthogonal to the direction of
integration. This model reproduces the observed dynamic representation of the task-related signals in FEF and
reveals a previously unknown mechanism of gating and integration.

II-59. Object completion along the ventral visual stream: neural signatures
and computational mechanisms
Dean Wyatte1
Hanlin Tang2
Calin Buia3
Joseph Madsen3
Randall O’Reilly1
Gabriel Kreiman4

DEAN . WYATTE @ COLORADO. EDU
HTANG @ FAS . HARVARD. EDU
BUIA @ PHYSICS . UNC. EDU
JOSEPH . MADSEN @ CHILDRENS . HARVARD. EDU
RANDY. OREILLY @ COLORADO. EDU
GABRIEL . KREIMAN @ TCH . HARVARD. EDU

1 University

of Colorado, Boulder
University
3 Children’s Hospital Boston
4 Harvard Medical School
2 Harvard

Object recognition in natural environments is characterized by significant variability and ambiguity due to object
overlap and occlusion. Foreground clutter can obscure critical object features, yet recognition remains robust using only partial information. Here we combine intracranial neurophysiological recordings along the human ventral
visual stream with computational models to study the neural circuits involved in recognizing occluded objects. We

132

COSYNE 2012

II-60
evaluate the hypothesis that neural circuits involved in object recognition utilize top-down projections to enhance
object representation by recovering occluded features from stored memories. To evaluate this hypothesis, we
construct a computational model of object recognition consisting of a hierarchy of reciprocally connected feature
processing layers. The model produces a representation in the highest levels capable of sustained recognition accuracy in spite of significant occlusion. Object completion emerges from the interplay of feedforward and feedback
connections. Purely feedforward versions of the model do not produce such complete representations. The model
constitutes a biologically plausible solution to the challenge of object occlusion and leads to testable predictions
about psychophysical performance and neurophysiological responses under occluded conditions. We confirm
previous psychophysical findings suggesting that recognition of occluded objects leads to longer reaction times.
To examine the neural circuitry underlying object completion, we record field potentials from electrodes implanted
in 12 epilepsy patients for clinical reasons. Subjects are presented with 100 ms flashes of 25 exemplar objects
drawn from 5 categories and perform a forced choice categorization task. In a subset of these trials, objects are
occluded by presenting them through random spatial arrangements of Gaussian “bubbles”. Consistent with the
model predictions, occlusion attenuates the physiological responses and leads to delays in object selectivity. The
computational, psychophysical and physiological results suggest that top-down projections may play a critical role
in object completion.

II-60. Decoding semantic content from fMRI responses to natural movies
Alexander G Huth
Tyler Lee
Shinji Nishimoto
An Vu
Jack Gallant

ALEX . HUTH @ BERKELEY. EDU
TYLERLEE @ BERKELEY. EDU
SHINJI @ BERKELEY. EDU
ANVU @ BERKELEY. EDU
GALLANT @ BERKELEY. EDU

University of California, Berkeley
Considerable interest has developed in decoding visual stimuli from brain responses measured using functional
magnetic resonance imaging (fMRI). A recent study from our laboratory showed that we can recover a great
deal of information about the visual structure of natural movies from evoked fMRI responses [Nishimoto et al,
2011]. Here we use a similar approach to decode the semantic content of natural movies. While the structural
features of visual stimuli (e.g. Gabor wavelets) are reasonably well understood, considerable debate surrounds
the representation of semantics in the brain. To address this problem we recorded brain activity while subjects
viewed two hours of silent natural movie clips. Other individuals then annotated each movie, writing either a
free prose description of each second of video or simply listing objects and actions that were present. These
annotations were then transformed into several different semantic feature spaces, each embodying a different
hypothesis about semantic representation in the brain. These included latent semantic analysis (LSA), latent
Dirichlet allocation (LDA), and WordNet. Regularized linear regression was then used to predict the response
of each voxel in the brain as a weighted sum of features in the candidate semantic space. Finally, direct and
Bayesian decoding methods were used to recover the semantic content of novel visual stimuli not used to train
the models. We found that all of these methods and models can recover significant semantic information from
fMRI data. Analysis of the estimated models revealed that as much as 27% of the cortex is involved in representing
visual semantic information, including inferior temporal, medial parietal, medial frontal and inferior frontal cortices.
These results show that we can accurately model and decode the representation of dynamic semantic information
in the brain.

COSYNE 2012

133

II-61 – II-62

II-61. Direction vs. category selectivity in LIP and MT neurons in delayed
match-to-category task
Warasinee Chaisangmongkon1
David Freedman2
Xiao-Jing Wang1
1 Yale

WARASINEE . CHAISANGMONGKON @ YALE . EDU
DFREEDMAN @ UCHICAGO. EDU
XJWANG @ YALE . EDU

University
of Chicago

2 University

Freedman and colleagues have conducted a series of neurophysiological experiments to examine neural responses during delayed match-to-category (DMC) tasks in lateral intraparietal cortex (LIP), and medial temporal
cortex (MT). Although much has been revealed about the information encoded in these areas, little is known about
how different signals interact and evolve over the time course of a trial. Such knowledge is crucial in determining
the neural mechanism of categorization. We took a fresh look at the work done by Freedman and Assad (2006)
that used random-dot motion stimuli to train DMC task. Here we report two major insights from our preliminary
analyses. A) In naïve monkeys, LIP neurons are tuned uniformly to directions of motion (Fanini and Assad, 2009)
but after DMC training they robustly reflect the categories of stimuli (Freedman and Assad, 2006). We characterize the dynamics of motion direction and category signals and found that most LIP neurons is tuned to motion
directions early in the sample period before category signal arises. We found that the tuning curves of LIP neurons are redistributed after learning, i.e. more LIP neurons are tuned to the directions near the center of the
categories than the directions near the category boundary. Such a shift in stimulus representation facilitates the
discriminatory decision that the task requires. B) We found that although MT activities straightforwardly represent
motion directions of stimuli during sample period, their activity is modulated by test stimulus categories in the
test period. This signal might be fed back from categorical decision area to guide synaptic learning. Accompanying this analysis, we construct a firing rate model to probe the mechanism and functional benefits associated
with these two findings. Specifically, we investigate how reward-dependent synaptic plasticity and feedback from
categorical decision area to MT may lead to the redistribution of LIP tuning curves.

II-62. Recovery of a shared spike-timing-dependent synaptic plasticity resource in natural spike trains
Jason Hunzinger1
Victor H Chan1
Robert Froemke2

JHUNZING @ QUALCOMM . COM
VCHAN @ QUALCOMM . COM
ROBERT. FROEMKE @ MED. NYU. EDU

1 Qualcomm
2 New

York University

Studies of spike-timing dependent plasticity (STDP) have revealed that long-term changes in the strength of
a synapse may be substantially modulated by temporal relationships between multiple pre- and post-synaptic
spikes. While long-term potentiation (LTP) and long-term depression (LTD) of synaptic strength have been modeled as distinct or separate functional mechanisms (Dan and Poo, Physiol Rev, 2006; Abbott and Nelson, Nature,
2000), here we present a new shared resource model. Motivation for this model originated from our analysis of
unsupervised learning stability with STDP for multi-spike trains. Our model allows fast, stable and diverse learning
of temporal spike patterns with a biologically-consistent spiking neural network. Surprisingly, the resource model
also accurately predicts in vitro observations of STDP in natural multi-spike trains. We discuss the potential
implications and candidate commonalities in natural short-term and long-term plasticity mechanisms.
We propose that LTP and LTD may be cooperative with closely-linked mechanisms, as opposed to competitive
with separate functional mechanisms. Specifically, we propose a shared resource model for synaptic plasticity with
three elements: 1) commitment to synaptic strength modification is modulated by availability of synaptic resources
shared between potentiation and depression mechanisms; 2) resources are depleted by commitment to synaptic

134

COSYNE 2012

II-63 – II-64
potentiation or depression, depending on the magnitude of modification; and 3) the shared resources available for
a synapse replenish over time. In a simple form, the resource model has only one parameter, the time constant
of the resource recovery. This resource is phenomenological, but could represent any one or combination of
biological factors (e.g., internal Ca2+, receptor expression, phosphorylation state, CaMKII activation, and/or gene
transcription).

II-63. Pairwise analysis can account for network structures arising from spiketiming dependent plasticity
Baktash Babadi1
Larry Abbott2
1 Harvard

BBABADI @ FAS . HARVARD. EDU
LFA 2103@ COLUMBIA . EDU

University
University

2 Columbia

Spike timing-dependent plasticity (STDP) modifies synaptic strengths on the basis of timing information available
locally at each synapse. Despite this, STDP can give rise to global structures when it acts within a recurrently
connected network. We analyze the types of structures that STDP can produce in a network of spiking neurons
both by running network simulations and through a mathematical analysis of the effects of STDP on interactions
between pairs of neurons. This provides an analytically tractable way of relating the structures arising in a network
to properties of the STDP model being used to modify synapses. We show how conventional pair-based STDP
acts as a loop-eliminating mechanism and organizes neurons into in- and out-hubs, depending on the external
input: sub-populations of neurons that receive higher external input turn into out-hubs once the synaptic weights
reach steady state, and subpopulations that receive lower external input turn into in-hubs. Loop-elimination increases when depression dominates and decreases when potentiation dominates. In addition, we find that STDP
with dominant depression implements a buffering mechanism for network firing rates. STDP with a shifted temporal window can enhance recurrent connections in a network, and also functions as a homeostatic mechanism that
maintains a roughly constant average value of the synaptic strengths. In general, studying pairwise interactions of
neurons provides a number of important insights about the structures that STDP can produce in large networks.

II-64. Similarity of spontaneous and sensory-evoked activity in cortex does
not imply learning
Michael Okun1
Pierre Yger1
Florian Gerard-Mercier2
Matteo Carandini3
Kenneth Harris1

MOKUN @ IC. AC. UK
P. YGER @ IMPERIAL . AC. UK
FLORIAN . GERARDMERCIER @ GMAIL . COM
MATTEO @ CARANDINILAB . NET
KENNETH . HARRIS @ IMPERIAL . AC. UK

1 Imperial

College London
Brain Science Institute
3 University College London
2 RIKEN

Sensory cortex is spontaneously active, and this spontaneous activity has statistical properties that are remarkably similar to those of sensory-evoked activity. The significance of this similarity, however, is unclear. One
interpretation comes from theories of learning in neural networks, such as Boltzmann machines. Through synaptic plasticity, such networks learn a model of the probability distribution of their inputs. When later presented with
a partial or absent input, the network produces activity patterns sampled from the posterior or prior of the learned
distribution, respectively. The success of this learning can then be observed in the similarity between spontaneous
and evoked activity. In support of this idea, a recent study analyzed multiunit activity in ferret V1 and found that
the Kullback-Leibler (KL) divergence between spontaneous and evoked firing patterns was substantially higher

COSYNE 2012

135

II-65 – II-66
in juveniles than in adults (Berkes et al., Science, 2011). Here we provide a more parsimonious explanation of
these findings. In population recordings of A1 and V1, we found that the joint structure of multiunit spike trains
was largely determined by network dynamics, which we summarized by the mean firing rate on each electrode
and the distribution (over time) of the population rate. Furthermore, even a randomly-connected recurrent spiking
model exhibits a transition between a state of poor spontaneous-evoked match to a state of high match, after
increasing a tonic conductance without any form of learning. Thus, in both cortical recordings and simulated
networks, the KL divergence is dominated by changes in network dynamics, and changes in multiunit distributions
can occur without synaptic plasticity. We conclude that the level of similarity between spontaneous and evoked
activity must be interpreted with caution: its changes need not be a signature of learning-related modifications in
synaptic strength; and high similarity does not imply that a network has learned a model of the environment.

II-65. Learning from positive and negative rewards in a spiking neural network
model of basal ganglia
Jenia Jitsev1
Abigail Morrison2
Marc Tittgemeyer1
1 Max

JENIA . JITSEV @ NF. MPG . DE
MORRISON @ BCF. UNI - FREIBURG . DE
TITTGEMEYER @ NF. MPG . DE

Planck Institute for Neurological Research
Freiburg

2 Albert-Ludwigs-University

Despite of vast amount of experimental findings collected on role of basal ganglia in reinforcement learning, there
is still lack in spiking neural network models that use plausible plasticity mechanisms to demonstrate rewardbased learning. In this work we extend a spiking actor-critic network model of basal ganglia introduced by Potjans
et. al., aiming to create a minimal realistic model of learning from positive and negative rewards. We hypothesize that not only the actor part, dorsal striatum, but also the critics part, the ventral striatum are subdivided in
distinct populations of medium spiny neurons (MSN), that carry either D1 or D2 dopamine (DA) receptor type.
This segregation allows explicit representation of both positive and negative expected reward within respective
population. In line with recent experiments, we further assume that D1 and D2 MSN populations have distinct,
opposing DA-modulated bidirectional synaptic plasticity, where high DA release induce potentiation (LTP) in D1
and depression (LTD) in D2 MSN, and reversely, low DA release leads to LTD in D1 and LTP in D2 MSN. We
implement the spiking network model in simulator NEST and conduct experiments involving application of delayed rewards in a grid world setting, where a moving agent has to reach a goal state while maximizing the total
obtained reward. We demonstrate that the network can learn to reach the goal equally well from both positive
and negative rewards distributed in the grid world environment. Specifically, it can be shown that the network is
also able to cope with the learning task even if only negative rewards are available, as opposed to the original
model. The spiking network model provides thus further hints on functional role of D1-D2 MSN segregation within
striatum and explains necessity for reversed direction of DA-dependent plasticity found at synapses converging
on distinct striatal MSN types.

II-66. On the “Site” and “Source” of Saccadic Countermanding: Reformulations of the Interactive Race Model
Motonori Yamaguchi
Gordon Logan
Thomas Palmeri
Jeffrey Schall

MOTONORI . YAMAGUCHI @ VANDERBILT. EDU
GORDON . LOGAN @ VANDERBILT. EDU
TOM . PALMERI @ VANDERBILT. EDU
JEFFREY. D. SCHALL @ VANDERBILT. EDU

Vanderbilt University
Countermanding saccades toward a visual target can be modeled as a race between two processes: one that

136

COSYNE 2012

II-67
produces a saccade (GO process) and the other that prevents the GO process from finishing (STOP process;
Logan & Cowan, 1984, Psych Rev). An interactive race model was formulated to explore how the STOP process
can prevent GO from finishing through lateral inhibition, motivated by the finding that firing rates in presaccadic
movement cells in the saccadic generator network, including the frontal eye field (FEF), superior colliculus (SC),
basal ganglia, etc., which may instantiate the GO process, decline markedly when saccades are successfully
inhibited (Boucher et al. 2007, Psych Rev). However, the interactive race model did not take into account the
full temporal dynamics of fixation cells in the network, which may instantiate the STOP process. We incorporated
this aspect of physiological data in revised interactive race models and tested several reformulations to examine
core assumptions of the original model. We found that models that assumed an external inhibitory control (e.g.,
blocking input to the GO unit, boosting input to the STOP unit, or strengthening the inhibitory connection from
the STOP unit to the GO unit) after the presentation of a countermanding signal fit the behavioral data better
than models that did not. The former models also exhibited the activation functions of the GO and STOP units
that resembled the activities of the movement and fixation cells. We conclude that the source of inhibitory control
exists outside of the site (i.e., FEF, SC) of saccadic countermanding.

II-67. Dimensionality in motor cortex: differences between models and experiment
Jeffrey Seely1
Matthew T Kaufman2
John Cunningham3
Stephen Ryu2
Krishna Shenoy2
Mark Churchland1

JSSEELY @ GMAIL . COM
MATT 235@ STANFORD. EDU
JPC 74@ CAM . AC. UK
SEOULMAN @ STANFORD. EDU
SHENOY @ STANFORD. EDU
MC 3502@ COLUMBIA . EDU

1 Columbia

University
University
3 University of Cambridge
2 Stanford

During movement, neurons in motor cortex exhibit complex, time-varying response patterns. Yet while it has
been difficult to determine what movement variables are represented by these responses, it is relatively easier to
ask how many variables are encoded. This can be done by assessing dimensionality using principal component
analysis (PCA). We analyzed multi-electrode recordings from two monkeys performing a reaching task involving
several different arm movements (e.g. right versus left, curved versus straight). We first assessed dimensionality
across neurons—i.e., given the responses of k neurons, is the response of the k+1th neuron a linear combination
of the first k? We then assessed dimensionality across movements—i.e. given the population responses for k
movements, is the population response for the k+1th movement a linear combination of the first k? We first examined simulated data from a traditional model, where each neuron is tuned to reach end-point (during planning) and
velocity (during movement). These simulated data had firing rates / noise properties matched to the real data. For
both simulated data sets, the across-neuron and across-movement dimensionalities were low and nearly equal.
This is expected; the model neurons represent a modest number of movement parameters. However, the results from the experimental data differed strikingly from those of the model. The experimental data shows high
dimensionality across neurons, yet remarkably low dimensionality across movements. This asymmetry across
dimensionality measures is dramatic in the data, yet absent in the traditional model. Thus, the data differ in both
quantitative and qualitative ways from the predictions of a traditional model. Nevertheless, this result is compatible
with what is expected of many classes of dynamical systems. For example, a high-dimensional dynamical system
could show such an effect if its initial states (one per movement) lay on a low-dimensional manifold.

COSYNE 2012

137

II-68 – II-69

II-68. Infinite-horizon optimal feedback control models for biological systems:
application to target jump
Zhai Fangwen1
Li Zhaoping1
Ning Qian2
1 Tsinghua
2 Columbia

FANGWENZ @ GMAIL . COM
Z . LI @ CS . UCL . AC. UK
NQ 6@ COLUMBIA . EDU

University
University

Optimal feedback control models have been highly successful in explaining many aspects of goal-directed reaching movements. However, most such models use a finite-horizon formulation and thus have to pre-fix movement
duration instead of predicting it. Consequently, to determine movement duration, these models have to be run
multiple times with different pre-fixed durations until an appropriate duration, according to some criteria, is found
via trial and error. The finite-horizon models are particularly inadequate to account for experiments in which the
target jumps to a new location at various times during reaching because these models have to know the new
movement duration, and compute a new solution for this duration, after the target jumps. To revolve this and other
problems of finite-horizon models, we and others recently proposed that an infinite-horizon formulation provides a
better framework for understanding biological motor control. Specifically, we implemented Phillis’s (1985) steadystate solution to an infinite-horizon optimal feedback control model. We showed that the model predicts movement
duration (Fitts’s law), and explains movement trajectories better than finite-horizon models. In the current study,
we extended the model in two important ways to make it more biologically plausible. First, we introduced statedependent noise into sensory feedback to incorporate Webber’s law. Second, we included delay for sensory
feedback. To our knowledge, these features are not properly incorporated into previous optimal feedback control
models. We then analytically derived the solution to the extended model, and applied it to explain key observations of target-jump experiments, including uni- and bi-modal speed profiles under various conditions. Contrary
to previous suggestions of multiple mechanisms, our model provides the first unified account for the target-jump
experiments.

II-69. Theta-phase coding by grid cells in two-dimensional environments
Eric Reifenstein1
Andreas VM Herz2
Richard Kempter1
Susanne Schreiber1
Martin Stemmler2
1 Humboldt

E . REIFENSTEIN @ GMAIL . COM
HERZ @ BIOLOGIE . UNI - MUENCHEN . DE
R . KEMPTER @ BIOLOGIE . HU - BERLIN . DE
S . SCHREIBER @ BIOLOGIE . HU - BERLIN . DE
STEMMLER @ BIO. LMU. DE

Universität zu Berlin

2 Ludwig-Maximilians-Universität

München

When a rat moves, grid cells in its entorhinal cortex become active in multiple regions of the external world that
form a hexagonal lattice. As the animal traverses one such “firing field”, spikes tend to occur at successively
earlier theta phases of the local field potential (“phase precession”). While phase precession when a rat runs
along a one-dimensional path has been demonstrated, whether the same phenomenon occurs in the open field is
less clear. Here, we analyze phase precession in both one- and two-dimensional environments. For rats running
on a linear track, we show that spike phases provide 80% more spatial information than spike counts, thereby
improving the position estimate derived from a single neuron down to a few centimeters. To understand how
spike phase variability limits the resolution, we analyze spike trains run by run. Phase precession on single runs
is significantly stronger than the pooled-run data suggest. Furthermore, no correlations in the spike sequences
exist across the multiple firing fields, suggesting that each field independently encodes physical space. In twodimensional environments, a rat’s path can curve, go through the center of the grid field, or swerve and miss
the center completely; additionally, running speed is highly variable—in contrast to the linear track. Despite
these differences, the slope and the correlation of phase precession in one and two dimensions are quite similar.

138

COSYNE 2012

II-70 – II-71
Interestingly, runs that graze a grid field tangentially lead to steeper phase precession, as opposed to runs through
the field center. If the run through one firing field is long and winding, however, phase precession decreases
midway, and the grid cell’s spikes lock to a new preferred phase. Such observations pose constraints on the
possible mechanisms of phase precession.

II-70. Suppressing Actions in the Basal Ganglia
Robert Schmidt
Daniel Leventhal
Jeff Pettibone
Alaina Case
Joshua Berke

ROSCHMID @ UMICH . EDU
DLEVENTH @ MED. UMICH . EDU
JRPETTIBONE @ GMAIL . COM
ALAINAN @ UMICH . EDU
JDBERKE @ UMICH . EDU

University of Michigan
Action suppression is thought to be a core component of executive function. Impaired action suppression plays an
important role in many pathologies such as Parkinson’s Disease, Tourette Syndrome, drug addiction or gambling.
A common clinical tool to detect impaired action suppression is the stop-signal task, in which human subjects
have to rapidly suppress imminent movements. To test previous proposals that parts of the basal ganglia act
as a “brake” on behavior, we developed a rat variant of the stop-signal task and recorded neural activity from
different basal ganglia subregions simultaneously (striatum, globus pallidus, substantia nigra, pars reticulata;
SNr, and subthalamic nucleus; STN). We found that movement initiation was marked by firing-rate decreases in
the output region SNr, in line with disinhibition of thalamocortical drive during action initiation. Importantly, during
successful action suppression the SNr firing-rate decreases were interrupted by a brief short-latency response to
the stop-signal. During failed action suppression there was no response to the stop-signal in SNr. In contrast, we
found STN units that exhibited fast responses to the stop-signal in both successful and failed action suppression.
This suggests that the STN-SNr transmission of sensory signals is crucial for reactive inhibition of behavior. We
conclude that SNr performs sensorimotor integration by combining the STN sensory stop-signal response with
action-related signals from the striatum.

II-71. Phase coding of trajectories by grid cells in unconstrained environments
Jason Climer
Michael Hasselmo

JRCLIMER @ BU. EDU
HASSELMO @ BU. EDU

Boston University
It is thought that EEG and LFP oscillations reflect large amounts of coherent activity in the brain and that neurons
represent information via use of temporal and rate coding. In phase coding, the firing of a neuron relative to
the phase of LFP rhythms also carries information. Hippocampal place cells fire at earlier phases (precess) in
hippocampal theta as the animal moves through the place field on a linear track and in the open field. Grid cells in
medial entorhinal cortex (MEC) fire in an array of positions falling on the vertices of a hexagonal grid, and precess
against MEC theta on the linear track. However, an in-depth analysis of the phase-coding of grid cells in the open
field has yet to be performed. We have developed a novel technique to aid in this analysis. We estimate how
“in field” an animal is using the rate map, and from the rising and falling of the “in-fieldness” along the animal’s
trajectory we extract phasic information using the Hilbert transform. This gives us a score for how much the animal
has completed each pass, which we call the “pass-index.” Similar to place cells, the firing of grid cells precesses
against theta as the animal moves through a field, irrespective of the direction or speed of the pass. In addition,
grid cells precess as the animal moves through the field regardless of the entrance and exit angle of the pass or
how close the pass comes to the center. This gives grid cells the capacity for finely coding their recent trajectories

COSYNE 2012

139

II-72 – II-73
via phase coding, and may have implications for models that generate grid cells. The pass-index can be applied
to any state space, and may provide insights into the mechanisms by which the brain uses phase coding in other
systems.

II-72. Statistics of junctions in natural images
James Golden1
Kedarnath Vilankar2
Damon Chandler2
David Field1
1 Cornell

JRG 265@ CORNELL . EDU
KEDAR . VILANKAR @ OKSTATE . EDU
DAMON . CHANDLER @ OKSTATE . EDU
DJF 3@ CORNELL . EDU

University
State University

2 Oklahoma

Studies on image features like edges and contours have provided insights into how the visual system efficiently
processes images from the natural world. Junctions are a loosely defined class of image features that has received
comparatively little attention. Using the Berkeley Segmentation Database, we measured several properties across
up to 10,000 junctions present in 500 natural images. These statistics shed light on what types of occlusions give
rise to junctions and why a large percentage of junctions cannot be detected locally by human observers or
automated methods. Junctions were defined as the points of intersection between three hand-segmented objects
with high agreement across segmentations by multiple subjects. In order to investigate occlusions, we measured
the most common angles found at Y-junctions. The most common angle was slightly less than 180 degrees; the
most common angle triplet was roughly (88 deg, 108 deg, 164 deg). We measured the probabilities of occurrence
of the Michelson contrast between the three wedges of Y-junctions. We found approximately 40% of Y-junctions
have an edge with a Michelson contrast of less than 0.05, making them invisible locally. The most common
triplet of contrasts has all three magnitudes below 0.15. Previous psychophysical work has found that 45-50% of
junctions cannot be identified by the visual system using only local information, which is in line with our estimated
fraction of junctions with one contour having near-zero contrast. These results will be discussed in the context
of efficient coding of junctions by the visual system and a Bayesian approach for junction identification in natural
images.

II-73. V1 and A1 maps: different topographies, a common organizing principle
Hiroki Terashima1,2
Masato Okada1,3

HTERASHIMA @ MNS . K . U - TOKYO. AC. JP
OKADA @ K . U - TOKYO. AC. JP

1 The

University of Tokyo
Society for the Promotion of Science
3 Riken Brain Science Institute
2 Japan

Contrary to the long-held idea that the sensory cortices exhibit similar topography, recent physiology has indicated
a difference between the primary visual cortex (V1) and the primary auditory cortex (A1). On a single-cell scale,
the V1 retinotopic map is smooth (Ohki et al., 2006; Smith & Hausser, 2010), whereas the A1 tonotopic map is
disordered (Bandyopadhyay et al., 2010). This discrepancy suggests that V1 and A1 use fundamentally different
information processing strategies; however, we hypothesize that the discrepancy emerges not from the difference
of organizing principles, but from that of environmental statistics. A V1 model study (Hyvarinen & Hoyer, 2001)
showed that a smooth retinotopic and orientation map can emerge from visual scene statistics; in addition, our
previous study on A1 (Terashima & Hosoya, 2009) suggested that one of the statistical characteristics of auditory
stimuli, compared to visual ones, is correlations between distant frequencies. In this study, to model the disordered
A1 map, we investigated how the V1 map model behaves when the input is “auditory”. First, to vary the input
“auditoriness”, i.e., the degree of correlations between distant coordinates, we used one-dimensional artificial

140

COSYNE 2012

II-74 – II-75
inputs. When the inputs were vision-like (simply locally correlated), the model produced smooth maps; however,
as the input “auditoriness” increased, maps were more disordered. Second, we used natural visual and auditory
stimuli, and compared the maps’ smoothness. As previously reported, the natural images resulted in a map with
smoothly changing retinotopy. In contrast, when the input was spectrograms of the human voice, the map was
significantly disordered. Therefore, the single model can produce both V1-like and A1-like maps, depending on
the input statistics, which are in fact different between vision and audition. The results suggest that the different
topographies of V1 and A1 may be a natural consequence of adapting to natural stimulus statistics.

II-74. Task-dependent feature representations of complex sounds in human
auditory cortex
Annika Linke1
Rhodri Cusack2
1 The

ANNIKALINKE @ GMAIL . COM
RHODRI @ CUSACKLAB . ORG

University of Western Ontario
and Mind Institute, University of Western Ontario

2 Brain

Real-world sounds vary in a plethora of different ways and it is unclear which features are encoded in auditory
cortex. Electrophysiological evidence of rapid plasticity in non-human auditory cortex suggests that the information represented varies depending on task demands. However, limitations in methods for neural measurement
have made it difficult to study auditory feature information coding in the human brain. Investigating how feature
tuning differs between individuals and under varying task demands could not only foster understanding of how
complex sounds are represented and how features are integrated to learn to categorize new and recognize familiar sounds, but also have implications for people suffering from abnormalities in auditory processing. In order
to assess which acoustic and abstracted features of complex natural sounds are represented in human auditory
regions during perception and two cognitive tasks (change detection and imagery), we used functional magnetic
resonance imaging (fMRI) and multivariate pattern analysis (MVPA), which exploits information in distributed neural networks and is robust to individual differences in anatomy. Our results show that the information encoded
in spatially distributed patterns of activity is task-dependent. Only during imagery but not during perception or
short-term memory maintenance was semantic information represented in auditory regions. During perception
and short-term memory maintenance, activity patterns in auditory regions contained stimulus-specific but not abstracted, categorical information. Additionally, differences in activation magnitude and pattern distinctiveness in
auditory regions were related to individuals’ memory capacity and imagery vividness. This indicates that auditory
cortex is recruited for processes beyond analyzing simple feature information, playing an important role in maintaining sustained representations of sounds in short-term memory and encoding abstracted information during
imagery. The demonstrated suitability of multivariate methods for analyzing feature coding in human auditory
cortex using fMRI provides exciting opportunities to investigate how neural representations vary within as well as
between individuals.

II-75. Predictive Coding with linear threshold neurons
Arjun Bharioke1,2
Dmitri Chklovskii1
1 HHMI,

BHARIOKE @ GMAIL . COM
CHKLOVSKIID @ JANELIA . HHMI . ORG

Janelia Farm Research Campus
of Cambridge

2 University

Neurons must faithfully code natural signals that may vary by orders of magnitude over a short time scale, for
example following a saccade, yet possess only a limited dynamic range. This dynamic range requirement may
be reduced by subtracting components of the signal that can be predicted from the past; this is a predictive
coding strategy (Srinivasan et al, 1982). Such compression of the dynamic range can be implemented by a

COSYNE 2012

141

II-76
feedback inhibition circuit, where Neuron 2 (long time constant) computes the prediction, which is then subtracted
from the full signal transmitted through Neuron 1 (short time constant). We studied the optimal parameters of a
linear predictive coding network for input with a mixture of low (predictable) and high frequency (unpredictable)
components. Minimizing transmission power, we find the predictor neuron’s time constant must be matched to
the temporal correlation length of the input. However, when the ratio of low and high frequency components is
varied, the optimal network was shown to require different feedback strengths, simultaneously varying the length
of time over which the predictive cell sums, as well as the amplitude of the returning feedback. This need for
a variable feedback immediately suggested that a nonlinearity applied to the output of the feedback cell would
provide a response better than the optimal linear threshold cell, over a range of contrast intensities. We present
here a model that utilizes a linear-threshold function on the output of the predictive cell. We demonstrate that this
model does indeed reduce the cost compared to the optimal linear network. Further, it is able to model many
of the nonlinear effects measured with changing contrast, without having to adapt its properties at each contrast
level (unlike alternative models) (Laughlin 1994; Victor 1987).

II-76. Serotonergic modulation of sensory information processing
Eran Lottem
Guillaume Dugué
Magor Lorincz
Patrícia Correia
Zachary Mainen

ERAN . LOTTEM @ NEURO. FCHAMPALIMAUD. ORG
GHYOMM @ GMAIL . COM
MAGOR . LORINCZ @ NEURO. FCHAMPALIMAUD. ORG
PATRICIA . CORREIA @ NEURO. FCHAMPALIMAUD. ORG
ZMAINEN @ NEURO. FCHAMPALIMAUD. ORG

Champalimaud Neuroscience Programme
Involved in a wide range of cognitive functions, serotonin is an important neuromodulator. A common theme that
emerges from many studies is that activation of the serotonergic system facilitates motor activity and suppresses
sensory information processing. Specifically, in the rodent olfactory system serotonin modulates both sniffing and
odor processing, and is therefore well positioned to control olfaction. In rats engaged in an olfactory discrimination task it was shown that neurons within the dorsal raphe nucleus, the major source of serotonin to the cerebral
cortex, modulate their activity in response to specific sensory, motor and reward variables. In addition, it was
shown that serotonin alters the response properties of olfactory bulb neurons. However the impact of this activity
on behaviour and perception remains largely unknown. To address this issue we use optogenetic, electrophysiological and behavioral approaches. By viral delivery of the light-activated cation channel channelrhodopsin-2
into serotonergic neurons, we are able to specifically identify these neurons during extracellular recording, and to
activate them with millisecond precision. We find that DRN illumination elicits a prominent response, which we
term optical field potential (OFP). The OFP displays a number of characteristics similar to those of typical serotonergic neurons, such as waveform shape and frequency adaptation, suggesting that it represents the summed
firing of synchronously active neurons. We are extending this method to study the effects of serotonin release on
the response properties of olfactory cortex by activating serotonergic neurons during odor presentations or afferent stimulation in-vivo and in brain slices. By controlling serotonin release in mice as they discriminate between
various odors, we hope to establish the relationship between serotonergic activity on the one hand and perception
and behavior on the other. We believe that the results of this study will provide new insight into the functioning of
this little-understood system.

142

COSYNE 2012

II-77 – II-78

II-77. Temporal aspect of odor stimuli and odor identity and intensity coding
by Olfactory Receptor Neurons
Carlotta Martelli
John R Carlson
Thierry Emonet

CARLOTTA . MARTELLI @ YALE . EDU
JOHN . CARLSON @ YALE . EDU
THIERRY. EMONET @ YALE . EDU

Yale University
Odor stimuli elicit spatio-temporal patterns of activity in the brain. Spatial patterns arise from the specificity of the
interaction between odorants and odorant receptors expressed in different olfactory receptor neurons (ORNs). But
which features of odor stimuli are represented in temporal patterns and their role in odor coding remains unknown.
We performed single sensillum recordings from the Drosophila antenna simultaneously with measurements of
the odorant concentration reaching the fly. We found that stimulus dynamics depend on odor type before any
interaction with the olfactory system occurs. Individual ORNs follow these differences in stimulus dynamics with
such precision that a linear-nonlinear model can predict the response of one ORN to different odorants solely from
measurements of the stimulus. This finding suggests that odorant structure might not always be the sole source
of information regarding odor identity and that stimulus dynamics also carries information about the identity of
an odorant. Importantly we found that within the sensitivity range of a single ORN the degree and time-scale of
adaptation are independent of stimulus intensity and background. As a consequence, ORN response dynamics
are remarkably similar across a large range of stimulus intensities. Hence, an individual ORN can independently
capture odor identity and intensity in the dynamics and magnitude of its response. The decomposition of odor
identity and intensity has been attributed to chemical structure recognition by a repertoire of ORNs and circuit
processing downstream from the ORNs. Our results suggest a mechanism by which temporal patterns of activity
in ORNs may also contribute to this decomposition by capturing odor-specific stimulus dynamics independently
of the intensity of the signal. Odor discrimination and tracking therefore may rely on features of odorant dynamics
in addition to features of odorant structure.

II-78. Adaptive sharpening of tuning in the auditory system of the cricket
Jan Clemens1,2
Florian Rau1
K. Jannis Hildebrandt3
R. Matthias Hennig1

CLEMENSJAN @ GOOGLEMAIL . COM
FLORIAN . RAU @ BIOLOGIE . HU - BERLIN . DE
J. HILDEBRANDT @ UCL . AC. UK
MATTHIAS . HENNIG @ BIOLOGIE . HU - BERLIN . DE

1 Humboldt

Universität zu Berlin
Center for Computational Neuroscience Berlin
3 University College London
2 Bernstein

Natural environments barrage the animal with complex stimuli. This constitutes a special challenge for sensory
systems, as interference between stimulus components can lead to information loss. To maximize information
transfer interference has to be minimized. In a linear encoder this can be achieved by suppressing the gain or
decorrelating the filters for individual components. The interference problem also pertains to crickets, whose auditory world is divided into a low-frequency channel associated with mating calls and a high-frequency channel
linked to predator signals. This clear behavioral partition is reflected in the simple layout of the auditory system: in
the prothoracic ganglion exist two ascending neurons (AN1, AN2) that are most excited at different carrier frequencies and receive inhibition from a broadly-tuned local neuron (ON1). However, both AN respond to predator and
mate signals at high intensities, leading to potential interference. We recorded responses from all three neurons in
the network while presenting amplitude-modulated stimuli with either single carrier frequencies or with a mixture
of mate and predator carriers. Temporal and spectral tuning was quantified by estimating linear-nonlinear models
consisting of a filter and a nonlinearity. Coding properties changed adaptively to minimize interference: while
tuning for carrier frequency was relatively broad for single-carrier stimuli, the system’s tuning was much sharper
when confronted with multiple carriers due to “side-band” suppression. Additionally, changes in filter shape led

COSYNE 2012

143

II-79 – II-80
to a decorrelation of the temporal selectivity for song- and predator-specific sounds. An abstract network model
reveals that this adaptive coding can arise through the logarithmic nonlinearity inherent in the dB transformation
of the auditory system. Untuned feed-forward inhibition further sharpens tuning through an “iceberg effect” and
decorrelates the filters. Logarithmic encoding of intensities and broadly-tuned inhibition are mechanisms also at
work in cortex and constitute thus general mechanisms underlying the efficient representation of complex stimuli.

II-79. Performing noise reduction using realistic spectro-temporal receptive
fields as modulation filters
Tyler Lee
R Channing Moore
Frederic Theunissen

TYLERLEE @ BERKELEY. EDU
CHANNING MOORE @ BERKELEY. EDU
THEUNISSEN @ BERKELEY. EDU

University of California, Berkeley
Natural auditory scenes are often composed of a multitude of competing sounds from distinct sound sources
that the brain must parse in order to extract relevant information. Despite decades of research into the so-called
cocktail party problem, it remains unclear how the brain isolates sounds from individual sound sources. Drawing
on recent work in the avian auditory cortex, we have devised a model to perform noise reduction in such complex
auditory scenes. Studies characterizing the spectro-temporal receptive fields (STRFs) of neurons in primary and
secondary avian auditory cortex have discovered tuning to an array of spectro-temporal modulations. Consequently, it has been shown that the ensemble modulation tuning of these neurons reflects the spectro-temporal
modulations present in bird song. Our model exploits this ensemble tuning by using model neurons, described by
their STRFs, as modulation filters, selectively responding to the presence of modulations corresponding to bird
song within a complex noisy stimulus. The STRFs used by the model can be taken from actual neurons, or they
can be learned using an iterative sparse coding algorithm. By projecting the noisy stimulus onto a population of
STRFs we can obtain a “neural response”. The set of responses provides a time-varying readout of the presence
of song in the noisy stimulus, and since we know the frequency response properties of each STRF, the responses
can be used to isolate song in time-frequency space. We thus utilize these filters to generate time-frequency
gains that are then applied to the noisy stimulus to produce an estimated reconstruction of the clean song. This
model performs at least as well as other state-of-the-art methods for noise reduction. This model also performs
well on speech stimuli, successfully separating a target speech stimulus from competing background noise, even
when the background noise is composed of competing speech or crowd noise.

II-80. Encoding of ultra-sonic vocalizations in the rodent primary auditory cortex
Isaac Carruthers
Ryan Natan
Maria Geffen

CIS @ SAS . UPENN . EDU
RNATAN @ MAIL . MED. UPENN . EDU
MGEFFEN @ MAIL . MED. UPENN . EDU

University of Pennsylvania
Rats produce complex vocalizations in communicating with each other. Over 10 types of distinct ultra- sonic calls
can be distinguished in their repertoire. Neurons in the primary auditory cortex respond selectively to con-specific
vocalizations. However, the precise mechanisms of how complex vocalizations are encoded in the auditory pathway are not well understood. To learn how the auditory cortex encoded information about rat vocalizations, we
presented a library of recorded and purified vocalizations to awake rodents, recorded neural activity in the auditory cortex and constructed a mathematical model that allowed to predict A1 responses to novel vocalizations.
To generate a library of acoustic stimuli, we first constructed a sparse representation of the recorded vocalizations. Each vocalization was modeled as an amplitude- and frequency- modulated tone (akin to a whistle). A1

144

COSYNE 2012

II-81 – II-82
responses were selective for a small fraction of vocalizations. To examine temporal invariance in responses of
A1 neurons, these vocalizations were slowed down (two-fold) and accelerated (two-fold). A1 neurons exhibited
limited temporal invariance in their responses. To measure the temporal parameters of neuronal responses, we
next presented a long USV-based stimulus, consisting of 350 purified vocalizations, concatenated into a long stimulus sequence. A1 neurons exhibited reliable and specific responses to this sequence. We used the responses
of populations of neurons to cluster the vocalizations into 10 categories. Furthermore, we used a generalized
linear-non-linear model to predict A1 responses. The GLNM was modified from standard methods, as the stimulus was represented in a sparse fashion, through amplitude and frequency modulation variables, rather than a
spectrogram. The GLNM, trained on the first 200 vocalizations, gave accurate predictions for neuronal responses
to the remaining 150 vocalizations (correlation coefficients up to .78). Our study shows that neurons in the primary
auditory cortex exhibit high temporal precision and selectivity in encoding ultra-sonic vocalizations.

II-81. Symmetry in the neural representation of visual motion
Alfred Kaye1,2
James H Marshel1
Edward Callaway1
Tatyana Sharpee1
1 Salk

AKAYE @ UCSD. EDU
JMARSHEL @ UCSD. EDU
CALLAWAY @ SALK . EDU
SHARPEE @ SALK . EDU

Institute for Biological Studies
of California, San Diego

2 University

The distribution of angular motion in natural scenes has an overrepresentation of horizontal and, to a lesser
extent, vertical movement. The mirror symmetries in natural optic flow distributions constrain the class of direction
selective (DS) functions which can maximize the information between the direction of motion and the neural
response. In this work, we develop a theory of optimal DS response for single neurons and for pairs of neurons,
and compare predictions with responses observed in DS neurons in the mouse LGN. Natural scene optic flow
statistics are compared with the responses of DS neurons in the mouse LGN as revealed by in vivo two photon
calcium imaging to determine whether those neurons are optimal for encoding information alone or as pairs of
neurons.

II-82. The omitted stimulus response originates in ON bipolar cells
Nikhil Deshmukh1
Frederick Soo1
Gregory Schwartz2
Michael Berry1
1 Princeton
2 University

NDESHMUK @ PRINCETON . EDU
FSOO @ PRINCETON . EDU
GREGWS @ UW. EDU
BERRY @ PRINCETON . EDU

University
of Washington, Seattle

Previous work showed that retinal ganglion cells respond robustly to violations of periodic stimulus patterns with
an extra burst of action potentials (Schwartz et al., Nat Neurosci. 2007). The latency of this burst increases as
the period of the flash sequence is increased, such that the peak firing rate coincides with the time of the next
expected flash. This phenomenon is called the omitted stimulus response (OSR), and is one example of complex
pattern detection in the retina (Schwartz et al., J. Neurophysiol. 2008). Several models have been proposed to
explain the origin of the OSR. One class of biophysical models proposes a resonant element in the inner retinal
circuit that continues to oscillate after the stimulus ends (Gao et al., Network 2009). A second hypothesis is that
the OSR is caused by the changes in mean light level that interact with the standard biphasic temporal tuning of
ganglion cells (Werner et al., J. Neurophysiol. 2008). To distinguish among these and other possibilities, voltageclamp recording of retinal ganglion cells were performed while presenting full-field flash sequences with periods

COSYNE 2012

145

II-83 – II-84
ranging from 8-20 Hz. The latency of the input currents to ganglion cells shifted linearly with the period of the flash
sequences, suggesting that the OSR originates upstream of ganglion cells. Whole-cell voltage clamp recordings
of bipolar cells from retinal slices show that ON bipolar cells, but not OFF bipolar cells, have an OSR in their input
currents. Additionally, the OSR is not present in suction electrode recordings of currents in cone outer segments.
Finally, the OSR is abolished by the bath application of APB, which specifically blocks the mGluR6 receptor found
in ON bipolar cells. These results suggest that the OSR originates in the dendrites of ON bipolar cells, possibly
due to the dynamics of the mGluR6 second messenger cascade.

II-83. Saccade-confounded image statistics explain visual crowding
Bosco Tjan1
Anirvan S Nandy2

BTJAN @ USC. EDU
NANDY @ SALK . EDU

1 University
2 Salk

of Southern California
Institute for Biological Studies

Processing of shape information in human peripheral visual fields is impeded beyond what can be expected by
poorer spatial resolution. Visual crowding—the inability to identify objects in clutter—has been shown to be the
primary factor limiting shape perception in peripheral vision. Despite the well documented effects of crowding, its
underlying causes are poorly understood. Since spatial attention both facilitates learning of image statistics and
directs saccadic eye movements, we propose that the acquisition of image statistics in peripheral visual fields is
confounded by eye-movement artifacts. Specifically, the image statistics acquired under a peripherally deployed
spotlight of attention is systematically biased by saccade-induced image displacements. These erroneously represented image statistics lead to inappropriate contextual interactions in the periphery and cause crowding. We
simulated the acquisition of joint orientation statistics from attended image regions within a spatial extent defined
by the lateral connections of a V1 neuron. The stimulations were conducted over a range of retinal eccentricities
and allowed for a non-zero temporal overlap between the trailing edge of attention and the onset of saccades.
We show that orientation statistics acquired under such conditions misrepresent the underlying image statistics
in two major aspects: (a) whereas the true image statistics dictate a co-circular pattern of connectivity (Sigman et
al., 2001), the connectivity pattern in the periphery would have a preference for iso-orientation; (b) the extent of
the mispresented statistics have a strong radial bias reflecting the fact that saccades are radial eye movements
connecting the fovea to the attended peripheral locations. With a single free parameter (temporal overlap between
saccade and attention), our results provide a simple quantitative explanation for the characteristic shape of the
spatial extent of crowding (Bouma, 1970; Toet & Levi, 1992) and suggest a root cause of form-vision deficits in
peripheral vision.

II-84. Neurons in macaque area CIP respect the geometric topology of 3D
object orientation
Ari Rosenberg1
Noah Cowan2
Dora Angelaki3

ROSENBERG @ PCG . WUSTL . EDU
NCOWAN @ JHU. EDU
ANGELAKI @ CABERNET. WUSTL . EDU

1 Washington

University
Hopkins University
3 Baylor College of Medicine
2 Johns

Interacting with objects in 3D space, for instance grasping them, often requires determining their spatial orientation. Here we investigate the visual encoding of 3D object orientation. An object’s spatial orientation generally
has three degrees of freedom represented by the special orthogonal matrices. Within this space, neuronal tuning
curves for 3D object orientation are constrained by symmetries and perceptual singularities that depend on the

146

COSYNE 2012

II-85
physical structure of the viewed object. For instance, the 3D orientation of a plane can be represented by its unit
normal, which lies on an antipodally symmetric sphere. Planar orientation therefore has two degrees of freedom,
which we parameterize as tilt (rotations about the line-of-sight) and slant (rotations in depth). When the plane’s
normal is perpendicular to the line-of-sight, the plane self-occludes, creating visibility constraints that eliminate
the sphere’s equator. This geometric topology introduces theoretical constraints on neuronal tuning curves and
suggests a metric for measuring perceptual distance between planes of different tilt-slants. To investigate if neuronal responses conform to these constraints, tuning curves were measured extracellularly from single neurons in
the caudal intraparietal area (CIP) of macaque monkeys. The stimuli consisted of checkerboard planes rendered
with monocular texture and binocular disparity cues. Many neurons were tuned for 3D surface orientation and,
as expected from the modeling, an antipodally symmetric spherical function fit the data well. Tilt and slant were
uniformly represented over the population. We also investigated the multiplicative separability of joint tilt-slant
tuning curves (i.e., whether they are defined by the outer-product of independent tilt and slant tuning curves).
Even though the geometry imposes strong constraints on this property, tilt-slant tuning was separable for most
neurons. Our results suggest that the representation of 3D object orientation in area CIP respects the topological
structure inherent in describing the visible configurations of objects.

II-85. Reconstruction of the connectome of the fruit fly visual system
Shin-ya Takemura1
Shiv Vitaladevuni1
Richard Fetter1
Zhiyuan Lu1
Stephen Plaza1
Arjun Bharioke1,2
Lou Scheffer1
Ian Meinertzhagen3
Dmitri Chklovskii1

TAKEMURAS @ JANELIA . HHMI . ORG
VITALADEVUNIS @ JANELIA . HHMI . ORG
FETTERR @ JANELIA . HHMI . ORG
LUZ @ JANELIA . HHMI . ORG
PLAZAS @ JANELIA . HHMI . ORG
BHARIOKE @ GMAIL . COM
SCHEFFERL @ JANELIA . HHMI . ORG
IAM @ DAL . CA
CHKLOVSKIID @ JANELIA . HHMI . ORG

1 HHMI,

Janelia Farm Research Campus
of Cambridge
3 Dalhousie University
2 University

In 1950s, Hassenstein and Reichardt proposed an elementary motion detector (EMD) that computes correlations
between visual signals offset in time and space (Figure 1A). Although most following experimental results (Borst,
2010) were largely consistent with the original proposal, there has never been a “smoking gun” that identifies
particular neurons with the specific computations within the EMD. In fruit fly (Figure 1B), the EMD circuit is thought
to be in the medulla (Figure 1C) because the upstream lamina does not contain directionally selective cells while
the downstream lobula plate contains cells, such as H1, which integrate motion over visual field. The medulla
was previously called an “impenetrable jungle” because of its complicated structure. To help uncover the neuronal
EMD we pursued a reconstruction of the medulla using electron microscopy. Such reconstruction is made easier
by the small size of the fly, stereotypic brain geometry from fly to fly, and the repeating columnar structure of
the medulla. We imaged 90x90x90um3 volume and obtained a dataset of ∼1012 3x3x40nm3 voxels. Next, we
developed a semi-automated pipeline to assign synapses to individual neurons by tracing neuronal processes
through this volume. The reconstructed circuit of 48 columnar neurons contains thousands of synapses. We
identified individual neurons by matching their shape (Figure 1D) to previously reported light microscopy images
of Golgi impregnations (Fischbach and Dittrich, 1989) and newly obtained GAL4-UAS lines (Pfeiffer et al. 2008,
Nern & Rubin, 2011). These lines will allow genetic access to neurons of specific classes to determine their
repertoire of neurotransmitters and synaptic receptors, manipulate their physiological properties, and monitor
their activity. The emerging connectome (Figure 2), along with future physiological investigations, should be
sufficient to conclusively resolve the nature of EMD mechanism in fruit fly, as well as provide insight into the other
computations of the visual system.

COSYNE 2012

147

II-86 – II-87

II-86. Developmental regulation of sensory processing by spontaneous cortical activity
Matthew Colonnese

COLONNESE @ GWU. EDU

The George Washington University
Spontaneous activity generated intrinsically in thalamocortex utilizes the vast majority of the brain’s energy, but
its function is poorly understood. Here we present evidence that the developmental initiation of spontaneous
activity regulates sensory processing and plasticity. During development, sensory cortex transitions from an
early period of relative sensory isolation, when the primary input is spontaneous bursting at the periphery, to
a later period of continuous sensory input and active exploration. In humans this transition occurs at birth; in
altricial mammals it occurs post-natally, for example at eye-opening in the visual system. In all sensory systems
this transition is associated with a profound and rapid increase in spontaneous activity that matures sensory
responses by eliminating immature bursts of thalamocortical network oscillations and replacing them with graded
sensory responses. We are investigating the network mechanisms and function of this switch in pre-term infants
and rodent models. Whole-cell patch clamp recordings in head fixed, but unanesthetized, rats show that the
increased spontaneous activity is driven by changes in sub-threshold membrane dynamics consistent with the
initiation of active states in the cortical network. This hypothesis is supported by a dependence on ascending mid
or hind brain connections and norepinephrine, but not the presence of the eye. Surprisingly, degraded sensory
experience (eye-closure) that induces regressive effects on receptive field formation, also modulates the nascent
active states causing hyperexcitability and neuronal synchrony in the form of aberrant alpha-beta oscillations
that modify spike-timing dynamics. We suggest that the regulation of cortical network dynamics contributes to
circuit development and plasticity by dividing early development into two periods: an early period concurrent with
spontaneous activity at the periphery that guides initial circuit formation in the absence of spontaneous cortical
activity, and a late period when spontaneous intra-cortical activity permits sensory exploration and the onset of
experience-dependent plasticity.

II-87. The combined micro-organization of orientation and spatial frequency
tuning in primate V1
Ian Nauhaus
Kristina Nielsen
Anita Disney
Edward Callaway

INAUHAUS @ SALK . EDU
KJNIELSEN @ GMAIL . COM
ANITA @ SALK . EDU
CALLAWAY @ SALK . EDU

Salk Institute for Biological Studies
Spatial frequency tuning is a highly salient property of neurons in primate V1 and has been well-characterized.
However, the organization of spatial frequency across the V1 surface is far less understood than that of other
properties, such as pinwheels of orientation preference. Discrepancy among previous attempts at characterizing
spatial frequency organization may be due to insufficient imaging resolution (review: Purushothaman et al 2009).
In this study, we use two-photon imaging to provide the first data demonstrating the cell-by-cell layout of spatial frequency maps in primate V1. We first show that similar to orientation, spatial frequency is organized continuously.
Next, we used these data to ask how spatial frequency and orientation are jointly represented in V1. In general,
the manner in which different functional maps are overlayed has important implications for the efficiency of local
populations in representing visual scenery. The combined organization of orientation and spatial frequency tuning
is of particularly interest, as any image can be represented by the combination of these two parameters. For efficient coding of both orientation and spatial frequency, we expect that a given region of retinotopic space should
represent all combinations (within range) of the two parameters. Our results show that orientation and spatial
frequency maps are indeed systematically organized at the fine spatial scale observed with two-photon imaging.
We find that not only does spatial frequency change more rapidly when orientation is more constant, but also that
the map gradients tend to be orthogonal within a 200 um region of interest . This organization may allow local

148

COSYNE 2012

II-88 – II-89
populations to encode a more complete representation of their dedicated visual space.

II-88. Effects of local orientation on large-scale representations in V1 bias
perceived global shape
Melchi Michel
Yuzhi Chen
Wilson S Geisler
Eyal Seidemann

MELCHI @ MAIL . CPS . UTEXAS . EDU
CHEN @ MAIL . CPS . UTEXAS . EDU
GEISLER @ PSY. UTEXAS . EDU
EYAL @ MAIL . CPS . UTEXAS . EDU

University of Texas at Austin
Population responses to visual stimuli in the primate primary visual cortex are topographically organized at multiple
spatial scales, each of which may be useful for a subset of visual judgments. At a fine scale, these responses
are organized into orientation columns; signals at this scale might be useful for discriminating between different
textures. At a larger (retinotopic) scale, the population responses seem to encode the contrast envelope of
visual stimuli; signals at this scale might be useful for computing global shape. Are responses at this larger
scale independent of the local orientation structure of the stimulus? To answer this question, we used voltagesensitive dye imaging in fixating monkeys to measure V1 population responses to small isolated Gabor patches
with a fixed contrast envelope and varying carrier orientations. We found that V1 response at the retinotopic
scale is significantly elongated along the direction corresponding to the orientation of the carrier. Moreover,
increasing the carrier frequency reduces this effect. Both of these results can be explained by an elongation of
the V1 population receptive field along the preferred orientation, similar to findings in single V1 neurons. If we
rely on these retinotopic-scale V1 population responses when making judgments about global shape, the results
above suggest that local orientation might bias these judgments. We tested this prediction in a psychophysical
task by having human subjects determine which of two small patches was more circular. Consistent with the
physiological results, we found that human subjects perceive Gabor patches to be elongated along the orientation
of the sinusoidal carrier, and that increasing the carrier frequency reduces this effect. Taken together, these results
suggest that we make use of information from the retinotopic scale of V1 population responses when determining
shape, and that for small stimuli, local orientation information can bias our perception of global shape.

II-89. Position-specific heterogeneity of orientation pooling in curvature-tuned
neurons of macaque area V4
Anirvan S Nandy
Tatyana Sharpee
John Reynolds
Jude Mitchell

NANDY @ SALK . EDU
SHARPEE @ SALK . EDU
REYNOLDS @ SALK . EDU
JUDE @ SALK . EDU

Salk Institute for Biological Studies
Previous studies (Pasupathy & Connor, 1999; 2001) have shown that neurons in monkey visual area V4 are
involved in the processing of shapes of intermediate complexity and are sensitive to curvature. These studies
also suggest that curvature tuned neurons are position-invariant. We sought to examine the detailed mechanisms
that endow V4 neurons with their shape-selective properties. We recorded responses of area V4 neurons in
two male macaques to bars or curves. Bars were presented at 8 orientations. Curves were composed of 3
bars, linked together to yield varying degrees of curvature. Stimuli were presented in a fast reverse correlation
sequence (30 per second) at various locations within the receptive field (RF) of peripheral V4 neurons, while
the animals maintained fixation for 4s. The curved stimuli were presented on a 5x5 location grid centered on
the RF, while the single oriented bars were presented on a finer 15x15 location grid. Consistent with previous
studies, we found that response rank-order to the most- and least-preferred stimuli were preserved througout the

COSYNE 2012

149

II-90 – II-91
RF. However, a fine-grained analysis of curvature tuning revealed a surprising result: curvature-tuned V4 neurons
exhibit considerable spatial variation in their curvature preference. At a finer scale, such neurons exhibited local
variation in orientation tuning. In contrast, neurons that preferred straight, rather than curved contours, exhibited
spatially invariant orientation tuning, and correspondingly homogenous fine-scale orientation tuning maps. Both
these patterns are consistent with a simple model in which orientation tuning is pooled, with tuning for straight or
curved contours resulting, depending on the heterogeneity of patterns of orientation tuning inherited from V1.

II-90. Inhibition controls the spatiotemporal spread of responses in awake
visual cortex
Bilal Haider
Michael Häusser
Matteo Carandini

B . HAIDER @ UCL . AC. UK
M . HAUSSER @ UCL . AC. UK
MATTEO @ CARANDINILAB . NET

University College London
The role of inhibition in shaping cortical activity is a topic of considerable debate. Numerous studies indicate
that excitation and inhibition are balanced in magnitude and time-course during both spontaneous and sensory
evoked activity. However, these studies have been performed in anesthetized animals; it is unknown how sensory
responses are shaped by inhibition during wakefulness. We measured excitation and inhibition with whole-cell
recordings in layer 2/3 of visual cortex in awake and urethane anesthetized mice. We recorded membrane potential responses in current clamp, and measured synaptic conductances in voltage clamp while pharmacologically
blocking intrinsic conductances. An additional electrode measured population activity via the local field potential
(LFP). Visual stimuli probed selectivity for spatial position (white and black bars, flashed for 100 ms in random
positions). Awake and anesthetized responses differed dramatically. During anesthesia, LFP responses were
evoked from large portions of visual space, and lasted hundreds of milliseconds after the stimulus. During wakefulness, in contrast, LFP responses to these same stimuli were localized in space and time. Current clamp
recordings showed equally short-lasting membrane potential responses in awake versus anesthetized animals.
Voltage clamp recordings revealed that inhibition during wakefulness is much larger than excitation and substantially less tuned for spatial position. It makes responses more transient by truncating the effects of excitation.
Accordingly, fewer spikes were produced by stimuli during wakefulness, even though the membrane potential was
∼3 mV more depolarized than during anesthesia. We conclude that during wakefulness, excitation and inhibition
are not strictly balanced in visual cortex. Rather, inhibition exerts a more dominant influence that constrains the
patterns of activity elicited in visual cortex during wakefulness.

II-91. Selectivity and invariance are greater in macaque V2 than V1
Corey M Ziemba1
Jeremy Freeman1
Tony Movshon1
Eero P Simoncelli1,2
1 New

ZIEMBA @ CNS . NYU. EDU
FREEMAN @ CNS . NYU. EDU
MOVSHON @ NYU. EDU
EERO. SIMONCELLI @ NYU. EDU

York University

2 HHMI

As visual information passes through the visual hierarchy, neuronal responses are thought to become more selective to complex image features, and more invariant to identity-preserving image transformations. This transition
has been observed for neural populations in areas V4 and IT (Rust and DiCarlo, 2010), although the precise
nature of representations in these areas remains elusive. We examined this hypothesis in areas V1 and V2,
using stimuli generated with a model of naturalistic image structure that has been linked to visual representation
in V2 (Freeman and Simoncelli, 2011). We generated novel stimuli with model responses matched to those of an

150

COSYNE 2012

II-92
original natural texture image. These images are perceptually similar, despite differences in their local features.
We assessed how accurately the identity of such images can be estimated from neural populations by recording
sequential single-unit responses in anesthetized macaque and training linear decoders on the combined population. To test for selectivity, we asked whether neural populations discriminated better among images generated
from the model than among spectrally-matched images which lack the complex features captured by the model.
Identification accuracy in V1 was indistinguishable for the two types of image, whereas V2 accuracy was 20%
better for the model-generated images. Thus the V2 population is more selective for the features captured by the
model than the V1 population. To test for invariance, we measured discrimination performance when the linear
decoder was forced to generalize across multiple distinct images matched according to the model. The performance decrement was 50% larger for V1 than V2, indicating that the representation in V2 is more invariant to
those features discarded by the model than the representation in V1. Our results demonstrate increases in both
selectivity and invariance from V1 to V2, and explicitly link the features of our model to population responses in
area V2.

II-92. Understanding V1 surround modulation with natural stimuli using a
principled statistical model
Ruben Coen-Cagli
Adam Kohn
Odelia Schwartz

RUBEN . COENCAGLI @ EINSTEIN . YU. EDU
ADAM . KOHN @ EINSTEIN . YU. EDU
ODELIA . SCHWARTZ @ EINSTEIN . YU. EDU

Albert Einstein College of Medicine
The response properties of neurons in primary visual cortex (V1) are usually measured with simple stimuli such as
gratings, but this characterization has limited explanatory power under natural stimulation. Here we address the
properties of V1 surround modulation recruited by naturalistic input. We show that a principled model of scenes
that captures statistical dependencies between oriented filters explains neuronal responses better than energy
models and canonical divisive normalization. Our model is a Mixture of Gaussian Scale Mixtures (MGSM; CoenCagli, Dayan, Schwartz, NIPS 2009) that characterizes linear and nonlinear dependencies between V1-like filters
in natural images. Bayesian inference in the model generalizes divisive normalization: given an input, the model
infers whether filter activations in center and surround locations are statistically coordinated (homogeneous) and
therefore to be jointly divisively normalized. While most scene statistics models have been compared against
neural responses to gratings, a stronger test is to assess how well they explain responses to natural inputs. We
therefore measured surround modulation in macaque V1 with static natural images. The majority of cells were
more strongly suppressed by images that the model classified as statistically homogeneous, than heterogeneous.
This was not due to differences in the firing rates evoked by the stimuli confined to the receptive field, nor to the
amount of energy in the surround, between the two classes of images. To compare to previous models, we then fit
the MGSM and several descriptive models to the neuronal responses. Both the MGSM and divisive normalization
with independent center and surround gains explained 50% of the variance, but the observed surround modulation
correlated with the MGSM twice as well as with any other model. Our results support the idea that surround effects
can be viewed as sensitivity in V1 to the inferred statistical homogeneity of visual input. Supported by the NIH
(CRCNS-EY021371).

COSYNE 2012

151

II-93 – II-94

II-93. Thalamic Synchrony and Visual Orientation Information Transmission
To Cortex
Sean Kelly1,2
Jianzhong Jin3
Yushi Wang3
Qi Wang1
Michael Black4
Jose-Manuel Alonso3
Garrett Stanley1

SKELLY 32@ GATECH . EDU
JJIN @ MAIL . SUNYOPT. EDU
YUSHIWANG @ SUNYOPT. EDU
QI . WANG @ BME . GATECH . EDU
BLACK @ TUEBINGEN . MPG . DE
JALONSO @ SUNYOPT. EDU
GARRETT. STANLEY @ BME . GATECH . EDU

1 Georgia

Institute of Technology
University
3 State University of New York
4 Max Planck Institute for Intelligent Systems
2 Emory

Thalamic synchrony is highly effective at driving cortical spiking (Alonso et al., 1996; Usrey et al., 2000), and is
an important part of the neural code that facilitates information transmission from the visual thalamus to cortex
(Wang et al., 2010). Given that thalamic neurons with overlapping receptive fields are likely to converge at common cortical targets (Alonso et al., 1996; Reid and Alonso, 1995), a potential role for the synchronous activity
of thalamic input in the establishment of cortical response properties emerges. We have found previously that
synchronous activity computed across pairs of neurons in the LGN can contain surprisingly sharp orientation tuning, but precisely how this relationship is modulated across the larger population and regulates cortical firing is
unknown. Here, we investigated this phenomenon using a leaky integrate and fire model driven by thalamic population activity whose synchrony we systematically controlled. At all levels of input synchrony the cortical response
showed well-tuned responses of both membrane potential and firing rate, and exhibited similar sharpening of
the orientation tuning in the cortical firing rate relative to the cortical membrane potential. We further found that
the orientation tuning for cortical firing rate sharpened with increasing levels of input synchrony while orientation
tuning for the cortical membrane potential was relatively invariant. Also, thalamic synchrony had a complex effect
on cortical firing statistics. Using a Fisher Information metric to estimate decoder performance, we found that
information in the cortical membrane potential about stimulus orientation was relatively invariant to the thalamic
synchrony. Information in the cortical firing rate, however, increased with increasing synchrony until reaching a
plateau at a thalamic synchrony level of approximately 12 ms, consistent with levels of timing precision observed
in LGN activity in the natural environment (Butts et al., 2007; Desbordes et al., 2010).

II-94. Differences in sensitivity to neural timing among cortical areas
Yang Yang1,2
Anthony Zador2

YANGYANG @ ION . AC. CN
ZADOR @ CSHL . EDU

1 Institute
2 Cold

of Neuroscience, Shanghai
Spring Harbor Laboratory

The basic circuitry of auditory, visual, somatosensory and other cortical areas is highly stereotyped. However, it
remains unclear whether this anatomical stereotypy implies functional homogeneity, or whether instead different
cortical areas are specialized to process the diverse sensory inputs they receive. Here we have used a two alternative choice task to assess modality-specific differences in the ability of rats to exploit cortical spike timing.
To isolate differences due to cortical circuitry rather than to sensory transduction and subcortical processing, we
used electrical stimulation to drive activity in the cortex. We previously showed (Cosyne 2010) that the minimum
detectable interstimulus interval varied over more than an order of magnitude, ranging from 1ms in barrel cortex
and 3 ms in auditory cortex to 15 ms in visual cortex. Here we extend these results by asking how these differences arise. The temporal statistics of auditory, visual and somatosensory stimuli may be quite different, raising
the possibility that the differential processing might arise as an experience-dependent adaptation to the different

152

COSYNE 2012

II-95 – II-96
stimulus ensembles. We therefore tested whether these differences are innate, or whether disruption of sensory
stimulus statistics during development could affect the ability to exploit fine timing. We focused on the barrel cortex, reasoning that because the barrel cortex-implanted subjects achieved the best performance, this area might
be the most sensitive to disruption. We did complete unilateral whisker trimming from birth to adulthood to disrupt
barrel cortex development. We found that animals subjected to whisker clipping initially showed impairment in
exploiting fine timing in barrel cortex. Surprisingly, behavioral training partially rescued this deficit. Our results
suggest that different cortical areas are adapted to the specific structure of the input signals they process, and
that precise spike timing may play a more important role for some cortical areas than for others.

II-95. Artificial synchronization across sensory cortical area is sufficient for
behavioral discrimination
Hachi Manzur1
Joel Alvarez2
Cecilia Babul2
Pedro Maldonado2
1 National

HEMANZUR @ NIH . GOV
JALVAREZRUF @ YAHOO. COM
CECI @ NEURO. MED. UCHILE . CL
PEDRO @ NEURO. MED. UCHILE . CL

Institue on Aging
de Chile

2 Universidad

Direct electrical microstimulation has been used to explore functional anatomy and to determine the participation of a particular cortical locus in sensory and motor coding. In these experiments, microstimulation has been
typically delivered to one electrode at a time, producing a local increase in neuronal rate as well as in neuronal
synchrony. Here we used electrical microstimulation of the visual and somatosensory cortices of the rat in both
hemispheres, to test whether distributed patterns of artificial synchrony evoke a coherent brain activation that
animals can signal by pressing different levers. To disambiguate synchrony from other related parameters, our
experiments independently manipulated the rate and intensity of stimulation, the spatial locations of stimulation,
the exact temporal sequence of stimulation patterns, and the degree of synchrony across stimulation sites. We
found that rats reliably distinguished between two microstimulation patterns, differing only in the topography of
synchrony among the electrodes. Also, their performance was proportional to the level of synchrony in the microstimulation patterns. We demonstrated that rats can recognize artificial current patterns containing precise
synchronization features, thus providing the first direct evidence that artificial synchronous activity can guide behavior. These precise temporal information that the animals were able to discriminate can be used as feedback
signals in machine interface arrangements.

II-96. Transient activation of distinct striatal pathways mimics changes in the
value of actions
Lung-Hao Tai1
A. Moses Lee1
Antonello Bonci2
Linda Wilbrecht1
1 University

LTAI @ GALLO. UCSF. EDU
MLEE @ GALLO. UCSF. EDU
ANTONELLO. BONCI @ NIH . GOV
LWILBRECHT @ GALLO. UCSF. EDU

of California, San Francisco

2 NIH/NIDA

In constantly changing environments, animals must adaptively select actions to achieve their goals. Recently,
the striatum has been implicated in goal-directed action selection and striatal neural activity has been shown to
represent the value of competing actions. This activity could generate a response bias toward an action of higher
value. While this model is consistent with numerous studies, no study to date has demonstrated the direct impact
of distinct striatal pathways on choice behavior in the context of a reward-based decision. Here we show that

COSYNE 2012

153

II-97 – II-98
unilateral transient optogenetic stimulation (500ms, 5∼20Hz) of the striatal direct or indirect pathway introduces
opposing biases in the distribution of choices during a probabilistic switching task. Stimulation of dopamine D1or D2- receptor expressing neurons biased choice but the effect of stimulation was dependent on recent choice
and reward history. The behavioral bias introduced by stimulation is consistent with theoretical predictions of an
additive change in the value for a given action and suggests the presence of a “winner-take-all” system within
or downstream of the basal ganglia for action selection. Together this data supports a model in which striatal
neurons pool a wide variety of cortical and thalamic inputs and convert them into a common currency of action
value to bias choice.

II-97. Short-axon Cells Provide Both Excitatory and Inhibitory Drive to the
Mitral/Tufted Cells
Arkarup Bandyopadhyay
Fred Marbach
Matthew Koh
Dinu F Albeanu

ABANDYOP @ CSHL . EDU
FMARBACH @ CSHL . EDU
MKOH @ CSHL . EDU
ALBEANU @ CSHL . EDU

Cold Spring Harbor Laboratory
Lateral interactions are thought to shape the olfactory bulb output (mitral/tufted—M/T cell firing rate), yet their
roles and extent remain largely unknown. Short-axon (SA) cells in the glomerular layer, receive inputs from olfactory sensory neurons and/or external tufted (ET) cells and release both GABA and Dopamine, synapsing onto
juxtaglomerular cells as far as tens of glomeruli away (Kiyokage et al., 2010). Computational models (Cleland et
al, 2007) have suggested that SA cells may be involved in long-range normalization of bulb outputs, but to date
their function in the intact brain has not been investigated. We used a Cre-loxP viral approach to express neuronal activity reporters (GCAMP3) or light gated switches (Channelrhodopsin2 and Halorhodopsin) in SA cells.
We imaged GCAMP3 responses to a wide range of odor concentrations via wide-field microscopy. Odors induced
transient, yet global SA responses, in contrast to focal glomerular patterns observed via intrinsic optical imaging (6 odors, 5 concentrations, 4 mice). To understand the effect of the SA network activity on the bulb output,
we recorded from M/T cells in anaesthetized mice using extracellular tetrodes. In conjunction, we selectively
activated/inactivated SA cells by shining blue/yellow light either throughout the dorsal bulb surface or in specific
spatial patterns using a digital micro-mirror device (DMD). Pairing light with odor presentation at various concentrations indicated that SA cells provide both excitatory and inhibitory drive to the M/T cells (n=11). At low odorant
concentrations excitatory input dominates, whereas in response to stronger stimuli, both excitatory and inhibitory
drives are present. We are currently performing pharmacological manipulations to dissect these dual effects of
SA cells. We propose that the SA network regulates the dynamic range of M/T cell firing by amplifying the weak
inputs and downscaling the stronger ones.

II-98. A cell-type-specific population analysis of optogenetically evoked response in primary visual cortex
Ali Mohebi1
Jessica A. Cardin2
Karim Oweiss1

MOHEBIAL @ MSU. EDU
JESS . CARDIN @ YALE . EDU
KOWEISS @ MSU. EDU

1 Michigan
2 Yale

State University
University

Precisely timed coordination between excitatory and inhibitory neurons within and between layers of the cerebral
cortex is a major element of brain function. Stimulus dynamics are believed to engage this E-I coordination in a
different regime. In this study, we combined optogenetics tools with tetrode recording to correlate the dynamics of

154

COSYNE 2012

II-99
network state transition in response to precisely controlled dynamics of the artificial light stimulus. By expressing
ChR2 in fast spiking interneurons of layer II/III primary visual cortex in PV-Cre mice, we could selectively excite
Fast Spiking (FS) interneurons in this layer by superficial illumination using very short pulses (<1ms) of light
(∼ 470 nm wavelength). Extracellular electrical activity was recorded using tetrode arrays in three mice under
different frequencies of light stimulation. We used Dynamic Bayesian Networks—previously demonstrated to be
more effective in capturing causal influence between simultaneously recorded neurons - to infer the functional
connectivity across sorted units. We found distinct patterns of connectivity corresponding to different frequencies
of stimulation. To examine the consistency of these connectivity patterns, we computed adjacency matrices for
each network graph and examined a reduced dimension feature space representation of these matrices. The
results showed 76%±16% similarity between connectivity patterns across multiple repeated trials, compared to
only 21%±14% similarity across different stimuli. We also examined Local Filed Potentials (LFPs) dynamics as a
proxy to assess distinct levels of population excitability, if any, under different stimulus conditions. We found distinct
patterns of LFP time-locked to light pulse onset at different frequencies. These results suggest that inhibitory drive
has a substantial influence on local network dynamics across multiple time scales. Taken together, our data and
our dynamic Bayesian inference analysis suggest a novel framework to characterize the dynamics of computations
in sensory cortical circuits.

II-99. A Model of I-Wave Generation during Transcranial Magnetic Stimulation
(TMS)
Catalin V Rusu1
Ulf Ziemann2
Jochen Triesch3

RUSU @ CONEURAL . ORG
U. ZIEMANN @ EM . UNI - FRANKFURT. DE
TRIESCH @ FIAS . UNI - FRANKFURT. DE

1 UBB
2 Department
3 Frankfurt

of Neurology, Goethe-University, Frankfurt, Germany
Institute of Advanced Studies

Transcranial magnetic stimulation (TMS) allows to manipulate neural activity non-invasively and much research is
currently dedicated to exploiting this ability in clinical settings. But the details of how TMS induces neural activity
patterns in cortical circuits remain poorly understood, which hampers targeted clinical application. In a standard
TMS paradigm, single-pulse stimulation over the left motor cortex produces high-frequency repetitive responses
of around 600Hz in descending motor pathways called I-waves. Although this paradigm is well-established experimentally and has been studied quite extensively, the detailed mechanisms of I-wave generation have remained
unclear. Here we present a model that reproduces I-waves similar to those observed in epidural responses during in vivo recordings of conscious humans. The model consists of a detailed layer 5 (L5) pyramidal cell and a
population of layer 2 and 3 (L2/3) neurons projecting to it. The model parsimoniously explains the mechanisms
underlying I-wave generation together with some of their basic properties such as frequency, timing, and size. We
argue that I-waves are a product of both extrinsic and intrinsic factors. By depolarizing large populations of L2/3
cells, magnetic stimulation causes a synchronized volley of EPSPs and IPSPs to impinge onto the dendritic trees
of L5 cells. The intrinsic membrane properties of the L5 cells are then responsible for generating trains of action
potentials at the characteristic I-wave frequency. Our model is shown to reproduce the effects of pharmacological interventions with drugs affecting GABA-ergic transmission on I-waves. By incorporating short-term synaptic
depression of synapses from L2/3 onto L5 cells, our model also accounts for facilitation and depression effects
observed in different paired-pulse stimulation protocols. Overall, our model reproduces findings from a range of
experiments and brings us one step closer to designing stimulation protocols for specific clinical purposes.

COSYNE 2012

155

III-1 – III-2

III-1. A model of the effect of visual saliency on ethologically relevant, valuebased decisions
R. Blythe Towal1
Mili Milosavljevic2
Christof Koch1,3

TOWAL @ CALTECH . EDU
MMILOSAV @ HSS . CALTECH . EDU
KOCH . CHRISTOF @ GMAIL . COM

1 California

Institute of Technology
University
3 Allen Institute for Brain Science
2 Stanford

Recent studies have shown that race models accurately describe value-based decisions and that fixation durations
at each alternative are directly related to the alternative’s relative value and are predictive of choices. However,
bottom-up visual saliency also has a large effect on where people look. One recent study has shown that during
two-alternative, value-based decisions, subjects’ decisions are biased towards more salient items. It’s unclear
whether this occurs during multiple-alternative decisions and how information about saliency and value are combined. Here, we develop a model of how saliency and value combine to drive eye-movements and decisions in a
multiple-alternative, forced-choice task. We hypothesize that each fixation location is the outcome of a decision
about the next fixation location. We model this decision process using the drift diffusion model (DDM) assuming
that each alternative has a DDM unit driven by a linear combination of saliency and value. By solving for the first
passage times, we calculate the probability that gaze will transition from one alternative to another and the probability of the transition time. These probabilities define the transition matrix for a Markov chain that determines
the probability of every gaze trajectory. Given a gaze trajectory and its temporal structure, a max-versus-next
decision process determines the conditional probability of deciding on each alternative, which is normalized to
find the chance of choosing each alternative. Based on psychophysical data from ten human subjects performing
a multiple-alternative, forced-choice task involving snack food items we show that this provides an accurate model
of the choice probabilities. We also present a side-by-side comparison of both the predicted fixation locations
and durations and those generated by human observers during this task. We show that combining the DDM with
the traditional saliency model provides an accurate model of fixation durations, fixation locations and value-based
choices.

III-2. Optimal placement of dynamic range by coordinated populations of retinal ganglion cells
David Kastner1
Stephen Baccus1
Tatyana Sharpee2

DKASTNER @ STANFORD. EDU
BACCUS @ STANFORD. EDU
SHARPEE @ SALK . EDU

1 Stanford
2 Salk

University
Institute for Biological Studies

Neurons have a limited dynamic range, and as such must change their sensitivity to encode their current sensory
environment. Previous work has derived the optimal placement of a cell’s dynamic range given an input, and
certain neurons in the early stages of the visual system conform well to that prediction. However, much of the
theoretical effort for understanding the encoding of time varying inputs considers a single neuron. Recent work
has shown that distinct populations of retinal ganglion cells encode similar spatiotemporal features of the visual
input, but have different thresholds–with one population encoding weaker inputs and the other stronger inputs. We
sought to derive theoretically whether this coordinated behavior conforms to an optimal solution to the problem
of representing a sensory input using two distinct neural populations. By modeling neurons with binary outputs,
either spiking or not, placing a constraint on the overall firing rate of the model neurons, and accounting for noise
in the responses, we show that experimentally measured response curves of ganglion cells together optimize the
amount of information the two populations can provide about the distribution of light intensities in the input. This
solution holds across a tenfold range of contrast distributions. Additionally, we show that the coordination that

156

COSYNE 2012

III-3 – III-4
we see within the retina is the optimal solution only if noise is considered, shedding light on the importance of
accounting for variability in neural responses.

III-3. Sparse codes for speech predict spectrotemporal receptive fields in the
Inferior Colliculus
Nicole Carlson1
Vivienne Ming1,2
Michael DeWeese1

N - CARLSON @ BERKELEY. EDU
NEURALTHEORY @ SOCOS . ME
DEWEESE @ BERKELEY. EDU

1 University
2 Socos

of California, Berkeley
LLC

We have developed a sparse representation of speech that minimizes the number of active model neurons needed
to represent typical speech sounds. Motivated by the fact that the number of neurons increases with each stage in
the ascending auditory pathway, we explore overcompleteness in our model to see if it can better capture properties of spectrotemporal receptive fields (STRFs) compared with previous models. When trained on spectrograms
of speech, our overcomplete, sparse coding model learns several well-known acoustic features of speech such
as harmonic stacks, formants, onsets and terminations. However, we also find more exotic structures in the spectrogram representation of sound such as localized checkerboard patterns and frequency-modulated excitatory
subregions flanked by suppressive sidebands. These localized checkerboard patterns are a hallmark of receptive
fields found in the Inferior Colliculus (IC). We also compare additional model STRFs such as broadband and localized ON and OFF cells and Gabor-like patterns to experimental receptive fields found in IC as well as auditory
thalamus and cortex. Additionally, our model neurons exhibit the same tradeoff in spectrotemporal resolution as
has been observed in IC. To our knowledge, this is the first demonstration that receptive fields of neurons in the
ascending mammalian auditory pathway beyond the auditory nerve can be predicted based on coding principles
and the statistical properties of recorded sounds.

III-4. Computing sparse representations using a network of integrate-and-fire
neurons
Tao Hu1
Alexander Genkin2
Dmitri Chklovskii1

HUT @ JANELIA . HHMI . ORG
ALEXGENKIN @ INAME . COM
CHKLOVSKIID @ JANELIA . HHMI . ORG

1 HHMI,
2 AVG

Janelia Farm Research Campus
Consulting

As most neurons in our brain communicate using action potentials, realistic models of neural computation must
contain spiking neurons. For many existing models, however, spikes represent a nuisance: it is difficult for a
spiking model to achieve the same performance as a corresponding graded potential or firing rate model (Vogels
et al., 2005, Deneve and Boerlin, 2011, Shapero et al., 2011). We address this issue in the context of the
sparse representation problem, in which an arbitrary vector must be represented as a linear combination of a few
feature vectors chosen from an over-complete dictionary (Olshausen and Field, 1996, Chen et al., 1998, Baraniuk,
2007). The sparse representation problem can serve as an example of neural computation because many natural
signals can be sparsely represented (Gallant and Vinje, 2000) and because neuronal representations are sparse
(DeWeese et al., 2003, Attwell and Laughlin, 2001, Lennie, 2003). Sparse representations can be computed by
the Local Competitive Algorithm (Rozell et al 2008) on a network of graded potential neurons. Here, we propose a
hybrid distributed algorithm (HDA), which computes sparse representations on the same network architecture as
(Rozell et al., 2008) but using biologically inspired integrate-and-fire neurons instead. We call our algorithm hybrid
because its neurons perform both gradient-descent-like steps on analog internal variables (membrane potentials)

COSYNE 2012

157

III-5 – III-6
and coordinate-descent-like steps via quantized external variables (spikes) communicated to each other. We
prove the convergence of HDA analytically
and show that HDA is stable against time-varying noise, specifically,
√
the representation error decays as 1/ t for Poisson noise. We show numerically that the performance of HDA
is on par with a corresponding network of graded potential neurons. Therefore, we demonstrate for the first time
that, in addition to being energetically efficient (Laughlin et al., 1998), spiking neurons may be as computationally
powerful as graded potential neurons are.

III-5. On the role of cortical feedback on invariant odor perception in the mammalian olfactory system
Gonzalo Otazu
Dinu F Albeanu

OTAZU @ CSHL . EDU
ALBEANU @ CSHL . EDU

Cold Spring Harbor Laboratory
Natural odor scenes are composed of odor plumes originating from multiple sources and traveling at fluctuating
intensities that span several orders of magnitude. Despite this turbulence rodents readily identify odors of interest
against varying, odor-rich backgrounds. This invariant identification poses in fact a challenging computational
problem that appears in many sensory modalities and has been described as the ‘cocktail party problem’. Olfactory bulb models (Koulakov & Rinberg, 2011; Hopfield, 1991) have been proposed that solve this problem
in the absence of strong cortical feedback, a prominent feature of bulb connectivity. We propose that cortical
feedback is essential for invariant representation. We applied a model that relies on cortical feedback, Corrected
Projections Algorithm (CPA) (Otazu & Leibold, 2011) into the olfactory domain. The model identifies odors in
complex scenes by querying the incoming inputs against a large dictionary of learned odor representations and
finds the sparsest combination of odorants that matches best the observed olfactory scene. CPA is unique in that
a sparse representation is found by minimizing the difference between the observed odor signal and the signal
estimate, without explicitly minimizing the number of active dictionary elements. The CPA architecture consists of
a two layer neuronal network with large feedback. This feedback impinges on a separate class of neurons that
form reciprocal inhibitory synapses with the principal neurons in the input layer. Interestingly, this bears striking
resemblance to the mammalian olfactory system anatomy. The model predicts that the cortico-bulbar feedback
onto granule cells represents binary variables encoding the presence or absence of a given odorant, invariant of
concentration, temporal fluctuations or presence of background odors. We are currently testing this prediction by
pairing simultaneous monitoring and optogenetic manipulation of cortical feedback and its bulbar targets in mice
engaged in invariant odor perception tasks.

III-6. Fast estimation of non-smooth non-stationary receptive fields
Eftychios A Pnevmatikakis
Liam Paninski

EFTYCHIOS @ STAT. COLUMBIA . EDU
LIAM @ STAT. COLUMBIA . EDU

Columbia University
State space models have been established as a fundamental tool for statistical analysis of time series, e.g. the
Kalman filter in the linear-Gaussian setting, with many neuroscience applications. However, their applicability is
limited in practice to low-dimensional state spaces, since the computational complexity of the inference algorithms
scales cubically with the state vector dimensionality d. Moreover, although non-Gaussian generalizations are
available, these in general pertain to smooth signals and ignore several important aspects that may be present,
e.g., sparsity. Here we present a general algorithm that addresses these issues to perform tractable inference
in penalized state-space models. Our algorithm is applicable when the state-space dynamics obey log-concave
distributions and the state vector is further penalized by appropriate convex norms. It computes the maximum-aposteriori estimate using Newton’s method. We show that if the number of measurements per timestep is small,

158

COSYNE 2012

III-7 – III-8
then Newton’s direction can be computed with O(d) complexity, using an efficient forward-backward scheme based
on a series of low rank updates. This leads to substantial computational gains and makes the algorithm applicable
to arbitrarily high-dimensional settings. A similar approach has been presented before in the Gaussian setting for
voltage filtering in dendritic trees and smooth dynamic receptive field estimation. Here we extend it to a general
non-smooth setting that can also model sparsity. Sparse penalties can act as regularizers for high-dimensional
inference from a limited number of observations. Moreover, they can lead to interpretable model selection methods
by promoting the inference of rare events. We apply our algorithm for the estimation of piecewise smooth functions
from generalized linear model observations. As one specific example, we derive an effective method for correcting
ocular motion artifacts during visual neuroscience experiments. In this case, the artifacts can be inferred by
imposing sparse variations penalties on the state transitions.

III-7. Improving individual classification learning using a predictive maximum
entropy model
Yarden Cohen
Elad Schneidman

YARDEN . COHEN @ WEIZMANN . AC. IL
ELAD. SCHNEIDMAN @ WEIZMANN . AC. IL

Weizmann Institute of Science
Learning to classify new complex stimuli into categories is a common cognitive task, which we seem to be doing
regularly and almost effortlessly. Yet, the number of potential classification “rules” is exponential in the number
of possible stimuli, which means that when we learn to classify from examples, we must be using priors and
simplifying assumptions. Models of how people learn such tasks have often focused on finding measures of rule
difficulty, and analysis of learning dynamics of first-order rules, such as “weather prediction” task. We conducted
psychophysical experiments in which subjects had to learn to classify binary patterns, x, according to rules of
different complexity. We found that in most cases people did not use single, simple stimulus features when learning
to classify but instead combined them dynamically. We used a maximum entropy based approach to identify
how subjects learned to combine stimulus features and a gradient ascent algorithm to model individual learning.
Specifically, for every subject we find a probabilistic classifier that uses an exponential probability distribution over
a weighted sum of stimulus features f i(x): the initial weights for each f i give the subject’s prior, and a learning
rate, h, determines how they are changed after each example, using gradient ascent. This modeling framework
reproduced with very high accuracy the individual learning curves of diverse population of subjects and rules,
even when subjects failed to learn the task. Furthermore, fitting a model to the first part of the learning session
enabled us to predict the future learning curve and single answers remarkably well. Finally, we used the model
learned for a subject in the beginning of their session to infer which examples would benefit them the most. We
then found that we can significantly improve subjects’ learning by such model-based teaching.

III-8. Internal representations of temporal statistics and feedback calibrate
sensorimotor interval timing
Luigi Acerbi1
Daniel Wolpert2
Sethu Vijayakumar1
1 University
2 University

L . ACERBI @ SMS . ED. AC. UK
WOLPERT @ ENG . CAM . AC. UK
SETHU. VIJAYAKUMAR @ ED. AC. UK

of Edinburgh
of Cambridge

Recent results have shown that bias and variance trade-offs in time perception can be accounted for by ‘optimal’
probabilistic inference. However, specific temporal statistics of the stimuli seem to induce sub-optimal behaviours;
for instance adaptor distributions, in which one inter-stimulus duration appears overwhelmingly often, typically
cause a temporal recalibration effect that defies a simple account based on prior expectations. The Bayesian

COSYNE 2012

159

III-9
ideal observer responses depend crucially on both the internal representation of the temporal context (subjective
prior and likelihoods) and on the loss function; observed ‘sub-optimal’ behaviours could be caused by a systematic
mismatch between the objective statistics of the experiment and their subjective counterparts. When, how and the
degree to which people can learn a correct internal representation of the temporal context can be revealing of the
underlying mechanisms. In this work, we studied how internal representations of temporal statistics are affected
by uniform and adaptor distributions of action-stimulus intervals in a time interval reproduction paradigm. By providing different shapes of performance feedback (i.e. loss functions) to the subjects, we also investigated how the
participants integrated external error signals with the temporal context. Our results show that temporal context
calibrates sensorimotor timing according to the ‘scalar property’ of sensorimotor error on short/long intervals in
the subsecond range. The subjects typically learnt smoothed approximations of the experimental distributions of
stimuli, with a good estimate of their mean and variance but also took into account higher-order statistics. The
responses were sensitive to the nature of the feedback provided, in general agreement with the behaviour predicted by the related loss function. Interestingly, the above results also held in the adaptor condition, implying that
there are no significant limitations in learning complex temporal distributions of stimuli with the help of corrective
feedback.

III-9. Generalization of uncertainty
Hugo Fernandes1
Ian Stevenson2
Konrad Kording1

HUGOGUH @ GMAIL . COM
I - STEVENSON @ BERKELEY. EDU
KK @ NORTHWESTERN . EDU

1 Northwestern
2 University

University
of California, Berkeley

Many studies have demonstrated that the nervous system takes into account the uncertainty associated with
state and feedback, in line with the predictions of Bayesian statistics. How populations of neurons represent the
associated probability distributions is consequently a central topic in computational neuroscience. Generalization
studies are a standard tool in movement neuroscience used to characterize underlying neural representations.
Subjects adapt to perturbations for one type of movement and we then examine how this affects other movements.
The resulting generalization curves are traditionally interpreted as a reflection of the tuning-curves for the underlying neural representations. Here we extended movement generalization experiments to ask how uncertainty is
generalized. We vary uncertainty by changing the variance of the stochastic visuomotor perturbation. We assess
uncertainty by introducing noisy visual information and measuring its influence on subjects’ reaches. In three
experiments we ask how uncertainty affects the generalization of the mean, how uncertainty itself generalizes,
and how mean and uncertainty interact. We then compare well-studied generalization-curves for the perturbation
means with our measured variance/uncertainty generalization-curves. We find that standard linear generalization
models in either the moments (Model M1) or probability space (M2) well describe the generalization of mean and
variance (Experiments E1 and E2). However, we find truly surprising effects on the generalization of uncertainty
when we perturb both the mean and the variance (E3): those directions where, during early training, subjects
made errors had high uncertainty, far higher than predicted by regular generalization models. However, the effects can be readily understood in terms of a supervised learning model (M3): in those directions with large errors
a high gain on the noisy feedback would have been optimal. We thus provide evidence of the involvement of
supervised learning mechanisms in the acquisition of Bayes-like behavior.

160

COSYNE 2012

III-10 – III-11

III-10. Exact inference for time series data on nonstandard state spaces
Carl Smith
Frank Wood
Liam Paninski

CAS 2207@ COLUMBIA . EDU
FWOOD @ STAT. COLUMBIA . EDU
LIAM @ STAT. COLUMBIA . EDU

Columbia University
A variety of methods are available for smoothing time series data in unconstrained vector spaces (e.g., splines,
Kalman filter approaches, or frequency domain methods). However, in many neuroscience settings we must perform inference and smoothing of time series in less- standard, non-vector state spaces. A prototypical example
involves the estimation of firing rates (which are constrained to be positive) given spike train data. Other examples include smoothing of angular variables (e.g., orientations of visual stimuli, or joint angle data in a motor task)
or oscillatory signals (e.g., local field potentials): in these cases, the relevant state space is a circle (or higherdimensional sphere or torus), instead of the real line. A final example involves decoding the position of a rodent
moving through a maze of some arbitrary topology. In each of these cases, standard vector-space smoothing
methods can be adapted to form approximations to the quantities of interest, via Monte Carlo methods or deterministic approximation approaches (e.g., Laplace approximation or Expectation Propagation). How- ever, exact
optimal Bayesian inference methods have the potential to be faster, more reliable, and more accurate. Here we
identify a broad model class in which exact inference can be performed efficiently by efficient dynamic programming methods. Our approach is based on a graphical (hidden Markov) model formulation in which the potential
functions connecting nearby time bins have a certain “low-rank” structure that enables fast exact computation: we
can draw exact samples and compute exact posterior distributions in time which scales linearly in the length of the
observed time series, and quadratically with a measure of the complexity (the “rank”) of the examples discussed
above.

III-11. Adaptive estimation of nonlinear response functions in V1 with Gaussian processes
Mijung Park1
Greg Horwitz2
Jonathan W Pillow1
1 University
2 University

MJPARK @ MAIL . UTEXAS . EDU
GHORWITZ @ U. WASHINGTON . EDU
PILLOW @ MAIL . UTEXAS . EDU

of Texas at Austin
of Washington

A common goal in neurophysiology experiments is to estimate a neuron’s tuning across multiple stimulus dimensions from a minimal amount of data. Here we describe a method for adaptive stimulus selection in closed-loop
experiments using a flexible model of the neural nonlinearity. Specifically, we model the nonlinearity f in a linearnonlinear-Poisson (LNP) neuron with a Gaussian process (GP). This provides a non-parametric description of
the mapping from feature space to spike rate, and offers a nice tradeoff between flexibility and computational
tractability relative to standard methods (e.g., parametric forms, histograms, cubic splines). We provide a simple
algorithm for updating the posterior distribution over the f after each trial, and select the stimulus that maximizes
the expected information gain about f for the next trial. Our approach relies on a method for rapidly updating
hyperparameters governing the GP prior using the Laplace approximation to the posterior over f. We demonstrate
the efficacy of our method using data from color-tuned neurons in macaque V1. Single neurons were stimulated
with spectrally modulated gabor patches, each of which defined a location in the three-dimensional space of cone
contrasts. The integration of cone contrasts in V1 neurons can be highly nonlinear, and the shape of this nonlinearity varies considerably across neurons. We show that adaptive stimulus selection under the GP-LNP model
substantially reduces the amount of data required to estimate these nonlinear functions. Finally, we extend the
model to incorporate spike-history dependence, allowing the effects of spike-rate adaptation (which may arise
when exploring high-response regions of the stimulus space) to be taken into account when estimating neural
nonlinearities.

COSYNE 2012

161

III-12 – III-13

III-12. On the interaction of excitatory and inhibitory synaptic plasticity
Henning Sprekeler1
Claudia Clopath2
Tim P. Vogels3

H . SPREKELER @ BIOLOGIE . HU - BERLIN . DE
CLAUDIA . CLOPATH @ GMAIL . COM
TIM . VOGELS @ EPFL . CH

1 Humboldt

Universität zu Berlin
University
3 École Polytechnique Fédérale de Lausanne
2 Columbia

Cortical neurons receive a balance of excitatory and inhibitory synaptic currents. This balance has attracted significant interest because it is thought to be a central mechanism by which cortical networks maintain stable activity
with the observed high degree of irregularity and little correlation across neurons and because it shapes cortical
responses to sensory stimuli. An open question, however, is by which mechanism this balance arises and how it is
maintained in the presence of excitatory plasticity. Recently, we suggested that inhibitory synaptic plasticity could
serve as a self-organization mechanism by which networks can robustly self-tune into a balanced state. Because
inhibition shapes neuronal responses to excitatory input, any form of excitatory synaptic plasticity that depends
on pre- and postsynaptic activity will in turn be influenced by a rebalancing form of inhibitory plasticity. Here,
we present a mathematical and compu- tational analysis of this interaction. We study the dynamics of synaptic
weights onto a single postsynaptic cell receiving both excitatory and inhibitory inputs. While inhibitory plasticity
follows a rebalancing rule [3], we consider a variety of excitatory learning rules: simple Hebbian learning, BCM
rules and spike timing dependent plasticity (STDP). Our analysis reveals that the learning dynamics can be nontrivial, depending on (i) the relative speed of learning of excitation and inhibition, (ii) the weight-limiting mechanism
for the excitatory synapses and (iii) the degree of correlation between the excitatory and the inhibitory inputs. In
particular, (i) homogeneous excitatory weights or transient, unstable input selectivity are common results of the
interaction so that (ii) input selectivity forms only when the two forms of plasticity act synergistically. Moreover, (iii)
inhibitory plasticity changes the temporal feature selectivity of STDP. These insights will help to understand how
excitatory and inhibitory plasticity interact to form functional neural circuits.

III-13. Dynamics of Gap Junctions Inspired Networks
Merav Stern1,2
Yosef Yarom1
Larry Abbott2
1 Hebrew

MS 4325@ COLUMBIA . EDU
YAROM @ VMS . HUJI . AC. IL
LFA 2103@ COLUMBIA . EDU

University of Jerusalem
University

2 Columbia

We are interested in whether gap junction connected networks can support complex dynamics. To this end, we
consider a network of neurons with gap junction connections, enhanced by additional intrinsic excitatory and
global inhibitory currents. This system turns out to be equivalent to a nonlinear network with a synaptic weight
matrix that has random off-diagonal elements with variance of order 1/N, and diagonal elements with variance
of order 1. Such large diagonal terms make the network behave differently from the more commonly studied
case where the diagonal terms make a negligible contribution. These networks have a trivial fixed point, which
does not support interesting dynamics, so we began by examining its stability. We examine the eigenvalues of
the stability matrix for this trivial fixed point, which are the same as the eigenvalues of the weight matrix. These
do not fall into the typical circular region of the eigenvalues of random matrices, and we discuss methods for
determining the shape of this region. For appropriate coupling strength, the trivial fixed point becomes unstable.
When this happens, the network exhibits long-lasting chaotic “exploratory” activity that eventually leads to a stable,
nontrivial fixed point. At the fixed point, the activity or voltage value of the neurons are predominantly determined
by the diagonal elements of the connectivity matrix, with the off-diagonal elements serving as a source of noise.
Simulations show that the lifetime of the transient chaotic stage grows exponentially with network size. Thus, a
reasonably sized network can sustain complex activity and signal generation over large time intervals. This in turn

162

COSYNE 2012

III-14 – III-15
suggests that these networks can serve as a source of internal activity for motor, learning and memory process.

III-14. Slow dynamics and high variability in balanced networks with clustered
connections
Ashok Litwin-Kumar1
Brent Doiron2
1 Carnegie

ALK @ CMU. EDU
BDOIRON @ PITT. EDU

Mellon University
of Pittsburgh

2 University

Anatomical studies demonstrate that excitatory connections in cortex are not uniformly distributed but instead
exhibit clustering into groups of highly connected neurons (Song et al., 2005, Perin et al. 2011). However, the
implications of clustering for cortical activity are unclear. We study the effect of clustered excitatory connections on
the dynamics of neuronal networks that exhibit high spike time variability due to a balance between excitation and
inhibition (van Vreeswijk and Sompolinsky, 1996). Even modest clustering substantially changes the behavior of
these networks, introducing slow dynamics where clusters of neurons transiently increase or decrease their firing
rate. These fluctuations over long timescales or across trials are not captured by networks with simple uniform
connection structures, which instead produce irregular firing with a fixed rate. Neurons in clustered networks
exhibit both short timescale spiking variability and long timescale firing rate variability. A simplified model shows
how stimuli bias networks toward particular attractor states in which certain clusters fire at high rates and others
at low rates. In this way, stimuli reduce firing rate variability compared to the spontaneous state, as observed
experimentally in many cortical systems (Churchland et al., 2010). Our model thus relates cortical architecture to
the reported variability in spontaneous and evoked spiking activity.

III-15. Phase precession through intrinsic neural resonance in continuous attractor models of grid cells
Sean Trettel
Ila Fiete

TRETTELS @ UTEXAS . EDU
ILAFIETE @ MAIL . CLM . UTEXAS . EDU

University of Texas at Austin
With one notable exception, the features of grid cells are remarkably well-modeled by recurrent network models
(called continuous attractor or CA models) whose weights stabilize a restricted set of patterns in the neural population. The exception is a feature ubiquitous in layer II entorhinal grid cells: phase precession. As an animal moves
through a grid cell’s activity field, the neuron’s spikes precess, or are emitted at progressively earlier phases of the
oscillating local field potential (LFP). The presence of phase precession in competing temporal-interference models has been cited as a primary advantage. We show here that if a simple model of the resonant properties of layer
II entorhinal neurons is included in CA network neurons, with appropriate phase coupling, the resulting grid cells
will phase precess. We consider a 1-dimensional CA grid cell model network, in which neurons interact through
local excitation and local surround inhibition. The neurons are intrinsic oscillators, with frequency f0 Hz. Neural
spiking leads to perturbations in the phase of synaptic target neurons based on the phase difference between
neurons and the sign of their interaction (excitatory or inhibitory). As the animal moves through a chain of grid
fields, the phases of cells along this chain are progressively retarded, resulting in an LFP with lower frequency,
f0-d. Spikes emitted by each cell at a fixed phase of its internal oscillation (e.g. at its peak), will precess forward
relative to the lower-frequency LFP. The model predicts that phase precession depends on location rather than
time spent in the response field, unlike models based on cellular processes with fixed time-constants. Further, the
model makes testable predictions about how the precession rate varies with peak neural firing rates and animal
velocity.

COSYNE 2012

163

III-16 – III-17

III-16. Error-driven learning within the Hippocampus; theta rhythm, and novelty based learning signals
Nicholas Ketz
Srinimisha Morkonda
Randall O’Reilly

NICHOLAS . KETZ @ COLORADO. EDU
SRINIMISHA @ COLORADO. EDU
RANDY. OREILLY @ COLORADO. EDU

University of Colorado, Boulder
Previous evidence has shown that the theta rhythm within the hippocampus is crucial for encoding and recall, but
its functional mechanism has yet to fully understood. Mathematical modeling work has suggested that different
phases of this oscillatory signal facilitate hippocampal encoding or retrieval(Hasselmo, Bodelon & Wyble 2002).
Similarly, neuroanatomical modeling has proposed the role of the subiculum, within a larger striatal circuit, as
providing a dopamine facilitated novelty signal within area CA1(Lisman & Grace 2005). The current work expands
upon these ideas by implementing and testing them within an existing, biologically plausible, neural network model
of the hippocampus(Norman & O’Reilly 2003). Representations are first encoded during the theta trough in the
auto-encoder like Entorhinal Cortex(EC) to CA1 connections. However, during the subsequent peak of the theta
wave, retrieval occurs such that the CA3 drives a completed pattern of activation in CA1 where it is compared
against the pattern encoded during the theta trough, providing an error-driven learning signal. Similarly, the
subiculum provides a novelty signal based on the mismatch of incoming stimuli and the completed pattern of
activation within EC, which in turn drives a dynamic learning rate in the CA3 to CA1 connections. Simulations
of this augmented model show enhanced performance through the error-driven learning signal, as well as the
dynamic learning rate. Performance is assessed in the AB-AC cued recall task, as well as a raw learning capacity
test, where the augmented hippocampal model is compared with a purely Hebbian based hippocampal model.
Results, explored across various network sizes, show a decrease in interference between studied items, as well
as an increase in raw capacity for the augmented model compared to the Hebbian model. The current work is
presented in the development of a mechanistic account of pre-frontal interactions with hippocampal encoding and
retrieval.

III-17. Intrinsic gradient networks: Highly recurrent neural networks with biologically plausible training
Jason Rolfe1
Matthew Cook2
Yann LeCun1
1 New
2 ETH

ROLFE @ CS . NYU. EDU
COOK @ INI . PHYS . ETHZ . CH
YANN @ CS . NYU. EDU

York University
Zürich

Artificial neural networks are computationally powerful and exhibit brain-like dynamics. However, it is generally
believed that the backpropagation algorithm, commonly used to train neural networks, is not biologically plausible;
backpropagation messages must not directly affect the original feedforward messages, in contradiction to the
pervasively recurrent architecture of the cortex. The recurrence of the cortex, together with the requirement
that training signals project directly to the trained structures in the brain, implies that the cortex uses a single
interdependent set of messages for both computation and learning. Moreover, when faced with complicated or
ambiguous input, the cortex can withhold a motor response until processing is complete. We further assume
that learning requires the approximate calculation of the gradient of a loss function, and restrict our attention to
networks in which this gradient can be calculated completely at a single network state. From this modest set of
constraints, we derive a novel class of recurrent neural networks, intrinsic gradient networks, for which the gradient
of the loss function with respect to the parameters is a simple function of the network state when a self-identified
output has been produced. Intrinsic gradient networks do not generally segregate “feedforward” computation
signals from “feedback” training signals, and so are potentially consistent with the pervasive recurrence observed
in the cortex. Within the class of intrinsic gradient networks, it is easy to identify highly recurrent instances for

164

COSYNE 2012

III-18 – III-19
which training is simple and local, thus satisfying all of the biological and computational constraints identified
above.

III-18. Plasticity in chaotic random recurrent networks leads to complex but
non-chaotic neural trajectories
Rodrigo Laje
Dean V Buonomano

RLAJE @ MEDNET. UCLA . EDU
DBUONO @ UCLA . EDU

University of California, Los Angeles
It’s clear that much of the computational power of the brain derives from the complex dynamics generated by
recurrent neural networks. Ongoing activity in the absence of input is hallmark of the brain and recurrent neural
networks operating in “high-gain” regimes. Analytical and numerical simulation results, however, show that the
dynamics in random recurrent networks capable of generating ongoing activity tend to be chaotic and difficult
to control (1,2,3). While synaptic plasticity within recurrent networks may provide a manner to control neural
dynamics, incorporating plasticity in recurrent networks has proven to be challenging. Learning rules in recurrent
networks have been heavily influenced by those in feedforward networks where weights are changed in order to
minimize the error between the actual output and the desired target. One successful approach has been to use
carefully controlled feedback from a readout (output) to partially drive the dynamics within the recurrent network
(4,5). Although recent studies have incorporated plasticity in the recurrent weights, e.g. (5), these approaches
have not dramatically enhanced the computational power of neural networks. Here we propose an alternate
strategy where the weights in an initially random, chaotic recurrent network of firing rate units are trained to
reproduce their natural or “innate” trajectory during a long time window—in essence making the network do more
robustly what it already does. After this initial phase the trained neural trajectory can then be used to drive the
desired output patterns as with previous state-dependent or reservoir computing approaches (6,7). We show
that the innate trajectory, originally a portion of the chaotic attractor, becomes a locally stable attracting trajectory
(“stable transient channel”) while maintaining its original complexity.

III-19. Reliable and unreliable spike times in sparsely connected networks
Guillaume Lajoie1
Kevin Lin2
Eric Shea-Brown1
1 University
2 University

GLAJOIE @ AMATH . WASHINGTON . EDU
KLIN @ MATH . ARIZONA . EDU
ETSB @ WASHINGTON . EDU

of Washington
of Arizona

If the same sensory stimulus is presented multiple times to a neuronal network, how similar are the spike trains
that it evokes? This question of the reproducibility, or reliability, of stimulus-induced spike times has a long history
in neuroscience. Here, we contribute new results on how features of spiking network dynamics constrain this reliability. We focus on networks with sparse, random connectivity and balanced excitation and inhibition. Such networks have attracted great interest, as they reproduce the irregular firing that typifies cortical activity. For several
models of neural dynamics, this activity is known to be chaotic, with extremely strong sensitivity of spike outputs
on tiny changes in a network’s initial conditions [VanVreeswijk and Sompolinsky, 1996, Monteforte and Wolf, 2010,
London et al., 2010]. These chaotic networks are by definition unreliable. Here, our goal is to understand whether
this is essentially always the case, or whether there are non-chaotic—and hence reliable—varieties of balanced
networks as well. We consider sparse networks of inhibitory and excitatory neurons, modeled as phase variables,
randomly coupled and driven by independent temporally structured inputs. We explore the impact of two key
parameters. The first is the amplitude of a signal impinging on the network. The second is the superthreshold vs.
subthreshold state of the individual cells—that is, whether they are in mean-driven vs. fluctuation-driven regimes.

COSYNE 2012

165

III-20 – III-21
We show that both parameters strongly impact network reliability. In particular, a range of networks with moderate
signal amplitudes and excitable single-cell dynamics exhibit sustained irregular activity that is not chaotic. Such
networks reliably respond to a given input, with high spike timing accuracy on several trials with distinct initial
conditions. These same networks become unreliable when individual cells come closer to superthreshold states,
connecting with results for sparse recurrent networks in the literature.

III-20. Response of a Hodgkin-Huxley model VCN octopus cell to sounds with
pitch
Brian Flynn
Laurel Carney

BRIAN . FLYNN @ ROCHESTER . EDU
LAUREL CARNEY @ URMC. ROCHESTER . EDU

University of Rochester
Pitch, a fundamental attribute of complex sounds, is vital for both music and vocal communication. Fundamental
frequency (F0) is the defining characteristic of any periodic sound; however, it is not known how the auditory
system determines F0. A prevailing hypothesis is that temporal fine structure (TFS) of the waveform is crucial
for proper pitch perception. Auditory-nerve (AN) fibers preserve both TFS and envelope by phase locking to the
dominant periodicities present in sounds. The temporal representation of periodicity is enhanced by octopus cells
in the ventral cochlear nucleus, but is not maintained in more central auditory nuclei such as the inferior colliculus
or auditory cortex. It is hypothesized that coincidence detection, a property of octopus cells, is a physiologically
feasible mechanism for re-encoding temporal information present in auditory-nerve fibers into a firing rate-based
representation. Octopus cells receive input from many AN fibers with a broad range of characteristic frequencies,
have a large low-threshold K+ conductance, and can entrain to click trains up to 1 kHz. In this study, it is shown
that a Hodgkin-Huxley model of an octopus cell with model AN fiber input responds with high firing rates and
strong synchrony to pitch stimuli. Stimuli tested include click trains, harmonic tone complexes (with cosine and
positive/negative Schroeder phase), broadband noise, and iterated rippled noise. Firing rate and synchrony of
a model octopus cell were positively correlated to pitch strength. These results support the hypothesis that
octopus cells are responsible for converting temporal information present in AN fibers to an average firing rate
representation of F0.

III-21. Ion Channels Overcome the Biophysical Constraints of Neuron Morphology
Corinne Teeter
Victor H Chan

CTEETER @ GMAIL . COM
VCHAN @ QUALCOMM . COM

Qualcomm
Neurons have a vast variety of different morphological architectures [1]. It is likely that this difference in neuron
morphology reflects a difference in function. However, it is unclear whether the purpose of this morphology
diversity is to allow a neuron to make the necessary connections in the neural circuit or whether the structure
itself is required to produce the appropriate input-output transfer function of a neuron. Here we show that in most
cases a set of ion channels can be found that enables a neuron with a distinct morphology to replicate the spiking
behavior of neurons of different morphologies. This suggests that the brain has substantial flexibility with regard
to its structure and information processing strategies. We show that the brain could, with the correct selection of
ion channels, vary neural morphology without changing the spiking activity of a neuron.

166

COSYNE 2012

III-22 – III-23

III-22. A Sequential Prediction Approach to Measure Time-Varying Causality
in Ensemble Neural Recordings
Sanggyun Kim
Todd Coleman

IFREE 77@ GMAIL . COM
TPCOLEMAN @ UCSD. EDU

University of California, San Diego
Tracking the dynamics of neural systems is crucial for understanding how they adapt their responses to relevant
biological information. Rather than relying on correlation, recent neuroscientific endeavors have evolved towards
developing statistical measures of causality based on Granger’s notion [1]. However, his detailed quantitative
formulation, based on the multivariate autoregressive (MVAR) models, is problematic when observations are not
continuous (for example, binary-valued neural spike trains). More recently, the log-likelihood ratio has been applied to embody Granger’s notion for arbitrary modalities [2,3]. Secondly, Granger’s original formulation assumes
a time-invariant MVAR model; but dynamic, time-varying causal interactions arise naturally in the brain, and
changes within different neural populations might arise at different times. Current attempts to provide a sequence
of discrete snapshots of the dynamics using temporal windows do not track the evolution in a fine time scale, assume that all changes occur at the same time, and assume time-invariant dynamics within windows [3]. Here, we
propose a time-varying causality measure in ensemble neural spike trains using a sequential prediction approach.
Our approach measures the reduction in loss between two dynamic, provably good, sequential predictors—one
with the past of candidate “effector” and the other without. When applied to neural activity recorded in the primary motor cortex of monkey, which was trained to perform a visuomotor task [3], the proposed method tracks
time-varying causal interactions and observes more interactions after visual stimulus, which is consistent with our
previous findings [3]. Additionally, it provides detailed temporal information causal dynamics, changes of which
occur at different times and durations (Fig. 1).

III-23. Memory formation, recall and forgetting in neuronal networks
Christian Tetzlaff1
Christoph Kolodziejski2
Marc Timme2
Florentin Wörgötter1

TETZLAFF @ PHYSIK 3. GWDG . DE
KOLO @ NLD. DS . MPG . DE
TIMME @ NLD. DS . MPG . DE
WORGOTT @ PHYSIK 3. GWDG . DE

1 University
2 Max

of Goettingen
Planck Institute for Dynamics and Self-Organization

Which processes are behind the ability of biological neuronal circuits for memory formation, recall, and forgetting
(e.g., objects or facts) is still under heavy debate in neuroscience. Although several theoretical approaches exist
(e.g. Hopfield networks [Hopfield, 1982], attractor networks [Mongillo et al., 2008], Liquid-state machines [Maass
et al., 2002]), each approach has difficulties like, for instance, arbitrary non-biological constraints (e.g. Hopfield
networks), predefined connectivity (e.g. attractor networks), or supervised recall (e.g. Liquid-state machines). In
Tetzlaff et al., 2011 we analyzed a combination of conventional plasticity [e.g. Hebb, 1949] and synaptic scaling
[Turrigiano et al., 1998] that builds up memory traces (cell assemblies) induced by external inputs. Here we
show that the combination of this mechanism with inhibition can also form (“learn”), retrieve and delete clusters
of highly connected neurons within recurrent neuronal networks. Synapses in such a cluster are significantly
stronger than those between different clusters, thus similar connectivity patterns recently observed in the cortex
[Perin et al., 2011]. In our model inter-cluster connection strength directly depends on duration and frequency
of the presentation of an unknown entity. Thus, for complete recall of a well-learned (often presented) entity, a
smaller fraction of cluster neurons have to be stimulated and recall is quick. Well-trained clusters are not only
quantitatively but also qualitatively different from sporadically-trained clusters: The time scale for forgetting the
former is significantly longer than for the latter so that it takes longer to forget a well-learned entity. Synaptic scaling
in combination with conventional plasticity thus leads to 1) learning of new memory entities, 2) recalling of clusters
while synaptic weights remain plastic which additionally supports memory formation through reconsolidation, and

COSYNE 2012

167

III-24 – III-25
3) forgetting, which depends on the state (well- or sporadically-learned) the cluster is in.

III-24. On the complementary strengths and weaknesses of spatial vs. hybrid
map formation algorithms
Rishabh Jain
Bartlett Mel

RISHABH @ USC. EDU
MEL @ USC. EDU

University of Southern California
Map formation is a core process in visual development. Conventional algorithms that use neighborhood-based
learning are well suited to learn retinotopic maps of one or two non-mutually-exclusive features—notably the
combined orientation and ocular dominance map in V1. Cells in more ventral areas including V4 and IT respond
to a wide range of more complex object features, potentially constituting hundreds or thousands of different feature
types [1,2]. Given that object recognition is location-invariant, every point in the visual field should in principle be
analyzed in parallel by all features, implying that every feature should form its own map tiling the visual field.
The physical interdigitation of multiple feature maps presents a challenge for conventional self-organizing map
(SOM) algorithms: within such a “multimap”, a neuron will be surrounded by neurons coding multiple different
feature types, preventing neurons from co-training based only on their spatial proximity. We developed a hybrid
algorithm in which co-training of neurons depends both on their spatial proximity and the correlation of their
responses to incoming stimuli. We contrasted the behavior of this hybrid spatial-correlational learning rule with
a conventional SOM algorithm, finding the following double dissociation: The conventional SOM was learned a
good retinotopic maps of a single feature, but when multiple feature maps were trained simultaneously, the cortical
space was partitioned into mutually exclusive single-feature islands with periodic gaps in retinotopic coverage. In
contrast, the hybrid algorithm produced locally jumbled (salt-and-pepper) structure for a single feature, but when
presented with multiple features, produced fully interdigitated maps, each with good coverage both in retinal and
feature dimensions. Our results suggest that pure spatial map formation algorithms could be specialized for
development of single-feature maps such as in V1, whereas hybrid spatial+correlation-based development allows
for simultaneous mapping of multiple independent features as found in higher visual areas.

III-25. Active self-organization of disordered arrangements of orientation preference in cortical networks
Juan Florez Weidinger1,2
Wolfgang Keil1
Dmitry Tsigankov1
Michael Schnabel3
Matthias Kaschube4,5
Fred Wolf1

CHEPE @ NLD. DS . MPG . DE
WOLFGANG @ NLD. DS . MPG . DE
DMITRY @ NLD. DS . MPG . DE
M - SCHNABEL @ NORTHWESTERN . EDU
KASCHUBE @ FIAS . UNI - FRANKFURT. DE
FRED - WL @ NLD. DS . MPG . DE

1 Max

Planck Institute
Center for Computational Neuroscience
3 Northwestern University
4 Frankfurt Institute of Advanced Studies
5 Johann Wolfgang Goethe University
2 Bernstein

Response characteristics of orientation tuned neurons in the visual cortex appear to be similar in mammalian
lineages widely separated in evolution. The spatial arrangements of tuning properties across the cortex, however,
show fundamental differences. While in primates and carnivores orientation preference varies progressively forming orientation maps, in rodents it appears to be randomly distributed. Recently, it has been shown that orientation
maps in different species realize a common design which can be explained by activity-dependent self-organization

168

COSYNE 2012

III-26 – III-27
of large scale neuronal circuits (Kaschube et al., Science 2010). It remains unclear, however, whether a similar approach can also explain rodent functional organization. Here we present an analytically tractable symmetry-based
model of the activity-dependent development of orientation selectivity that can describe both types of organization.
In the model neurons interact in a distance dependent manner both with isotropically inhibition and excitation and
with orientation selective excitation. By symmetry this model has a large set of exact map solutions. Analytically
examining their stability, we find that, independent of the fraction of selective interactions, with strong short range
inhibition all map solutions become unstable. We show numerically that in this regime, disordered arrangements
of orientation preferences become the attractor state of the network. We examine generalizations of the model
to binocular tuning and determined a parameter regime in which an initially independent tuning for each eye is
matched only after the emergence of selectivity, as was recently reported (Wang et al., Neuron 2010). Even while
the neurons independently organize their inputs to achieve binocular matching, the disordered structure in the
network is actively generated. Our results demonstrate, that a disordered arrangement of orientation selectivity
can be actively generated by a dynamical process of self-organization.

III-26. Nonnormal amplification in random balanced neuronal networks
Guillaume Hennequin
Tim P Vogels
Wulfram Gerstner

GUILLAUME . HENNEQUIN @ EPFL . CH
TIM . VOGELS @ EPFL . CH
WULFRAM . GERSTNER @ EPFL . CH

École Polytechnique Fédérale de Lausanne
In dynamical models of cortical networks, noisy inputs can be amplified into structured activity fluctuations by
the recurrent connectivity, essentially through a combination of two distinct mechanisms. First, those patterns
that self-reproduce by passage through the connectivity matrix W display large but slow fluctuations (dynamical
slowing). Second, if W is nonnormal in the mathematical sense, it may hide a functionally feedforward network of
strongly coupled activity patterns, allowing for transient amplification on fast time scales. The latter mechanism of
nonnormal amplification has recently emerged in the neuroscience literature, though only in the context of a network with specific structure [Murphy and Miller (2009)], or networks explicitly designed to exhibit the phenomenon
[Ganguli et al. (2008); Goldman (2009); Benayoun et al. (2010)]. It is not clear to what extent nonnormality affects
the dynamics of more generic models of cortex. Here we investigate the tradeoff between nonnormal amplification
and dynamical slowing in large random neuronal networks composed of excitatory and inhibitory neurons. Assuming linear stochastic dynamics, we derive an exact expression for the expected amount of purely nonnormal
amplification. We find that nonnormality primarily gives rise to macroscopic fluctuations of the global population
firing rate, which explains the positive mean pairwise correlation among network neurons. Amplification along
more detailed spatial patterns is microscopic, however, and its total amount is very restricted if dynamical slowing
needs to be kept low. Thus, in order to achieve strong transient amplification with little slowing, the connectivity
must be structured, so that the synaptic strengths can afford larger values – a region where amplification in fact
explodes – while self-reproducing patterns are discouraged. We discuss why this could be a desirable feature for
sensory cortices that need to track fast-changing signals, and we give a plausible example of structure that favors
nonnormal amplification.

III-27. Non-linear predictive coding and dynamic decorrelation in early sensory systems
Shaul Druckmann
Dmitri Chklovskii

DRUCKMANNS @ JANELIA . HHMI . ORG
CHKLOVSKIID @ JANELIA . HHMI . ORG

HHMI, Janelia Farm Research Campus
Reacting to stimuli is crucial to animals—survival yet must be done accurately—a conflict known as the speed-

COSYNE 2012

169

III-28
accuracy tradeoff. Correct reactions become challenging if functionally different stimuli elicit similar neural activation patterns. Discriminability can be improved by pattern decorrelation—processing stimuli that are functionally
different, yet elicit similar activity, to make output activity more distinct (Wiechart et al. 2010). Such a process inherently involves amplifying differences between activity patterns. However, even repetitions of the same stimulus
have differences in activity, i.e., noise, which does not contribute to discriminability and will degrade performance.
Thus, amplifying differences between patterns is potentially risky. If signal always has given characteristics e.g.,
low spatial frequency, and noise different ones e.g., high spatial frequency, then noise can be filtered out. However, signals may be similar to what is noise in a different context e.g., high spatial frequency components induced
by edges, making decorrelation difficult. This is similar to overfitting in statistics, for which a common solution is
using powerful models that can in principle fit the noise, but enforcing regularization, making overfitting less likely.
Here we show that a nonlinear recurrent network, a simplified model of early sensory processing, performing regularized predictive coding decorrelates activity. Solving the nonlinear dynamics, we demonstrate that interneurons
initially subtracts highly regularized, i.e., less susceptible to noise, predictions but with time subtract progressively
less regularized predictions, resulting in increased decorrelation, and improving discriminability. Consistent with
experiments, decorrelation develops over time and is only partial (Gridhar et al. 2011). This is explained in our
model by the temporal dynamics of regularization enforced by interneuron nonlinearity. Thus, our study provides
insight into the fundamental question of speed-accuracy tradeoffs, and a theoretical framework explaining the
mechanistic basis and dynamics of decorrelation in early sensory circuits.

III-28. Frequency analysis of short-term memory in nonlinear network models
Ann Kennedy1
Haim Sompolinsky2
Larry Abbott1

AK 3024@ COLUMBIA . EDU
HAIM @ FIZ . HUJI . AC. IL
LFA 2103@ COLUMBIA . EDU

1 Columbia
2 Hebrew

University
University

Recurrent neuronal networks have been proposed as generic models for storing temporal signals in their dynamic states. However, previous analysis of the short term memory capacity of recurrent networks has largely
focused on linear networks with no intrinsic dynamics. Here we study short-term memory of randomly connected
rate-based neural networks with strong, balanced connectivity and nonlinear firing rate functions. When these
networks are driven by a strong input pulse, its effect resonates in the network 50–100 times longer than the
time constant of individual network neurons. This memory is of limited computational use, however, because the
strong input erases any traces of preceding input. To study memory capacity for continuous signals, networks
were driven with zero-mean Gaussian white-noise input, and a linear readout of the network state was used to
reconstruct the input history. Signal reconstruction error of this readout was computed as a function of input
frequency and elapsed time. Network memory profiles fall into two regimes. When the input signal has a small
amplitude, it fails to fully suppress the intrinsic chaotic dynamics of the network, most notably at lower frequencies.
When input to the network is strong, new inputs overwrite the representation of preceding inputs. Between these
two regimes we find an optimum input amplitude at which network memory is greatest. The interaction between
memory and network architecture—including size, connection density and strength—was characterized through
simulation. Spatial targeting of input populations was also explored as a means of exploiting network architecture
to improve suppression of chaos and to increase memory duration. In conjunction, these findings provide guidelines by which network architecture may be tailored to achieve greater memory capacity in networks of neurons
with limited dynamic range.

170

COSYNE 2012

III-29 – III-30

III-29. Local control of non-local information routing in spiking neuronal networks
Christoph Kirst1,2
Marc Timme1
Demian Battaglia1
1 Max

CKIRST @ NLD. DS . MPG . DE
TIMME @ NLD. DS . MPG . DE
DEMIAN @ NLD. DS . MPG . DE

Planck Institute for Dynamics and Self-Organization
Center for Computational Neuroscience Göttingen

2 Bernstein

To delve with behaviorally relevant changes in context or to subserve top-down control of neural processing dynamic routing of information between neuronal assemblies is essential in a functional brain. Recent experimental
and theoretical studies [1-3] show that coherent oscillations among local circuits may create such information
channels. Here we show how network-wide information flow patterns in oscillatory networks can be controlled by
local changes within a sub-region only. Thus local structural and dynamical changes, induced e.g. by local stimulation or synaptic plasticity are capable of controlling the information routing at distant locations. We first derive
an analytical expression for the information flow in hierarchical networks of reduced phase-amplitude oscillators
with strongly connected clusters and weaker inter-cluster couplings. Our analytics unravel that local changes can
remotely control the information transmission between two other distant, physically unchanged groups. Moreover,
by switching among local multi-stable dynamical states the global information flow pattern in the network can
be changed in a dynamical way. We link our theoretical findings to more realistic clustered networks of spiking
neurons exhibiting pyramidal interneuron gamma (PING) oscillations. We derive an semi-analytical expression for
the information flow between the PING clusters and identify the same local information flow control mechanisms.
Moreover, by introducing a coding scheme based on the spike ordering we show that these information channels
act as “carrier channels” for complex representations encoded in precise patterns of neuronal firing. Collective
dynamics of interacting brain rhythms thus equip a network with local control mechanisms for non-local routing of
large amounts of information in a way akin to a “clocked” combinatorial circuit. [1] T. Womelsdorf et al., Science
316, 1609–1612 (2007). [2] A. Buehlmann, G. Deco, PLoS Comput. Biol. 6, e1000934 (2010). [3] D. Battaglia, A.
Witt, F. Wolf, T. Geisel, in review (2011).

III-30. Between-pair spike-field coherence comparison
Kyle Lepage1
Mark Kramer1
Georgia Gregoriou2
Steve Gotts3
Robert Desimone4
Uri Eden1

KYLE . LEPAGE @ GMAIL . COM
MAK @ MATH . BU. EDU
GREGORIOU @ MED. UOC. GR
GOTTSS @ MAIL . NIH . GOV
DESIMONE @ MIT. EDU
TZVI @ BU. EDU

1 Boston

University
of Crete
3 National Institute of Mental Health
4 Massachusetts Institute of Technology
2 University

Recent experiments in neuroscience have compared the strength of association between neural spike trains
and oscillations present in local field potential (LFP) recordings. The measure employed in these comparisons,
“spike-field coherence”, is a frequency dependent measure of linear association, and is shown to depend on
overall neural activity. Dependence upon overall neural activity, that is, dependence upon the total number of
spikes, renders comparison of spike-field coherence across experimental context difficult. In this presentation,
an inferential procedure based upon a generalized linear model is shown to be capable of separating the effects
of overall neural activity from spike train-LFP oscillatory coupling. This separation provides a means to compare
the strength of oscillatory association between spike train-LFP pairs independent of differences in spiking rate.
Following a review of the generalized linear modelling framework of point process neural activity a specific class

COSYNE 2012

171

III-31 – III-32
of generalized linear models are introduced. This model class, employing a piece-wise constant link function
relating an LFP rhythm to neural response, is used to develop an hypothesis test capable of detecting changes in
spike train-LFP oscillatory coupling. The performance of this hypothesis test is validated, both in simulation and
on real data, and is compared to the method of spike thinning previously employed to facilitate across-context
spike-field coherence comparison. By equalizing the overall neural activity through a spike removal process
prior to spike-field coherence computation, this latter method seeks to equalize the dependence of spike-field
coherence estimates upon neural activity prior to across-pair comparison. It is shown to be deleterious in at least
one example, and is approximately equivalent to the proposed method when neural spiking rates are comparable.
The proposed method of inference provides a principalled statistical procedure by which across-context change
in spike train-LFP rhythmic association can be directly inferred, independent of neural spiking rates.

III-31. Bayesian entropy estimation for infinite neural alphabets
Evan Archer
Il Memming Park
Jonathan W Pillow

EARCHER @ UTEXAS . EDU
MEMMING @ AUSTIN . UTEXAS . EDU
PILLOW @ MAIL . UTEXAS . EDU

University of Texas at Austin
Shannon entropy quantifies the information that may be conveyed by a vector of neural responses, and has
featured prominently in the analysis of neural codes. However, entropy is notoriously difficult to estimate from
data, particularly in the “undersampled” regime, where the number of possible response patterns (or “words”)
K is larger than the number of observed responses (”samples”) N. Here we describe a Bayesian method for
estimating entropy from datasets where the number of possible words (i.e., the size of the neural alphabet) is
arbitrarily large or unknown. Our approach follows that of Nemenman et al, who formulated a Bayesian entropy
estimator using a “mixture-of-Dirichlets” prior over the space of discrete distributions on K bins. Here we extend
this approach in several directions. First, we formulate two priors over discrete, countably infinite distributions
using mixtures of Dirichlet processes (DP) or Pitman-Yor (PY) processes. (These processes play a central role in
nonparametric Bayesian statistics, and are useful when the number of parameters or “bins” is not known a priori.)
We analytically derive a set of mixing weights over these processes so that the resulting improper prior over
entropy is approximately flat across a semi-infinite range. Secondly, we consider the posterior over entropy given
a dataset (which contains some observed number of words but an unknown number of unobserved words), and
show that the posterior mean can be efficiently computed via a simple 1D or 2D numerical integral. Remarkably,
for most datasets the expected entropy given data is finite, even though the distributions have positive probability
on infinitely many bins and the prior is improper. We compare our approach to previous methods, including an
approximate Bayesian entropy estimator and a frequentist “coverage-adjusted” estimator designed for unknown
or infinite K.

III-32. Parallel pathways for information processing in the retina: the ON and
OFF dichotomy
Julijana Gjorgjieva1
Haim Sompolinsky2
Markus Meister1
1 Harvard
2 Hebrew

GJORGJIEVA @ FAS . HARVARD. EDU
HAIM @ FIZ . HUJI . AC. IL
MEISTER @ FAS . HARVARD. EDU

University
University of Jerusalem

Over twenty different ganglion cell types have been identified in the mammalian retina, believed to represent
parallel channels of information transmission. In an effort to understand the origins and functional implications of
this diversity, we examine the division into ON and OFF channels for the processing of light and dark increments

172

COSYNE 2012

III-33 – III-34
in a visual scene. The ON and OFF pathways have been hypothesized to emerge for the rapid and metabolically
efficient signaling of changes in light intensity (Schiller 1992), however, this idea has never been formalized.
We quantify the information gain in a system with two cell types of opposite light responses, ON and OFF, and
compare it to a system with one cell type, ON-ON. In the regime of low firing rates (one spike per integration
time) typical for retinal ganglion cells, the optimal mutual information between a stimulus and a cell’s response is
transmitted by Poisson neurons which use only two firing rates: zero and maximal (Stein 1967, Shamai, 1990,
Nikitin et al. 2009). We extend the single cell analysis to a system of two cells with binary response functions,
described by the response threshold. Constraining the maximum or the mean firing rate in both systems, ON-OFF
and ON-ON, results in only a minor information gain for the ON-OFF system. This unexpectedly small difference
between the two schemes, as measured by the mutual information, suggests the need to explore other criteria for
optimality: (1) We examine natural images which may contain ecologically-relevant information not captured by
the mutual information. (2) In addition to static images, we also compare the performance of the two schemes for
their speed of encoding of temporally fluctuating light intensities.

III-33. Fisher and Shannon information in finite neural populations
Stuart Yarrow
Peggy Series

S . YARROW @ ED. AC. UK
PSERIES @ INF. ED. AC. UK

University of Edinburgh
The precision of the neural code is commonly investigated using two different families of statistical measures: (i)
Shannon mutual information and derived quantities when investigating very small populations of neurons and (ii)
Fisher information when studying large populations. These statistical tools are no longer the preserve of theorists,
and are being applied by experimental research groups in the analysis of empirical data. Although the relationship
between information theoretic and Fisher-based measures in the limit of infinite neural populations is relatively well
understood, how these measures compare in finite size populations has not yet been systematically explored. We
aim to close this gap. We are particularly interested in understanding which stimuli are best encoded (in terms
of discrimination) by a given neuron within a population and how this depends on the chosen measure. We
use a novel Monte Carlo approach to compute a stimulus-specific decomposition of the mutual information (the
stimulus-specific information) for model populations of up to 256 neurons and show that Fisher information can be
used to accurately estimate both mutual information and stimulus-specific information (SSI) for populations of the
order of 100 neurons, even in the presence of biologically realistic variability, noise correlations and experimentally
relevant integration times. According to both measures, the stimuli that are best encoded are then those falling at
the flanks of the neuron’s tuning curve. In populations of less than around 50 neurons, however, Fisher information
can be misleading.

III-34. Maximally informative stimulus energies in the analysis of neural responses to natural signals
Kanaka Rajan
William Bialek

KRAJAN @ PRINCETON . EDU
WBIALEK @ PRINCETON . EDU

Princeton University
We have developed Maximally Informative Energies as a method for describing the responses of neurons to
complex and potentially high-dimensional stimuli, in cases where the response has a quadratic dependence on
the stimulus. This extends the maximally informative approach to neurons with receptive fields that are sensitive
to the covariance structure of the stimulus preceding a spike rather than depending only on a linear projection. We
illustrate the practical feasibility of this procedure in three model systems: 1) Non-phase-locked auditory neurons
responding to complex, natural, acoustic stimuli. These neurons respond to sound energy in a selected bandwidth,

COSYNE 2012

173

III-35 – III-36
but do not discriminate among signals shifted by small times. 2) Complex cells in the visual cortex responding to
natural scenes. These exhibit invariance to small spatial displacements of the relevant, oriented stimulus feature.
3) Motion-sensitive visual neurons that compute a velocity, independent of the absolute position of the objects
that are moving. The invariance that is common to all three systems, means that the neurons are sensitive to
multiple stimulus dimensions, although these different dimensions effectively correspond to the same stimulus
feature occurring at different times relative to the spike. To capture this intuition concisely, our simplest starting
point is a model in which a quadratic projection of the stimulus onto a receptive field is computed. To extract
this quadratic dependence in the presence of a naturalistic stimulus distribution, we extend the earlier notion of
maximizing mutual information to include quadratic stimulus dependence and demonstrate that this approach
leads to consistent inference. Our approach yields unbiased receptive field estimates, but requires optimization
in a possibly rugged information landscape. We expect that this approach will help elucidate systematically the
prevalence of sensitivity to high-order statistical features in sensory cortices for naturalistic stimuli.

III-35. A principle of brain communication based on compressive sampling
and sparse coding
Guy Isely
Christopher Hillar
Fritz Sommer

GUYI @ BERKELEY. EDU
CHILLAR @ MSRI . ORG
FSOMMER @ BERKELEY. EDU

University of California, Berkeley
Axonal projections between brain regions may constitute wiring bottlenecks for communication. Thus, some
form of compression may be necessary for communication within the brain. Here we explore whether projecting
neurons could compress local patterns of activity by simple random subsampling. Although there are many
different algorithms for compression, few of them are well suited to be implemented in a biologically plausible
fashion. We consider different alternatives for communication across wiring bottlenecks in the brain and argue that
a combination of compressive sampling and sparse coding is a compelling potential mechanism. Previous work on
compressive sampling has shown that under the assumption of sparsity, compressed signals can be reconstructed
even when they are sampled at a rate less than the Nyquist rate. Here we show that meaningful recovery is
possible through the use of sparse coding even when the subsampling matrix is not known. This is of particular
interest in the brain where presumably a downstream region cannot know the local pattern of the connectivity in
the upstream region. In simulations, we demonstrate this approach to compressive communication. Although the
downstream representations learned in our simulations are not identical to the patterns in the upstream region,
we are able to show that they bear a precise correspondence to the original signals in a way that preserves their
information content.

III-36. Fitting receptive fields in V1 and V2 as linear combinations of nonlinear
subunits
Brett Vintch1
Andrew D Zaharia1
Tony Movshon1
Eero P Simoncelli1,2
1 New

VINTCH @ CNS . NYU. EDU
ZAHARIA @ CNS . NYU. EDU
MOVSHON @ NYU. EDU
EERO. SIMONCELLI @ NYU. EDU

York University

2 HHMI

Responses of early visual neurons are commonly described with a linear sensory filter followed by a spiking
nonlinearity. In many cases these linear-nonlinear (LN) models capture a substantial fraction of the response
variance. However, many cell types are not well fit (e.g. V1 complex cells), and are described instead as a

174

COSYNE 2012

III-37 – III-38
linear combination of LN “subunits” (altogether, LNL). The stimulus subspace spanned by these subunits can be
estimated by analyzing the covariance of the spike-triggered stimulus distribution (STC), but this requires large
amounts of data and cannot uniquely determine the form of the subunits. To overcome these limitations we
introduce a procedure that fits an LNL subunit model directly. The model assumes that a single linear subunit
is applied convolutionally over space, and that the field of linear responses is then transformed with a single,
replicated nonlinearity. We fit the model (linear subunit, nonlinearity, and linear weighting over subunits) with
alternating gradient descent. The model performs well on three sets of neural data (two collected in V1 and one in
V2); the fitted subunit models capture similar visual information as STC with many fewer parameters and produce
unique well-localized subunit features. In V1, the model can fit 2D (x-t) subunits for both simple and complex
cells. For simple cells, the linear weighting is small at all but one location, indicating a single subunit; complex
cell subunits are spatially dispersed. Fitting 3D (x-y-t) subunits to another set of V1 data reveals the expected
combinations of orientation and direction tuning. For V2 neurons, instead of computing subunits directly from
the stimulus, the model pools a spatial array of model V1 neurons tuned to local orientation and phase, creating
subunits that are selective for the conjunction of V1-afferent features.

III-37. Two-layer synaptic integration in pyramidal neurons
Bardia Behabadi
Bartlett Mel

BEHABADI @ USC. EDU
MEL @ USC. EDU

University of Southern California
The two-layer model of synaptic integration is an abstract spike rate model analogous to a 2-layer artificial neural
network, where the dendrites and soma constitute the first and second layers, respectively. In specific terms, the
first layer consists of a set of independent sigmoidally-thresholded dendritic subunits whose outputs are represented as currents. These currents are summed linearly at the soma and run through the axo-somatic f-I curve
to determine the cell’s output firing rate. A key assumption of the 2-layer model is that nonlinear interactions
between synapses within a dendritic subunit are “private”, that is, whatever output is supposed to be produced
by a subunit for a given pattern of inputs is always produced, and delivered to the soma, regardless of the “goings on” inside other subunits. However, somatic action potentials, which represent the combined outputs of all
subunits, back-propagate into the dendritic arbor sometimes 10’s or even 100’s of times per second. These largeamplitude voltage signals sweeping repeatedly into the dendrites act as a form of crosstalk between subunits,
and seem likely to disrupt and/or homogenize the subunit-specific voltage-based “calculations” taking place in
different dendrites at any given time. How could subunit independence be maintained under such conditions? We
used simple (abstract) and complex (compartmental) models to address this question. We found that an abstract,
rate-based, 2-layer model can predict the mean firing rate of a detailed compartmental model with remarkable
accuracy (R2>0.999, NRMSE<1%). In an analysis of the conductance, voltage, and current waveforms at various locations inside the cell during stimulation, we discovered why BPAPs leave functional compartmentalization
essentially intact (Figure 1). Our results suggest that compartmentalized 2-layer processing is a ‘natural’ feature
of pyramidal neuron dendrites.

III-38. A Population Approach to Coding and Decoding with Adapting Neurons
Richard Naud
Wulfram Gerstner

RICHARD. NAUD @ EPFL . CH
WULFRAM . GERSTNER @ EPFL . CH

École Polytechnique Fédérale de Lausanne
Theoretical explorations of adaptation in spiking neurons often fall flat due to a dependance on the infinitely
many spiking histories possible. Basic observables such as the Peri-Stimulus Time-Histogram (PSTH) and linear
filter could not be expressed as an explicit function of the model’s parameters. In the present article, we use

COSYNE 2012

175

III-39 – III-40
an approximation of the full dynamics which becomes exact in the limit of small population activity and/or weak
refractoriness. This approximation allows us to derive an expression for encoding and decoding time-dependent
stimulus in the population activity. In like manner, we derive an expression for the linear filter which shows how
high-pass and band-pass properties can arise from distinct shapes of the spike after-potential. In all cases the
approximation matches very well with direct simulations of large neuronal populations. An analytical expression
can shed light onto previously obscure processes. Here we discover that the decoding of a population of weakly
active neurons only requires two quantities: i) the instantaneous population activity and ii) an accumulation of the
past history weighted by a factor that relates to the effective spike after-potential. The results presented here can
be used to make mean-field theory models of neuron networks closer to experimental observations.

III-39. Improving neural control of a simulated arm by decoding intended future movement
Francis R Willett1
Aaron Suminski1
Andrew Fagg2
Nicholas Hatsopoulos1
1 University
2 University

FWILLETT @ UCHICAGO. EDU
ASUMINSKI @ UCHICAGO. EDU
FAGG @ CS . OU. EDU
NICHO @ UCHICAGO. EDU

of Chicago
of Oklahoma

A brain-machine interface (BMI) records neural signals in real time from a subject’s brain, interprets them as motor commands, and reroutes them to a device (e.g., a computer cursor or prosthetic arm) in order to restore the
subject’s lost motor function. Typically, a BMI that enables the control of a prosthetic arm decodes an intended
hand position or velocity from the subject and uses a controller to generate joint torques to drive the arm accordingly. Previous studies taking this approach have chosen to decode the subject’s desired arm state in the present
moment and to use it as the command signal. However, this approach causes the prosthetic arm to lag behind
the state desired by the user, as the dynamics of the arm constrain how quickly the controller can bring the arm’s
state in accordance with the commanded state. If the command signal is smoothed, by filtering the neural data
or the signal itself, the arm will lag further behind the user’s intent. To compensate for delay introduced by the
controller and/or smoothing, we used a regularized Weiner filter to decode a subject’s intended hand position
in the future at a time lead equal to the known system delay, and used this value as the command signal. In
our experiment, a monkey (Macaca mulatta) used a BMI implementing this approach to control a simulated arm
to hit targets on a screen. Results from experiments with two BMIs with different system delays (100 and 200
milliseconds) show that the monkey can make significantly straighter and faster movements when the decoder
predictively compensates for the delay. By varying the time by which we decode into the future, we also show that
performance peaks near the time of the known system delay and degrades otherwise.

III-40. Long-term Decoding Stability without Retraining for Intracortical Brain
Computer Interface
William Bishop1
Cynthia A Chestek2
Vikash Gilja2
Paul Nuyujukian2
Stephen Ryu2
Krishna Shenoy2
Byron Yu1

WBISHOP @ CS . CMU. EDU
CINDYC @ STANFORD. EDU
GILJA @ STANFORD. EDU
PAUL @ NPL . STANFORD. EDU
SEOULMAN @ STANFORD. EDU
SHENOY @ STANFORD. EDU
BYRONYU @ CMU. EDU

1 Carnegie
2 Stanford

176

Mellon University
University

COSYNE 2012

III-41 – III-42

Most current intracortical brain computer interface (BCI) systems rely on daily retraining. While this is feasible in
a lab, it is not clear the burden of daily retraining will be viable in the clinic. We therefore sought to investigate
the long-term stability of an intracortical BCI system without retraining. We recorded neural activity using a 96electrode array implanted in the motor cortex of a rhesus macaque performing center-out reaches in 7 directions
over 41 sessions spanning 48 days. One simple way to avoid retraining is to hold the decoder static from day to
day. As expected, we found that when decoding reach direction based on threshold crossings collected during
arm movement, the performance of such a static decoder was diminished compared to one which was retrained
daily. However, we found no significant decline in performance across time for this decoder, though variability
(standard deviation) from day to day was large. We then considered a second static model which allowed for a
greater dispersion of spike counts than the standard method. Mean decoding performance increased from 59.4%
to 70.0% while the standard deviation of day-to-day performance decreased from 12.1% to 7.9%. While these
results must be reproduced in a closed-loop setting, we believe such insights into the role of decoder training will
be important for the clinical translation of BCI systems.

III-41. An objective approach to learning movement-related features from local field potentials
Kelvin So
Michael Gastpar
Jose Carmena

SOKELVIN @ EECS . BERKELEY. EDU
GASTPAR @ EECS . BERKELEY. EDU
CARMENA @ EECS . BERKELEY. EDU

University of California, Berkeley
Past studies have shown that local field potentials (LFP) in the primary motor cortex (M1) encode information
related to movement, enabling the possibility of using LFP as a control signal for brain-machine interfaces (BMI).
A recent approach to incorporating LFP for BMI focused on the power of specific low frequency bands (0-10Hz and
20-40Hz), which are known to be modulated during movement execution. Conversely, Rickert et al. found multiple
other frequency bands (<4, 6-13, 63-200Hz) that contain direction related information. Given the wide range of
frequencies that appear to contain movement information, it is unclear how to determine the features most relevant
to movement from all these frequency bands. Here, we present a novel approach to extracting features from LFP
that are well modulated by movement. Specifically, this method searches for an optimal projection of the LFP
that maximizes the variance of the activity across different movement directions, while minimizing the variance
of the activity during movements in the same direction. This method objectively determines features in the LFP
that differentiate between movement directions, instead of relying on a priori assumptions about particular LFP
frequency bands. The learned features capture the aspect of the LFP that changes with movement direction but is
consistent during movements in the same direction. Our results obtained from LFP recordings in M1 of macaque
monkeys performing center-out reaching indicate that these features reflect contributions from the 0-40Hz and
120-200Hz bands of the LFP. Lastly, we demonstrate that these features can provide high accuracy in predicting
hand position during movement with a correlation coefficient of 0.7.

III-42. Synaptic input correlations and membrane potential decorrelation in
spontaneous cortical activity
Michael Graupner
Alex D Reyes

MICHAEL . GRAUPNER @ NYU. EDU
REYES @ CNS . NYU. EDU

New York University
Spiking correlations between neurons have been found in many regions of the cortex and under multiple ex-

COSYNE 2012

177

III-43
perimental conditions. Despite their important consequences for neural population coding, the origin and the
magnitude of such correlations remains a highly debated issue. A large body of extracellular data reports spiking correlations of various strengths, but how synaptic input correlations translate into membrane potential and
spike-output correlations remains largely unexplored. Using an in vitro thalamocortical slice preparation, we
perform simultaneous recordings from pairs of layer IV neurons in the auditory cortex and measure synaptic potentials/currents, membrane potentials and spiking outputs. We calculate cross-correlations between excitatory
and inhibitory inputs to investigate correlations emerging from the network topology. We furthermore evaluate
membrane potential correlations at depolarized potentials in order to study how excitation and inhibition combine and give potentially rise to spike-output correlations. Irregular, spontaneous activity is induced through high
potassium and low magnesium and calcium concentrations in the artificial cerebrospinal fluid. The impact of
space clamp and the number of presynaptic neurons projecting to the recorded cells is investigated using recurrent network simulations in combination with compartmental modeling. We find that nearby neurons receive
correlated excitatory and inhibitory input. Excitatory correlations are broader than inhibitory correlations which
hints to more presynaptic excitatory than inhibitory neurons projecting to the recorded cell pair, in agreement with
cortical architecture. We measure directly that excitation is correlated with inhibition thereby partially canceling
each other and resulting in weak membrane potential correlations between neurons. Inhibition follows excitation
with a short delay of a couple of milliseconds. Our data suggests that cortical networks are set up to partially
cancel correlations emerging from the coupling between neurons. That active decorrelation is achieved through
close tracking of excitation and inhibition. Our results provide the cellular correlate possibly leading to low spiking
correlations.

III-43. Correlations in Spatially Heterogeneous Neuronal Networks
James Trousdale
Kresimir Josic

JRTROUSD @ MATH . UH . EDU
JOSIC @ MATH . UH . EDU

University of Houston
How does network architecture together with response properties of individual neurons shape the response of a
population of cells? How are the properties of neuronal networks related to the computations they have evolved
to perform? To address these fundamental questions of systems neuroscience we have recently generalized
linear response methods previously used to analyze the population responses of specific neuronal networks.
This technique allows us to explicitly describe the correlation structure in the output of a network of spiking
neurons in terms of synaptic architecture and the response properties of the constituent cells. Our technique
also allows us to address a question that has recently received much attention: What is the magnitude and
distribution of correlations in cortex? In particular, Renart, de la Rocha et al. have shown that even in densely
connected networks, balance between excitation and inhibition can lead to a cancellation of correlations and an
asynchronous state. However, experimental evidence has so far not conclusively demonstrated whether cortical
dynamics is asynchronous. The question is complicated by recent simulations of a realistic model of the visual
area V1 that demonstrated that different layers can exhibit differing correlation patterns. We examined the impact
of spatial structure in networks on the statistics of population activity. Although earlier theoretical studies have
frequently considered spatially homogeneous networks, it is known that in cortex the probability that two cells
have a synaptic contact is dependent on physical distance. We explore the impact of these connectivity patterns
by imposing spatial profiles on the synaptic weights in recurrent networks of spiking neurons. We successfully
apply linear response methods to predict the effects altering the spatial structure of the network. Our aim is to
help describe how input and architecture determine population activity, a question central to understanding the
neural code.

178

COSYNE 2012

III-44 – III-45

III-44. Filtering and recurrent connectivity shape higher-order correlations in
retinal circuits
Andrea Barreiro1
Julijana Gjorgjieva2
Fred Rieke3
Eric Shea-Brown3

ABARREIRO @ SMU. EDU
GJORGJIEVA @ FAS . HARVARD. EDU
RIEKE @ WASHINGTON . EDU
ETSB @ WASHINGTON . EDU

1 Southern

Methodist University
University
3 University of Washington
2 Harvard

Neural circuits can, in principle, produce an enormous number of distinct multi-cell outputs—a number so large
that it could never be measured experimentally. Pairwise maximum entropy (PME) methods suggest a dramatic
simplification: the distribution of outputs of many circuits are well captured by models that rely only on activity
of single neurons and neuron pairs. Recent empirical studies find that the activity patterns of some circuits are
well described by PME models (Shlens et al., J Neurosci, 2006; 2009; Schneidman, et al., Nature, 2006), while
other circuits show significant departures (e.g. Ohiorhenuan et al., Nature, 2010). What circuit properties will
lead to success or failure of this model? In particular, what factors contribute to the remarkable success of PME
models in retinal ganglion cells (RGCs)? We study spike patterns in RGC circuits with different architectures and
inputs. When circuit parameters are matched to observed values from primate ON parasol RGCs, spike outputs
were well described by PME models across a broad range of light stimuli. Two mechanistic features contribute
to this success; the largely feedforward structure of the circuit and the temporal filtering of synaptic inputs. By
modulating filtering properties and recurrent connectivity, we find the conditions under which, all other factors
being equal, the circuit generates the greatest higher-order interactions (HOIs). The first is when bimodal light
stimuli are processed by an “integrating” temporal filter vs. a “differentiating” one. The second is when coupling
interactions are present, and are of intermediate strength. As filters and coupling differ across retinal ganglion cell
classes, this leads to a number of predictions. For example, networks of OFF-parasol cells could produce larger
HOIs than their ON counterparts due to their coupling, and networks of primate midget cells could generate HOIs
due to their distinct filtering properties.

III-45. Neuronal populations model of associative retrieval
Sandro Romani1,2
Itai Pinkoviezsky3
Alon Rubin3
Misha Tsodyks3

SANDRO. ROMANI @ GMAIL . COM
ITAIPINK @ GMAIL . COM
ALON . RUBIN @ WEIZMANN . AC. IL
MISHA @ WEIZMANN . AC. IL

1 Columbia

University
van Amsterdam
3 Weizmann Institute of Science
2 Universiteit

Voluntary memory retrieval is one of the underpinnings of human cognitive functions. ‘Free recall’ experiments,
where subjects have to repeat as many words as possible after a quick exposure, have long been used as a
simplified setting to investigate this fundamental process. Nevertheless, the mechanisms underlying voluntary
retrieval of information from memory are yet to be clarified. Interestingly, the number of retrieved items slowly
grows with the number of presented items, excluding from the candidate mechanisms short-term memory, which
has a limited fixed capacity. We study cue-less retrieval within a general theoretical framework inspired by the
hypothesis of population coding of information. Items in memory are represented by overlapping groups of neurons, whose activation is a proxy for retrieval. Once an item is retrieved, it triggers the retrieval of the next one in
an associative manner. The model dynamics can be mapped to a graph search algorithm, allowing its complete
analysis. Consistently with experimental results, we found a sub-linear scaling between the number of retrieved
items and the number of presented items. Moreover, the model predicts a non-trivial scaling for the variance of

COSYNE 2012

179

III-46 – III-47
the number of items, which we tested on existing experimental data. Due to its general nature, the model can be
readily implemented in associative neural networks. We suggest that the validity of our results extends beyond
the classic free recall paradigm.

III-46. The correlation structure induced by fluctuations in attention
Alexander Ecker1
Philipp Berens1
Andreas Tolias2
Matthias Bethge3

AECKER @ TUEBINGEN . MPG . DE
BERENS @ TUEBINGEN . MPG . DE
ATOLIAS @ CNS . BCM . EDU
MBETHGE @ TUEBINGEN . MPG . DE

1 Max

Planck Institute for Biological Cybernetics
College of Medicine
3 Max Planck Institute
2 Baylor

Attention has traditionally been associated with an increase in firing rates, reflecting a change in the gain of the
population. More recent studies also report a change in noise correlations, which is thought to reflect changes
in functional connectivity. However, since the degree of attention can vary substantially from trial to trial even
within one experimental condition, the measured correlations could actually reflect fluctuations in the attentionrelated feedback signal (gain) rather than feed-forward noise, as often assumed. To gain insights into this issue
we analytically analyzed the standard model of spatial attention, where directing attention to the receptive field
of a neuron increases its response gain. We assumed conditionally independent neurons (no noise correlations)
and asked how uncontrolled fluctuations in attention affect the correlation structure. First, we found that this
simple model of spatial attention explains the empirically measured correlation structure quite well. In addition to
a positive average level of correlations, it predicts both an increase in correlations with firing rates, as observed
in many studies, and a decrease in correlations with the difference of two neurons’ tuning functions—a structure
generally referred to as limited range correlations. Second, we asked how fluctuations in attention would affect
the accuracy of a population code, if treated as noise by a downstream readout. Based on previous theoretical
results, it would be expected that they negatively affect readout accuracy because of the limited range correlations
they induce. Surprisingly, we found that this is not the case: correlations due to random gain fluctuations do not
affect readout accuracy because their major axis is orthogonal to changes in the stimulus orientation. Our results
can be readily generalized to include feature-based attention. The model has very few free parameters and can
potentially account for a large fraction of the observed spike count (co-)variance.

III-47. Attentional effects in V1 are related to spatial gating but not to allocation of limited resources
Eyal Seidemann
Yuzhi Chen

EYAL @ MAIL . CPS . UTEXAS . EDU
CHEN @ MAIL . CPS . UTEXAS . EDU

University of Texas at Austin
Attention can modulate neural responses in sensory cortical areas and can improve behavioral performance in
detection and discrimination tasks. However, the nature and purpose of these modulations in V1 remain under
debate. To examine the precise spatiotemporal dynamics of attentional modulations at the level of neural populations in V1, and to determine their potential purpose, we used voltage-sensitive dye imaging to measure V1
responses while monkeys performed a difficult detection task under focal or distributed attention. We found that
despite improved behavioral performance under focal attention, V1 population responses at a single attended
location (under focal attention) or at one of four attended locations (under distributed attention), are indistinguishable. This finding is inconsistent with the hypothesis that an important goal of attention is to allocate limited
representational resources in V1 based on task demands. However, we found that V1 responses at all attended

180

COSYNE 2012

III-48 – III-49
locations are significantly elevated relative to actively ignored or irrelevant locations. This elevation operates on
a spatial scale larger than the stimulus evoked response, and is initiated ∼80 ms before stimulus onset. This
widespread baseline elevation is consistent with the hypothesis that an important goal of attention in V1 is to limit
the behavioral effect of task-irrelevant visual stimuli. The elevated baseline at attended locations could contribute
to this selective spatial gating by biasing competition in subsequent processing stages in favor of task-relevant
stimuli.

III-48. Probabilistic palimpsest memory: multiplicity, binding and coverage in
visual short-term memory
Loic Matthey
Paul Bays
Peter Dayan

LOIC. MATTHEY @ GATSBY. UCL . AC. UK
P. BAYS @ UCL . AC. UK
DAYAN @ GATSBY. UCL . AC. UK

University College London
According to a standard view of short term memory, there is a rather rigid set of fixed ‘slots’ that can be independently filled (Vogel 1997), with recall errors arising from items that did not fit. Recent results instead suggest a
unified shared working memory resource, with the signal/noise ratio for one item being reduced by the storage of
others (Ma 2004, Husain 2008). There are substantial experimental and theoretical results based on this latter
view, including evident sequential effects. Considering this shared working memory as a distributed code leads
to a rich set of computational problems that have not been well addressed in conventional accounts of population coding. In particular, it becomes necessary to consider multiplicity (Sahani 2003, Amari 2005), with multiple
items coexisting simultaneously in memory, overlaid on top of one another like a palimpsest (Savin 2011), and
multidimensional binding, with some dimensions of an item providing information about the others during recall.
We built a probabilistic model of a finite capacity working memory network, capable of reproducing the form of
experimental psychophysical human data, only assuming a simple palimpsest-like storage process and a normative Bayesian recall process where uncertainty is propagated. The precision of recall decayed smoothly as the
number of items increased because of interference between stored objects. Our model provided a parsimonious
account of recall precision, binding errors and temporal effects. Interestingly, it also showed a similar distribution
of errors to those that were thought to support a ‘slots’ model. At the heart of our approach was a specific proposal
for the distributed representation of objects in memory. We showed how simple representations based on single
features failed when multiple objects have to be distinguished, and how the addition of conjunctive neurons and
appropriately tiling and covering the object space could rescue them.

III-49. What does information seeking tell us about reinforcement learning?
Ethan Bromberg-Martin
Okihide Hikosaka

NEUROETHAN @ GMAIL . COM
OH @ LSR . NEI . NIH . GOV

National Eye Institute
Conventional theories of reinforcement learning explain how we choose actions to gain rewards, but we also often
choose actions to help us predict rewards. This behavior is known as information seeking (or ‘early resolution
of uncertainty’) in economics and a form of “observing behavior” in psychology. We recently found that many
neurons involved in conventional reward seeking also send signals appropriate for information seeking, suggesting that these behaviors share a common neural mechanism. Thus it is crucial to understand how theories of
reinforcement learning can be revised to produce information seeking, and what this implies about the underlying
neural computations. Two major proposals to produce information seeking are nonlinear reinforcement, in which
reinforcement from a state is a nonlinear function of its expected value, and distributional reinforcement, in which
reinforcement is influenced by additional statistics of the value distribution such as variance and entropy. It is

COSYNE 2012

181

III-50 – III-51
believed that nonlinear theories are promising, making only small changes to conventional theories and giving a
good account of existing data; while distributional theories are believed to be dubious, making predictions that
lack experimental support. Here, however, we make three theoretical advances which show that exactly the opposite is the case. We first derive methods to measure reinforcement nonlinearities, and show that nonlinear
theories can only explain existing neural and behavioral data by invoking severe nonlinearities that deviate far
from conventional theories. Furthermore, we show that nonlinear theories can only produce robust information
seeking by using a very specific family of nonlinearities, a family that was not considered in previous proposals
and which makes several unlikely predictions about behavior. Finally, we show that distributional theories escape
these limitations, and that upon careful inspection distributional theories are fully compatible with the experiments
that were previously believed to exclude them.

III-50. Corticostriatal projections mediate auditory decisions
Petr Znamenskiy1,2
Anthony Zador1
1 Cold

ZNAMENSK @ CSHL . EDU
ZADOR @ CSHL . EDU

Spring Harbor Laboratory
School of Biological Sciences

2 Watson

How does sensory information get out of the cortex to drive actions? Representations of the acoustic world are
constructed in the auditory cortex (ACx), but how these representations are used to effect decisions is largely
unknown. Here we report for the first time that the projection of the ACx to the striatum drives decisions in rats,
and that changes in corticostriatal synaptic strength may underlie learning in this task. We first developed a
novel auditory task—modeled after the random dot motion task used to study area MT—designed to exploit the
tonotopic organization of the ACx. We trained rats to discriminate low- and high-frequency “cloud-of-tones” stimuli
(overlapping 30msec pure tones distributed over three octaves) in a two-alternative choice task. Subjects were
required to report whether low or high tones were overrepresented; performance varied smoothly with stimulus
frequency. We hypothesized that the ACx drives subjects’ choices through its projection to the striatum. To
test this, we used ChR2 to selectively activate corticostriatal neurons in rats performing the task. Activation of
corticostriatal neurons biased the subjects’ choices as predicted by the neurons’ frequency tuning. These results
suggest not only that the ACx is causally involved in this task, but also that information propagates beyond the
ACx via the corticostriatal projection. To test the role of the corticostriatal synapse in encoding associations
between sounds and motor responses, we are using ChR2-evoked corticostriatal local field potentials as a proxy
for synaptic strength. Preliminary results indicate that acquisition of the task increases the strength of these
connections. Although it would be naive to suggest that we have identified the “engram” for this task, our findings
suggest a simple model in which learning the association between frequency and action occurs by strengthening
of a subset of feedforward projections.

III-51. Task set switching: dissecting ideal observer models and their approximation
Jan Drugowitsch1,2
Etienne Koechlin3

JDRUGO @ GMAIL . COM
ETIENNE . KOECHLIN @ UPMC. FR

1 INSERM
2 École

Normale Supérieure
ENS, UPMC, Paris

3 INSERM,

Learning task sets and switching between them is ubiquitous in everyday behavior. Despite this, little is known
about how efficient humans are in their use of information when performing either learning or switching. Previous
task set switching models either contained heuristic components, or were unable to reproduce important features

182

COSYNE 2012

III-52
in human behavior. Here, we address task set learning and switching from the ideal observer perspective, based
on an episodic task in which—based on noisy feedback—humans had to learn stimulus/response associations
(the task sets), and in which these associations changed over time. The ideal observer model resulting from
this task structure is similar to inference in sticky Hidden Markov Models, in which both the feedback noise and
the switching structure are inferred. When compared to a large dataset of performing humans, we found that
these humans receive on average above 90% of the optimal reward, as determined by the ideal observer model.
Furthermore, and in contrast to standard change point detection models, our model captured well important behavioral features, such as faster switching and reduced exploration when switching into previously learned task
sets. Optimal behavior requires subjects to memorize all previous observations and actions, which is clearly intractable. By limiting the model’s memory, we found that it is sufficient to remember the last 6 trials while still
featuring human-like performance. Still, even the limited memory variant required extensive MCMC sampling to
perform accurate inference. We alleviated the latter by the introduction of a variational Bayesian approximation
that significantly simplified the inference with a negligible drop in performance, thus providing a tractable explanation for how humans are able to feature the observed behavior. The use of particle filters did not provide a good
alternative explanation, as they struggled to handle the discontinuous nature of the task.

III-52. Evidence for incidental structured learning and abstraction in cognitive
reinforcement learning
Anne Collins
Michael J Frank

ANNE COLLINS @ BROWN . EDU
MICHAEL FRANK @ BROWN . EDU

Brown University
Executive functions and learning share common neural substrates essential for their expression, notably in prefrontal cortex and basal ganglia. Understanding how they interact requires studying how cognitive control facilitates learning, but also how learning provides the (potentially hidden) structure, such as abstract rules or
task-sets, needed for cognitive control. We investigate this question from four complementary angles. First, we
present a new experimental paradigm to assess whether subjects spontaneously build structure into a learning
problem when not cued to do so, as evidenced by patterns of errors and response times. Second, we develop a
new computational model inspired by non-parametric Bayesian methods, specifying how the learner might infer
hidden structure in the form of task-sets and decide whether to re-use that structure, or to create a new task set,
in new contexts. Third, we develop a neurobiologically explicit model to assess potential mechanisms of such
interactive structured learning in multiple circuits linking frontal cortex and basal ganglia. Last, we use electroencephalography to explore the neural correlates of these cognitive processes. Results showed that subjects
have a strong predilection to build structure into learning, potentially allowing them to generalize this abstract
structure in subsequent opportunities, in a way accounted for by the computational model. The neural network robustly exhibited the behavioral pattern of data predicted by the model and observed in human subjects, providing
a biologically realistic implementation of the functional model and thus providing a firm grounding for predictions
on the neural correlates implicated in the process, such as prefrontal, parietal cortex and basal ganglia. Thus,
this work exemplifies how linking of two levels of computational modeling offers the means to relate behavioral
and neural observation to further our understanding of the mechanisms of structure learning in humans.

COSYNE 2012

183

III-53 – III-54

III-53. Inactivation of rat frontal and parietal cortex during a temporal integration of evidence task
Chunyu Duan1,2
Jeffrey Erlich2,1
Timothy Hanks2,1
Bingni Brunton2,1
Carlos Brody2,1

CDUAN @ PRINCETON . EDU
JERLICH @ PRINCETON . EDU
THANKS @ PRINCETON . EDU
BWEN @ PRINCETON . EDU
BRODY @ PRINCETON . EDU

1 HHMI
2 Princeton

University

Decision-making has been modeled as a process of noisy integration of evidence over time until reaching a
decision-commitment bound. To study the neural mechanisms underlying this process in rats, we used a task
previously developed in our lab, the Poisson Clicks Accumulation task. Rats were trained to hear two independent
Poisson click trains, one from a left speaker and one from a right speaker, and at the end of the stimulus, orient
towards the one that played the greater number of clicks. Trial difficulty was controlled by varying the difference
between the two Poisson rates. Stimulus durations were controlled by the experimenter. Perfect decisions require
counting clicks over time, to compare the two totals. We investigated the role of two cortical areas, the posterior parietal cortex (PPC) and the frontal orienting fields (FOF), both of which have been implicated in sensory
integration and planning orienting movements. We reversibly inactivated these areas by applying the GABAA receptor agonist muscimol while animals performed the task. Unilateral inactivation of the FOF profoundly impaired
orienting contralateral to the infusion site, in a manner independent of trial difficulty. Unilateral inactivation of the
PPC also impaired contralateral orienting, but to a lesser degree than FOF inactivation, and the degree of impairment was greater for hard than for easy trials. These data show that both areas are involved in performance
of the task, and are consistent with a feedforward model in which graded evidence integration is performed in
the PPC while final motor planning is performed in the FOF. We are further testing and developing this model by
performing bilateral inactivations, as well as simultaneous inactivation of both the FOF and the PPC. Our data
will provide constraints on the functional relationship and differential roles of the PPC and the FOF in perceptual
decision-making.

III-54. Coherent network-wide fluctuations of neural activity in the PFC during
behavioral uncertainty
Mattias Karlsson
Dougal Tervo
Alla Y Karpova

KARLSSONM @ JANELIA . HHMI . ORG
GOWANTERVO @ GMAIL . COM
KARPOVAA @ JANELIA . HHMI . ORG

HHMI, Janelia Farm Research Campus
When unpredictable changes occur in the environment, decisions based on outdated information can become unprofitable or even hazardous. During these moments of uncertainty, animals tend to rapidly adjust their behavioral
strategy from one based on experience (exploitation) to one that re-evaluates the possible options (exploration)
(Quilodran et al. 2008, Daw et al. 2006). It has been proposed that widespread ‘resets’ in information processing
might be beneficial during such behavioral transitions, with new information taking precedence over older, potentially outdated representations (Gers et al. 2000, Bouret & Sara 2005, Dayan & Yu 2006, Nassar et al. 2010).
Such resets would be most likely to occur in areas of the brain that monitor task performance and guide future
decisions, such as the prefrontal cortex. It has been demonstrated that single-cell activity in the prefrontal cortex
can fluctuate rapidly as shifts occur in the state of the environment (Durstewitz et al. 2010, Hayden et al, 2011)
or the internal state of the animal (Critchley et al. 2001, Huettel et al. 2005, Kepecs et al. 2010, Kiani & Shadlen
2009) but neurophysiological evidence for widespread, coordinated dynamics indicative of possible network resets
is missing. We present evidence that in the rodent anterior cingulate cortex, an area known to be important for
flexible behavior, fluctuations in activity often happen in unison across a large fraction of simultaneously recorded

184

COSYNE 2012

III-55 – III-56
cells. Using a novel task that provides a behavioral readout for periods of uncertainty, we find that such network
fluctuations preferentially occur as animals transition from a strategy of exploitation to less-certain or exploratory
behavior. These observations suggest that neurons in the prefrontal cortex can modulate task representations in
unison when it is computationally advantageous.

III-55. Dissecting the Contributions of Sensory and Category Uncertainty in
Perceptual Decision-Making
Andre Mendonca1
Maria Vicente1
Alexandre Pouget2
Zachary Mainen1

ANDRE . MENDONCA @ NEURO. FCHAMPALIMAUD. ORG
MARIA . VICENTE @ NEURO. FCHAMPALIMAUD. ORG
ALEX . POUGET @ GMAIL . COM
ZMAINEN @ NEURO. FCHAMPALIMAUD. ORG

1 Champalimaud
2 University

Neuroscience Programme
of Rochester

It is commonly assumed that perceptual decisions are difficult because of sensory uncertainty, i.e. noise in
the stimulus or stimulus transduction process. This phenomenon can be captured using drift-diffusion models
(DDMs). However, some decisions are difficult even given unlimited sampling time, and it is less clear what limits
performance in such cases. Here, we studied this problem using a combination of psychophysical experiments
in rats and computational modeling. We compared two closely related tasks: odor mixture categorization and
odor detection. We found that, as problem difficulty was increased in the two tasks, for the same change in
accuracy, the change in reaction time was substantially larger in detection than in categorization (41% vs. 13%
increase). This result suggested that difficulty in odor categorization results from non-sensory variability that is not
subject to integration. We first explored this hypothesis by introducing a new source of noise in a standard DDM:
in addition to the normal fast, within-trial noise (i.e. within the diffusion process) we added a slow, trial-to-trial
noise, represented by drift rate variance. We found that fast noise plays the main role in odor detectability, while
slow noise is critical for categorization. We hypothesized that one source of drift rate variance may be on-going
changes in stimulus-response mapping caused by reinforcement learning. To test this idea, we used an extended
delta-rule learning model (DRM) to examine the effect of trial-by-trial updating on performance accuracy. We show
that the relative importance of different noise sources in this model shifts depending on the task. While detection
performance is limited by stimulus noise, categorization performance is dominated by noise in weight updating.
These results provide an account of which sources of noise will dominate in different kinds of perceptual decisions
and explain why different tasks will show different susceptibility to integration.

III-56. Optimal integration of multisensory event streams in rats and humans
David Raposo1,2
John Sheppard2,3
Paul Schrater4
Anne Churchland2

DRAPOSO @ CSHL . EDU
SHEPPARD @ CSHL . EDU
SCHRATER @ UMN . EDU
CHURCHLAND @ CSHL . EDU

1 Champalimaud

Neuroscience Programme
Spring Harbor Laboratory
3 Watson School of Biological Sciences
4 University of Minnesota
2 Cold

In recent years, studies have examined how subjects combine evidence across time within a single sensory
modality, and separately how subjects combine static evidence across modalities. We trained rats and humans
on a decision task that invited subjects to accumulate sensory information both over time and across modalities.
Subjects were trained to discriminate lower- from higher-rate stimuli that consisted of auditory or visual pulses

COSYNE 2012

185

III-57 – III-58
separated by short or long inter-event intervals (IEIs). Varying the proportion of long relative to short IEIs in a
1-second trial allowed us to generate audiovisual stimuli over a range of difficulties. Here, we report three novel
findings: (1) We now extend to rats our previous finding that when the reliabilities (i.e., signal-to-noise ratios) of
auditory and visual stimuli are adjusted to obtain equal performance for both modalities, subjects’ performance on
multisensory trials is near the level predicted by statistically optimal (i.e., maximum likelihood) cue combination;
(2) When we systematically vary the relative reliability of auditory and visual stimuli and present trials in which the
two modalities provide conflicting information, human subjects optimally weight sensory information accumulated
over time in proportion to the relative reliabilities of each event stream; and (3) By modeling decision-making using
an evidence accumulation framework, subjects’ multisensory performance can be predicted by fitting weights to
either modality based upon subjects’ single sensory performance. Together, these findings indicate that subjects
are sensitive to cue reliability when integrating sensory information across time and modalities. Furthermore, the
ability to optimally integrate information across time and sensory modalities appears to be a general feature of the
mammalian brain that is not restricted to a particular species.

III-57. Dissociable Influences of D1 and D2-mediated Frontal Eye Field Activity
on Target Selection
Alireza Soltani1,2
Behrad Noudoost1
Tirin Moore1
1 Stanford

ASOLTANI @ STANFORD. EDU
BEHRAD @ STANFORD. EDU
TIRIN @ STANFORD. EDU

University

2 HHMI

Recent evidence suggests an important role for dopamine in decision making. We explored this possibility by
testing the influence of D1 and D2 receptormediated activity in the frontal eye field(FEF) on the selection of visual
targets for saccades. We manipulated dopaminergic activity in the FEF of monkeys performing a saccadic doubletarget, choice task. By systematically altering the onset asynchrony of the two targets(TOA) we could measure
the bias in target selection toward either target. Next, we manipulated D1R and D2Rmediated FEF activity with
local, intracortical infusions of the D1R antagonist SCH and the D2R agonist quinpirole. The effects of these
manipulations were fourfold. First, we found that the D1R and D2R manipulations both increased the selection of
Tin targets, and to a similar extent; however, neither manipulation changed the sensitivity of the monkeys’ choices
to the TOA. Second, the amount of shift in the psychometric function with the D1R manipulation was inversely
correlated with the baseline sensitivity to TOA.The D2R manipulation showed an opposite trend. Third, we found
that the probability of repeated choices on consecutive trials was reduced by the D1R manipulation, yet it was
enhanced by the D2R manipulation. Fourth, we examined how selection on a given trial depended on the choice
and TOA on the previous trial and found that relative to the D2R manipulation, the D1R manipulation decreased
the preference for targets that appeared first on the previous trial if the target was previously selected. Our results
reveal stark differences between the influence of D1R and D2Rmediated activity on target selection and how it is
adjusted based on past outcomes.

III-58. Joint probability of independent events is consistent with weighted
combination of log probabilities
James Tee
Hang Zhang
Laurence Maloney

JAMES . TEE @ GMAIL . COM
HANG . ZHANG @ NYU. EDU
LTM 1@ NYU. EDU

New York University
In standard probability theory, the conjunctive probability of two independent events is the product of the proba-

186

COSYNE 2012

III-59 – III-60
bilities of the events: P(A and B)=P(A)P(B). Humans distort probability in many tasks, including conjunction. We
examined whether human performance in a conjunction task could be predicted by standard probability distortion models. During each trial, a subject chose between a single roulette wheel and a pair of roulette wheels,
displayed on a computer screen. After the subject has chosen, all three wheels were spun simultaneously at different speeds. If the subject chose the single wheel, he won a monetary reward precisely when the wheel stopped
with the needle pointing at the orange zone. If the subject chose the pair, he won only when both wheels in the
pair stopped in orange zones. Design: There were 12 conditions, each corresponding to a choice of probabilities
P1, P2. Across trials, the size of the orange zone of the single wheel was varied by an adaptive procedure to
measure the Pe of the single wheel that the subject considered equivalent to each P1, P2 pair. Ten naïve subjects
completed the experiment. For all subjects, we rejected the standard model Pe=P1P2. We considered several
distortion functions w(p) and found that subjects treated the larger in each P1, P2 pair differently from the smaller.
We relabeled P1, P2 as P+, P- where P+ is greater or equal to P-, and discovered that data fitted the model:
log(Pe)=(alpha)log(P+)+(beta)log(P-) with weights alpha and beta. We rejected the hypothesis alpha=beta for
each subject individually: subjects assigned different weights to larger and smaller probabilities. The model is
readily interpreted neurally but the differential weighting of large and small probabilities is not previously reported.

III-59. Representation of multiple stimuli in the macaque middle face patch.
Akinori Ebihara
Winrich Freiwald

AEBIHARA @ ROCKEFELLER . EDU
WFREIWALD @ ROCKEFELLER . EDU

The Rockefeller University
Face recognition is mediated by specialized brain regions. In the macaque temporal lobe, face selective neurons
are aggregated into face selective regions that are tightly interconnected. This functional organization allows for
a systematic study of the neural mechanisms of face recognition. Neurons within the face patches have large
receptive fields. Consequently, in a natural scene, these large receptive fields will frequently encompass multiple
objects. Thus a question central to theories of object recognition arises: how is the representation of a face
affected by the presence of other objects? Two psychophysical phenomena lead to opposite predictions: faces
have been observed to pop out amongst non-face objects. Thus face representation may not be compromised
in the presence of clutter. On the other hand, the “crowding effect”, a reduced discriminability of an object in
clutter, affects faces, too. Thus face representations may be impaired by clutter. To address this question, we
conducted electrophysiological recordings in the middle face patch MF. For each neuron, the most preferred face
stimulus was selected and the receptive field was mapped. Then, the preferred face was presented together
with distracter stimuli in several configurations: distractor category (face or non-face), number and spacing were
varied to study if and how this would change the representations of the preferred face. We found that a small
proportion of neurons in MF showed responses that were largely uncompromised by the presence of distracters,
thus potentially providing a correlate of the face pop-out effect. However, the large majority of MF neurons reduced
their response to the preferred face stimulus with increasing number of distracters and with increased proximity of
distracters. Effects were stronger for face distracters than object distracters. These three findings are consistent
with the psychophysics of crowding and might thus represent a neurophysiological correlate of crowding.

III-60. A neural network model of the primate visuo-motor system
Christopher Kanan
Garrison Cottrell

CHRISKANAN @ GMAIL . COM
GARY @ ENG . UCSD. EDU

University of California, San Diego
Most models of the primate visual system fail to incorporate eye movements. Humans make about three saccades
per second, and this is an integral part of how the visual system works. Although binary well separated categoriza-

COSYNE 2012

187

III-61
tion judgments can be made in a single fixation (e.g., Fabre-Thorpe, Ghislaine, & Thorpe, 1998), harder decisions
involving many categories may demand many fixations (e.g., Holm, Eriksson, & Andersson, 2008). Moreover,
learning new categories requires multiple fixations (e.g., Henderson, Williams, & Falk, 2005). A few simple neural
network models of the primate visuo-motor system exist (e.g., Lo & Wang, 2006), but they cannot be used with
actual images, limiting their utility for making predictions about cognitive processes and behavior when viewing
images.
We created a recurrent probabilistic neural network model that uses a simulated retina to sample its environment.
To learn visual features for our retina model, we use independent component analysis (ICA) to acquire sparse V1like filters from natural images. These features are non-linearly weighted by how rarely they occur in the natural
environment (Shan & Cottrell, 2008), with rarer features given greater weight. This is followed by spatial pooling
of the visual features over multiple localized regions of the current visual field.
The features acquired in a fixation are compared to compressed object memories using a probabilistic neural
network (PNN; Specht, 1990). Unlike the standard PNN model, we have incorporated a form of competitive
divisive normalization, which significantly improves performance. A recurrent multinomial logit network is used to
make the final prediction, with the recurrent connections used to retain information from fixations across time. A
saccade controller determines where to look next.
To assess our model’s predictive capability, we evaluate it on computer vision datasets from several domains, including objects (Caltech-256), flowers (Oxford Flowers-102), and birds (CUB-200). Our preliminary results exhibit
performance comparable to recent methods in computer vision. We are currently investigating improvements to
the model, including training the saccade controller using reinforcement learning, and we intend to compare the
model’s predictions with human eye movements.

III-61. Short-Term Plasticity Optimizes Synaptic Information Transmission
Ziv Rotman
Panyue Deng
Vitaly Klyachko

ZIV. ROTMAN @ WUSTL . EDU
PYDENG @ WUSTL . EDU
KLYACHKO @ WUSTL . EDU

Washington University
Short-term plasticity (STP) is widely believed to play important roles in information processing. This major function of STP has recently been challenged, however, by several computational studies indicating that transmission
of information by dynamic synapses is broadband, i.e. frequency independent. Here we developed an analytical
approach to quantify time- and rate-dependent synaptic information transfer during arbitrary spike trains using a
realistic model of synaptic dynamics in excitatory hippocampal synapses. We found that STP indeed increases
information transfer in a wide range of input rates, which corresponds well to the naturally occurring spike frequencies at these synapses. This increased information transfer is observed both during Poisson-distributed spike
trains with a constant rate as well as during naturalistic spike trains recorded in hippocampal place cells in actively
exploring rodents. Interestingly, we found that presence of STP in low-release probability excitatory synapses
leads to optimization of information transfer specifically for short high-frequency bursts, which are indeed commonly observed in many excitatory hippocampal neurons. In contrast, more reliable high-release probability
synapses that express dominant short-term depression are predicted not to have optimal information transmission for spike bursts, but rather for single spikes. This prediction is verified in inhibitory hippocampal synapses
that exhibit dominant depression and fits well with the observation that inhibitory hippocampal interneurons do
not commonly fire spike bursts. We conclude that STP indeed contributes significantly to information transfer and
may serve to maximize information transfer for specific firing patterns of the corresponding neurons. .

188

COSYNE 2012

III-62 – III-63

III-62. Stochastic short term depression imposes a frequency-dependent filter
on information transmission
Robert Rosenbaum
Jonathan Rubin
Brent Doiron

ROBERTR @ PITT. EDU
JONRUBIN @ PITT. EDU
BDOIRON @ PITT. EDU

University of Pittsburgh
Synapses act as information gates in neuronal networks. Hence, a comprehensive description of neural coding
requires an understanding of synaptic dynamics and their impact on signal and information transfer. Recent theoretical studies show that a widely used deterministic model of synapses exhibiting short term synaptic depression
(STD) transmits information encoded at all frequencies equally. However, vesicle release and uptake dynamics
responsible for STD are known to be stochastic. We use stochastic calculus techniques to derive a compact
description of the filtering properties of a deterministic and a detailed stochastic model of STD. Taking synaptic
stochasticity into account imposes a frequency dependent filter on information transfer. Using parameters consistent with experimental studies of cortical synapses, we show that a population of presynaptic cortical neurons with
stochastically depressing synapses transmits information optimally at frequencies higher than 10Hz. In addition,
the postsynaptic conductance induced by these neurons exhibits a power spectrum that is peaked within the beta
frequency band even when the presynaptic inputs have a white or broadband power spectrum. Our results have
important and general implications for coding in neuronal networks.

III-63. Optimizing online learning capacity in a biologically-inspired memory
structure
Xundong Wu1
DJ Strouse2,1
Bartlett Mel1
1 University
2 University

WUXUNDONG @ GMAIL . COM
DANIELJSTROUSE @ GMAIL . COM
MEL @ USC. EDU

of Southern California
of Cambridge

To function in a complex world, our brains must somehow stream our experiences into memory in real time as
they occur. An “online” memory of this kind must form durable memory traces based on a single exposure to
each incoming pattern, while preserving older experiences as long as possible. Using computer models and
mathematical analysis, we studied the online learning capabilities of a biologically inspired memory, with the
goal to understand how the properties of neurons, dendrites, and synapses interact to determine online storage
capacity. A key assumption was that dendrites, rather than whole neurons, are the main representational units
used to encode learned information. In previous work (Wu and Mel, 2009), we focused on synaptic plasticity rules
and their impact on online storage capacity. In this work, given that learning operates at the level of dendrites, we
set out to identify the factors that determine optimal dendrite “size” (i.e. the number of synapses on each dendrite).
We show that capacity is maximized when dendrites are of “medium” size, that is, when each dendrite contains
a few hundred synapses rather than 10’s or 1000’s of synapses. We show why both short and long dendrites
suffer from severe capacity costs: long dendrites lead to wasteful over-representation, while short dendrites suffer
from greater susceptibility to noise as well as a previously undescribed problem of “dendrite availability”. We also
studied the relationship between optimal dendrite size and properties of the input patterns: increased pattern
density and noise both reduced capacity, but denser patterns push for shorter dendrites while noisier patterns
push for longer dendrites. Our results can help clarify which morphological changes that occur with aging, stress,
neurological disorders, and mental retardation are likely to be most detrimental to online recognition memory, and
why. Funding provided by NSF CRCNS grant no. IIS-0613583.

COSYNE 2012

189

III-64 – III-65

III-64. Prediction error signals in ACC are scaled according to rational adjustments of learning
Matthew Nassar
Joshua Gold

MATTNASSAR @ GMAIL . COM
JIGOLD @ MAIL . MED. UPENN . EDU

University of Pennsylvania
Behavior often depends on the ability to update beliefs according to new information. Optimization of this process
often requires preferential use of information occurring after likely environmental changes. Here we examined
the role of the anterior cingulate cortex (ACC) in this process by measuring behavior in two rhesus monkeys and
single-unit activity in one monkey performing a ten-alternative choice task that included both static fluctuations
(noise) and abrupt changes (change-points) in the identity of the rewarded target. Subject performance was consistent with an optimal-inference model: they tended to switch choice targets more frequently after errors, most
notably for errors that were unlikely to result from noise or that occurred shortly after a change-point. Moreover, they exhibited information-seeking behaviors during the feedback interval that revealed the extent to which
feedback would be incorporated into future choices. In particular, the fraction of time searching through target
locations and the latency to begin this search process (measured after the rewarded target was revealed visually
but before juice delivery) reflected rational adjustments of influence, as reflected in subsequent choice behavior.
We recorded the activity of 47 single units in the ACC of one monkey performing the task. Consistent with previous studies, we found units that responded preferentially to either reward or error feedback. To examine how
adjustments of influence might affect these responses, we computed an error-response metric specific to each
neuron that reflected how much the neural response on a given trial resembled that of the average error trial.
This metric was larger for error trials and smaller for correct trials in which the monkey displayed more extreme
information-seeking behaviors (e.g. shorter latency to search or larger search fraction), suggesting that ACC neurons scale reward prediction error signals according to the extent that those signals should be incorporated into
future beliefs.

III-65. A high-performance, robust brain-machine interface without retraining
Paul Nuyujukian
Jonathan Kao
Joline Fan
Sergey Stavisky
Stephen Ryu
Krishna Shenoy

PAUL @ NPL . STANFORD. EDU
JCYKAO @ STANFORD. EDU
JMFAN @ STANFORD. EDU
SSTAVISK @ STANFORD. EDU
SEOULMAN @ STANFORD. EDU
SHENOY @ STANFORD. EDU

Stanford University
Brain-machine interfaces (BMIs) translate neural activity into control signals for prosthetic systems, such as computer cursors and robotic arms. BMIs strive to offer people with movement disabilities greater interaction with the
world. Despite compelling proof-of-concept demonstrations, barriers to translation still remain (Ryu & Shenoy,
Neurosurgical Focus, 2009). One such barrier is that without frequent decoder retraining and recalibration by expert technicians, system performance becomes unusably poor over time. We present here a robust BMI that does
not require any retraining and can sustain high performance for up to a month. We trained a rhesus macaque
(Monkey J) implanted with two 96-channel electrode arrays in M1/PMd to perform a 2D free-paced randomized
grid keyboard selection task for a juice reward. A BMI decoder was built using spike threshold crossings (4.5xRMS) utilizing the ReFIT-KF algorithm (Gilja et al., COSYNE, 2010), which then continuously controlled the
on-screen cursor. The decoder’s parameters were held constant for thirty consecutive days, and the monkey
performed the BMI grid task for thousands of trials each day until satiated. Monkey J successfully achieved high
performance, regularly sustaining 3-4 bits per second across hours. Over the course of the month long experiment, nearly 900 kilobits in total were communicated, comparable to the content in a short novel. These results
were enabled by the inherent stability of spike threshold crossings across weeks and the high performance offered

190

COSYNE 2012

III-66 – III-67
by the ReFIT-KF algorithm. These findings demonstrate that without retraining or intervention, a high performing
BMI can transmit significant, meaningful information over several weeks. As frequent retraining is untenable for
long-term patient use, such robust performance is crucial for the successful translation of BMIs and should further
increase their clinical viability.

III-66. Cerebellar granule cell activity during behavior: dynamics in light of
the adaptive filter model
Sherika Sylvester1
Kayvon Daie1
Melanie Lee1
Mark Goldman2
Emre Aksay1
1 Cornell

SJS 2006@ MED. CORNELL . EDU
KPD 7@ CORNELL . EDU
MEL 2010@ MED. CORNELL . EDU
MSGOLDMAN @ UCDAVIS . EDU
EMA 2004@ MED. CORNELL . EDU

University
of California, Davis

2 University

For nearly forty years, the Marr-Albus-Ito model of the cerebellum has provided the dominant framework for understanding cerebellar control of motor learning. In this model and subsequent adaptive-filter models of cerebellar
function, the cerebellar granule layer is proposed to separate sensory inputs into a large array of basis functions
that can be reinforced by error signals during learning. However, direct tests of the role of granule cells during
behavior have been prohibitive due to the miniscule size and close packing of these cells. Here, we overcome this
barrier by using two-photon calcium imaging to monitor the activity of GFP-positive granule neurons during the
horizontal optokinetic reflex. To quantify the activity of granule cells, the fluorescence time series of each neuron is
averaged across optokinetic stimulus cycles and regressed against a linear model incorporating velocity, position,
and calcium-buffering components. In total, we examined 44 GFP-positive granule neurons whose activity was
well described by model fits (rˆ2 ≥ 0.7). 89% carried signals that were dominated by stimulus velocity (velocity/position: 3.3±2.9). Of these neurons, a third exhibited activity that increased with stimulus velocity irrespective
of direction, while the remainder exhibited direction-selective responses. Analysis of the relative sensitivity to leftward vs. rightward directed stimulation as a function of pair-wise distance suggests neighboring granule neurons
encode similar information, and direction-specific velocity neurons tend to be more caudal in the zebrafish cerebellum. To describe the dynamics of granule activity, drifts following cessation of stimuli were quantified by fitting
the average fluorescence response to a model of the underlying firing rate. The approximated time constants
from this study ranged from 5 msec to 6 sec, consistent with the heterogeneity of dynamics hypothesized to be a
feature of the granule layer in the adaptive-filter model (n=17 direction-selective neurons).

III-67. Action valuation in multi-effector decision-making
Seth Madlon-Kay
Bijan Pesaran
Nathaniel Daw

SETH . MADLONKAY @ NYU. EDU
BP 31@ NYU. EDU
NDD 204@ NYU. EDU

New York University
Models of action selection posit parietal and motor areas containing effector-specific maps that represent values
of available motor actions. Effector-specific maps, however, cannot represent the values of actions that involve
multiple effectors acting in coordination, so the mechanisms by which values of conjoint movements are represented and used to guide effectors are unknown. One possibility is that effector-nonspecific valuation areas step
in to guide action selection via connectivity with motor or motor-planning regions. We investigated multieffector
decision-making using model-based fMRI, exploiting the contralateral organization of hand control to contrast
choices executed with coordinated bimanual versus unimanual hand movements. Subjects (N=20) performed a

COSYNE 2012

191

III-68 – III-69
four-armed bandit task, alternating between selecting choices by either pressing one of two buttons on the left or
right hand or, in the bimanual condition, simultaneously pressing one button with each hand. Using trial-by-trial
estimates of action values derived from a reinforcement learning model fit to choice behavior, we found at choice
time activity correlating with chosen option value in the bilateral mid-cingulate cortex, extending into the supplementary motor area (p<0.01, cluster-size corrected). Examining the cingulate ROI for effector specificity, we found
a hand-by-hemisphere interaction indicating stronger value representation for actions by the contralateral hand
(p<0.05). The medial PFC also showed value-related activity (p<0.05, small-volume cluster-size corrected), but
unlike the mid-cingulate, no effector specificity (p>0.5). Testing whether the mPFC influenced motor areas more
during coordinated movements, we performed a psycho-physiological interaction seeking areas showing higher
mPFC connectivity during bimanual than right or left-handed actions, and identified an area of mid-cingulate cortex (p<0.05, small-volume peak corrected). These results suggest that while effector-specific value maps may
drive simpler actions, complex actions involving coordination between effectors implicate top-down control from
frontal areas where value is represented over options rather than effector-specific actions.

III-68. Internal metabolic state determines human motor control strategies
Scott V Taylor
Aldo Faisal

S . TAYLOR 10@ IMPERIAL . AC. UK
ALDO. FAISAL @ IMPERIAL . AC. UK

Imperial College London
Computational motor control successfully explains human motor coordination in a principled manner by deriving
optimal control policies from minimization of cost functions (e.g. Todorov&Jordan,2002). However, little is known
about the neuronal implementation of the controllers and their cost functions (Scott et al.,2004). Two possible
cost functions stand out as being biologically principled: signal-dependent motor noise (Harris&Wolpert,1998)
and metabolic cost. These are evolutionarily sensible; maximizing biological fitness by minimizing task-relevant
variability and energetic expenditure. We previously found that the optimal activations of the 6 muscles involved
in planar reaching movements differ whether noise or energy consumption are considered, due to their muscle
fibre compositions and size (Taylor&Faisal, 2011). Here we test the hypothesis that the internal metabolic state
produces trade-offs between energy and noise in motor planning. Ten right handed participants performed centerout reaching tasks under two different metabolic regimes. Participants acted in a low-friction haptic-virtual-reality
setup executing reaches to 8 targets 15 cm away (N=800 trajectories). Each subject was tested during the
morning on two separate days: 5 subjects fasted from 8pm the evening before the first session and followed their
normal nutritional routine for the second session (vice versa for the other subjects, counterbalancing potential
learning effects). We find that reaching trajectories changed significantly between high and low metabolic regimes.
Controls independent of the metabolic state or velocity profiles showed no significant change. We reconstructed
a weighted energy-noise cost function using inverse nonlinear optimal control by fitting the trajectories for both
metabolic regimes. As predicted, the energy cost function was consistently dominant in the low metabolic regime.
Our findings suggest a change in motor coordination strategies based on internal metabolic state. To our best
knowledge this is the first demonstration that internal metabolic state affects neuronal computation strategies.

III-69. A stable, long-range motor pattern in the songbird brain
Jeffrey Markowitz
Timothy Gardner

JMARKOW @ CNS . BU. EDU
TIMOTHYG @ BU. EDU

Boston University
Modern neuroscience has uncovered a wide range of machinery for cellular plasticity that underlies the neuron’s
ability to change. In parallel, an ancient question has become more pressing: how are memories maintained
for a lifetime given the plasticity of single neurons? In vertebrate motor learning, a related question is whether

192

COSYNE 2012

III-70
learned neural sequences are stable. A musician may activate muscles in a fast sequence with a remarkable
stereotypy that persists for years. However, little is known about the stability of the underlying neural patterns
over these time scales. Do the same microscopic (i.e. single cell) or mesoscopic (i.e. multi-unit) patterns of
neural activity persist across time at central levels of motor control, or does the brain drift among “degenerate”
states that produce the same effective muscle output? The answer to this question will vary depending on the
system considered. We continuously monitored neural activity from the cortical pre-motor nucleus HVC in awakebehaving zebra finches both in bilateral and unilateral implants, using minimally invasive carbon-fiber electrodes.
The recordings reveal a detailed pattern of neural activity stereotyped on a 5 ms time scale that is synchronous
between the two hemispheres. These stereotyped patterns persist for months. Furthermore, after nerve damage
to the syrinx (vocal organ), we find evidence that central motor patterns remain unchanged for a minimum of one
month, despite disruption of the acoustic form of the song. We conclude that the neural correlates of song include
both precise single unit firing and stable mesoscopic, patterns. Here we describe progress towards understanding
the relationship between these two levels of motor representation.

III-70. Are grid-cell responses very low-dimensional?
KiJung Yoon1
Caswell Barry2
Michael A Buice1
Neil Burgess2
Ila Fiete1
1 University
2 University

KIJUNG . YOON @ UTEXAS . EDU
CASWELL . BARRY @ UCL . AC. UK
MABUICE @ MAIL . CLM . UTEXAS . EDU
N . BURGESS @ UCL . AC. UK
ILAFIETE @ MAIL . CLM . UTEXAS . EDU

of Texas at Austin
College London

What mechanisms could underlie grid cell activity in rodents is the subject of debate, with different models positing starkly divergent neural architectures and dynamics. One model class is based on the conversion of cellular
temporal oscillations into spatially periodic responses, while another is based on strong lateral network connectivity that leads to low-dimensional periodic pattern formation in the neural population, which is then converted
into periodic spatial responses. Despite these differences, current analyses of experimental data have not ruled
out either model. We examine spikes from multiple simultaneously recorded grid cells, with the aim of elucidating
the dynamics underlying grid cell activity. We demonstrate evidence of a 2-dimensional continuous attractor in
the response of grid cells to animal location. The responses of grid cells with similar spatial period are identical
(upto measurement uncertainty), differing from each other along only 2 dimensions of their response, through
translations of their preferred spatial phases. The relationships between grid cell responses – specifically, their
relative spatial phases – remain absolutely stable (upto measurement error) over time, even if the responses themselves do not. Relative phases remain absolutely stable when the grids are significantly deformed by anisotropic
stretching in response to a rapid resizing of the environment, as well as when the grids uniformly expand in novel
environments. The stabilization of relative phase during dramatic changes in single-cell responses cannot be
ascribed to input from external cues or from the hippocampus, because we ascertain that relative phases are
stable even when these inputs are not. The findings together provide unequivocal support for the hypothesis that
the brain computes using low-dimensional continuous attractors. Finally, we assess the implications for proposed
mechanisms of grid cell activity, and show that the data specifically support the pattern-forming recurrent network
models.

COSYNE 2012

193

III-71 – III-72

III-71. Scale-dependence of orientation statistics in natural scenes
Tatyana Sharpee
Anirvan S Nandy
John Reynolds

SHARPEE @ SALK . EDU
NANDY @ SALK . EDU
REYNOLDS @ SALK . EDU

Salk Institute for Biological Studies
Vision evolved to extract information from natural scenes replete with contour and texture elements. However,
contour-integration and texture-processing have traditionally been treated as separate phenomena. Here we show
that these two universal types of information can be extracted from natural scenes, simply by varying the ratio of
two scales of processing: (a) edge-extraction and (b) orientation sampling. We analyzed pair-wise statistics of
more than 106 image patches taken from natural scenes (van Hateren & van der Schaaf, 1998). We found
that contour-like smooth continuation patterns emerge when orientations are sampled at a spatial scale that
matches the scale at which the edges were extracted. In contrast, texture-like iso-orientation patterns emerge
when orientations are sampled at coarser scales. The relevant scales of processing accord with the dimensions
of receptive Uields observed in the geniculostriate pathway (Alonso et al., 2001), suggesting that natural selection
has endowed the visual system with the capacity to extract contour and texture from natural scenes via a single
mechanism. This leads to testable hypotheses about the relationship between the receptive Uield sizes of visual
cortical neurons and the degree to which they exhibit distinct patterns of anatomical connectivity.

III-72. Population decoding algorithms for change detection and discrimination
Nicholas Price1
Richard Born2
1 Monash
2 Harvard

NICHOLAS . PRICE @ MONASH . EDU
RICHARD BORN @ HMS . HARVARD. EDU

University
Medical School

Over what timescales is neuronal activity informative about both constant, and changing, stimuli? To explore this
issue, we recorded from MT and MST neurons in two macaque monkeys performing a task requiring the detection
and discrimination of unpredictable speed changes. Previously, we have reported that on timescales of tens of
milliseconds, the activity of single neurons encodes the sign of the speed change and can be decoded to predict
the animals’ behavioral judgments. Here, we examine maximum-likelihood methods of decoding the responses
of a population of neurons in order to predict the precise timing of speed changes. We focus on how the raw
spike trains of each neuron in the population might be filtered, or pre-processed, to facilitate change detection
and discrimination on behaviorally relevant timescales. For example, spike trains might be low pass filtered, or
converted to an instantaneous spiking rate, or spike-rate derivative. The most successful decoder, which had
similar performance to the animal’s behavior, was designed to simultaneously categorize recent speed variations
using two independent change detectors: faster versus not faster, and slower versus not slower. The decoder
was unable to extract information about veridical speed throughout a trial, and only performed at levels better than
chance if the raw spiking times were pre-processed to give a spike rate derivative. This derivative was formed by
taking the difference between spike trains convolved with exponential functions with short and long time constants.
The optimal prefilter time constants were 20 ms and 120 ms, with these times critically affecting discrimination
performance, the rate of false detections (noise susceptibility), and the timing of when changes were correctly
detected. Our results demonstrate that stimulus changes, but not veridical stimulus properties, can be reliably
decoded from short (<100 ms) timescales of activity across a population of MT/MST neurons.

194

COSYNE 2012

III-73 – III-74

III-73. Adaptive Gating of Information Flow to Cortex
He Zheng1
Douglas Ollerenshaw1,2
Bilal Bari1
Qi Wang1
Garrett Stanley1

NEURON @ GATECH . EDU
D. OLLERENSHAW @ GATECH . EDU
B . BARI @ GATECH . EDU
QI . WANG @ BME . GATECH . EDU
GARRETT. STANLEY @ BME . GATECH . EDU

1 Georgia
2 Emory

Institute of Technology
University

A ubiquitous property of sensory pathways is that they continuously adapt to changes in the properties of the
sensory input. Adaptation is not simply fatigue or attenuation of neural activity, but instead can fundamentally
change the features to which the sensory pathway are sensitive and thus changes what the pathway encodes. We
have recently shown that through multi-site, multi-electrode recording in the thalamocortical circuit that adaptation
through ongoing tactile stimulation in the rat vibrissa system induces a fundamental change in what behaviorally
relevant features the cortex encodes: from detection of a tactile contact to discrimination between different speeds
of the tactile/whisker input (Wang et al., Nat. Neurosci., 2010), and does so through the desynchronization of the
projecting thalamic inputs. To what extent this observation is general and whether it manifests perceptually has
not been well studied. Using ideal observer analysis of voltage sensitive dye imaging of cortical activation, we
now show that adaptation produces an enhanced spatial acuity at the level of S1 at the expense of a degraded
detectability of tactile inputs. Further investigation reveals a frequency dependent enhancement of discriminability
that reflects a tradeoff involving restricting spatial extent of cortical activation while maintaining good signal-tonoise. A separate behavioral paradigm reveals that these phenomena manifest perceptually. Adaptation degrades
detectability of vibrissa contact, consistent with our observations that detectability is increased in the absence of
self-motion. Discriminability, on the other hand, is enhanced by adaptation, improving spatial acuity as measured
in a go/no-go discrimination task using deflections of individual whiskers. Taken together, we believe these results
provide a compelling framework for switching between coding of different kinds of information when behaviorally
relevant.

III-74. Detection of weak sensory signals by molecular dynamic transformations of interspike interval seque
William Nesse1
Gary Marsat2
Andre Longtin2
Leonard Maler2
1 University
2 University

WILLNESSE @ GMAIL . COM
GMARSAT @ GMAIL . COM
ALONGTIN @ UOTTAWA . CA
LMALER @ UOTTAWA . CA

of Utah
of Ottawa

Spike-activated molecular-dynamical processes are common in neural systems, including both neural adaptation
and synaptic transmission. These molecular processes form a dynamical representation of the fine-timescale
pattern of interspike interval sequences that can be used to decode sensory information encoded in afferent spike
trains. A recent theoretical discovery by our lab has shown how negatively correlated interspike interval (ISI)
sequences common to sensory afferents paradoxically induce molecular activation patterns in the intracellular
adaptation processes that are statistically independent from spike to spike (Nesse et al., 2010). Here, in our most
recent work, we put the above theoretical discovery to the test by stimulating sensory afferents of the weakly
electric fish Apteronotus with weak stimulus intensities near known behavioral detection thresholds. Negatively
correlated afferent ISI sequences recorded from afferents in vivo were then transformed into a spike-activated
model-based molecular-dynamic code. This dynamical code was discovered to exhibit the independence property
predicted in Nesse et al. (2010). Moreover, the molecular-dynamic code was able to achieve the known behavioral
detection levels for the fish, and exhibited distinct advantages over more conventional firing-rate-based models of

COSYNE 2012

195

III-75 – III-76
sensory detection.
Because independent and identically distributed (IID) data allow for computationally efficient statistical analyses,
our discovery that a putative intracellular adaptation process underlying in vivo spike trains may exhibit such an
IID property from correlated ISI sequences suggests a plausible theory into how neurons, sensory afferents in
particular, may efficiently represent sensory information in their internal molecular states and transmit information
onward through synaptic dynamics.

III-75. Multisensory integration in the rat - behavioral benefits and neural correlates in parietal cortex
Stephanie Gleiss1
Michael T Lippert1
Kentaroh Takagaki2
Nikos K Logothetis1
Frank W Ohl2
Christoph Kayser1
1 Max

STEPHANIE @ GLEISS . AG
BIOLIPPI @ GMX . DE
KENTAROH . TAKAGAKI @ IFN - MAGDEBURG . DE
NIKOS . LOGOTHETIS @ TUEBINGEN . MPG . DE
FRANK . OHL @ LIN - MAGDEBURG . DE
CHRISTOPH . KAYSER @ TUEBINGEN . MPG . DE

Planck Institute for Biological Cybernetics
for Neurobiology

2 Leibniz-Institute

The complementary information provided by our different senses greatly enhances our ability to perceive and
interact with the environment. Rodent models offer the possibility to study the underlying neural mechanisms and
computations using a range of methodologies. However, suitable behavioral tasks and cortical candidate areas
for the rodent remain to be elucidated. We developed a two-response forced-choice stimulus detection paradigm
where rats (Long Evans) were required to detect lateralized audio-visual targets presented in either uni- or multisensory configuration. After training, the animals exhibit faster reaction times and enhanced detection rates in
congruent multisensory conditions and this multisensory response enhancement is strongest for weak unisensory
stimuli. These multisensory behavioral benefits mirror those described for similar tasks in humans. To localize
target areas of multisensory convergence, we performed high-resolution intrinsic imaging experiments in urethane
anaesthetized rats. We found a consistent overlap of responses to visual, somatosensory and auditory stimuli in
an elongated region which had the cytoarchitectonic properties of an association area (sparse layer IV) and which
overlapped well with parietal region PtA, as defined by the Paxinos atlas. Laminar recordings confirmed the functional convergence of unisensory inputs both in current source densities and multi-unit activity. These recordings
also demonstrated multisensory response interactions and the magnitude and sign of response enhancement /
suppression was dependent on temporal stimulus order. Control experiments confirmed the specificity of the multisensory response patterns to the parietal region (in comparison to visual cortex). We developed a rodent model
of behavioral multisensory integration similar to paradigms known from human psychophysics and we show the
presence of key criteria of multisensory processing in a region in the parietal cortex. Ongoing experiments directly
study the neural underpinnings of behavioral benefits for enhanced stimulus detection in the behaving animal.

III-76. Multisensory calibration with external feedback is contingent on cuereliability
Adam Zaidel1
Mandy Turner1
Dora Angelaki2

AJZAIDEL @ GMAIL . COM
MANDY @ PCG 3. WUSTL . EDU
ANGELAKI @ CABERNET. WUSTL . EDU

1 Washington
2 Baylor

196

University
College of Medicine

COSYNE 2012

III-77
Accurate perception of a dynamic environment requires continuous multisensory calibration. When present, external feedback is particularly beneficial for multisensory calibration, since it serves as a “teacher”. However, the
principles of interaction between external feedback and relative cue-reliability, and their combined influence on
multisensory calibration are currently unknown. Five monkeys were trained to perform a heading discrimination
task in which they were required to report whether self-motion was to the right/left of straight ahead. Stimuli comprised either visual (optic-flow), vestibular (motion platform) or combined (visual-vestibular) passive motion. For
each experimental session, coherence of the visual stimulus was manipulated such that either visual or vestibular reliability was relatively higher. A systematic heading discrepancy was introduced between the visual and
vestibular stimuli and external feedback was congruent with either the more-reliable or less-reliable cue. When
external feedback was aligned with the more-reliable cue, the less-reliable cue shifted towards the feedback, and
the more-reliable cue (which was already accurate) did not shift. However, when external feedback was aligned
with the less-reliable cue, a surprising form of calibration occurred: cues were yoked and shifted together in
the same direction. Hence, whilst the more-reliable cue shifted to become more accurate, the less-reliable cue
simultaneously shifted away from the external feedback, becoming less accurate. We propose two different mechanisms of multisensory calibration: 1) cue-specific (local) calibration, and 2) reference-frame (global) calibration.
When the more-reliable cue is incongruent with external feedback, the entire reference frame (zero) is considered to be inaccurate. Hence cues are yoked and shift in conjunction. When the more-reliable cue is congruent
with external feedback, the global reference frame is considered accurate and the less-reliable cue is calibrated
individually/locally. These results suggest that the Bayesian-optimal cue-combination is used to assess global
accuracy.

III-77. Odors sum linearly in time in the rat olfactory bulb
Priyanka Gupta1,2
Upinder S Bhalla1

PGUPTA @ CSHL . EDU
BHALLA @ NCBS . RES . IN

1 National
2 Cold

Centre for Biological Sciences, India
Spring Harbor Laboratory

Odor stimuli in the natural environment travel away from the source via turbulent carriers. At the sensory end, signals from multiple sources thus arrive intermittently, varying both in intensity and duration. Computational models
show that significant information about the location of the source can be extracted from the temporal dynamics
of this signal (Hopfield 1991, Murlis et al. 1992). However, it is unknown how these time-varying odor inputs are
represented in the rodent olfactory system. To this end, we studied responses of mitral/tufted cells (output neurons of the olfactory bulb) to precisely controlled, time-varying patterns of 9 odors and their binary combinations,
in anesthetized, tracheotomized rats using extracellular single unit recordings. We find that individual cells (n=76)
modulate their firing rate in synchrony with the odor waveform, in a surprisingly linear manner. The response to
an arbitrary odor waveform is accurately predicted by convolving the estimated “impulse response” (response to
a brief stimulus) with the odor waveform (measured by a photo-ionization detector). Excitatory and/or inhibitory
responses evoked by two odors sum arithmetically upon concurrent presentation of the two odor patterns (n=36).
Further, we compared responses across free breathing and tracheotomized conditions. We show that the phasic,
respiration-tuned responses of cells (n=12) are explained by simply convolving the “odor impulse” with the respiration waveform (measured by a pressure sensor). This summation of responses at the mitral cell level is striking
and unexpected, given the complex wiring diagram of the bulb and the richness of inhibitory interactions. Contrasting observations in the insect olfactory system (Broome et al. 2005, Geffen et al. 2009) suggest that different
mechanisms of information processing may be used in the mammalian and invertebrate olfactory systems. This
may be attributed to inherent differences in circuit architecture and/or odor sampling behavior.

COSYNE 2012

197

III-78 – III-79

III-78. Noradrenergic control of long-term cortical synaptic receptive field plasticity
Ana Raquel Martins1,2
Robert Froemke2

RAQUEL . MARTINS @ MED. NYU. EDU
ROBERT. FROEMKE @ MED. NYU. EDU

1 University
2 New

of Coimbra
York University

Neuronal networks of the cerebral cortex are plastic, maintaining the capacity to reorganize throughout life. While
neuromodulator release is required for cortical plasticity, it is uncertain how subcortical neuromodulatory systems,
such as the noradrenergic locus coeruleus, interact with and refine cortical circuits. Here we determine the dynamics of cortical receptive field plasticity at the synaptic and spiking levels using in vivo whole-cell recording. Adult
rats were anesthetized, stimulation electrodes implanted in the locus coeruleus, and a craniotomy performed over
the primary auditory cortex (AI). After mapping AI, whole-cell and cell-attached recordings were made from AI
neurons, and pure tones of varying frequencies and intensities were presented to the animal to characterize tonal
receptive fields. Pairing sensory stimulation (pure tones) with locus coeruleus activation (to release noradrenalin)
dramatically changed the tuning properties of AI neurons. In most cases, pairing induced large increases of
tone-evoked synaptic and spiking responses. While in some cases, these changes could be stimulus-specific, in
most other cases the magnitude of tone-evoked responses was greatly increased across all stimuli, de-tuning AI
receptive fields. This degradation of frequency tuning was observed immediately after pairing, before emergence
of new stimulus preference occurred 30+ minutes later. Unlike the effects of nucleus basalis pairing (Froemke
et al., Nature 2007), locus coeruleus pairing seemed to enhance tone-evoked excitation and inhibition together.
Multiple cell recordings from the same animal for hours after pairing suggested that these changes in AI tuning
could persist for 10+ hours. We hypothesize that such changes to cortical tuning curves have important implications for the detection and/or discrimination of different sensory inputs. In particular, increased excitability could
make it easier for a network or animal to detect the presence of a stimulus, in exchange for reduced discriminative
abilities.

III-79. Real-time changes in single neurons during auditory object recognition
learning
Daniel Knudsen
Timothy Gentner

DKNUDSEN @ UCSD. EDU
TGENTNER @ UCSD. EDU

University of California, San Diego
Adult sensory systems do not maintain a static representation of the external world, but instead modify their
response properties as a function of experience and real time behavioral demands. In the auditory system,
studies of adult experience-dependent plasticity have been generally limited to responses to artificial stimuli, and
usually the effects of experience are compared between animals or at discrete time points. Here, we expand the
scope of these studies by investigating changes in single neurons as animals performed ethologically realistic
behaviors mediated by natural stimuli. We recorded extracellular action potentials from single neurons in an avian
analogue of auditory cortex while European starlings learned to classify sets of novel auditory objects (segments
of starling song) in a vocal recognition task. Across the population, recently learned objects elicited stronger
neural responses on average than did objects for which birds had extensive training. In addition to these longertimescale population effects, by maintaining single unit isolation while new objects were introduced (sessions
lasted between one and four hours) we were able to observe changes in the same neuron over the course of
learning. Here, responses to newly learned objects changed more than responses to over-trained reference
objects presented in the same behavioral session. On average, neuronal firing rates for all stimuli decreased
during a learning session, and this decrease was largest for the set of newly learned objects. Our results support
a model in which natural experience leads to progressive sparsening of representations in the auditory system.
We propose that initial learning is tied to a period of over-representation of multiple stimulus features followed by

198

COSYNE 2012

III-80 – III-81
selective retention (or suppression) of responses tied to subsets of features that are more (or less) informative for
natural behaviors.

III-80. Encoding of motion onset by retinal ganglion cells
Eric Chen1,2
Joshua Levy1
Clark Fisher1,3
Rava da Silveira4
Michael Berry1

ERICCHEN @ PRINCETON . EDU
JMLEVY @ PRINCETON . EDU
CLARK . FISHER @ GMAIL . COM
RAVA @ LPS . ENS . FR
BERRY @ PRINCETON . EDU

1 Princeton

University
of Medicine and Dentistry of New Jersey
3 The Rockefeller University
4 École Normale Supérieure
2 University

Sudden onset of motion signals changes in the environment that can have great behavioral significance. Recent
psychophysics studies have shown that motion onset is especially effective at capturing attention and is more
salient than smooth motion itself (Abrams and Christ, Psych. Sci. 2003, Christ and Abrams, Vision 2008). Here,
we investigated whether the retina contributes to the recognition of sudden motion onset. Using multi-electrode
array recordings, we found that a subset of salamander ganglion cells, fast OFF cells, respond significantly more
strongly a bar’s motion onset than to smooth motion. This was despite aligning our stimuli such that once motion
onset began, the smooth motion and motion onset stimuli were exactly identical. Using stimuli with motion of only
one edge as well as pharmacology, we found that the OFF pathway dominates the response to motion onset. We
explored different computational models of the motion onset effect. The classical receptive field model (LN model)
failed to predict a larger response to motion onset than to smooth motion, sometimes predicting no response or
even an inhibitory response. Next we considered a model with rectified bipolar subunits within the ganglion cell’s
receptive field, as for a Y-type cell (Victor and Shapely, J. Gen. Phys. 1979). Although the subunit model correctly
predicted excitation for motion onset at all locations on the receptive field center, this model was unable to produce
a stronger response to motion onset than to smooth motion. Finally, we added contrast gain control mechanisms
(Berry, Nature 1999) to both the bipolar and ganglion cells of the subunit model. This Adaptive Cascade model
could reproduce all of the qualitative features of the ganglion cell responses to smooth motion, motion onset, and
bar appearance over a range of contrasts and speeds.

III-81. A wide-field neuron enhances visual contrast sensitivity in the fly
John Tuthill1,2
Aljoscha Nern1
Gerald M Rubin1
Michael Reiser1
1 HHMI,

TUTHILLJ @ JANELIA . HHMI . ORG
NERNA @ JANELIA . HHMI . ORG
RUBING @ JANELIA . HHMI . ORG
REISERM @ JANELIA . HHMI . ORG

Janelia Farm Research Campus
of Chicago

2 University

Peripheral visual circuits are highly adapted for efficient coding of image contrast. However, the mechanisms by
which retinal circuits maintain contrast sensitivity across diverse luminance conditions are not well understood.
We identified a new class of wide-field feedback neurons in the fly lamina that contributes to visual contrast
sensitivity. Silencing these lamina wide-field (lawf2) neurons decreased the fly’s ability to detect low contrast visual
motion stimuli. Using targeted whole-cell patch-clamp recordings in vivo, we found that lawf2 neurons encode low
frequency luminance fluctuations, and that their excitability increases when the fly is flying. Application of the
neuromodulator octopamine mimicked flight conditions, suggesting that octopamine release during flight alters

COSYNE 2012

199

III-82 – III-83
the coding properties of lawf2 neurons. These data suggest that lawf2 neurons enhance contrast sensitivity by
providing feedback to the lamina, and that the quality of this feedback depends on the behavioral state of the
fly. Behavioral state modulation of peripheral visual circuits may compensate for the shift in image statistics
experienced during self-motion.

III-82. Perceptual relevance of neurally-inspired natural image models evaluated via contour discrimination
Holly Gerhard1
Matthias Bethge2
1 Max
2 Max

HGERHARD @ GMAIL . COM
MBETHGE @ TUEBINGEN . MPG . DE

Planck Institute for Biological Cybernetics
Planck Institute

A key hypothesis in sensory system neuroscience is the idea that sensory representations are formed by the
statistical regularities in sensory signals and thus acquire knowledge about the outside world (Barlow, 1997). In
vision, several probabilistic models of local natural image regularities have been proposed which intriguingly
replicate neural response properties (Attick&Redlich 1992, Bell&Sejnowski 1997, Schwartz&Simoncelli 2001,
Karklin&Lewicki 2009). To evaluate how such models relate to functional vision, we previously measured their
perceptual relevance using a discrimination task pitting model image patches against true natural image patches
(Gerhard, Wichmann, Bethge, 2011). Observers were remarkably sensitive to the regularities of grayscale
patches, even for patches as small as 3x3 pixels. Performance relied greatly on how well the models captured
luminance features like contrast fluctuation. Here we focus on how well the models capture local contour information in natural images. In a two-alternative forced choice task, observers viewed two tightly-tiled textures of
binary image patches, one comprised of natural image samples, the other of model patches. The task was to
select the natural image samples. We measured discrimination performance at patch sizes from 3x3 to 8x8 pixels
for 8 models spanning the range from low likelihood to one among the current best in terms of likelihood. We
compared human performance to an ideal observer with perfect knowledge of the natural distribution for patch
sizes at which we could empirically estimate the distribution and tested potential texture cues with a classification
analysis. While human performance suggested suboptimal strategies were used to discriminate contour statistics
relative to grayscale statistics, observers were well above chance with binary 4x4 pixel patches and larger, meaning that neuronally-inspired models do not yet capture enough of the contour regularities in natural images that
functional human vision can detect, even in very small natural image patches.

III-83. Continuous Time Infomax Models of Oculomotor Control
Walter Talbott
He Huang
Javier Movellan

WTALBOTT @ UCSD. EDU
CRANE 081@ GMAIL . COM
MOVELLAN @ MPLAB . UCSD. EDU

University of California, San Diego
Recent models of oculomotor control have been successful at describing saccade velocity profiles using optimal
control principles. Some models postulate that the eyes minimize the expected deviation from a target end point
(Minimum Variance models). Other models postulate that eye movements minimize the time required to reach the
target point (Minimum Duration models). These models have a common assumption that the goal of oculomotor
control is to reach target points. Not surprisingly, much of the empirical data used in these models is based on
tasks in which the explicit goal is for the eyes to move to predefined target end points. However, such tasks seldom
occur in daily life. Instead, the eyes typically play a supportive role, providing other actuators (e.g., the hands) with
the information they need to efficiently achieve their goals (e.g., grasp objects). Here, we use a rapid-pointing task
to study eye movement in different conditions where the eyes serve either a supporting role (where the reward

200

COSYNE 2012

III-84 – III-85
depends on the hand endpoint) or an executive role (where the reward depends on the fixation endpoint). Our
results suggest that Minimum-variance and Minimum-duration models cannot account for key properties of the
eye movements observed in our data. To address this issue, we present an alternative class of models (Infomax
models) in which the eyes move to maximize the information needed to achieve goals. The approach relies on
a novel algorithm (PIC2) developed at our laboratory to find approximate solutions to continuous time partially
observable stochastic optimal control problems. We present our progress solving Infomax Control problems and
show how they explain the complex saccadic movements observed in our experiments.

III-84. Dissection of cortical microcircuits by single-neuron stimulation in vivo
Alex Kwan1
Yang Dan1,2
1 University

ALEXKWAN @ BERKELEY. EDU
YDAN @ BERKELEY. EDU

of California, Berkeley

2 HHMI

A fundamental process underlying all brain functions is the propagation of spiking activity in networks of excitatory and inhibitory neurons. To understand how spikes are routed in cortical circuits in vivo, we used two-photon
calcium imaging to monitor ensemble activity and targeted patching to stimulate a single neuron in layer 2/3 of
mouse visual cortex. Here we show that burst spiking of a single pyramidal neuron can drive spiking activity in
both excitatory and inhibitory neurons nearby. For inhibitory neurons, ∼30% of the somatostatin interneurons
can be reliably activated by a burst of ≥5 spikes. In contrast, parvalbumin interneurons showed no detectable
responses to single-neuron stimulation, but their spiking is highly correlated with the local network activity. Our
results demonstrate the feasibility of mapping functional connectivity at cellular resolution in vivo and reveal distinct operations of two major inhibitory circuits, one detecting single-neuron spike bursts and the other reflecting
distributed network activity.

III-85. Selectivity of neurons in area MT to complex motion features
Yuwei Cui1
Liu Liu2
Farhan A Khawaja2
Christopher Pack2
Daniel A Butts1

YWCUI @ UMD. EDU
IU. LIU 2@ MAIL . MCGILL . CA
FKHAWAJA @ GMAIL . COM
CHRISTOPHER . PACK @ MCGILL . CA
DAB @ UMD. EDU

1 University
2 McGill

of Maryland
University

Visual perception is constructed from successive computations on visual stimuli by a hierarchy of cortical areas.
The computations in a given area can be estimated from spiking responses to stimuli that are sufficiently complex
to activate the appropriate combinations of excitatory and inhibitory afferents to a given cell. In this work we have
used a naturalistic, continuously varying optic flow stimulus to explore the responses of neurons in the middle
temporal area (MT) of the alert macaque monkey. These responses were interpreted using a hierarchical model
that took into account the nonlinear properties of the V1 neurons that likely provide direct and indirect inputs to
MT. In this rich stimulus context, the putative inputs to MT are rectified, resulting in each being only excitatory
or inhibitory. While the resulting model’s excitation is generally spatially localized, its inhibition is direction-tuned,
and appears to be distinct from typical surround suppression and normalization, which are largely not direction
selective and can be separately identified. Roughly half of MT neurons have motion opponency, although inhibition
usually did not spatially overlap with excitation completely. In other MT neurons, inhibition has similar direction
tuning to excitation, or is orthogonal, and is usually not spatially co-localized. Taking these additional features into
account lead to better cross-validated performance in every case. The observed direction-tuned inhibition can

COSYNE 2012

201

III-86 – III-87
result in selectivity to more complex features of natural motion stimuli, such as motion boundaries and particular
types of optic flow. In total, considering MT neurons in a complex stimulus context reveals a diverse set of
computations likely relevant for perception in the behaving animal.

III-86. Compressive spatial summation: a characteristic of extrastriate computation
Kendrick Kay
Jonathan Winawer
Aviv Mezer
Brian Wandell

KENDRICK @ POST. HARVARD. EDU
WINAWER @ STANFORD. EDU
AVIVM @ STANFORD. EDU
WANDELL @ STANFORD. EDU

Stanford University
The visual system pools image features across increasingly large extents of the visual field, from small receptive
fields in the retina to very large receptive fields in anterior temporal cortex. We studied pooling using simple
contrast patterns and fMRI measurements of human visual cortex. Subjects viewed a systematic set of contrast
patterns occupying different portions of the visual field. In extrastriate cortex the response to a contrast pattern
is substantially less than the sum of the responses to individual parts of the pattern presented on separate trials.
We modeled this sub-additivity using a compressive power-law nonlinearity that is applied after linear summation
of contrast across the visual field. We find that the amount of compression grows larger in anterior extrastriate
maps and cannot be explained by increased receptive-field size. An important prediction of the compressive spatial summation (CSS) model is that the more compressive the nonlinearity, the more tolerant the response is to
changes in the position and size of a viewed object. In independent datasets we measured responses to objects
varying in position and size and confirmed that the CSS model accurately predicts the size and position tolerance
exhibited by extrastriate cortex. The CSS model is largely consistent with HMAX, an influential object recognition
model that also seeks to explain position and size tolerance. A key feature of HMAX is hierarchy, and we demonstrate through simulations that the CSS model can be re-expressed in hierarchical form. Furthermore, there is an
interesting parallel between compressive spatial summation and the observation that responses in visual cortex
saturate with increasing stimulus contrast. We are currently developing a general model that quantitatively accounts for both the spatial and contrast nonlinearities. This model can be viewed as a specific instantiation of the
canonical cortical circuit model proposed by Kouh and Poggio (2008).

III-87. Direction selectivity within large receptive fields in a three-layer visual
cortex
Jeff Pobst
David Morton
Ralf Wessel

JEFFSPOBST @ GMAIL . COM
DAVIDLMORTON @ GMAIL . COM
RW @ PHYSICS . WUSTL . EDU

Washington University
Cortical direction selectivity emerges de novo in neurons of mammalian primary visual cortex (V1), i.e. direction
selectivity is not derived from retinal responses; instead it emerges from computations made in V1. Since its discovery some 50 years ago, elucidating the cellular mechanisms subserving directional selectivity in V1 continues
to be an important area of intense research. Cortical direction computation starts in V1 and becomes increasingly
complex in higher cortical areas (V2, MT, MST); directional information is combined to produce neurons sensitive
to complex motions, such as rotation, expansion, and contraction. The increased complexity is paralleled by an
increase in the size of the receptive fields. In MSTd, the receptive fields can cover most of the visual field. We
investigate direction selectivity in the three-layer cortex of turtle, which serves as a model to study the circuit
mechanisms of distributed processing in primary visual cortex (V1). In contrast to the six-layered structure of

202

COSYNE 2012

III-88 – III-89
mammalian neocortex, the turtle V1 contains a monolayer of pyramidal neurons, which receive sensory inputs
from Lateral Geniculate Nucleus (LGN) axons, excitatory inputs from other pyramidal neurons, and inhibition from
GABAergic interneurons. We characterize visual response properties of neurons in turtle visual cortex in an eyeattached whole-brain preparation. We use single electrode extracellular recordings to record both isolated units
and multiunit activity while presenting visual stimuli to a hemisected eye. We find that individual neurons respond
to motion (small black spot, 2 deg diameter) in distinct regions spanning the entire visual field and to all directions
(8 tested). These regions show strong directional tuning, but this tuning varies greatly from region to region. In
summary, a subset of units in turtle visual cortex display large nonconcentric receptive fields, reminiscent of higher
cortical areas in mammals, with mixed direction-sensitive structure when probed with moving spots.

III-88. Trial-to-trial variability of MT neurons reveals the nature of their engagement in a motion discrimi
Mark Zarella
Tatiana Pasternak

MZARELLA @ CVS . ROCHESTER . EDU
TANIA @ CVS . ROCHESTER . EDU

University of Rochester
We have recently shown that neurons in the motion processing area MT and in the prefrontal cortex (PFC) are
actively engaged in all stages of a task in which monkeys compare two directions of motion, S1 and S2, separated
by a delay. Neurons in both areas showed direction selective responses, were active during the delay, and showed
comparison effects that correlated with perceptual decision. In the PFC, this engagement was also reflected in
trial-to-trial variability of spiking activity (Fano Factor, FF) of putative pyramidal neurons, a likely source of top-down
influences on MT. The FF tracked consecutive task components and was predictive of the upcoming neuronal
events, dropping with stimulus onset, decreasing prior to salient events and flagging neurons participating in
sensory comparisons. Here, we report that the variability of spiking activity in MT during the same behavioral
task followed a similar pattern. The FF showed a typical rapid drop with stimulus onset, which was present even
for stimuli that appeared remotely from the neuron’s receptive field, revealing that even in the absence of overt
activity MT neurons were engaged in discrimination. The FF also reflected stimulus identity during several trial
components, even in the absence of selective spiking activity. With time in delay, variability of many neurons
increased, the pattern opposite to that observed in PFC, suggesting possible interactions between the two areas
in preparation for comparison stimulus. Towards the end of the delay, variability of neurons with future comparison
effects decreased, an effect analogous but delayed relative to that observed in the PFC, suggesting its possible
top-down influences on MT neurons participating in sensory comparisons. Our results demonstrate that the FF
provides a sensitive measure of the engagement of MT neurons in motion discrimination tasks and suggest the
nature of their interactions with PFC during such tasks.

III-89. Reductions of correlated firing arise from attention-dependent depolarization in a spiking network
Jude Mitchell
John Reynolds

JUDE @ SALK . EDU
REYNOLDS @ SALK . EDU

Salk Institute for Biological Studies
Recent studies have found that spatial attention reduces the variability of neuronal responses (Mitchell et al, 2007;
Cohen et al, 2009; Mitchell et al, 2009). Variability arises both from sources specific to individual neurons, such as
the tendency to fire in bursts, and to sources shared across populations, such as spreading waves of activity. In
particular, reductions in variability shared across neurons can lead to large improvements in sensory read-out and
behavioral performance (Cohen et al, 2009). These correlated fluctuations in firing rate occur both in awake and
anaesthetized animals, and have slow dynamics from tens to hundreds of milliseconds (Bair et al, 2001; Smith

COSYNE 2012

203

III-90
and Kohn, 2008; Mitchell et al, 2009). While previous models of attention have done well at explaining the mean
firing rate modulation observed with spatial attention (Lee and Maunsell, 2009; Reynolds and Heeger, 2009),
they are ill suited to explain realistic sources of firing variability. Here we introduce a model cortical circuit that
exhibits realistic noise fluctuations, and show that depolarization introduced by attentional feedback reduces the
magnitude of these fluctuations. The network is implemented as a two-dimensional grid of spiking units that are
recurrently connected with shift-invariant center-surround antagonism. The spiking units use a reduced parameter
neuron model introduced by Izhikevich (2003) that captures a wide range of behaviors including burst firing. In the
absence of input, the network exhibits spontaneous activity that is marked by brief bursts and slower correlated
waves of activity resembling those seen in array recordings (Smith and Kohn, 2008). Attentional feedback is
modeled as a weak excitatory input that depolarizes units at the attended location. This modest depolarization
leads to the increases in firing rate typically observed with attention, and also reduces correlated fluctuations,
leading to improved sensory coding.

III-90. Dynamics of correlated variability in evoked and spontaneous responses
of V4 neurons
Matt A Smith1
Marc Sommer2

SMITHMA @ PITT. EDU
MARC. SOMMER @ DUKE . EDU

1 University
2 Duke

of Pittsburgh
University

The activity of nearby cortical neurons is correlated on both short and long time scales, measured by both synchronous spiking and correlated trial-to-trial response variability (rsc). In primary visual cortex (V1), the spatial
and temporal characteristics of this correlation are well characterized, although the structure and magnitude of
neuronal correlation has recently been a topic of debate (Cohen & Kohn, 2011, Nat Neurosci). In area V1, a
number of studies have found that correlation is larger in spontaneous activity than in evoked responses. In
extrastriate cortex, however, this issue has not been studied in detail. In area V4, a pair of recent studies demonstrated that correlation is modulated by attentional state (Cohen & Maunsell, 2009, Nat Neurosci; Mitchell et al,
2009, Neuron). However, the detailed spatial and temporal structure of correlation in V4 is unknown. We recorded
from populations of neurons in area V4 of two awake, behaving macaque monkeys performing a fixation task. We
used a chronically implanted 100-electrode “Utah” array, which consists of a 10 x 10 grid with 400 micron spacing
between adjacent electrodes (each 1.0 mm in length). We presented drifting sinusoidal grating stimuli that varied
in orientation but were fixed in spatial frequency, temporal frequency, and size, and recorded all waveforms that
exceeded a threshold for offline sorting. We analyzed data from pairs of well-isolated single units recorded on
different electrodes. We found that the spatial and temporal structure of neuron correlation in V4 was strikingly
similar to that we observed in V1, although roughly one-third the magnitude. In a fixation task with no stimulus
(spontaneous activity), the value of rsc was roughly two-fold higher than in the evoked condition. These results
indicate that the structure of neuronal correlation is likely governed by features of neuronal circuits that are shared
across visual cortex.

204

COSYNE 2012

III-91 – III-92

III-91. The mesencephalic locomotor region modulates layer-specific activity
in V1 independent of locomotion
Cristopher Niell1
A. Moses Lee2
Linda Wilbrecht2
Antonello Bonci3
Michael P Stryker2
1 University
2 University

CNIELL @ UOREGON . EDU
MLEE @ GALLO. UCSF. EDU
LWILBRECHT @ GALLO. UCSF. EDU
ANTONELLO. BONCI @ NIH . GOV
STRYKER @ PHY. UCSF. EDU

of Oregon
of California, San Francisco

3 NIH/NIDA

Behavioral state is known to affect sensory processing. Our previous study in awake, head-fixed mice demonstrated a cell-type specific effect of locomotion on visual processing–when an animal began to walk or run, responses to visual stimuli in layer 2/3 of mouse V1 more than doubled, accompanied by an increased LFP in the
gamma range, but spontaneous activity and tuning did not change. Our more recent experiments have also revealed that locomotion has different effects on layer 5, primarily resulting in increased baseline firing. However,
the source of the signal modulating cortical responsiveness is unknown. In order to investigate the neural circuits
underlying this change with behavioral state, we measured the effect of optogenetic stimulation of glutamatergic neurons in the mesencephalic locomotor region (MLR), which includes the cholinergic pedunculo-pontine
tegmental nucleus (PPTg) and has both descending projections to motor pathways and ascending projections to
the thalamus and other cholinergic areas. Above a certain threshold, stimulation of the MLR in awake, head-fixed
mice induced walking and then running, as stimulation frequency increased, along with the same effects in visual
cortex as we previously described for spontaneous locomotion. Surprisingly, stimulation at a frequency below the
threshold for locomotion also induced similar effects on visual responses, suggesting that the MLR acts directly
to modulate cortical responsiveness via a pathway that is independent of locomotion. Furthermore, the effects of
MLR stimulation varied by cortical layer exactly as did those of spontaneous locomotion, with superficial layers
showing increased gain while deep layers showed primarily changes in spontaneous firing rates. These findings
demonstrate that a midbrain cholinergic region can control both ongoing and evoked activity in primary visual
cortex in a layer-specific manner.

III-92. Probing motion perception with spatiotemporal reverse correlation
Jacob Yates
Lawrence Cormack
Alex Huk
Jonathan W Pillow

JLYATES @ UTEXAS . EDU
CORMACK @ MAIL . UTEXAS . EDU
HUK @ MAIL . CPS . UTEXAS . EDU
PILLOW @ MAIL . UTEXAS . EDU

University of Texas at Austin
We used psychophysical reverse correlation to gain insight into the mechanisms of motion processing in human observers. Specifically, we investigated whether short-range stroboscopic (apparent) motion processing can
be explained by canonical motion processing mechanisms, or by a template-matching mechanism tuned to the
apparent-motion stimulus. Observers were asked to detect a 3-step apparent-motion bar embedded in spatiotemporal Gaussian white noise. Half the trials contained the signal bar plus noise; the other half contained only noise.
The ideal observer for this task consists of a linear template matched to the expected space-time locations of the
white bar, followed by a threshold. To probe the mechanisms underlying human performance, we estimated a linear kernel for each observer, using the maximum a posterior estimate under a generalized linear observer model
(GLM), with a prior encouraging smooth and sparse filters. (These estimates were significantly better at predicting
responses on held-out data than filters fit by “classical” psychophysical reverse correlation). We also estimated a
second-order psychophysical kernel that operated on the squared stimulus pixels. The resulting model is insensitive to the sign of stimulus contrast (white vs. black), and exhibits an accelerating nonlinear response to absolute

COSYNE 2012

205

III-93 – III-94
pixel luminance. Surprisingly, this model provided a more accurate description of observers’ data than the linear
GLM. The observed quadratic kernels exhibited compelling direction selectivity (i.e., spatiotemporal orientation)
and integrated smoothly across a larger range of space-time locations than the actual apparent motion signal.
These results reveal that motion-energy processes (Adelson & Bergen, 1985) are likely involved in the detection
of apparent motion, and also demonstrate the feasibility of extending reverse correlation methods for analyzing
nonlinear mechanisms of motion processing in both psychophysics and physiology.

III-93. Dissociating interneuron types in frontal cortex by firing variability and
spike timing specificity
Salva Ardid1
Martin Vinck2
Daniel Kaping1
Stefan Everling3
Thilo Womelsdorf1

SARDID @ YORKU. CA
MARTINVINCK @ GMAIL . COM
DKAPING @ UWO. CA
SEVERLIN @ UWO. CA
THIWOM @ YORKU. CA

1 York

University
of Amsterdam
3 University of Western Ontario
2 University

To understand how a neocortical microcircuit transforms input to output it is pivotal to elucidate the functional
roles of specific neuron types within the circuit. Previous studies have critically implicated three separable types
of inhibition participating in this processing: (1) Modulatory, inhibitory control that sets the tone for excitability and
responsiveness to input (relying on dendritic targeting ‘CR’ interneurons); (2) localized dis-inhibition of pyramidal cells (through I-to-I interactions relying on ‘CB’ interneurons); and (3) widespread (per-somatic) gain control
of pyramidal cell firing through fast spiking ‘PV’ interneurons. The implicated interneuron types are particularly
well characterized anatomically within prefrontal cortex, but their electrophysiological identification is largely lacking. We set out to identify subtypes of putative interneurons in extracellular recordings within lateral and medial
prefrontal cortex of two awake macaques. We identified 92 putative interneurons - among 420 recorded highly
isolated neurons (21%) - by their unique narrow width of action potentials. We used a measure of ‘Local variability’ (Lv) to index spiking activity as regular (Lv<1), irregular Poisson-like (Lv∼1), or bursty (Lv>1). Conjointly
with average firing rate, Lv reliably separated three interneuron types that were validated by higher dimensional
k-means clustering. Analysis of spike timing to the local field potential revealed that 90% of neurons showed a
highly consistent phase of firing with varying spectral/timing specificity: Large subsets of putative interneurons
synchronized significantly (and more likely than broad spiking neurons) at 12±1 Hz, or at 30±5 Hz, with a subset
displaying clock-like, regular spike train patterns. Smaller subsets (∼ 15% of interneurons) displayed either highly
specific preferences to synchronize at either 8±1 Hz, at 21±3 Hz, or had two apparent peaks at 5±1 Hz and
45±5 Hz. These findings distinguish putative interneurons, which may correspond to separable anatomical cell
classes and/or relate to separable inhibitory regimes within the cortical microcircuit.

III-94. Transient motion analysis reveals differential motion dynamics of synaptic vesicle populations
Ziv Rotman
Amy Peng
Vitaly Klyachko

ZIV. ROTMAN @ WUSTL . EDU
AMYWPENG @ WUSTL . EDU
KLYACHKO @ WUSTL . EDU

Washington University
Central synapses typically contain a very small number of release-competent vesicles and synaptic function thus
critically depends on an efficient vesicle recycling program to sustain and control neurotransmitter release. De-

206

COSYNE 2012

III-95 – III-96
spite several decades of research on synaptic function, the basic vesicle cycling mechanisms remain poorly
understood due to the relative inaccessibility of central synapses to conventional recording techniques. Although
recent advances in high-resolution microscopy techniques has permitted tracking of single synaptic vesicles, the
quantitative analysis of vesicle motion has been limited by the transient nature of the motion, challenges of detecting rapid changes in vesicle behavior in noisy tracks and wide range of timescales at which the motion occur,
spanning from tens of milliseconds to tens of seconds. Here we describe a novel transient motion analysis approach that allows identification of rapid changes in vesicle motion and detection of directed motion episodes
within vesicle trajectories. The analysis is based on directional correlations between subsequent steps over short
time scales and estimation of vesicle mobility based on the traveled distance. The time resolution obtained by this
analysis offers a major improvement over the currently available mean square displacement analysis. We demonstrate the application of our motion analysis approach to synaptic vesicle motion during their recycling process
in hippocampal neurons. We compared the dynamic properties of vesicles endocytosed via different pathways,
specifically, via fast, slow and spontaneous forms of retrieval and found that vesicles recycled by different retrieval
pathways exhibit differential motion dynamics, particularly the ability to engage in cytoskeleton-based directed
motion. We further combine single-vesicle imaging and pharmacological tools to link specific forms of vesicle
motion to identified cellular mechanisms, particularly motor-based cytoskeletal transport.

III-95. A mobile imaging system to monitor the cortex in behaving rodents
Joon Hyuk Park1
Ahmad Osman2
Jelena Platisa2
Eugenio Culurciello3
Vincent Pieribone1

JOONHYUK . PARK @ YALE . EDU
AOSMAN @ JBPIERCE . ORG
JPLATISA @ JBPIERCE . ORG
EUGE @ PURDUE . EDU
VAP 5@ EMAIL . MED. YALE . EDU

1 Yale

University
John B. Pierce Laboratory
3 Purdue University
2 The

Optical monitoring of electrical activity has benefits over traditional electrode-based methods of recording neuronal
activity. Electrode arrays have limited, inaccurate spatial resolution and penetrating electrodes damage brain
tissue. Most optical studies of neuronal activity in rodents are performed on anesthetized and/or head-fixed
animals. These restrains have dramatic effects on all levels of neuronal function, and behavioral performance
is substantially altered. There are published head-mountable optical systems for rodents, but they are limited
in temporal resolution (< 100 Hz) and signal to noise ratio (< 40 dB). Therefore, a novel imaging system is
necessary to measure rapid cortical activity over a wide cortical area using optical probes in behaving rodents.
We have designed a miniature microscope system that measures neuronal activity over a wide area (> 2 mm)
with high temporal (> 400 Hz) and spatial resolution. This system is light enough to be mounted on a head of
a rodent, and generates little to no heat. Thus, only our system is suitable to record neuronal activity over a
wide area at high speeds in freely moving animals with high spatial and temporal resolution without damaging the
brain. We present the challenges encountered when designing such a system, in both the imaging circuitry and
microscope. We also present what types of dynamic information can be collected by the system to allow scientists
to observe neuronal activity in the cortex generated by voltage-sensitive optical probes.

III-96. Relating patterns of EEG activity to natural scene categories
Dirk Walther
Bart Larsen

BERNHARDT- WALTHER .1@ OSU. EDU
LARSEN .116@ OSU. EDU

The Ohio State University

COSYNE 2012

207

III-97
Our ability to categorize natural scenes is essential for visual tasks such as navigation or the recognition of objects
in their natural environment. Using functional magnetic resonance imaging (fMRI), we have previously shown that
spatial patterns of brain activation in the parahippocampus and throughout visual cortex are specific for natural
scene categories (Walther et al. 2009, 2011). Several recent fMRI studies have attempted to illuminate the
complex interactions between scene and object perception (e.g., MacEvoy and Epstein 2011; Kim and Biederman
2011). However, its low temporal resolution limits the use of fMRI for exploring the dynamics of these interactions.
Electroencephalography (EEG) with its high temporal resolution may serve as a complementary technique. Here
we present a first step in this direction. We recorded EEG signals from scalp electrodes while participants viewed
images of natural scenes from six categories (beaches, city streets, forests, highways, mountains, offices). We
then employed partial least-squares analysis in order to identify activity patterns in the (time x frequency x scalp
location) space that significantly contributed to the discrimination between these six categories. Significance
of activity patterns was assessed using permutation analysis. We found a cluster of significant activity around
eight parietal and occipital electrodes in the theta band (4-8 Hz) between 172-192 ms after stimulus onset. In
remarkable consistency with our prior fMRI results, the activity patterns that we found were more pronounced
when participants viewed line drawings than when they viewed color photographs of the same types of scenes.
Our results illustrate the potential of using EEG for measuring precise time information of neural activity related to
natural scene categories. This opens the door to further exploration of scene categorization and its interactions
with object perception.

III-97. An open-source system for combining multi-electrode recording with
closed-loop feedback
Joshua H Siegle1
Jakob Voigts1
Stuart Layton1
Caleb Kemere2
Loren Frank2
Christopher Moore3
Matthew Wilson1

JSIEGLE @ MIT. EDU
JVOIGTS @ MIT. EDU
SLAYTON @ MIT. EDU
CKEMERE @ PHY. UCSF. EDU
LOREN @ PHY. UCSF. EDU
CHRISTOPHER MOORE @ BROWN . EDU
MWILSON @ MIT. EDU

1 Massachusetts

Institute of Technology
of California, San Francisco
3 Brown University
2 University

The advent of optogenetic techniques has driven neuroscience toward experiments which attempt to demonstrate
the causal role of particular cell populations in behavior. Such experiments often involve detecting network-level
phenomena in awake, behaving animals and intelligently perturbing neural circuits in real time. To this end, we
have developed a low-cost, open-source hardware and software solution for multi-electrode recordings in rodents.
Our system is not merely a more affordable version of existing commercial products; it goes beyond anything
currently available by prioritizing low-latency control of feedback devices in response to neural events. Data
acquisition is performed via hardware built around head-mounted amplifier chips (Intan Technology). Digitization
is coordinated by an FPGA with custom firmware, which uses USB for high-speed data transfer to and from
standard computers. Our open-source hardware and software will allow users to customize all aspects of the
data-processing pathway, but we also aim to provide an out-of-the-box solution down the road. Our graphical
interface, written in C++, not only enables the user to view and record spikes and LFP, but also provides eventtriggered visualizations and an intuitive system for configuring signal processing chains on the fly. We hope
this system will become a flexible platform for developing online feedback algorithms that can be easily shared,
eliminating the need for redundancy between labs. The software, firmware, and hardware designs are freely
available, and we invite and encourage all members of the community to make use of them and contribute to
their development. One of our first applications will be online detection of sharp-wave–ripple complexes in the
hippocampus. Through stimulating electrodes in the ventral hippocampal commissure, this approach enables us
to selectively disrupt ripples with millisecond latency in order to test their role in memory processes.

208

COSYNE 2012

III-98 – III-99

III-98. Fast methods for mapping the full dendritic synaptic connectivity
Ari Pakman
Jonathan Huggins
Liam Paninski

ARIPAKMAN @ GMAIL . COM
JHH 2143@ COLUMBIA . EDU
LIAM @ STAT. COLUMBIA . EDU

Columbia University
A detailed understanding of the organization of local neural circuits requires determining not just which neurons
are interconnected, but also the exact location and strength of synaptic interactions in the dendritic tree. To
achieve this goal, we can combine the ability to stimulate individual presynaptic neurons with simultaneous imaging of postsynaptic neurons at subcellular resolution. In this work we develop fast methods to filter voltage measurements and determine the location and strength of synaptic dendritic connections. For this we use two types
of data: 1) anatomical measurements of the postsynaptic neuron’s shape and dendritic arborization, used to build
a dynamical model of the cell, 2) voltage-sensitive fluorescence observations at subcellular resolution, sampled at
several controlled, changing locations, which provide the key dynamical variable, the spatiotemporal subthreshold voltage. The relatively low signal-to-noise ratio of imaging methods implies that optimal filtering methods are
needed in order to fully exploit the measurements. We formulated the problem in a Bayesian state-space framework. Under reasonable assumptions, determining the synaptic weights becomes a convex optimization problem.
To enforce sparseness, we used an l1 prior with coefficient λ. For Gaussian noise, we get a quadratic program
with a solution path piecewise-linear in λ, which we obtain by adapting known l1 methods to exploit the structure
of this problem and enforce the excitatory/inhibitory nature of the synapses. A major computational challenge
comes from the large number of compartments (N ∼ 10ˆ4) of the dendritic arbor and the O(N ˆ3) computational
cost of the exact solution. Using previously developed fast approximation ideas, we find the location and strength
of the synapses in O(N k +kˆ3) time, where k, the total number of synapses, is determined using model selection
techniques and generally k  N . We illustrate our results on simulated measurements in toy and real neurons
for different sampling techniques.

III-99. Casting light on the interplay between perception and decision making
in active sensing
Alex Gomez-Marin1
Aljoscha Schulze1
Vani Rajendran1
Gus K Lott2
Eric Trautman2
Parvez Ahammad2
Chris Werner2
Vivek Jayaraman2
Matthieu Louis1
1 Center
2 HHMI,

ALEX . GOMEZ @ CRG . EU
ALJOSCHA . SCHULZE @ CRG . EU
VANI . RAJENDRAN @ CRG . EU
GUSLOTT @ YARCOM . COM
TRAUTMANE @ JANELIA . HHMI . ORG
AHAMMADP @ JANELIA . HHMI . ORG
WERNERC @ JANELIA . HHMI . ORG
VIVEK @ JANELIA . HHMI . ORG
MATTHIEU. LOUIS @ ME . COM

for Genomic Regulation
Janelia Farm Research Campus

Active sensing couples motor responses to sensory processing in feedback loops that can be challenging to
investigate experimentally. We combined high-resolution behavioral analysis, electrophysiology, modeling, and
optogenetics to dissect the sensorimotor integration underlying chemotaxis in Drosophila melanogaster larvae.
When exposed to a static odor gradient, larvae orient through a series of runs punctuated by turns. The timing
and directionality of turning events proceed from active sampling. Runs are elongated when the larva moves upgradient. Turns are facilitated when the larva moves down-gradient. Prior to turning, the local gradient is resolved
through side-to-side head movements (casts). Larvae genetically engineered to retain function in a single olfactory
sensory neuron (OSN) demonstrate the same basic orientation strategy. We reconstructed the sensory dynamics
that the larva experiences at key decision points during unconstrained chemotaxis. Using electrophysiology,

COSYNE 2012

209

III-99
we recorded the responses of the single functional OSN to a replay of the stimulus time course. Two types of
signals were characterized in detail: rapid fluctuations in concentration associated with sub-second head casts,
and slower odor ramps detected during runs lasting several seconds. These neural responses constitute the
mechanistic basis for a model underlying the spatiotemporal integration of dynamical olfactory inputs and its
conversion into orientation responses. Our sensorimotor model was tested in a closed-loop tracker where the
behavioral state of a larva is classified in real-time. Using light sequences to trigger controlled activity patterns in
a single-functional OSN expressing channelrhodopsin, we can induce predictable changes in behavior in response
to well-defined sensory inputs delivered during specific behavioral states. We exploited this closed-loop paradigm
to establish a relationship between the neural integration of sensory evidence and the probability of implementing
stereotyped motor responses (timing of runs, directionality of turns). Overall, our work clarifies how a simple brain
uses active sensing and decision making to direct behavior.

210

COSYNE 2012

Author Index

B

Author Index
Abbott L., 29, 109, 115, 135, 162, 170
Acerbi L., 159
Acimovic J., 55
Aertsen A., 109
Afraz A., 78
Agarwal G., 117
Ahammad P., 209
Aitchison L., 52
Akrami A., 87
Aksay E., 60, 81, 191
Albanna B. F., 119
Albeanu D. F., 154, 158
Ales J., 92
Alonso J., 93, 152
Alvarez J., 153
Ames K., 25
Ammer J., 61
Andersen R., 68
Anderson E., 125
Anderson W. S., 117
Angelaki D., 146, 196
Aptekar J., 99
Aravinthan S., 32
Archer E., 172
Ardid S., 206
Asari H., 89
Assad J., 34
Averbeck B., 40
Ayaz A., 46
Babadi B., 53, 135
Babiak M., 24
Babul C., 153
Baccus S., 156
Ballard D., 30
Ban H., 95
Bandyopadhyay A., 154
Bao S., 119
Barack D., 76
Barak O., 102, 115
Bari B., 195
Barker A., 37
Barreiro A., 179
Barrett D., 53
Barry C., 193
Barthelmé S., 50
Bathellier B., 44

COSYNE 2012

Battaglia D., 171
Bays P., 125, 181
Bear M., 80
Beck J., 28, 70, 86, 130
Behabadi B., 175
Behrens T., 127
Beier K., 83
Bejjanki V., 70
Ben Dayan Rubin D., 112
Ben-Shaul Y., 88
Berens P., 180
Bergan J. F., 88
Berke J., 139
Berkes P., 69
Bernacchia A., 74
Berning M., 27
Bernstein H., 37
Berry M., 96, 145, 199
Besserve M., 114
Bethge M., 180, 200
Bhalla U. S., 197
Bharioke A., 141, 147
Bialek W., 173
Billings S., 92
Bishop W., 176
Black M., 152
Bodenhamer M., 67
Bohte S., 107
Bonci A., 153, 205
Born R., 194
Bornstein A., 131
Bossaerts P., 129
Bouchard K., 24
Boyden E., 33
Branson K., 129
Breton Y., 73
Briggs F., 42
Brody C., 184
Bromberg-Martin E., 181
Brown E., 23, 62, 117, 123
Brumby S. P., 103
Brunel N., 37
Brunton B., 184
Buckley C., 116
Buesing L., 114
Buia C., 132

211

D–E
Buice M. A., 108, 193
Bullock D., 71
Bumbacher E., 44
Buonomano D. V., 165
Burak Y., 54
Burge J., 46
Burgess N., 193
Burrows B., 36
Buschman T. J., 71
Butler V., 32
Butts D. A., 90, 201
Buzsaki G., 117
Callaway E., 145, 148
Carandini M., 46, 94, 135, 150
Carcea I., 37
Cardin J. A., 154
Carlson J. R., 143
Carlson N., 157
Carmena J., 81, 177
Carney L., 166
Carruthers I., 69, 144
Case A., 139
Cash S., 117
Cavanagh P., 78
Chaisangmongkon W., 134
Chaisanguanthum K., 24
Chan V. H., 134, 166
Chandler D., 140
Chang E., 24
Charles A., 56
Chase S., 51
Chen A., 99
Chen E., 199
Chen S., 32
Chen Y., 149, 180
Chestek C. A., 176
Chichilnisky E., 41
Chklovskii D., 32, 59, 141, 147, 157, 169
Christopoulos V., 127
Chung S., 54
Churchland A., 185
Churchland M., 83, 137
Clandinin T., 82
Clemens J., 143
Climer J., 139
Clopath C., 162
Coca D., 92
Coen-Cagli R., 121, 151
Cohen J., 56
Cohen Y., 159
Coleman T., 167
Collins A., 183
Colonnese M., 148
Conover K., 73

212

Author Index
Constantinidis C., 80
Cook M., 164
Cormack L., 205
Correia P., 142
Costa G., 76
Cottrell G., 187
Cowan N., 146
Cox S., 65
Crocker B., 114
Cueva C., 75
Cui Y., 201
Culurciello E., 207
Cummins G., 64
Cunningham J., 137
Curto C., 55, 57, 110, 122
Cusack R., 141
da Silveira R., 199
Daie K., 60, 81, 191
Dan Y., 33, 201
Daniel T., 65
Daw N., 75, 112, 131, 191
Dayan P., 49, 51, 73, 181
de Vries S., 82
Degeratu A., 110
Deisseroth K., 33
Dellu-Hagedorn F., 126
Demb J. B., 90
Deng P., 188
Denovellis E., 71
Derdikman D., 30
Deshmukh N., 145
Desimone R., 171
DeWeese M., 119, 157
DeWitt E., 76
Di Pietro N. C., 98
Diamond M. E., 87
DiCarlo J., 45, 78
Dimitrov A., 64
Diogo C., 71
Disney A., 148
Djamshidian A., 40
Doiron B., 163, 189
Dongre S., 41
Donoghue J., 117
Dougherty D., 36
Druckmann S., 169
Drugowitsch J., 111, 182
Duan C., 184
Dugué G., 142
Dulac C., 88
Ebihara A., 187
Ebitz R., 73
Ecker A., 180

COSYNE 2012

Author Index
Eden U., 171
Eldar E., 56
Emberly E., 98
Emonet T., 143
Engbert R., 50
Erlich J., 184
Ermentrout B., 116
Eskandar E., 36, 117
Esmaeili V., 87
Everling S., 206
Fagg A., 67, 176
Fairhall A., 65
Faisal A., 47, 57, 78, 192
Fan J., 190
Fang-Yen C., 32
Fangwen Z., 138
Farris H., 123
Fassihi A., 87
Felmy F., 61
Fernandes H., 160
Fernandez Bujan A., 109
Fetter R., 147
Field D., 140
Field G., 41
Fiete I., 84, 108, 118, 163, 193
Finkelstein A., 30
Fisek M., 87
Fiser J., 69
Fisher C., 199
Fisher D., 81
Fitzgerald J., 120
Flannery J., 33
Fleischer J., 58
Floresco S. B., 98
Florez Weidinger J., 168
Flynn B., 166
Foerster J., 30
Forkosh O., 99
Fox J., 99
Fründ I., 97
Frank L., 31, 208
Frank M. J., 74, 183
Freedman D., 134
Freeman J., 45, 150
Freiwald W., 187
Fried I., 62
Friederich U., 92
Friesen A., 72
Froemke R., 37, 134, 198
Frye M., 99
Fukunaga I., 27
Fulvio J., 128
Furl N., 40
Fusi S., 35, 102, 112

COSYNE 2012

G–H
Gütig R., 58
Gale J., 36
Gallant J., 133
Ganguli D., 105
Ganmor E., 50
Gardner T., 192
Gastpar M., 177
Gavornik J., 80
Gedalin M., 91
Geffen M., 69, 144
Geib T., 131
Geisler W. S., 46, 149
Genkin A., 157
Gentner T., 89, 198
Gerard-Mercier F., 135
Gerhard H., 200
German W., 31
Gerrard J., 36
Gershow M., 32
Gerstner W., 79, 112, 169, 175
Ghosh A., 63
Gilja V., 176
Gjorgjieva J., 172, 179
Gleiss S., 196
Gold J., 190
Golden J., 140
Goldman M., 60, 70, 81, 191
Golub M., 51
Gomez-Marin A., 209
Goodhill G., 49
Goris R., 67
Gotts S., 171
Gouvea T., 76
Graf A., 68
Granot-Atedgi E., 66
Graupner M., 177
Gray C., 96
Green C. S., 128
Greenberg B., 36
Gregoriou G., 171
Greschner M., 41
Griffiths T., 28
Gunning D., 41
Guo C., 129
Gupta P., 197
Gutnisky D., 38
Häusser M., 150
Hadizadeh S., 98
Haefner R. M., 69
Haider B., 150
Ham M. I., 103
Hanks T., 184
Hardie R., 41
Harnett M., 39

213

K

Author Index
Harrell P. G., 62
Harris K., 46, 135
Haslinger R., 124
Hasselmo M., 139
Hatsopoulos N., 67, 176
Hayden B., 76
Hedrick K., 65
Heinen S., 72
Hellmundt F., 106
Hennequin G., 169
Hennig R. M., 143
Hensch T., 40
Herz A. V., 61, 79, 138
Hikosaka O., 181
Hildebrandt K. J., 143
Hillar C., 96, 174
Hirokawa J., 27
Hochberg L., 117
Hong H., 45
Horwitz G., 34, 161
Hu R., 123, 124
Hu T., 59, 157
Huang H., 200
Huang J., 33
Huang Y., 72, 110
Huber D., 38, 39
Huggins J., 209
Huh D., 25
Huk A., 205
Hulme E., 32
Hunt J., 49
Hunt L., 127
Hunzinger J., 134
Husain M., 125
Huth A. G., 133
Hyman J. M., 98
Ikegaya Y., 102
Imaizumi K., 123
Insanally M., 119
Isely G., 174
Itskov V., 55, 57, 110, 122
Jadhav S., 31
Jain R., 168
Jaramillo S., 43
Jasinski P., 108
Jayaraman V., 209
Jazayeri M., 34
Jeffery K., 46
Jepson L., 41
Ji Ma W., 105
Jin J., 93, 152
Jitsev J., 136
Jocham G., 127

214

Johnson K., 24
Josic K., 124, 178
Juusola M., 41, 92
Kabra M., 129
Kanan C., 187
Kanitscheider I., 86
Kao J., 190
Kaping D., 206
Kapoor V., 114
Karklin Y., 100
Karlsson M., 184
Karpova A. Y., 129, 184
Kaschube M., 168
Kastner D., 156
Kaufman M. T., 83, 137
Kay K., 202
Kaye A., 145
Kayser C., 196
Keil W., 168
Kelly S., 152
Kemere C., 31, 208
Kempter R., 138
Kennedy A., 170
Kenyon G. T., 103
Kepecs A., 27
Keshvari S. O., 105
Ketz N., 164
Khawaja F. A., 201
Kiani R., 75
Kiggins J., 89
Kilpatrick Z., 116
Kim H., 119
Kim S., 167
Kirst C., 61, 171
Klyachko V., 188, 206
Knudsen D., 198
Koch C., 156
Koechlin E., 182
Koh M., 154
Kohn A., 111, 151
Kollo M., 27
Kolodziejski C., 113, 167
Komban S. J., 93
Koralek A. C., 81
Kording K., 160
Koster U., 96
Kover H., 119
Koyluoglu O. O., 118
Kozarev A., 58
Kramer M., 171
Kreiman G., 132
Kremkow J., 93
Kumar A., 109
Kwan A., 33, 201

COSYNE 2012

Author Index
Laje R., 165
Lajoie G., 165
Lara A., 71
Larsen B., 207
Lashgari R., 93
Latham P., 28, 53
Lawlor M., 84
Layton S., 208
Lazar A., 94, 121
LeCun Y., 164
Lee A. M., 153, 205
Lee C., 41
Lee D., 120
Lee M., 81, 191
Lee S., 33
Lee T., 133, 144
Lee Y., 110
Lees A., 40
Leibold C., 106
Leifer A., 32
Lengyel M., 51, 52, 64
Lepage K., 171
Leventhal D., 139
Levy J., 199
Lewis L. D., 117
Li P., 41
Li X., 41
Lim S., 70
Lin K., 165
Lin Y., 110
Lindbloom-Brown Z., 34
Linke A., 141
Linne M., 55
Lippert M. T., 196
List O., 41
Litke A., 41
Little D., 59
Litwin-Kumar A., 163
Liu L., 201
Liu X., 32
Lo C., 110
Logan G., 136
Logothetis N. K., 114, 196
Longtin A., 195
Looger L., 38
Lopour B., 62
Lorincz M., 142
Lott G. K., 209
Lottem E., 142
Louis M., 209
Lu Z., 147
Mäki-Marttunen T., 55
Ma W. J., 104, 106, 130
Macke J., 97, 114

COSYNE 2012

M
Madlon-Kay S., 75, 191
Madsen J., 117, 132
Magee J., 39
Mainen Z., 26, 76, 100, 142, 185
Majaj N., 45
Maldonado P., 153
Maler L., 195
Maloney L., 186
Mangun G. R., 42
Mante V., 132
Manzur H., 153
Marbach F., 154
Marchand A., 126
Markowitz J., 192
Marre O., 96
Marsat G., 195
Marshel J. H., 145
Martelli C., 143
Martins A. R., 37, 198
Masmanidis S., 33
Mathieson K., 41
Matthey L., 181
Mayko Z., 64
Mazyar H., 104
McCulloch M. E., 41
McDermott J., 88
McNamee D., 77
Mehraban Pour Behbahani F., 78
Meinertzhagen I., 147
Meister M., 89, 172
Mel B., 168, 175, 189
Memmesheimer R., 113, 122
Mendonca A., 100, 185
Merzenich M., 37
Mesgarani N., 24
Meyers E., 80
Mezer A., 202
Mian M., 36
Michel M., 149
Miller E., 71
Milosavljevic M., 156
Ming V., 44, 157
Mitchell J., 125, 149, 203
Mohebi A., 154
Molkov Y., 108
Moore C., 33, 208
Moore R. C., 144
Moore T., 36, 93, 186
Moreno R., 28, 104, 111
Morkonda S., 164
Morrison A., 136
Morrison K., 122
Morton D., 202
Movellan J., 200
Movshon T., 45, 67, 150, 174

215

Q–R
Mukamel E., 62, 117
Murray J., 128
Nandy A. S., 146, 149, 194
Nassar M., 190
Natan R., 69, 144
Naud R., 175
Nauhaus I., 148
Near J., 127
Nehrkorn J., 79
Neishabouri A., 57
Nern A., 199
Nesse W., 195
Newsome W., 75, 132
Niell C., 205
Nielsen K., 148
Nikolic D., 94
Nishimoto S., 133
Nitzany E., 85
Niv Y., 56
Niyogi R. K., 73
Norcia A., 92
Noudoost B., 186
Nuyujukian P., 176, 190
O’Connor D., 38, 39
O’Doherty J., 77
O’Kane C., 41
O’Reilly R., 132, 164
Ohl F. W., 196
Okada M., 140
Okun M., 135
Ollerenshaw D., 195
Olshausen B., 48, 96
Orbán G., 52
Osborne L., 86
Osman A., 207
Ostojic S., 29
Otazu G., 158
Oweiss K., 154
Pack C., 201
Pagan M., 119
Pakman A., 209
Palmeri T., 136
Panagiotaropoulos F., 114
Paninski L., 47, 158, 161, 209
Panzeri S., 114
Park I. M., 66, 172
Park J. H., 207
Park M., 161
Pasternak T., 203
Patel S., 36
Patzelt F., 98
Pawelzik K., 98

216

Author Index
Pearson J., 76
Pehlevan C., 29
Peng A., 206
Peron S., 38
Pesaran B., 191
Petreska B., 83
Pettibone J., 139
Phoumthipphavong V., 33
Pierce E. T., 62
Pieribone V., 207
Pillow J. W., 66, 161, 172, 205
Pinkoviezsky I., 179
Pitkow X., 103
Platisa J., 207
Platt M., 73, 76
Plaza S., 147
Plenz D., 60
Pnevmatikakis E. A., 158
Pobst J., 202
Polley D., 37
Portfors C., 64
Pouget A., 28, 70, 86, 100, 111, 185
Powers M., 123
Preuschoff K., 129
Price N., 194
Priebe N., 95
Pritchett D., 33
Purdon P., 62, 117
Qi X., 80
Qian N., 138
Radulescu A., 56
Rajan K., 173
Rajendran V., 209
Ramirez A., 47
Rangel A., 77
Rao R., 72
Raposo D., 185
Rau F., 143
Ray S., 72
Reifenstein E., 138
Reinagel P., 90
Reiser M., 199
Reppas J., 75
Reshef E., 78
Reyes A. D., 177
Reynolds J., 125, 149, 194, 203
Rieke F., 23, 179
Rigotti M., 102, 112
Ringach D., 62
Roelfsema P., 107
Rolfe J., 164
Romani S., 179
Rombouts J., 107

COSYNE 2012

Author Index
Romero Arandia I., 111
Romo R., 115
Rosenbaum R., 124, 189
Rosenberg A., 146
Roth Z., 122
Rothkopf C., 101
Rotman Z., 188, 206
Rowekamp R., 120
Rozell C., 48, 56
Rubin A., 179
Rubin G. M., 199
Rubin J., 189
Rubin R., 113, 122
Rumpel S., 44
Ruohonen K., 55
Rust N., 119
Rusu C. V., 155
Rybak I., 108
Ryu S., 25, 83, 137, 176, 190
Sabatini B., 83
Sabes P., 24
Sadeghi K., 102
Sahani M., 83, 94, 114
Saleem A., 46
Saunders A., 83
Savin C., 51
Saxe R., 39
Schaefer A., 27
Schafer B. J., 36
Schafer W., 32
Schaffer E., 29
Schall J., 136
Scheffer L., 147
Schmidt R., 139
Schnabel M., 168
Schneider D., 85
Schneidman E., 50, 63, 66, 99, 159
Schrater P., 101, 126–128, 185
Schreiber S., 138
Schreiner C., 37
Schulz D., 94
Schulze A., 209
Schwartz G., 145
Schwartz O., 121, 151
Seamans J. K., 98
Seely J., 137
Segal I. Y., 91
Segev R., 50, 63, 66, 91
Seidemann E., 149, 180
Seitz A., 52
Sejnowski T., 25
Series P., 52, 126, 173
Seybold B. A., 37
Shanechi M., 123

COSYNE 2012

T
Sharpee T., 120, 145, 149, 156, 194
Shea-Brown E., 165, 179
Shemesh Y., 99
Shen H., 24
Shenoy K., 25, 83, 132, 137, 176, 190
Sheppard J., 185
Sheth S., 36
Shevtsova N., 108
Shi K., 120
Shimazaki H., 102
Shin H., 106
Shizgal P., 73
Shoemaker P., 99
Siegle J. H., 33, 208
Simoncelli E. P., 45, 67, 88, 100, 105, 150, 174
Sincich L., 120
Singer W., 94
Skatova A., 75
Slutskiy Y. B., 121
Smith C., 161
Smith J., 108
Smith M. A., 204
Snow M., 121
So K., 177
Sobel N., 26
Solomon E., 45
Solomon R., 73
Soltani A., 36, 186
Sommer F., 59, 117, 174
Sommer M., 204
Sompolinsky H., 29, 53, 54, 113, 122, 170, 172
Soo F., 145
Sotiropoulos G., 52
Sponberg S., 65
Sprekeler H., 79, 162
Sriram B., 90
Srivastava N., 126
Stanley G., 152, 195
Stavisky S., 190
Steinmetz N., 93
Stemmler M., 61, 138
Stern M., 162
Stevenson I., 160
Stinson P., 86
Stocker A., 120
Strouse D., 64, 189
Stryker M. P., 205
Suminski A., 67, 176
Sussillo D., 115, 132
Svoboda K., 38, 39
Sylvester S., 81, 191
Sztainberg Y., 99
Tai L., 153
Takagaki K., 196

217

W–Y
Takemura S., 147
Talbott W., 200
Tan A. Y., 95
Tang H., 132
Tang S., 41
Taniguchi H., 33
Tanimoto H., 79
Tavassoli A., 62
Taylor S. V., 192
Tee J., 186
Teeter C., 166
Terashima H., 140
Tervo D., 129, 184
Tetzlaff C., 113, 167
Theunissen F., 144
Thomik A. A., 47
Thurley K., 106
Tian L., 38
Timme M., 113, 167, 171
Ting C., 41
Tittgemeyer M., 136
Tjan B., 146
Tkacik G., 63, 66, 96
Tolias A., 114, 180
Towal R. B., 156
Toyoizumi T., 102, 116
Trautman E., 209
Trettel S., 163
Triesch J., 155
Trousdale J., 178
Trukenbrod H., 50
Tsigankov D., 168
Tsodyks M., 115, 179
Tsuchida T., 131
Turner M., 196
Tuthill J., 199
Ulanovsky N., 30
Urban L., 119
Ushakova L., 44
Usrey W. M., 42
Valton V., 126
Van Den Berg R., 104–106, 130
Vaziri-Pashkam M., 78
Veliz-Cuba A., 55
Vicente M., 100, 185
Victor J., 85
Vijayakumar S., 159
Vilankar K., 140
Vinck M., 206
Vintch B., 174
Vitaladevuni S., 147
Vogels T. P., 79, 162, 169
Voigts J., 208

218

Author Index
Vu A., 133
Wörgötter F., 113, 167
Walker J., 122
Wallis J., 71
Walsh J. L., 62
Walther D., 207
Wandell B., 202
Wang Q., 152, 195
Wang X., 74, 128, 134
Wang Y., 93, 152
Wang Y. V., 90
Wang Z., 120
Wardill T. J., 41
Wayne G., 109
Wei Z., 59
Weiner V., 117
Weiss Y., 48
Welchman A., 95
Wen Q., 32
Werner C., 209
Wessel R., 202
Whitesides G., 32
Wichmann F., 50, 97
Widloski J., 84
Wiecki T., 74
Wilbrecht L., 153, 205
Willett F. R., 67, 176
Williams S., 39
Williams Z., 123, 124
Wilson M., 208
Wilson R., 87
Winawer J., 202
Wohl M., 119
Wolf F., 168
Wolpert D., 159
Womelsdorf T., 206
Wong K. F. K., 62
Wood F., 161
Woolley S., 43, 85
Wornell G. W., 123
Wu X., 189
Wyart M., 32
Wyatte D., 132
Xu N., 39
Yamaguchi M., 136
Yang H., 60
Yang Y., 152
Yap H. L., 56
Yarali A., 79
Yarom Y., 162
Yarrow S., 173
Yates J., 205

COSYNE 2012

Author Index

Z

Yger P., 135
Yoon K., 193
Yu A. J., 131
Yu B., 51, 176
Yu S., 60
Yuan K., 37
Zador A., 43, 152, 182
Zaharia A. D., 174
Zaidel A., 196
Zaika N., 37
Zarella M., 203
Zenke F., 79
Zhang H., 186
Zhang S., 33
Zhaoping L., 138
Zheng H., 195
Zhu M., 48
Ziegler L., 112
Ziemann U., 155
Ziemba C. M., 45, 150
Znamenskiy P., 43, 182
Zoran D., 48
Zucker S., 84

COSYNE 2012

219

