www.cosyne.org

COSYNE 2009

CE
N
E
I
C
S
O
UR
E
N
S
M
E
ST
Y
S
ing
t
D
e
e
N
M
A
h
t
6
AH
NAL
T
O
I
U
,
T
Y
A
T
T
I
U
KE C
A
COMP
L
T
L
A
S

THE SWARTZ FOUNDATION

www.TheSwartzFoundation.org

Program Summary
Thursday, 26 February
4:00pm

Registration opens

6:00pm

Welcome reception

7:30pm

Keynote talk

8:30pm

Poster Session I

Friday, 27 February
7:30am

Breakfast

8:30am

Morning Session (break 10:00 – 10:30)

12:00pm

Lunch break (and last chance to see Session I posters)

2:00pm

Afternoon Session (break 3:15 – 3:45)

5:00pm

Dinner break

7:30pm

Poster Session II

Saturday, 28 February
7:30am

Breakfast

8:30am

Morning Session (break 10:00 – 10:30)

12:00pm

Lunch break (and last chance to see Session II posters)

2:00pm

Afternoon Session (break 3:15 – 3:45)

5:00pm

Dinner break

7:30pm

Poster Session III

Sunday, 1 March
7:30am

Breakfast

8:30am

Morning Session (break 10:00 – 10:30)

11:45am

Lunch break (hotel checkout, and last chance to see Session III posters)

2:00pm

Afternoon Session

4:00pm

Closing remarks

COSYNE 09

i

Frontiers in Neuroscience
“Access to knowledge
is a right,
not a privilege”

> Is a printed
and online journal
series for researchers
by researchers

> Is a new open-access,
equal opportunity
publishing model

> Is a community space
for scholars

> Promotes and freely
disseminates scientific
discoveries worldwide

> Counts over
60’000 neuroscience
readers, 1’700 world class
neuroscience editors,
and about 8’000 pages
on the Frontiers website

www.frontiersin.org
Frontiers in Neuroscience (www.frontiersin.org) is currently composed of 22 journals and
will expand to cover the full field with more than 30 specialty journals. Take the opportunity to shape the future of scientific publishing and submit your article for consideration
to Frontiers!
Computational Neuroscience Enteric Neuroscience Evolutionary Neuroscience Human Neuroscience
Integrative Neuroscience Molecular Neuroscience Neural Circuits Neuroanatomy Neuroenergetics Neuroengineering Neurogenesis Neurogenomics Neuroinformatics Neuromethods Neuropharmacology Neuroprosthetics Neurorobotics Neuroscience and Society Synaptic Neuroscience Systems Neuroscience
ii

Looking for what Neuron has
published in Computational and

Systems Neuroscience?

nisms.
l network
ard mecha
in a neura
y feed-forw
k
el
c
ur
a
p
b
g
d
in
e
patterns
ntained us
ithout fe
6 issue.
ral activity
u
Memory w . Neuron. 2009 Feb 2 t memories can be mai
e
n
f
o
n
atio
man
gges
e amplific
Mark Gold
g efforts su
of selectiv sue.
hanisms.
nal modellin
m
io
is
at
n
ut
a
p
h
m
rward mec
c
o
e
is
C
new m
ely feed-fo
9 Feb 26
a
ur
0
:
p
0
n
2
g
o
in
n.
ti
a
us
ro
c
eu
ed
amplifi
D. Miller. N
issue.
n be achiev
Balanced
d Kenneth
09 Jan 15
lification ca

These papers
available FREE at
www.neuron.org
through 3/26/09!

de.
ing electro
t amp
ron. 20
Murphy an
rts sugges
andini. Neu
d the record
fo
ar
ef
un
Brendan K
C
o
g
o
ar
lin
te
n
el
at
d
io
rtex
ach, M
nal mo
ow reg
Visual Co
ario L. Ring
a fairly narr
Computatio
otentials in ucci, Vincent Bonin, D , reflect this activity in
P
ld
ie
nkey
i.
F
f
Ben
tivity
in o
. Bandettin
n and Mo
Local Orig , Ian Nauhaus, Andrea reflecting population ac
rtex of Ma y, Keiji Tanaka, Peter A
o
C
l
ra
er
le
o
zn
hi
p
,w
r Tem
ein Estek
Steffen Kat
LFP signals
d monkey.
urka, Hoss
s in Inferio
ggest that
een man an
sentation zbeh Kiani, Jerzy Bod
w
re
et
p
b
Results su
e
ns
R
io
t
c
resentat
al Obje
. Ruff, Roo
coders
cortical rep
Categoric
Douglas A
Image De
lationship in
re
Matching eskorte, Marieke Mur,
se
o
cl
ale Local yasu Kamitani.
a
c
e
is
at
g
lt
tr
u
rie
ns
K
o
M
f
Yuki
on o
Nikolaus
thors dem
issue.
iro Sadato,
Combinati
proach, au
08 Dec 26
y using a , Hiroki C. Tanabe, Norih
n-based ap
it
Neuron. 20
o
v
is
ti
c
ar
p
A
m
in
ique co
Morito
an Bra
Using a un
from Hum Masa-aki Sato, Yusuke
n alone.
struction
,
n
ta
o
hi
c
as
e
m
R
l informatio
e
Ya
g
fMRI signa
e
Visual Ima i, Hajime Uchida, Okito
th
m
o
fr
ak
ior
tructed
tely recons
lar Behav
Yoichi Miyaw
sue.
nvariant
n be accura
ar Vestibu ct 23 issue.
8 Dec 10 is
e
ca
0
0
in
s
2
L
an
a
n.
m
ity for rate-i
s
ro
hu
rt
Neu
Suppo
ceived by
n. 2008 O
n
usual capac
er
ro
io
un
p
s
eu
’s
is
es
N
se
m
ag
c.
p
s
Tran
a du La
Visual im
rent syna
t Synaptic
stich, Sasch m the vestibular affe
dependen McElvain, Michael Faul
fro
n
lt
-I
y
su
c
re
n
e
to
u
Freq
appears
uren E.
Bagnall, La
vertebrates
reflexes in
Martha W.
ar
ul
ib
st
ve
f
yo
The linearit
release.
glutamate

w)
RTICLES
on (Revie
REVIEW A
l of Attenti n. 2009 Feb 12 issue.
e
d
o
M
n
o
ro
. Neu
alizati
The Norm lds and David J Heeger
no
ey
R
H
rspective)
hn
Jo
Rising (Pe
e
c
n
ie
c
s
al Neuro
6 issue.
Theoretic
2008 Nov
n.
bott. Neuro
Larry F. Ab

www.neuron.org
iii

januar y 2009 volume 10 no. 1
www.nature.com/reviews

NEUROSCIENCE

LEARNING TO READ FACES
Development of facial-emotion
recognition

Nature Neuroscience
Nature Neuroscience , the primary research
journal in the field, provides the international
community with a highly visible forum in which
the most exciting developments in all areas of
neuroscience can be communicated to a broad
readership. A lively front half, including News &
Views, Reviews, Book Reviews and editorials
help place the primary research in context,
providing readers with a broad perspective.

I feel, therefore I am

Awareness in the anterior insula

Nature Reviews Neuroscience
Nature Reviews Neuroscience is the leading
monthly review journal in the neurosciences. It
publishes articles that review and comment on
recent progress in nervous system research.
Topics include molecular, cellular, circuit and
computational aspects of neuronal development
and function, behaviour, cognition and disorders
of the nervous system.
Chief Editor:

Editor: Kalyani Narasimhan
Impact Factor:

15.664*
4/211 in Neurosciences

Impact Factor:

Claudia Wiedemann
24.520*
2/211 in Neurosciences

www.nature.com/reviews/neuro

Nature Clinical Practice Neurology
Nature Clinical Practice Neurology delivers
up-to-date developments in neurology with clear,
concise interpretations of clinical applications and
a format that allows for quick and easy reference.
Providing scientifically sound evaluations of new
studies and insightful commentaries from leading
experts, as well as in-depth reviews on topics of
current interests, readers will always be informed
of the latest findings as they relate to practical
patient care.
Editor-in-Chief:

John W Griffin, MD

www.nature.com/ncpneuro

www.nature.com/natureneuroscience
*2007 Journal Citation Report (Thomson Reuters, 2008)

www.nature.com/neurosci

iv

COSYNE 09

The MIT Press

Visit our booth for a 30% discount

Eye, Retina, and Visual System
of the Mouse

edited by Leo M. Chalupa and Robert W. Williams
“This comprehensive and well-produced volume
collects our essential knowledge of mouse vision
into a single extraordinarily useful volume. It will
be the standard reference for years to come.”
— J. Anthony Movshon, New York University
872 pp., 264 illus. in color and b&w, $135 cloth

Protocells
Bridging Nonliving and Living Matter
edited by Steen Rasmussen, Mark A. Bedau,
Liaohai Chen, David Deamer, David C. Krakauer,
Norman H. Packard, and Peter F. Stadler
“Protocells are playing increasingly important
roles in studies on the origin of life, artificial life,
and synthetic biology. This book serves as a
bridge for both nonexperts and experts in the
field, providing introductory and primer material
on protocells, as well as more advanced, cuttingedge updates on this exciting subject.”
— J.J. Collins, Boston University and Co-Director,
Center for BioDynamics
776 pp., 20 color illus., 100 b&w illus., $75 cloth

Toward Brain-Computer
Interfacing

edited by Guido Dornhege, José del R. Millán,
Thilo Hinterberger, Dennis J. McFarland,
and Klaus-Robert Müller
The latest research in the development of technologies that will allow humans to communicate,
using brain signals only, with computers, wheelchairs, prostheses, and other devices.
512 pp., 150 illus., $45 cloth

Modeling Biology
Structures, Behaviors, Evolution
edited by Manfred D. Laubichler
and Gerd B. Müller
“The contributors offer critical reflections on the
intricate interplay of experiment and modelbuilding, as creative imaginations equipped with
the power of modern computing search for the
patterns that underlie and inform the complex
variety of the living world.” — Michael S. Mahoney,
Princeton University
400 pp., 103 illus., $50 cloth

Bayesian Brain
Probabilistic Approaches to
Neural Coding
edited by Kenji Doya, Shin Ishii,
Alexandre Pouget and Rajesh P. N. Rao
Experimental and theoretical neuroscientists
use Bayesian approaches to analyze the brain
mechanisms of perception, decision-making, and
motor control.
340 pp., 102 illus., 10 color, $55 cloth

The Computational Neurobiology
of Reaching and Pointing
A Foundation for Motor Learning
Reza Shadmehr and Steven P. Wise
“This is a scholarly, comprehensive, and sophisticated view of motor learning.” — Emilio Bizzi,
Institute Professor, MIT
595 pp., 165 illus., $70 cloth

Dynamical Systems in
Neuroscience
The Geometry of Excitability and Bursting
Eugene M. Izhikevich
“This book will be a great contribution to the
subject of mathematical neurophysiology.”
— Richard Fitzhugh, former researcher, National
Institutes of Health
457 pp., 409 illus., $62 cloth

Theoretical Neuroscience
Computational and Mathematical
Modeling of Neural Systems
Peter Dayan and L. F. Abbott
“Not only does the book set a high standard for
theoretical neuroscience, it defines the field.”
— Dmitri Chklovskii, Neuron
476 pp., $40 paper

Exploring the Thalamus and Its
Role in Cortical Function
Second Edition
S. Murray Sherman and R. W. Guillery
“A scholarly study with great depth, touching not
only its immediate subject but also addressing
broad questions about the whole organization of
the brain… [I]t belongs on the shelves of all serious students of CNS.” — Richard Maslund, Trends
in Neurosciences
508 pp., 101 illus., $68 cloth

To order call 800-405-1619 • http://mitpress.mit.edu

The MIT Press publishes a wide variety of books in Neuroscience and related fields, with a special emphasis on Computational
Neuroscience, and welcomes the submission of proposals and manuscripts for possible publication. Please contact Bob Prior,
Executive Editor for the Life Sciences, for information on our publishing program (prior@mit.edu / 617-253-1584).

iv

Visit TDT in Booth 823

Only one other clip was
so perfectly designed.
ZIF-Clip Headstages from TDT
TM

32 Channels
128 Channels
64 Channels

96 Channels

Handy, but Ø channels

Self-aligning - automatically positions the high

TDT’s new miniature, low insertion-force set of connectors and active

density connectors on the headstage and probe

headstages support electrode arrays of up to 128 channels. The ZIF-Clip

Auto-locking - ﬁrmly locks the
headstage and probe in place

system features an innovative, hinged headstage design that ensures quick,
easy headstage connection with almost no insertion force applied to the subject. ZIF-Clip™ headstages and compatible probes are featured in our com-

Low insertion force - hinge design

plete system solutions for both chronic and acute applications. Headstages,

directs almost no force toward the subject

probes and manufacturer’s development kits are now available!

Low impact - stress-free connection
Tucker-Davis Technologies

without anesthetizing the subject

11930 Research Circle
Maximum return - lasts longer than

Alachua, Florida 32615

traditional micro connector designs

Made by TDT - ensures highly reliable
quality, performance and availability

vi

Tel: 386.462.9622

Tucker-Davis Technologies
www.tdt.com

Fax: 386.462.5365
E-mail info@tdt.com

COSYNE 09

About Cosyne
The annual Cosyne meeting provides a forum for the exchange of experimental
and theoretical results in systems neuroscience. Presentations are arranged in a
single track, so as to encourage interdisciplinary interactions.
The 2009 meeting consists of 13 invited talks selected by the Executive
Committee, along with 25 talks and 300 posters selected from the submitted
abstracts by the Program Committee. Ten of the poster presentations will also
give a four-minute “spotlight” presentation summarizing their submitted work.
The abstracts of the 2009 meeting will be published by Frontiers in Systems
Neuroscience. Similar to the abstracts of the Society for Neuroscience meeting,
these abstracts are citeable, but they are not full-length proceedings and
therefore do not preclude further publication.

Cosyne 2009 Organizing Committee
Organizing Committee:





General Chair: Matteo Carandini (University College London)
Program Chair: Maneesh Sahani (University College London)
Workshop Chairs: Adam Kohn (Yeshiva University), Alex Huk (UT Austin)
Publications Chair: Alex Wade (Smith-Kettlewell Eye Research Institute)

Executive Committee:




Tony Zador (Cold Spring Harbor Laboratory)
Alex Pouget (University of Rochester)
Zach Mainen (Instituto Gulbenkian de Ciencia)

Advisory Board:





Eero Simoncelli (New York University and HHMI)
Peter Dayan (University College London)
Steven Lisberger (UC San Francisco and HHMI)
Karel Svoboda (HHMI Janelia Farms)

vii

viii

Travel Grants
Thanks to the generosity of The Swartz Foundation and Neuron, many travel
grants were made available to support student and postdoc participation in the
main Cosyne meeting.
Awards were allocated based on need as well as a program committee
assessment of the submitted abstract. The recipients are listed below.
Recipients of a Swartz Cosyne Fellowship:
Uwe Friederich, Pietro Berkes, Jonathan D. Drover, Pavel
Itskov, Sam Gershman, Sacha van Albada, Alberto
Bernacchia, Gasper Tkacik, Jason Prentice, Matthieu Gilson,
Ashutosh Mohan, Salva Ardid, Alexandra Smolyanskaya,
Jian Liu, David Kastner, Shuzo Sakata, Nate Smith, Joseph
Schumacher, Florian Raudies, Edward Wallace.

Recipients of a Neuron Cosyne Fellowship:
Mark Churchland, Anita M. Schmid, Zhuoyi
Song, Philipp Berens, Jakob Macke.

Conference Support
Registration/Hotel arrangements: Denise Soudan, Conference and Events Office,
University of Rochester
Online Submissions/Reviews: Thomas Preuss, confmaster.net
Poster/Program cover design: Sabina Carandini, Studio InkLink

ix

x

Program

Program
(Note: institutions listed in the program are the primary affiliation of the first author. For the complete list,
please consult the abstracts.)

Thursday, 26 February
4:00pm

Registration opens

6:00pm

Welcome reception, including cocktails and buffet

7:25pm

Introductory Remarks

7:30pm

Keynote address: Internal representations of the olfactory world
R. Axel, Howard Hughes Medical Institute and Columbia University . . . . . . . . . 23

8:30pm

Poster Session I

Friday, 27 February
7:30am

Continental breakfast

8:30am

Multiple-electrodes, brain rhythms, and cognition
E. K. Miller, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . . . 23

9:15am

Burst spiking of single cortical neuron switches global brain state
C. Li, M.-M. Poo, Y. Dan, University of California, Berkeley . . . . . . . . . . . . . . 24

9:30am

Parvalbumin interneurons and oscillations enhance information processing in cortical microcircuits
V. Sohal, F. Zhang, O. Yizhar, K. Deisseroth, Stanford University . . . . . . . . . . . 25

9:45am

The asynchronous state in the cerebral cortex
A. Renart, J. de la Rocha, L. Hollender, B. Haider, A. Duque, D. McCormick, N.
Parga, A. Reyes, K. D. Harris, Rutgers University . . . . . . . . . . . . . . . . . . . . 26

10:00am

Refreshment break

10:30am

The neuroeconomics of simple goal-directed choices
A. Rangel, California Institute of Technology . . . . . . . . . . . . . . . . . . . . . . 27

11:15am

Neural activity in frontal eye field during flexible decision-making
V. Ferrera, M. Yanike, C. Cassanello, Columbia University . . . . . . . . . . . . . . . 27

11:30am

An Infinite Mixture Model of Context-dependent Learning and Extinction
S. Gershman, D. Blei, Y. Niv, Princeton University . . . . . . . . . . . . . . . . . . . 28

11:45am

An ideal-observer model for optimal inference in the presence of different types of
uncertainty
R. Wilson, M. Nassar, J. Gold, University of Pennsylvania . . . . . . . . . . . . . . . 29

12:00pm

Lunch break (and last chance to see Session I posters)

COSYNE 09

1

Program
2:00pm

Computational role of short-term synaptic plasticity in the neocortex
M. Tsodyks, Weizmann Institute of Science . . . . . . . . . . . . . . . . . . . . . . . 30

2:45pm

Reward enhances reactivation of experience in the hippocampus
A. Singer, L. Frank, University of California, San Francisco . . . . . . . . . . . . . . 30

3:00pm

State-dependent cortical processing: Cholinergic modulation of visual responses
M. Goard, Y. Dan, University of California, Berkeley . . . . . . . . . . . . . . . . . . 31

3:15pm

Refreshment break

3:45pm

Half a wiring diagram is better than none
C. Bargmann, The Rockefeller University . . . . . . . . . . . . . . . . . . . . . . . . 32

4:30pm

How neural systems adjust to different environments: an intriguing role for gap
junction coupling
S. Nirenberg, C. Pandarinath, I. Bomash, J. Victor, W. Tschetter, Weill Medical
College of Cornell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

4:45pm

Spotlights
Summation properties of frequency-dependent disynaptic inhibition between pyramidal cells
T. Berger, G. Silberberg, R. Perin de Campos, H. Markram, Ecole Polytechnique
Federale, Lausanne . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
Distinct adaptive modes for weak and strong signals in a retinal population
D. Kastner, S. Baccus, Stanford University . . . . . . . . . . . . . . . . . . . . . . . 185
Extracting MAX-pooling receptive fields with natural image fragments
M. Vidal-Naquet, S. Ullman, M. Tanifuji, Riken - Brain Science Institute . . . . . . . 200
Visual response properties of V1 neurons projecting to V2 in macaque
Y. El-Shamayleh, R. Kumbhani, N. Dhruv, J. A. Movshon, New York University . . . 202
OFF direction-selective cells in the mouse retina
Y. Zhang, I. Kim, J. Sanes, M. Meister, Harvard University . . . . . . . . . . . . . . . 205

5:00pm

Dinner break

7:30pm

Poster Session II

Saturday, 28 February

2

7:30am

Continental breakfast

8:30am

Bayesian reconstruction of perceptual experiences from human brain activity
J. Gallant, University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . . 33

9:15am

Attention reduces trial-to-trial and correlated variability in V4 neurons
M. Cohen, J. Maunsell, Harvard Medical School . . . . . . . . . . . . . . . . . . . . 34

9:30am

Statistical decision theory and the allocation of cognitive resources in multiple object
tracking
E. Vul, M. Frank, G. Alvarez, J. Tenenbaum, Massachusetts Institute of Technology

35

9:45am

Unlimited-capacity, metabolically constrained visual short-term memory in multiple
object tracking
W. J. Ma, W. Huang, Baylor College of Medicine . . . . . . . . . . . . . . . . . . . . 36

10:00am

Refreshment break

COSYNE 09

Program
10:30am

The fly lobula plate: a sensory network for ego-motion estimation based on optic
flow
A. Borst, Max Planck Institute of Neurobiology . . . . . . . . . . . . . . . . . . . . . 36

11:15am

Efficient spike encoding for mapping visual receptive fields
G. Pipa, Z. Chen, S. Neuenschwander, B. Lima, E. Brown, Max-Planck Institute for
Brain Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

11:30am

Inferring functional connectivity in an ensemble of retinal ganglion cells sharing a
common input
M. Vidne, J. Kulkarni, Y. Ahmadian, J. Pillow, J. Shlens, E. Chichilnisky, E. Simoncelli, L. Paninski, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . 38

11:45am

Different strategies for coding What and When in the archer fish retina
G. Vasserman, M. Shamir, R. Segev, Ben Gurion University . . . . . . . . . . . . . 39

12:00pm

Lunch break (and last chance to see Session II posters)

2:00pm

Learning, and learning to learn, with hierarchical Bayesian models
J. Tenenbaum, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . 40

2:45pm

Matching spontaneous and evoked activity in V1: a hallmark of probabilistic inference
P. Berkes, G. Orban, M. Lengyel, J. Fiser, Brandeis University . . . . . . . . . . . . 40

3:00pm

A walk through the woods explains the space variant oblique effect
T. Weisswange, C. Rothkopf, J. Triesch, Frankfurt Institute for Advanced Studies . . 41

3:15pm

Refreshment break

3:45pm

Interpreting primary motor cortex based on optimal feedback control
S. H. Scott, Queen’s University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

4:30pm

Representation of Motor Learning in the Smooth Eye Movement Region of the
Frontal Eye Fields
J. Li, S. Lisberger, University of California San Francisco . . . . . . . . . . . . . . . 42

4:45pm

Spotlights
Detection of extracellular potentials using a mechanical-based nanosensor
A. Sadek, R. Karabalin, J. Du, M. Roukes, C. Koch, G. Laurent, S. Masmanidis,
California Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
Motor planning in the rat superior colliculus
G. Felsen, Z. Mainen, Cold Spring Harbor Laboratory . . . . . . . . . . . . . . . . . 216
Localizing the origin of executive control over distributed processing to prefrontal
cortex
M. Chafee, S. Jain, R. Blackman, University of Minnesota . . . . . . . . . . . . . . . 222
Using Brainbow and GRASP for detailed reconstruction of complete circuits with
light microscopy
Y. Mishchenko, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
Towards fast in vivo neuronal imaging using objective coupled planar illumination
microscopy
D. Turaga, T. Holy, Washington University School of Medicine . . . . . . . . . . . . . 254

5:00pm

Dinner break

7:30pm

Poster Session III

COSYNE 09

3

Program

Sunday, 1 March

4

7:30am

Continental breakfast

8:30am

The building blocks of cerebellum-dependent learning
J. Raymond, Stanford University School of Medicine . . . . . . . . . . . . . . . . . . 43

9:15am

Developing a working memory with reward-modulated STDP
C. Savin, J. Triesch, Frankfurt Institute for Advanced Studies . . . . . . . . . . . . . 44

9:30am

Robust learning of position invariant visual representations with OFF responses
H. Sprekeler, W. Gerstner, Brain Mind Institute, EPFL . . . . . . . . . . . . . . . . . 44

9:45am

Sparse neural representations of odor and associative learning
I. Ito, C. Ong, B. Raman, M. Stopfer, NICHD . . . . . . . . . . . . . . . . . . . . . . 45

10:00am

Refreshment break

10:30am

Synaptic mechanisms of whisker sensory perception
C. Petersen, Brain Mind Institute, EPFL . . . . . . . . . . . . . . . . . . . . . . . . . 46

11:15am

Nonlinear receptive field mapping reveals two subpopulations of orientation selective neurons in V2.
A. Schmid, J. Victor, Weill Medical College of Cornell . . . . . . . . . . . . . . . . . 46

11:30am

Imperfect receptive fields can enhance performance of neural populations
Y. Liu, T. Sharpee, The Salk Institute . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

11:45am

Lunch break (hotel checkout, and last chance to see Session III posters)

2:00pm

Cortical analysis of auditory scenes and speech
S. Shamma, University of Maryland . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

2:45pm

Natural sound selectivity in the auditory forebrain is strongly shaped by the acoustic
environment
N. Amin, F. Theunissen, Helen Wills Neuroscience Inst., UC Berkeley . . . . . . . . 49

3:00pm

Synaptic mechanisms underlying sustained responses in auditory cortical neurons.
M. Wehr, B. Scholl, University of Oregon . . . . . . . . . . . . . . . . . . . . . . . . 49

3:15pm

Connecting brain to mind through computation: The birth of computational psychiatry
P. R. Montague, Baylor College of Medicine . . . . . . . . . . . . . . . . . . . . . . . 50

4:00pm

Closing remarks

COSYNE 09

Posters I

Poster Session I

8:30pm Thursday 26th February

I-1. Network adaptation improves temporal representation of naturalistic stimuli in Drosophila eye
M. Juusola, A. Nikolaev, T. J. Wardill, C. J. O’Kane, G. G. de Polavieja, L. Zheng, University of
Sheffield . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
I-2. Offset adaptation in the inner hair cell synapses enhances speech coding
H. Wang, M. Rudnicki, W. Hemmert, Infineon Technologies, AG . . . . . . . . . . . . . . . . . . . . 52
I-3. Phase Response Curve of Spike Response Model
M. IIDA, T. Omori, T. Aonishi, M. Okada, The University of Tokyo . . . . . . . . . . . . . . . . . . . . 52
I-4. Intrinsic membrane properties control gamma-frequency input integration
S. Otte, A. Hasenstaub, T. Sejnowski, E. Callaway, Salk Institute . . . . . . . . . . . . . . . . . . . . 53
I-5. The spatial extent of attentional facilitation and inhibition of return in humans and monkeys
A. Khan, N. Takahashi, S. Heinen, R. McPeek, Smith-Kettlewell Eye Research Institute . . . . . . . 54
I-6. Saccade related phase resetting of theta and delta rhythms modulates cortical high gamma
activity
C. Kovach, H. Kawasaki, N. Tsuchiya, M. Howard, R. Adolphs, University of Iowa Hospitals and
Clinics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
I-7. Causal role of auditory cortex in mediating attention to moments in time
S. Jaramillo, A. Zador, Cold Spring Harbor Laboratory . . . . . . . . . . . . . . . . . . . . . . . . . . 55
I-8. Computing the cost function in decision making
J. Drugowitsch, R. Moreno-Bote, A. Pouget, University of Rochester . . . . . . . . . . . . . . . . . . 56
I-9. Estimation and reproduction of time intervals by LIP neurons
M. Jazayeri, M. Shadlen, University of Washington . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
I-10. Behavior-dependent responses in primate frontal cortex neurons during natural behavior
C. Miller, X. Wang, Johns Hopkins University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
I-11. Adaptive decision making in monkeys during a non-stationary rock-paper-scissors game
H. Abe, D. Lee, Yale University School of Medicine . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
I-12. The Human Brain Computes Two Different Prediction Errors
J. Glascher, N. Daw, P. Dayan, J. O’Doherty, California Instititute of Technology . . . . . . . . . . . . 60
I-13. A Computational and Neurobiological Account of Theory of Mind
W. Yoshida, B. Seymour, K. Friston, R. Dolan, University College London . . . . . . . . . . . . . . . 61
I-14. Bayesian decision making predicts adaptive sensory weights and decision threshold
S. Deneve, Ecole Normale Supérieure, Paris . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
I-15. Computational implications of a normalized value representation in decision circuits
K. Louie, L. Grattan, P. Glimcher, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . 62
I-16. Analysis of neural activity during error trials in decision-making task
R. J. Cotton, A. S. Tolias, Baylor College of Medicine . . . . . . . . . . . . . . . . . . . . . . . . . . 63
I-17. Evaluating signal detection models of perceptual decision confidence
B. Maniscalco, H. Lau, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
I-18. Dynamical state spaces of cortical networks representing various horizontal connectivities
N. Voges, L. Perrinet, INCM / CNRS – Univ. Aix-Marseille . . . . . . . . . . . . . . . . . . . . . . . . 65
I-19. Typical behaviors in co-evolving recurrent network of oscillatory neurons
T. Aoki, T. Aoyagi, Kyoto Univesity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

COSYNE 09

5

Posters I
I-20. A constructive mean-field analysis of multi population neural networks with random synaptic
weights
O. Faugeras, J. Touboul, B. Cessac, INRIA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
I-21. Insights into Parkinson’s disease from mean-field modeling of brain electrical activity
S. van Albada, R. Gray, P. Drysdale, P. Robinson, The University of Sydney . . . . . . . . . . . . . . 67
I-22. Neural correlations in a heterogeneous network model dominated by recurrent inhibition
A. Bernacchia, X.-J. Wang, Yale University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
I-23. Generalized Wilson-Cowan rate equations for correlated activity in neural networks.
M. Buice, J. Cowan, C. Chow, National Institutes of Health . . . . . . . . . . . . . . . . . . . . . . . 69
I-24. A role for symmetric head-angular-velocity cells: Tuning the head-direction network.
P. Stratton, G. Wyeth, J. Wiles, The University of Queensland . . . . . . . . . . . . . . . . . . . . . . 69
I-25. Improved (I)CA-noise elimination of electrophysiological data using band-pass filtered components
K. Görgen, C. Bosman, T. Womelsdorf, R. Oostenveld, P. Fries, BCCN Berlin . . . . . . . . . . . . . 70
I-26. State Dependent Frequency Modulation of Hippocampal Theta Activity
D. Nguyen, M. Wilson, E. Brown, R. Barbieri, Massachusetts General Hospital . . . . . . . . . . . . 71
I-27. Complex Bayesian Inference in Neural Circuits using Divisive Normalization
J. Beck, P. Latham, A. Pouget, Gatsby Computational Neuroscience Unit, UCL . . . . . . . . . . . . 71
I-28. Neural model of action-selective neurons in STS and area F5
M. Giese, A. Casile, F. Fleischer, Hertie Institute for Clinical Brain Research . . . . . . . . . . . . . . 72
I-29. Deciding without remembering
S. Ganguli, R. Guetig, H. Sompolinsky, UCSF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
I-30. Anthropic correction for mutual information and its application to neural redundancy estimations.
F. Theunissen, M. Gastpar, P. Gill, S. Munro, University of California, Berkeley . . . . . . . . . . . . 74
I-31. Separation of single neurons from optical recordings in Tritonia diomedea using ICA
C. Moore-Kochlacs, E. Hill, W. N. Frost, J. Wang, T. Sejnowski, Salk Institute for Biological Studies . 75
I-32. Towards linking blood flow and neural activity : Petri net-based energetics model for neurons
V. Nagarajan, A. Mohan, WAran Research FoundaTion . . . . . . . . . . . . . . . . . . . . . . . . . 76
I-33. Task-driven Saliency Using Natural Statistics (SUN)
M. Tong, C. Kanan, L. Zhang, G. Cottrell, University of California, San Diego . . . . . . . . . . . . . 77
I-34. Predictors of successful memory encoding in the human hippocampus and amygdala
U. Rutishauser, A. N. Mamelak, I. B. Ross, E. Schuman, California Institute of Technology . . . . . 77
I-35. Topological stability of the hippocampal spatial map
Y. Dabaghian, F. Memoli, G. Singh, L. Frank, G. Carlsson, Keck Center for Integrative Neuroscience 78
I-36. Dynamic hippocampal remapping using recurrent inhibition on realigning grid cell inputs
J. Monaco, L. Abbott, Center for Theoretical Neuroscience, Columbia . . . . . . . . . . . . . . . . . 79
I-37. Synaptic decision making: flipping switch-like synapses with cubic autocatalysis.
G. Wittenberg, Siemens Corporate Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
I-38. Projection Neurons in Medial and Lateral Striatum Show Different Ensemble Patterns during
Learning
C. Thorn, A. Graybiel, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . . . . . . 81
I-39. Frequency selectivity using spike-timing-dependent plasticity
M. Gilson, M. Buerck, A. Burkitt, J. L. van Hemmen, The University of Melbourne . . . . . . . . . . . 81

6

COSYNE 09

Posters I
I-40. Biologically plausible model of synapse formation in the neocortex
G. Escobar, A. Stepanyants, Northeastern University . . . . . . . . . . . . . . . . . . . . . . . . . . 82
I-41. Associative representations in lateral intraparietal (LIP) area
J. Fitzgerald, J. Assad, D. Freedman, Harvard Medical School . . . . . . . . . . . . . . . . . . . . . 83
I-42. Specificity versus associativity in models of paired-stimulus learning
M. Bourjaily, P. Miller, Volen Center for Complex Systems . . . . . . . . . . . . . . . . . . . . . . . . 84
I-43. Sensory input balances excitation and inhibition to close the auditory cortical critical period
A. Dorrn, C. Schreiner, R. Froemke, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
I-44. The attention-gated reinforcement learning model: performance and predictions.
L. Watling, P. Roelfsema, A. Van Ooyen, CNCR, VU University Amsterdam . . . . . . . . . . . . . . 85
I-45. Neural activity in nigrostriatal circuits can signal action value and action sequence
X. Jin, R. Costa, Lab for Integrative Neuroscience, NIAAA/NIH . . . . . . . . . . . . . . . . . . . . . 86
I-46. Value function uncertainty as a cognitive map for reinforcement learning
E. Chastain, N. Daw, University of Washington . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
I-47. Effects of learning on the motor gestures of birdsong
J. Mendez, A. Dall’Asén, B. Cooper, F. Goller, University of Utah . . . . . . . . . . . . . . . . . . . . 87
I-48. Context dependent movement encoding and variability in the motor cortex
M. Nawrot, J. Rickert, A. Riehle, A. Aertsen, S. Rotter, Freie Universitat Berlin . . . . . . . . . . . . 88
I-49. Can divergent connectivity generate reliable sparse activity patterns?
T. Nowotny, R. Huerta, University of Sussex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
I-50. Bifurcation analysis of neural mass equations
R. Veltz, O. Faugeras, INRIA Sophia Antipolis, NeuroMathComp Team . . . . . . . . . . . . . . . . 90
I-51. Selective in vivo activation of fast- or regular-spiking barrel cortex neurons with Channelrhodopsin
J. Cardin, M. Carlen, K. Meletis, U. Knoblich, F. Zhang, K. Deisseroth, L.-H. Tsai, C. Moore, McGovern Institute, MIT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
I-52. Stimulus onset quenches neural variability: a widespread cortical phenomenon
M. Churchland, B. Yu, J. Cunningham, L. Sugrue, M. Cohen, G. Corrado, W. Newsome, A. Clark, P.
Hosseini, B. Scott, D. Bradley, M. Smith, A. Kohn, J. Movshon, K. Armstrong, T. Moore, S. Chang,
L. Snyder, S. Ryu, G. Santhanam, M. Sahani, K. Shenoy, Stanford University . . . . . . . . . . . . . 92
I-53. Columnar coding of neuronal populations in primary visual cortex
R. Herikstad, J. Baker, C. Gray, S.-C. Yen, National University of Singapore . . . . . . . . . . . . . . 93
I-54. A multivariate phase distribution and its estimation
C. Cadieu, K. Koepsell, University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . . . 94
I-55. Towards a physiological exploration of sensorimotor processing in behaving Drosophila
J. Seelig, E. Chiappe*, G. Lott, T. Adelman, M. Reiser, V. Jayaraman, Janelia Farm Research Campus 95
I-56. In what regimes do regular-spiking excitatory neurons drive gamma oscillations?
D. Vierling-Claassen, J. Cardin, C. Moore, S. Jones, MGH-MIT-HMS Martinos Cntr for Biomed.
Imaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
I-57. Perception as Modeling: A neural network that extracts and models predictable elements from
input.
D. Sussillo, L. Abbott, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
I-58. Modelling Adaptive Coding of Sound Localization Cues in the Inferior Colliculus.
P. Hehrmann, J. Maier, N. Harper, D. McAlpine, M. Sahani, Gatsby Computational Neuroscience
Unit, UCL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98

COSYNE 09

7

Posters I
I-59. Quantitative analysis of the learning dynamics of a two-phase neural model of classical conditioning
I. Herreros-Alonso, A. Giovannucci, R. Zucca, M. Mintz, P. F. M. J. Verschure, SPECS. Universitat
Pompeu Fabra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
I-60. Possible differential sensitivity to the matrix vs core thalamocortical systems by MEG vs EEG
N. Dehghani, E. Halgren, S. Cash, MGH, Neurology Dep. Harvard . . . . . . . . . . . . . . . . . . . 100
I-61. High-performance halorhodopsin variants for improved genetically-targetable optical neural
silencing
B. Chow, X. Han, X. Qian, M. Li, E. Boyden, MIT Media Lab . . . . . . . . . . . . . . . . . . . . . . 101
I-62. Adaptation in simple neurons: dependence of feature selectivity on stimulus statistics
M. Famulare, A. Fairhall, University of Washington . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
I-63. Decoding of stimulus velocity using a model of ganglion cell populations in primate retina.
E. Lalor, Y. Ahmadian, J. Pillow, E. Simoncelli, L. Paninski, Trinity College Dublin . . . . . . . . . . . 102
I-64. Decoding dynamic patterns of neural activity using a ‘biologically plausible’ fixed set of weights
E. Meyers, D. Freedman, G. Kreiman, E. Miller, T. Poggio, Massachusetts Institute of Technology . . 103
I-65. A decoder-based spike train metric for analyzing the neural code in the retina
Y. Ahmadian, J. Pillow, J. Shlens, E. Simoncelli, E. Chichilnisky, L. Paninski, Columbia University . . 104
I-66. Precise spike synchronization in the gamma band increases information gain in awake monkey
V1
T. Womelsdorf, B. Lima, M. Vinck, R. Oostenveld, W. Singer, S. Neuenschwander, P. Fries, Donders
Inst for Brain, Cognition & Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
I-67. Temporal processing with plastic short term synaptic dynamics
R. Guetig, H. Sompolinsky, M. Tsodyks, Racah Institute of Pysics . . . . . . . . . . . . . . . . . . . 106
I-68. Which model can properly describe dynamics and smoothness of firing rate?
K. Takiyama, K. Katahira, M. Okada, The University of Tokyo . . . . . . . . . . . . . . . . . . . . . . 107
I-69. How far the decoding process in the brain can be simplified?
M. Oizumi, T. Ishii, K. Ishibashi, T. Hosoya, M. Okada, The University of Tokyo . . . . . . . . . . . . 107
I-70. Extensions to copula based modeling of spike counts
A. Onken, S. Grünewälder, K. Obermayer, Berlin Institute of Technology . . . . . . . . . . . . . . . . 108
I-71. Decoding of regular Purkinje cell spiking based on synaptic depression in a model of a DCN
neuron
J. Luthman, R. Maex, R. Adams, N. Davey, V. Steuber, University of Hertfordshire . . . . . . . . . . 109
I-72. Overcoding-and-paring: a bufferless neural chunking model
G. Rinkus, Brandeis University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
I-73. Computational models of millisecond level neuronal timing mechanisms
B. Aubie, S. Becker, P. Faure, McMaster University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
I-74. Temporal response properties in auditory cortex are layer-dependent
G. B. Christianson, M. Sahani, J. Linden, UCL Ear Institute . . . . . . . . . . . . . . . . . . . . . . . 112
I-75. Targeting auditory cortex: combining physiology and anatomy to identify higher auditory regions
P. Crum, E. Issa, T. Hackett, X. Wang, Johns Hopkins School of Medicine . . . . . . . . . . . . . . . 113
I-76. Linking stimulus response properties and functional circuitry in the mouse auditory cortex
H. Oviedo, I. Bureau, K. Svoboda, A. Zador, Cold Spring Harbor Laboratory . . . . . . . . . . . . . 113
I-77. Temporal coding in the olfactory bulb of awake behaving rats during active sampling.
K. Cury, N. Uchida, Harvard University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

8

COSYNE 09

Posters I
I-78. Receptive field size and spike threshold control decoding of information from synchronous
afference
J. Middleton, A. Longtin, J. Benda, L. Maler, Department of Neurobiology . . . . . . . . . . . . . . . 115
I-79. Receptive field maps depend on high order stimulus structure: evidence for nonlinear feedback
J. Victor, F. Mechler, A. Schmid, I. Ohiorhenuan, K. Purpura, Weill Medical College of Cornell . . . . 116
I-80. Learning Natural Image Structure with a Horizontal Product Model
U. Köster, J. Lindgren, A. Hyvärinen, University of Helsinki . . . . . . . . . . . . . . . . . . . . . . . 117
I-81. A point process model for the parabigeminal nucleus as a recursive estimator
R. Ma, T. Coleman, J. Malpeli, Coordinated Science Laboratory . . . . . . . . . . . . . . . . . . . . 117
I-82. Inference of object attributes from local image features caused by occlusion
X. Pitkow, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
I-83. Sensory input statistics and network mechanisms in primate primary visual cortex
P. Berens, J. Macke, A. Ecker, R. J. Cotton, M. Bethge, A. Tolias, MPI for Biological Cybernetics . . 119
I-84. Modelling of light responses of Drosophila Photoreceptor
Z. Song, D. Coca, S. Billings, M. Juusola, University of Sheffield . . . . . . . . . . . . . . . . . . . . 120
I-85. Adaptation-induced changes in orientation tuning and tilt aftereffect in network models of V1
K. Wimmer, K. Obermayer, Berlin Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . 121
I-86. Roles of feedforward, recurrent and feedback connections in visual processing
S. Moldakarimov, M. Bazhenov, T. Sejnowski, The Salk Institute for Biological Studies . . . . . . . . 122
I-87. Motion discrimination unimpaired during silencing of binocular disparity information in macaque
MT
A. Smolyanskaya, R. Born, Harvard Medical School . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
I-88. Compensating fixational eye movements: A network model
Y. Burak, U. Rokni, H. Sompolinsky, M. Meister, Harvard University

. . . . . . . . . . . . . . . . . . 123

I-89. Probing the early visual system with naturalistic, synthetic images
R. Coen-Cagli, S. Wissig, A. Kohn, O. Schwartz, Albert Einstein College of Medicine

. . . . . . . . 123

I-90. Experience-dependent changes in perceptual capacity: behavior and brain states
M. Wenger, L. Blaha, J. Townsend, R. Von Der Heide, The Pennsylvania State University . . . . . . 124
I-91. What is the ”contrast” in contrast adaptation?
K. Simmons, G. Tkacik, J. Prentice, V. Balasubramanian, University of Pennsylvania . . . . . . . . . 125
I-92. Adaptation in MT neurons shifts speed tuning laterally, facilitating change discrimination
N. Price, R. Born, Harvard Medical School . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
I-93. Color Constancy of V1 Double Opponent Cells to Natural Images
D. Fisher, B. Conway, M. Goldman, University of California, Davis . . . . . . . . . . . . . . . . . . . 126
I-94. Distinct functional populations within macaque area MT as revealed by waveform analysis
F. Roemschied, F. Bremmer, B. Krekelberg, Rutgers University . . . . . . . . . . . . . . . . . . . . . 127
I-95. Divisive normalization provides summation and competition of population responses in visual
cortex
L. Busse, S. Katzner, A. Benucci, M. Carandini, University College London . . . . . . . . . . . . . . 128
I-96. Area MT pattern motion selectivity by integrating 1D and 2D motion features from V1 – a
neural model
C. Beck, H. Neumann, University of Ulm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
I-97. Rats’ Detection of Oriented Visual Target is Impaired by Collinear Flankers
P. Meier, E. Flister, P. Reinagel, UCSD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130

COSYNE 09

9

Posters I
I-98. Suboptimal selection of initial saccade in a visual search task
C. Morvan, L. Maloney, New York University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
I-99. Changes of visual response properties in area MT due to eye movements
T. Hartmann, T. Albright, F. Bremmer, B. Krekelberg, Rutgers University . . . . . . . . . . . . . . . . 131
I-100. High-speed imaging of local population activity in mouse visual cortex
V. Bonin, M. Histed, R. C. Reid, Harvard Medical School . . . . . . . . . . . . . . . . . . . . . . . . 132

10

COSYNE 09

Posters II

Poster Session II

7:30pm Friday 27th February

II-1. Predicting spike times of any cortical neuron
R. Kobayashi, Y. Tsubo, S. Shinomoto, Kyoto University . . . . . . . . . . . . . . . . . . . . . . . . . 133
II-2. Power-law distributions of inter-spike intervals in in vivo cortical neurons
Y. Tsubo, Y. Isomura, T. Fukai, Neural Circuit Theory, RIKEN BSI . . . . . . . . . . . . . . . . . . . . 134
II-3. Modeling the Electrical Function of Dendritic Spines
T. Vogels, R. Araya, R. Yuste, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
II-4. Unequal partitioning of AMPA and NMDA conductances leads to robust temporal order sensitivity
Y. Wei, B. Mel, University of Southern California . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
II-5. Reconciling inter-areal gamma-range synchrony with neural irregular activity in selective attention
S. Ardid, X.-J. Wang, A. Compte, Yale University School of Medicine . . . . . . . . . . . . . . . . . . 136
II-6. Spatial attention modulates steady state VEPs in retinotopic human visual cortex
T. Lauritzen, A. Wade, University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . . . . 137
II-7. Estimates of spike-LFP coherence based on finite spiking data vary with mean firing rate
J. Curtis, J. Mitchell, J. Reynolds, The Salk Institute . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
II-8. Where to look? Dissociating the effect of reward, salience and attention
V. Navalpakkam, C. Koch, A. Rangel, P. Perona, California Institute of Technology . . . . . . . . . . 138
II-9. Category learning and decision making: a cortical circuit model
T. Engel, X.-J. Wang, Yale University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
II-10. Serotonin modulates choice stickiness through an outcome-independent striatal mechanism.
B. Seymour, N. Daw, P. Dayan, J. Roiser, R. Dolan, Wellcome Trust Centre for Neuroimaging . . . . 140
II-11. Computation of value functions based on gains and losses in the cortico-striatal network
H. Seo, X. Cai, D. Lee, Yale University School of Medicine . . . . . . . . . . . . . . . . . . . . . . . 141
II-12. An optimality framework for understanding inhibitory control in countermanding tasks
A. Yu, University of California, San Diego . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
II-13. A computational theory of prefrontal executive control
A. Collins, E. Koechlin, Institut National Santé Et Recherche Médicale . . . . . . . . . . . . . . . . . 142
II-14. Active updating of decision boundaries in rats can be explained using bayesian classifiers
P. Shenoy, E. Chastain, A. Kepecs, R. Rao, University of Washington . . . . . . . . . . . . . . . . . 143
II-15. Competitive acceleration: A surprising consequence of neural decision-making
M. Wojnowicz, M. Spivey, M. Ferguson, Cornell University . . . . . . . . . . . . . . . . . . . . . . . 144
II-16. Probabilistic population coding of action selection in the basal ganglia
E. Kimchi, N. Narayanan, M. Laubach, Yale University School of Medicine . . . . . . . . . . . . . . 145
II-17. Structure learning in human sequential decision-making
D. Acuna, P. Schrater, Unversity of Minnesota . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
II-18. Towards inferring neural circuits from population calcium imaging
J. Vogelstein, A. Packer, R. Yuste, L. Paninski, Johns Hopkins University . . . . . . . . . . . . . . . 146
II-19. A neuronal network model for the detection of binary odor mixtures
A. Zavada, T. Nowotny, University of Sussex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
II-20. Nonlinear identification for modeling and analysis of adaptive neuronal systems
U. Friederich, D. Coca, S. Billings, M. Juusola, University of Sheffield . . . . . . . . . . . . . . . . . 148

COSYNE 09

11

Posters II
II-21. A Bayesian method to predict the optimal diffusion coefficient in random fixational eye movements
D. Pfau, X. Pitkow, L. Paninski, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
II-22. Temporal memory and network dynamics
P. Latham, E. Wallace, Gatsby Computational Neuroscience Unit, UCL . . . . . . . . . . . . . . . . 150
II-23. An analysis of functional connectivity across timescales
I. Stevenson, K. Kording, Northwestern University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
II-24. Efficient coding of binocular spontaneous activity for innate learning in V1 development
M. Albert, D. Field, Cornell University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
II-25. Correlation-based learning in resonate-and-fire neurons
D. Bouchain, F. Hauser, G. Palm, Ulm University, Germany . . . . . . . . . . . . . . . . . . . . . . . 152
II-26. Functional Connectivity between Neuronal Ensembles through Nonlinear Modeling
T. Zanos, R. Hampson, S. Deadwyler, T. Berger, V. Marmarelis, University of Southern California . . 153
II-27. A Biophysically Inspired Model for Contrast Adaptation
Y. Ozuysal, S. Baccus, Stanford University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
II-28. Modeling the Temporal Bisection Task in humans and rats.
C. Kopec, C. Brody, Princeton University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
II-29. Nonlinearity, memory, and phase transitions in animal learning
I. Nemenman, Los Alamos National Laboratory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
II-30. When can rates be reliably transmitted in feedforward networks?
K. Burbank, G. Kreiman, Children’s Hospital, Boston . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
II-31. A very general linear-nonlinear model for the spatio-temporal characterization of visual cells
from
J. Rapela, G. Felsen, J. Touryan, J. M. Mendel, N. M. Grzywacz, University of Southern California . 157
II-32. Validating a Bayesian model of conflicting sensory inputs
R. Natarajan, I. Murray, L. Shams, W. D. Hairston, R. Zemel, University of Toronto . . . . . . . . . . 157
II-33. Sleep as a Monte-Carlo: offline training of grammar-like models of semantic memory in the
neocortex
F. Battaglia, C. Pennartz, Universiteit van Amsterdam . . . . . . . . . . . . . . . . . . . . . . . . . . 158
II-34. What is stored in the hippocampus during tactile discrimination behavior?
P. Itskov, E. Vinnik, M. Diamond, International School For Advanced Studies . . . . . . . . . . . . . 159
II-35. The Secret Life of Kernels: Reconsolidation in Flexible memories
D. Nowicki, H. Siegelmann, University of Massachusetts Amherst, CS Dept. . . . . . . . . . . . . . 160
II-36. A study on medial temporal lobe online learning neuronal network model with active dendrites
X. Wu, B. Mel, University of Southern California . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
II-37. Tag-Trigger-Consolidation: A model of early and late long-term potentation and depression
C. Clopath, L. Ziegler, L. Buesing, E. Vasilaki, W. Gerstner, LCN . . . . . . . . . . . . . . . . . . . . 161
II-38. Structural plasticity and memory: Catastrophic forgetting, amnesia, and the spacing effect
A. Knoblauch, M.-O. Gewaltig, U. Körner, E. Körner, Honda Research Institute Europe . . . . . . . 162
II-39. Instructive cues for ON-OFF RGC segregation in the LGN of the developing mouse visual
system
J. Gjorgjieva, S. Eglen, University of Cambridge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
II-40. Decision making and perceptual learning for speed discrimination
S. Ringbauer, F. Raudies, H. Neumann, University of Ulm . . . . . . . . . . . . . . . . . . . . . . . . 164

12

COSYNE 09

Posters II
II-41. Optimizing microcircuits through reward modulated STDP
P. Joshi, J. Triesch, Frankfurt Institute for Advanced Studies . . . . . . . . . . . . . . . . . . . . . . . 165
II-42. Brain’s strategy for perceptual estimates: model averaging, model selection, or prob. matching?
L. Shams, U. Beierholm, UCLA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
II-43. TD learning versus motivational salience accounts of dopamine in animal models of OCD
T. Toulouse, H. Szechtman, S. Becker, McMaster University . . . . . . . . . . . . . . . . . . . . . . . 166
II-44. A spiking network model for learning reward timing in cortex:
H. Shouval, J. Gavornik, M. Shuler, Department of Neurobiolgy UT Houston . . . . . . . . . . . . . 167
II-45. The role of frontostriatal circuits in shifting between goal-directed actions and habits
C. Gremel, R. Costa, Laboratory for Integrative Neuroscience . . . . . . . . . . . . . . . . . . . . . 168
II-46. Recurrent neural network modeling of hierarchical motor control and analysis
D. Huh, E. Todorov, UCSD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
II-47. The interaction of Purkinje cell and Inhibitory Interneuron plasticity during classical conditioning
I. Herreros-Alonso, R. Zucca, A. Giovannucci, P. F. M. J. Verschure, SPECS. Universitat Pompeu
Fabra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
II-48. The relationship between correlations and network states is explained by balanced network
dynamics
A. Lerchner, P. Latham, Gatsby Computational Neuroscience Unit, UCL . . . . . . . . . . . . . . . . 171
II-49. Analysis of biologically inspired artificial neural networks
T. Fares, A. Stepanyants, Northeastern University . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
II-50. Graph coloring predicts the dynamics of neuronal networks
C. Assisi, M. Bazhenov, University of California, Riverside . . . . . . . . . . . . . . . . . . . . . . . . 172
II-51. Neuronal circuit reconstruction using serial block-face scanning electron microscopy
K. Briggman, M. Helmstaedter, T. Euler, W. Denk, Max Planck Institute for Medical Research . . . . 173
II-52. Biologically plausible saliency networks for object recognition
S. Han, D. Gao, N. Vasconcelos, University of California, San Diego . . . . . . . . . . . . . . . . . . 174
II-53. Summation properties of frequency-dependent disynaptic inhibition between pyramidal cells
T. Berger, G. Silberberg, R. Perin de Campos, H. Markram, Ecole Polytechnique Federale, Lausanne175
II-54. Constructing dopaminergic signals in response to transient inputs in the ventral tegmental
area
B. Gutkin, M. Graupner, Ecole Normale Supérieure, Paris . . . . . . . . . . . . . . . . . . . . . . . . 175
II-55. Phase locking of pulse-coupled oscillators with delays is determined by the phase response
curve
M. Woodman, C. Canavier, LSU Health Sciences Center . . . . . . . . . . . . . . . . . . . . . . . . 176
II-56. Phase Resetting Curves Predict Network Activity in Networks of Neural Oscillators
S. Achuthan, C. Canavier, LSU Health Sciences Center . . . . . . . . . . . . . . . . . . . . . . . . . 177
II-57. On the origin of low frequency fluctuations in the brain resting state
E. Hugues, G. Deco, Universitat Pompeu Fabra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
II-58. Stimulus space topology vs. network topography in the ring model
V. Itskov, C. Curto, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
II-59. The spatial profile of inhibitory circuitry modulates oscillatory activity in auditory cortex
A.-M. Oswald, B. Doiron, J. Rinzel, A. D. Reyes, New York University . . . . . . . . . . . . . . . . . 179

COSYNE 09

13

Posters II
II-60. Generation of short-term memory for items and order by unsupervised learning
M. Bourjaily, M. Tierz, P. Miller, Volen Center for Complex Systems . . . . . . . . . . . . . . . . . . . 180
II-61. Probing mechanisms of gamma rhythmogenesis with cell type-specific optical neural control
G. Talei Franzesi, X. Qian, M. Li, X. Han, C. Borgers, N. Kopell, F. LeBeau, M. Whittington, E.
Boyden, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
II-62. Detection of non-stationary higher-order spike correlation
H. Shimazaki, S.-i. Amari, E. Brown, S. Gruen, RIKEN Brain Science Institute . . . . . . . . . . . . 182
II-63. On the cumulants of the spike count distribution of stationary stochastic point processes.
C. van Vreeswijk, CNRS UMR 8119 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
II-64. Neurons as Monte Carlo Samplers: Sequential Bayesian Inference in Spiking Neural Populations
H. Yanping, R. Rao, University of Washington . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
II-65. Sparse and Invariant Representations of Multi-component Odors in the Mushroom Body
K. Shen, S. Tootoonian, A. Narayan, G. Laurent, California Institute of Technology . . . . . . . . . . 184
II-66. Distinct adaptive modes for weak and strong signals in a retinal population
D. Kastner, S. Baccus, Stanford University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
II-67. Feedback inhibition in the mushroom body and gain control
M. Papadopoulou, G. Turner, G. Laurent, California Institute of Technology . . . . . . . . . . . . . . 185
II-68. A likelihood framework to decode the timing of information in spiking and LFP activity
A. Banerjee, H. Dean, B. Pesaran, New York University . . . . . . . . . . . . . . . . . . . . . . . . . 186
II-69. Towards a biophysical basis of spike based inference
T. Lochmann, S. Deneve, B. Gutkin, Ecole Normale Supérieure, Paris . . . . . . . . . . . . . . . . . 187
II-70. Optimal correlation codes in populations of noisy spiking neurons
G. Tkacik, J. Prentice, E. Schneidman, V. Balasubramanian, University of Pennsylvania . . . . . . . 188
II-71. Phase coding of faces and objects in the superior temporal sulcus
K. Hoffman, H. Turesson, A. Ghazanfar, N. Logothetis, York University . . . . . . . . . . . . . . . . . 189
II-72. Transformation of Neural Representations in Probabilistic Population Codes
L. Shi, T. Griffiths, Helen Wills Neuroscience Institute . . . . . . . . . . . . . . . . . . . . . . . . . . 190
II-73. The significance of a nonlinear transformation and a role of local neurons in an olfactory circuit
R. Satoh, M. Oizumi, H. Kazama, M. Okada, The University of Tokyo . . . . . . . . . . . . . . . . . 191
II-74. Laminar-, event-, and state-dependent population coding in the auditory cortex
S. Sakata, K. Harris, Cntr for Molecul and Behav Neurosci, Rutgers . . . . . . . . . . . . . . . . . . 192
II-75. Contextual Effects in Neuronal Responses to Complex Sounds Differ between Areas AI and
AAF
M. Ahrens, M. Sahani, J. Linden, Gatsby Computational Neuroscience Unit, UCL . . . . . . . . . . 192
II-76. Auditory Learning Involving Complex Sounds Affects Nonlinear Integration within Cortical
Responses
J. Linden, I. Orduna, R. Williamson, M. Ahrens, E. Mercado III, M. Merzenich, M. Sahani, UCL Ear
Institute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
II-77. Spectrotemporal Modulations Underlying Speech and Timbre Perception
T. Elliott, L. Hamilton, F. Theunissen, Helen Wills Neuroscience Inst., UC Berkeley . . . . . . . . . . 194
II-78. An ideal observer for passive tactile spatial perception
D. Goldreich, McMaster University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
II-79. What you show is what you get: sampling biases in determining biological sensory function
A. Dimitrov, Montana State University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195

14

COSYNE 09

Posters II
II-80. Adaptive precision pooling of model neuron activities predicts efficiency of human visual learning
R. Jacobs, University of Rochester . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
II-81. Stability and persistence in visual cortex.
P. Ulinski, University of Chicago . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
II-82. Orientation processing without orientation maps in the pigeon analogue to the primary visual
cortex.
S. W. B. Ng, A. Grabska-Barwinska, O. Guentuerkuen, D. Jancke, Ruhr-University Bochum . . . . . 197
II-83. Timing precision in population coding of natural scenes in the early visual system
G. Desbordes, J. Jin, C. Weng, N. Lesica, G. Stanley, J.-M. Alonso, Georgia Institute of Technology 198
II-84. Temporal dynamics of surround suppression in the corticogeniculate feedback pathway
F. Briggs, W. M. Usrey, University of California, Davis . . . . . . . . . . . . . . . . . . . . . . . . . . 199
II-85. Contributions of single neurons in visual area MT to variability in smooth pursuit eye movements
S. Hohl, S. Lisberger, University of California San Francisco . . . . . . . . . . . . . . . . . . . . . . 199
II-86. Extracting MAX-pooling receptive fields with natural image fragments
M. Vidal-Naquet, S. Ullman, M. Tanifuji, Riken - Brain Science Institute . . . . . . . . . . . . . . . . 200
II-87. The effect of global context on the encoding of natural scenes.
R. Haslinger, B. Lima, G. Pipa, E. Brown, S. Neuenschwander, Massachusetts General Hospital . . 201
II-88. Visual response properties of V1 neurons projecting to V2 in macaque
Y. El-Shamayleh, R. Kumbhani, N. Dhruv, J. A. Movshon, New York University . . . . . . . . . . . . 202
II-89. Homeostatic and gain control mechanisms in a developmental model of orientation map formation in V1
J. Law, J. Antolik, J. Bednar, University of Edinburgh . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
II-90. Simultaneous multielectrode recording and multi-photon imaging of retinal visual responses
M. Menz, S. Baccus, Stanford University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
II-91. Inhibition facilitates fast, robust motion detection in the visual cortex
A. Sederberg, J. Liu, M. Kaschube, Princeton University . . . . . . . . . . . . . . . . . . . . . . . . . 204
II-92. OFF direction-selective cells in the mouse retina
Y. Zhang, I.-J. Kim, J. Sanes, M. Meister, Harvard University . . . . . . . . . . . . . . . . . . . . . . 205
II-93. Controlling response gain and input gain with noisy synaptic input
A. Ayaz, F. Chance, UC Irvine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
II-94. Context effects on visual search and rapid animal detection
J. Drewes, J. Trommershäuser, K. R. Gegenfurtner, Department of Psychology, Giessen University

206

II-95. A motion model based on a recurrent network
J. Joukes, B. Krekelberg, Rutgers University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
II-96. Effects of GABAa somatic inhibition on orientation tuning and contrast sensitivity in visual
cortex
S. Katzner, L. Busse, A. Benucci, M. Carandini, University College London . . . . . . . . . . . . . . 208
II-97. The negative BOLD response and its behavioral correlates
A. Wade, J. Rowland, Smith-Kettlewell Eye Research Institute . . . . . . . . . . . . . . . . . . . . . 209
II-98. Functional analysis of identified interneurons in the mouse visual cortex
H. Zariwala, J. Berzhanskaya, T. Zwingman, A. Jones, E. Lein, H. Zeng, K. Ahrens, Allen Institute
for Brain Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210

COSYNE 09

15

Posters II
II-99. The increasing importance of secondary stimulus dimensions to V1 firing with kurtosis
R. Rowekamp, T. Sharpee, University of California - San Diego . . . . . . . . . . . . . . . . . . . . . 211
II-100. Contextual interactions in natural image contours and their possible neural implementation
C. Ramachandra, B. Behabadi, R. Jain, B. Mel, University of Southern California . . . . . . . . . . . 212

16

COSYNE 09

Posters III

Poster Session III

7:30pm Saturday 28th February

III-1. Fly VS neurons compared to optimized motion detectors
B. Torben-Nielsen, R. Sinclair, K. M. Stiefel, Okinawa Institute of Science and Technology . . . . . . 212
III-2. Control of output gain in CA1 pyramidal cells using somatic shunting inhibition
F. Fernandez, J. White, University of Utah . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
III-3. Gene expression analysis and metabolic optimization in cortical fast-spiking interneurons
A. Hasenstaub, T. Sejnowski, E. Callaway, Crick-Jacobs Center, Salk Institute . . . . . . . . . . . . 214
III-4. Detection of extracellular potentials using a mechanical-based nanosensor
A. Sadek, R. Karabalin, J. Du, M. Roukes, C. Koch, G. Laurent, S. Masmanidis, California Institute
of Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
III-5. Location of modulatory inputs influences the ”scaling competence” of the NMDA channels.
M. Jadi, B. Mel, University of Southern California . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
III-6. Localizing the origin of executive control over distributed processing to prefrontal cortex
M. Chafee, S. Jain, R. Blackman, University of Minnesota . . . . . . . . . . . . . . . . . . . . . . . . 216
III-7. Dynamic allocation of limited resources in human visual working memory
P. Bays, M. Husain, University College London . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
III-8. Qualitative differences between decision-making for strongly and weakly attended stimuli
D. Rahnev, B. Maniscalco, E. Huang, H. Lau, Columbia University . . . . . . . . . . . . . . . . . . . 218
III-9. Anterior cingulate cortex encodes action-outcome associations in working memory
C.-H. Luk, J. Wallis, University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . . . . . 219
III-10. Role of Medial Prefrontal Cortex and Secondary Motor Cortex in Withholding Impulsive Action
M. Murakami, Z. Mainen, Champalimaud Neuroscience Programme . . . . . . . . . . . . . . . . . . 219
III-11. A spiking neural circuit model for conflict resolution between reflexive and voluntary responses
C.-C. Lo, X.-J. Wang, National Tsing Hua University . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
III-12. Chronic stress affects decision-making strategies: structural and physiological correlates
E. Dias-Ferreira, I. Melo, X. Jin, J. C. Sousa, J. J. Cerqueira, N. Sousa, R. Costa, National Institutes
of Health . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
III-13. Dissociations in ensemble dynamics between rat dorsal striatum, ventral striatum, and hippocampus
M. van der Meer, A. Johnson, N. Schmitzer-Torbert, A. D. Redish, University of Minnesota . . . . . 221
III-14. Motor planning in the rat superior colliculus
G. Felsen, Z. Mainen, Cold Spring Harbor Laboratory . . . . . . . . . . . . . . . . . . . . . . . . . . 222
III-15. Decision making without bounds? Evidence from humans and monkeys
J. Gao, J. McClelland, A. Rorie, R. Tortell, W. Newsome, Stanford University . . . . . . . . . . . . . 223
III-16. Abstract rule representations in a bilinear model.
K. Krueger, P. Dayan, Gatsby Computational Neuroscience Unit, UCL . . . . . . . . . . . . . . . . . 224
III-17. Causal model attribution in sequential decision making
P. Schrater, C. S. Green, C. Benson, D. Kersten, University of Minnesota . . . . . . . . . . . . . . . 225
III-18. Neurobiological foundations for ”dual system” theory in decision making under uncertainty
U. Beierholm, C. Anen, S. Quartz, P. Bossaerts, Gatsby Computational Neuroscience Unit, UCL . . 226
III-19. Active learning using uncertainty: behavioral evidence and neural correlates in rats
A. Kepecs, N. Uchida, Z. Mainen, Cold Spring Harbor Laboratory . . . . . . . . . . . . . . . . . . . 227

COSYNE 09

17

Posters III
III-20. Dynamics of coupled thalamocortical modules
J. Drover, N. Schiff, J. Victor, Weill Medical College of Cornell . . . . . . . . . . . . . . . . . . . . . . 228
III-21. Point Process Model for Precisely Timed Spike Trains
I. Park, M. Rao, T. DeMarse, J. Principe, University of Florida . . . . . . . . . . . . . . . . . . . . . . 229
III-22. Using Brainbow and GRASP for detailed reconstruction of complete circuits with light microscopy
Y. Mishchenko, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
III-23. Temporal precision of spikes in pulse-coupled networks of oscillating neurons
J.-n. Teramae, T. Fukai, Brain Science Institute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
III-24. Dynamics and cortical distribution of figure-ground activity in human visual cortex
A. Norcia, V. Vildavski, R. Miller, Smith-Kettlewell Eye Research Institute . . . . . . . . . . . . . . . 231
III-25. Extremely high-speed simulation of sparsely connected networks
T. Sanger, Stanford University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
III-26. Implementation of a probabilistic inference in a photoreceptor cell
A. Houillon, P. Bessière, J. Droulez, CNRS/Collège de France . . . . . . . . . . . . . . . . . . . . . 232
III-27. Transfer of correlations in neural oscillators
A. Barreiro, E. Shea-Brown, E. Thilo, University of Washington . . . . . . . . . . . . . . . . . . . . . 233
III-28. Neural mechanisms of visual motion integration and ego-motion estimation – a modelling
investigation
F. Raudies, J. Bouecke, H. Neumann, University of Ulm . . . . . . . . . . . . . . . . . . . . . . . . . 233
III-29. A Volterra kernel approach to non-linear functional connectivity
R. Miller, V. Vildavski, A. Norcia, Smith-Kettlewell Eye Research Institute . . . . . . . . . . . . . . . 234
III-30. A model shows how closed loop motor control can spontaneously develop in the rat whisker
system
L. Gruendlinger, M. Tsodyks, Weizmann Institute of Science . . . . . . . . . . . . . . . . . . . . . . 235
III-31. Complementary roles of the Left Posterior Parietal Lobe and Basal Ganglia in reference
frame usage
E. Torres, K. Heilman, H. Poizner, Rutgers University . . . . . . . . . . . . . . . . . . . . . . . . . . 236
III-32. Gaussian-process factor analysis for low-d single-trial analysis of neural population activity
B. Yu, J. Cunningham, G. Santhanam, S. Ryu, K. Shenoy, M. Sahani, Stanford University . . . . . . 237
III-33. Rapid, scalable neuronal network simulations using MNet
C. Acker, J. White, University of Connecticut Health Center . . . . . . . . . . . . . . . . . . . . . . . 237
III-34. Prediction of pairwise maximum entropy model parameters by maximizing information
J. Fitzgerald, T. Sharpee, University of California, San Diego, CTBP . . . . . . . . . . . . . . . . . . 238
III-35. Unsupervised learning of Lie group operators from image sequences
J. Wang, J. Sohl-Dickstein, B. Olshausen, Redwood Center for Theoretical Neuroscience . . . . . . 239
III-36. Virtual brain reading: A neural network approach to understanding fMRI patterns.
R. Cowell, G. Cottrell, University of Kent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
III-37. Attention and location effects on spatial memory: Testing the predictions of a computational
model
X. Han, S. Becker, P. Byrne, M. Kahana, McMaster University . . . . . . . . . . . . . . . . . . . . . 240
III-38. Speed versus accuracy in spiking attractor networks
J.-P. Pfister, M. Lengyel, University of Cambridge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
III-39. Entorhinal cortex — dentate gyrus system implements an optimal device for distance rep
Z. Somogyvari, M. Bányai, Z. Huhn, T. Kiss, P. Erdi, KFKI Res. Inst. Particle and Nuclear Physics . 242

18

COSYNE 09

Posters III
III-40. Embedding multiple trajectories in recurrent neural networks in a self-organizing manner
J. Liu, D. Buonomano, University of California, Los Angeles . . . . . . . . . . . . . . . . . . . . . . . 243
III-41. An ecological basis for the oblique effect
M. Falconbridge, D. MacLeod, Univeristy of California, San Diego . . . . . . . . . . . . . . . . . . . 244
III-42. Low frequency oscillations facilitate STDP-based pattern learning and decoding
T. Masquelier, E. Hugues, G. Deco, S. Thorpe, Universitat Pompeu Fabra . . . . . . . . . . . . . . . 244
III-43. Modeling the interaction of biological elements involved in Hebbian and homeostatic plasticity
T. Toyoizumi, K. Miller, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
III-44. Temporal difference learning does not always lead to STDP
R. Florian, C. Rusu, Center for Cognitive and Neural Studies . . . . . . . . . . . . . . . . . . . . . . 246
III-45. Neural noise shapes perceptual landscapes for different forms of plasticity in perceptual
learning
C.-L. Teng, J. Gold, University of Virginia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
III-46. Multiple timescales of reward memory in lateral habenula and midbrain dopamine neurons
E. Bromberg-Martin, M. Matsumoto, K. Nakamura, H. Nakahara, O. Hikosaka, Brown University,
Providence RI, USA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
III-47. A single cell with active conductance’s can learn timing and multi-stability
J. Gavornik, H. Shouval, The University of Texas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
III-48. MI self- vs. instructed initiation of reaches during force-field learning: A multi-electrode study
J. José, E. Torres, K. Ganguli, J. Carmena, SUNY at Buffalo . . . . . . . . . . . . . . . . . . . . . . 250
III-49. Central sources for acoustic variation in birdsong
S. Sober, M. Brainard, University of California, San Francisco . . . . . . . . . . . . . . . . . . . . . . 250
III-50. Irregular vs. Synchronized activity in Basal Ganglia Circuits
C. Park, R. Worth, L. Rubchinsky, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
III-51. Ramping activity is an inefficient estimator of time intervals
J. Kesseli, C. Machens, Ecole Normale Supérieure, Paris . . . . . . . . . . . . . . . . . . . . . . . . 252
III-52. Computing correlations analytically
D. Barrett, P. Latham, Gatsby Computational Neuroscience Unit, UCL . . . . . . . . . . . . . . . . . 253
III-53. A biophysical model of cortical tuning and invariance operations using a transient population
code
J. Mutch, U. Knoblich, T. Poggio, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . 253
III-54. Towards fast in vivo neuronal imaging using objective coupled planar illumination microscopy
D. Turaga, T. Holy, Washington University School of Medicine

. . . . . . . . . . . . . . . . . . . . . 254

III-55. Central roles of bipolar cells in retinal neural circuits.
H. Asari, M. Meister, Harvard University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
III-56. Detecting functional connectivity in networks of phase-coupled neural oscillators
A. Huth, C. Cadieu, C. Dale, D. Weber, D. Pantazis, F. Darvas, R. Leahy, G. Simpson, K. Koepsell,
University of California, Berkeley . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
III-57. Decoding center-surround interactions in population of neurons for the ocular following response
L. Perrinet, N. Voges, J. Kremkow, M. Guillaume, INCM / CNRS – Univ. de la Méditerranée . . . . . 257
III-58. Better luck next time: How prior behavioral outcomes influence network state in the frontal
cortex
N. Smith, N. Narayanan, M. Laubach, Yale University School of Medicine . . . . . . . . . . . . . . . 258

COSYNE 09

19

Posters III
III-59. Avalanches in simple stochastic spiking network models.
E. Wallace, M. Benayoun, W. van Drongelen, J. Cowan, Dept. of Mathematics, University of Chicago 259
III-60. How the optic nerve allocates space, energy capacity and information
J. Perge, K. Koch, R. Miller, P. Sterling, V. Balasubramanian, University of Pennsylvania . . . . . . . 259
III-61. A neural circuit to read out the temporal population code
A. Luvizotto, C. Renno-Costa, P. F. M. J. Verschure, SPECS - Universitat Pompeu Fabra

. . . . . . 260

III-62. Withdrawn
, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
III-63. Correlation transfer in neuronal populations
J. Ma, K. Josic, A. Barua, R. Rosenbaum, F. Marpeau, University of Houston . . . . . . . . . . . . . 261
III-64. Robust resolution of neuronal population dynamics on a single trial basis
K. Miller, B. Jagadeesh, A. Hebb, J. Ojemann, R. Rao, University of Washington . . . . . . . . . . . 262
III-65. Change-based inference in attractor nets: Linear analysis
R. Moazzezi, P. Dayan, Gatsby Computational Neuroscience Unit, UCL . . . . . . . . . . . . . . . . 263
III-66. Bayesian Population Decoding of Spiking Neurons
S. Gerwinn, J. Macke, M. Bethge, MPI for Biological Cybernetics . . . . . . . . . . . . . . . . . . . . 263
III-67. Contrast responses of P-cells and V1 neurons are optimized for a winner-takes-all encoding
R. Martin, Y. Tadmor, K. Obermayer, Berlin Institute of Technology . . . . . . . . . . . . . . . . . . . 264
III-68. Sampling and Optimal Cue Combination during Bistable Perception
R. Moreno-Bote, D. Knill, A. Pouget, University of Rochester . . . . . . . . . . . . . . . . . . . . . . 265
III-69. Hierarchical novelty-familiarity representation in the visual cortex
B. Vladimirskiy, W. Senn, R. Urbanczik, University of Bern . . . . . . . . . . . . . . . . . . . . . . . 266
III-70. A sparse coding model with “imperfect” feed-forward circuitry
W. Coulter, F. Sommer, University of California Berkeley . . . . . . . . . . . . . . . . . . . . . . . . . 267
III-71. Comparison between decoding algorithms in open-loop and closed-loop performance
S. Koyama, S. Chase, A. Whitford, M. Velliste, A. Schwartz, R. Kass, Carnegie Mellon University . . 268
III-72. Correlated physiological and perceptual effects of noise in a tactile stimulus
A. Lak, E. Arabzadeh, J. Harris, M. Diamond, International School for Advanced Studies . . . . . . 269
III-73. Correlation-rate relation enhances information throughput in layer networks of spiking neurons
B. Doiron, J. de la Rocha, E. Shea-Brown, K. Josic, A. Reyes, University of Pittsburgh . . . . . . . . 270
III-74. Map Dynamics for Rhythmically Perturbed Model Neurons
J. Engelbrecht, K. Loncich, R. Mirollo, Boston College . . . . . . . . . . . . . . . . . . . . . . . . . . 270
III-75. Modeling adaptation in the auditory cortex to causally link neural synchrony to tinnitus
M. Chrostowski, S. Becker, I. Bruce, McMaster University . . . . . . . . . . . . . . . . . . . . . . . . 271
III-76. The influence of theta (4-8Hz) structure in vocalizations on neural activity in auditory cortex
H. Turesson, C. Chandrasekaran, A. Ghazanfar, Princeton University . . . . . . . . . . . . . . . . . 272
III-77. Control of single neuron activity by global network dynamics in auditory cortex
C. Curto, S. Sakata, S. Marguet, K. D. Harris, Courant Institute, NYU . . . . . . . . . . . . . . . . . 273
III-78. The predictive power of spectrotemporal receptive fields at the single spike train level.
J. Schumacher, D. Schneider, S. Woolley, Columbia University . . . . . . . . . . . . . . . . . . . . . 273
III-79. Direction-dependence of inter- and intra-vibrissa adaptation in rat primary somatosensory
cortex
U. Knoblich, C. Moore, McGovern Institute for Brain Research . . . . . . . . . . . . . . . . . . . . . 274

20

COSYNE 09

Posters III
III-80. Fast and dynamic odor encoding in rat olfactory cortex during odor discrimination task
K. Miura, Z. Mainen, N. Uchida, Japan Science and Technology Agency . . . . . . . . . . . . . . . . 275
III-81. Radial orientation and direction biases in the response of human visual cortex to kinetic
contours
C. Clifford, D. Mannion, S. McDonald, School of Psychology, University of Sydney . . . . . . . . . . 276
III-82. Bayesian estimation of orientation preference maps
J. Macke, S. Gerwinn, L. White, M. Kaschube, M. Bethge, MPI for Biological Cybernetics . . . . . . 277
III-83. Distinct laminar zones of coherent local field potentials in monkey V1
A. Maier, G. Adams, C. Aura, D. Leopold, National Institutes of Health . . . . . . . . . . . . . . . . . 277
III-84. Spatially periodic activity without periodic connectivity requires an inhibition-stabilized network
D. Rubin, K. Miller, Columbia University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
III-85. A dynamic receptive field of a motion-sensitive neuron in the fly
F. Weber, C. Machens, A. Borst, Max Planck Institute of Neurobiology . . . . . . . . . . . . . . . . . 279
III-86. Estimating surface structure and eye position from local derivatives of the disparity vector
field
J. Read, Institute of Neuroscience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
III-87. A Feed Forward-Lateral Inhibition Dynamical Model of Predictive Remapping of the Tilt Aftereffect
A. Ashourvan, J. Brown, N. Port, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
III-88. A model for the distortions of space and time perception during saccades
G. M. Cicchini, P. Binda, M. C. Morrone, Universit Vita-Salute San Raffaele Milan . . . . . . . . . . . 281
III-89. Local fourth-order statistics in natural scenes predict the saliency of synthetic textures
G. Tkacik, J. Prentice, J. Victor, V. Balasubramanian, University of Pennsylvania . . . . . . . . . . . 282
III-90. Population coding of ground truth motion in natural scenes in the early visual system
G. Stanley, M. Black, J. Lewis, G. Desbordes, J. Jin, J.-M. Alonso, Georgia Institute of Technology . 283
III-91. Speed sensitive excitation shapes the tuning of a collision-detecting neuron
P. Jones, F. Gabbiani, Baylor College of Medicine . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
III-92. Reading the mind’s eye: Decoding object information during mental imagery from fMRI patterns
T. Serre, L. Reddy, N. Tsuchiya, T. Poggio, M. Fabre-Thorpe, C. Koch, Massachusetts Institute of
Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
III-93. A Gamma-phase Model of Receptive Field Formation
D. Ballard, University of Texas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
III-94. Complex cells in the early visual cortex have multiple disparity detectors in the 3D binocular
RFs
K. Sasaki, Y. Tabuchi, I. Ohzawa, Osaka University . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
III-95. FEF neural responses during uninstructed, optimal eye movements in a free-viewing scan
task
T. Desrochers, A. Graybiel, Massachusetts Institute of Technology . . . . . . . . . . . . . . . . . . . 287
III-96. Contrast dependence of spatial frequency tuning in macaque LGN
S. Sokol, N. Dhruv, J. A. Movshon, New York University . . . . . . . . . . . . . . . . . . . . . . . . . 288
III-97. Decoding sequences of population responses in visual cortex
A. Benucci, L. Busse, S. Katzner, M. Carandini, University College London . . . . . . . . . . . . . . 288

COSYNE 09

21

Posters III
III-98. Is Sensory Processing about Detecting Abrupt Changes in the Sensory World?
N. Bouaouli, S. Deneve, Ecole Normale Supérieure, Paris . . . . . . . . . . . . . . . . . . . . . . . . 289
III-99. Wireless Recording from the Cortex of a Freely Roaming Rat
T. A. Szuts, V. Fadeyev, A. Leifer, W. Dabrowski, N. Uchida, A. Litke, M. Meister, Harvard University 290
III-100. Sensory adaptation as an optimal redistribution of neural resources
S. Gepshtein, L. Lesmes, I. Tyukin, T. Albright, The Salk Institute, USA . . . . . . . . . . . . . . . . 291

22

COSYNE 09

T-1 – T-2

Abstracts
Abstracts for talks appear first, in order of presentation; those for posters next, in order of poster
session and board number. An index of all authors appears at the back.
DOI links will not be active until the conference.

T-1. Internal representations of the olfactory world
Richard Axel

RA 27@ COLUMBIA . EDU

Howard Hughes Medical Institute
Columbia University
Olfactory perception is initiated by the recognition of odorants by a large repertoire of receptors in the
sensory epithelium. A dispersed pattern of neural activity in the nose is converted into a segregated map
in the olfactory bulb. In mice, as well as flies, projections from the olfactory bulb (antennal lobe) extend to
multiple, higher order processing centers in the brain. We have identified a spatially invariant, dimorphic
circuit responsive to pheromones that dictates components of sexually dimorphic courtship behavior in
Drosophila. We have also asked how the representation in the bulb is transformed at the next processing
center for olfactory information in the mouse, the piriform cortex. Optical imaging of odor responses in
the cortex of the mouse reveals that the piriform discards spatial segregation as well as chemotopy and
returns to a highly distributed organization in which different odors activate unique but dispersed ensembles
of cortical neurons. Neurons in piriform cortex, responsive to a given odorant, are not only distributed
without apparent spatial preference but exhibit discontinuous receptive fields. This representation suggests
organizational principles that differ from those in neocortical sensory areas where cells responsive to similar
stimulus features are clustered and response properties vary smoothly across the cortex.
doi:10.3389/conf.neuro.06.2009.03.221

T-2. Multiple-electrodes, brain rhythms, and cognition
Earl Miller

EKMILLER @ MIT. EDU

Massachusetts Institute of Technology
Our laboratory has developed unique technology to record neural activity from many electrodes simultaneously in behaving monkeys. This has revealed new insights into the role of the precise timing of neural
activity in cognitive functions, including attention and short-term memory. Attention regulates the flood of
sensory information into a manageable stream, and so understanding how it is controlled is central to understanding cognition. Different theories suggest that searching a visual scene involves serial and/or parallel

COSYNE 09

23

T-3
allocation of attention, but there is little direct evidence. We found both behavioral and neural evidence in
the frontal eye fields for spontaneous, serial, covert shifts of attention. Attention shifts in FEF spiking activity
were correlated with oscillations in the local field potential, suggesting a ‘clocking’ signal. Our results suggest that serial covert shifts of attention during search are directed by the FEF and that neuron population
oscillations may regulate the timing of cognitive processing. The ability to hold multiple objects in memory is
fundamental to intelligent behavior, but its neural basis remains poorly understood. It has been suggested
that multiple items may be held in memory by oscillatory activity across neuronal populations, but yet there
is little direct evidence. We found that neuronal information about two objects held in short-term memory is
enhanced at different phases of an underlying neuronal population oscillation. We recorded neuronal activity from the prefrontal cortex of monkeys remembering two visual objects over a brief interval. We found
that, during this memory interval, there was an oscillation of neuron population activity in the prefrontal cortex. Further, spikes carried most information about the individual memorized objects at different oscillatory
phases according to their order of presentation. Optimal encoding of the first object was significantly earlier
in the oscillatory cycle than that for the second object. These results suggest that oscillatory neuronal synchronization mediates a phase-dependent coding of memorized objects in prefrontal cortex. Encoding at
distinct phases may help to disambiguate information about multiple objects in short-term memory. These
results illustrate how synchronized oscillatory brain rhythms may act as “clocking signals” that help regulate
cognitive functions.
doi:10.3389/conf.neuro.06.2009.03.230

T-3. Burst Spiking of Single Cortical Neuron Switches Global Brain
State
Chengyu Li
Mu-Ming Poo
Yang Dan

TONY LI @ BERKELEY. EDU
MPOO @ BERKELEY. EDU
YDAN @ BERKELEY. EDU

University of California, Berkeley
Howard Hughes Medical Institute
Different global patterns of brain activity are associated with distinct behavioral states of the animal. The
slow oscillation, a hallmark of slow-wave sleep, is characterized by UP-DOWN membrane potential transitions in cortical neurons and high-amplitude, low-frequency local field potential (LFP) and EEG signals.
In contrast, low-amplitude high-frequency cortical activity prevails during rapid-eye-movement (REM) sleep
and wakefulness, with neuronal membrane potentials fluctuating around a depolarized level. The mechanisms controlling the switch between these brain states are only beginning to be examined. Inspired by
recent findings that activation of a single cortical neuron can significantly modulate sensory perception and
motor output, we set out to test whether single neuron stimulation can modify the global brain state. We
here report that repetitive high-frequency (>20 Hz) burst spiking of a single visual cortical neuron could
trigger a switch between the cortical states characteristic of slow-wave and REM sleep. This is reflected
in the switching of the membrane potential of the stimulated neuron from slow UP/DOWN oscillations to a
persistent-UP state or vice versa, with concurrent changes in the temporal pattern of cortical LFP recorded
several millimeters away, or EEG recorded at the opposite hemisphere to the patched neuron. These results
point to the power of single neuron activity in influencing the arousal and behavioral state of the animal.
doi:10.3389/conf.neuro.06.2009.03.128

24

COSYNE 09

T-4

T-4. Parvalbumin interneurons and oscillations enhance information
processing in cortical microcircuits
Vikaas Sohal
Feng Zhang
Ofer Yizhar
Karl Deisseroth

VIKAAS @ STANFORD. EDU
ZHANGF @ STANFORD. EDU
YIZHAR @ STANFORD. EDU
DEISSERO @ STANFORD. EDU

Stanford University
Oscillations in fast-spiking neurons that express parvalbumin may be involved in information processing,
and may be deficient in neural circuit pathologies including schizophrenia. However, it has not been possible to demonstrate a quantitative impact of this oscillatory system on information processing. Here we use
dynamic clamp and optogenetic tools to deconstruct the pyramidal cell – parvalbumin interneuron microcircuit, and information theory to measure how oscillations affect the function of each circuit element. First, we
studied how oscillations affect single neocortical pyramidal neurons. Using dynamic clamp, we stimulated
neocortical pyramidal neurons with trains of simulated excitatory post-synaptic currents (sEPSCs). During
each train, the rate of sEPSCs varied, and in some cases, we additionally modulated the sEPSC rate at
theta (8 Hz) or gamma (40 Hz) frequencies. Theta frequency modulation increased mutual information
between the output spike rate of pyramidal neurons and the number of input sEPSCs from 1.13 ± 0.08 to
1.39 ± 0.06 bits per 125-msec cycle (p < 0.001; n = 14 cells). Similarly, gamma oscillations increased the
sEPSC-spike rate information from 0.42 ± 0.05 to 0.63 ± 0.05 bits per 25-msec cycle (p < 0.001; n = 14
cells). Theta, but not gamma, frequency oscillations also increased the information carried by the time of the
first spike in each cycle (p < 0.001; n = 14 cells). To extend this approach to microcircuits, we used light to
activate layer V pyramidal neurons expressing channelrhodopsin (ChR2). We varied the rate of light flashes,
and again modulated the rate at theta or gamma frequencies. We recorded from both pyramidal neurons
directly activated by light (n = 13), and downstream neurons, using the fast-spiking phenotype to identify
parvalbumin interneurons (n = 7). Both theta and gamma frequency modulation markedly increased mutual
information between the output spike rate of post-synaptic fast-spiking interneurons and the rate of spikes
in pre-synaptic pyramidal neurons. Theta oscillations increased the information / 125 msec cycle from 0.92
± 0.09 bits to 1.12 ± 0.09 bits (p < 0.05), whereas gamma oscillations increased information / 25 msec
cycle from 0.36 ± 0.05 bits to 0.71 ± 0.07 bits (p < 0.001). These increases resulted from decreased noise
entropy in fast-spiking interneurons, and were specific to fast-spiking interneurons, as oscillations did not
enhance information transmitted from pyramidal neurons to other types of post-synaptic neurons. Finally,
to complete the pyramidal neuron – parvalbumin interneuron loop, we activated parvalbumin interneurons,
and measured their effects on information processing. In mice selectively expressing ChR2 in parvalbumin
interneurons, we stimulated pyramidal neurons with sEPSC trains, and used pyramidal neuron spikes to
trigger light flashes and recruit feedback inhibition from parvalbumin interneurons. Such inhibition generated emergent gamma oscillations in pyramidal neurons, and also increased the efficiency of rate coding,
measured by the mutual information per spike. These results demonstrate and quantify a powerful synergy
between fast-spiking parvalbumin interneurons and oscillations: parvalbumin interneurons generate emergent gamma oscillations, which amplify incoming signals into pyramidal neurons, and increase the fidelity
with which these signals are transmitted to parvalbumin interneurons.
doi:10.3389/conf.neuro.06.2009.03.089

COSYNE 09

25

T-5

T-5. The asynchronous state in the cerebral cortex
Alfonso Renart1
Jaime de la Rocha1
Liad Hollender
Bilal Haider
Alvaro Duque
David McCormick
Nestor Parga2
Alex Reyes
Kenneth D. Harris
1
2

ARENART @ ANDROMEDA . RUTGERS . EDU
JAIME . DELAROCHA @ RUTGERS . EDU
LIADH @ PEGASUS . RUTGERS . EDU
BILAL . HAIDER @ YALE . EDU
ALVARO. DUQUE @ YALE . EDU
DAVID. MCCORMICK @ YALE . EDU
NESTOR . PARGA @ UAM . ES
REYES @ CNS . NYU. EDU
KDHARRIS @ ANDROMEDA . RUTGERS . EDU

Rutgers University
Dept Fisica Teorica-Univ Autonoma de Madrid

Cortical circuits can operate in different dynamical regimes. Extra-cellular recordings over the last decade,
and recent paired whole-cell recordings in vivo suggest that, under certain conditions, the spike trains
of nearby cortical neurons are very weakly correlated. The stability of this type of activity is mysterious,
as both the high connection probability and the large cortical synaptic strengths would seem to lead to
the amplification of small synchronous events into globally synchronous network states. We investigated
this issue by considering the effect of common input on balanced recurrent neural networks composed
of N excitatory and inhibitory binary neurons. Neurons are connected randomly with probability p and
receive projections with the same probability from an excitatory external population, leading to an average
fraction p of shared inputs. Synaptic connections are ’strong’, i.e., they scale as O(1/sqrt(N)). This has
two consequences: a) The excitatory and inhibitory components of the total synaptic input become very
large. b) Weak correlations in the activity of the neurons are greatly amplified, leading to strong correlations
among these components. Despite of this, we show analytically that, under general conditions, the activity
in the network is asynchronous, i.e. the average pair-wise correlation scales as O(1/N). This is possible
due to a cancellation, not only between the mean excitatory and inhibitory inputs, but also between the
large positive terms of the input correlation (arising from common input as well as network amplification of
exc-exc and inh-inh correlations) and a large negative term resulting from exc-inh correlations. These excinh correlations reflect the self-generated tracking of excitatory fluctuations by inhibition. Asymptotically, the
exc. and inh. populations track the fluctuations in the external input perfectly. We have tested the predictions
of the model at three different levels: (1) Numerical simulations of biophysically plausible cortical circuits
confirm that exc-inh tracking underlies weakly synchronized activity in spiking networks. (2) According to
the model, both the mean and the variance of the distribution of pair-wise correlations should scale as
O(1/N). This implies that the distribution of correlations should be ’wide’, with approximately equal numbers
of positively and negatively correlated pairs. Simultaneous extra-cellular recordings of large populations of
cortical neurons in the absence of slow global oscillations (activated state) confirmed this prediction with
remarkable accuracy: over many thousands of pairs, the number of positively and negatively correlated
pairs differs by only a few percent, and the standard deviation of the distribution ( 0.1) is much larger than the
mean ( 0.005). (3) We are currently measuring correlations between membrane potential fluctuations from
simultaneously recorded pairs of cortical neurons in vivo. The theory predicts that the correlations between
EPSPs and between IPSPs should be larger than the correlation between the unperturbed membrane
potential. Our results establish theoretically the feasibility of asynchronous states in densely connected
balanced networks and provide strong experimental evidence that the cortex can resemble such states.
These results unify a large body of work and provide a mechanistic foundation for the firing rate code in the
cortex.
doi:10.3389/conf.neuro.06.2009.03.112

26

COSYNE 09

T-6 – T-7

T-6. The neuroeconomics of simple goal-directed choices
Antonio Rangel

RANGEL @ HSS . CALTECH . EDU

California Institute of Technology
Goal-directed decision-making requires, at a minimum, two computational steps: (1) a value must be assigned to the different options under consideration, and (2) these values need to be compared in order
to make a choice. In this talk we describe a series of experiments designed to characterize the neurobiological and computational basis of these two steps. Despite the pervasiveness of the value assignment
computation in goal-directed choice, until recently little was known about how the brain carries it out. In the
first part of the talk we will report on a series of fMRI experiments showing that the valuation computation
seems to be encoded in a common area of the medial orbitofrontal cortex in a variety of decision-making
tasks (e.g., binary choice vs. bidding) and in a variety of items (e.g., foods, trinkets, social decisions, and
gambles). Furthermore, the experiments suggest that this common area is responsible for evaluating both
appetitive and aversive items. In the second part of the talk, we will describe a new computational model
of how the values are compared in order to make a choice. The model is a variant of the race-to-barrier
models of perceptual decision-making with an important modification: visual attention guides the path of
integration and comparison of the value signal. The model makes several novel stark predictions about
the relationship between visual attention and choices, and about the performance of the decision-making
processes. Among others, it predicts that there is a first-fixation bias (the first seen item is more likely to
be chosen), a last fixation bias (the last seem item is more likely to be chosen), an exposure bias (items
seem longer are more likely to be chosen), and a left-bias (for Westerners, items placed on the left visual
field are more likely to be chosen). We will describe tests of the critical assumptions of the model, as well
as its predictions, using an eye-tracking decision-making task in which hungry subjects choose between
snacks. The results suggest that visual attention is critical in the computation and comparison of values
during simple goal-directed choice.
doi:10.3389/conf.neuro.06.2009.03.291

T-7. Neural activity in frontal eye field during flexible decision-making
Vincent Ferrera1
Marianna Yanike1
Carlos Cassanello2
1
2

VPF 3@ COLUMBIA . EDU
MY 2252@ COLUMBIA . EDU
CARLOS . CASSANELLO @ ANU. EDU. AU

Columbia University
Australian National University

Categorical decision-making with shifting boundaries can provide a flexible linkage between sensory stimuli
and behavioral responses. The frontal eye field (FEF) is a region of prefrontal cortex that is involved in
linking visual stimuli to motor responses. FEF is thought to be involved in selecting visual targets for eye
movements. However, it is not known whether FEF is capable of playing a role in categorizing visual stimuli
independently of a specific motor response. To investigate this, we developed a speed categorization
task in which monkeys were presented with a random dot motion stimulus. They were required to make
a saccadic eye movement to one of two response targets to indicate whether the stimulus was “fast” or
“slow”. The task was designed so that monkeys associated the speed categories with the colors of the
response targets (“fast” = green, “slow” = red). The locations of the response targets were randomized.
Hence, the categorical decision was independent of the motor response. The category boundary was
determined arbitrarily by the computer and the monkeys learned it by trial and error. Once the monkeys
had learned one boundary speed, the boundary was shifted to a new speed and the monkeys learned the

COSYNE 09

27

T-8
new boundary. Once learned, monkeys were able to shift rapidly between the two category boundaries.
We recorded the activity of 96 FEF neurons from two monkeys. Activity during stimulus presentation was
significantly modulated by stimulus speed in 33 (34%) neurons. Neural activity changed significantly when
the category changed in 43 (44%) cells (2-way ANOVA, EVs = stimulus speed, boundary speed, p < 0.05).
Some cells encoded both the physical speed of the stimulus and its category, which could be considered a
categorical representation. Others simply showed enhanced firing to all stimuli of one category, but were
not affected by stimulus speed. Neurons that preferred faster speeds tended to be more active on trials with
the slower boundary speed, while neurons that preferred slow speeds were more active on trials with the
faster boundary speed. This pattern of activity allowed a simple rule to be used to read out neural activity:
neurons were considered as casting a vote for their preferred speed category if their firing rate was higher
than average, and a vote for their non-preferred category if their firing rate was lower than average. Using a
subpopulation of 46 neurons, this rule could be used to categorize stimuli on a trial-by-trial basis. Averaged
over all recording sessions, neuronal performance approximated the monkeys’ behavioral performance (r
= 0.98; p <0.0001). These results demonstrate that FEF activity is influenced by stimulus category and
suggest a possible model for categorical decision-making.
doi:10.3389/conf.neuro.06.2009.03.216

T-8. An Infinite Mixture Model of Context-dependent Learning and Extinction
Samuel Gershman
David Blei
Yael Niv

SJGERSHM @ PRINCETON . EDU
BLEI @ CS . PRINCETON . EDU
YAEL @ PRINCETON . EDU

Princeton University
Dominant theories of human and animal learning posit that in classical conditioning, one of the most basic
forms of learning, animals learn to make associations between different stimuli in the environment. These
associations are then used to predict future events based on current observations. Although this framework
is powerful, we argue that a wealth of behavioral evidence is more consistent with an account of learning
in which animals make more complex inferences about the environment. Most notably, responding to a
conditioned stimulus appears to depend not only on the reinforcement history of the stimulus but also on its
relationship to contextual stimuli (such as the experimental chamber). We thus model learning as inference
over latent causes of observed stimuli, rather than of direct causal relationships between them. We focus
on two extensively-studied animal conditioning paradigms, latent inhibition and renewal. In latent inhibition,
the acquisition of a conditioned response to a cue that is paired with reinforcement is retarded if the cue
was previously presented without reinforcement. A change of context between the initial exposure to the
unreinforced cue and the subsequent training reduces this retardation. In renewal, the cue is first presented
paired with reinforcement (conditioning) in one context and then presented without reinforcement (extinction) in another context, leading to loss of conditioned responding. Returning the animal to the conditioning
context renews conditioned responding. Renewal also occurs when the animal is tested in a completely
novel context. In both latent inhibition and renewal, lesioning the hippocampus prior to the second phase of
training eliminates context-dependence. A similar pattern is observed in infant rats, whose behavior seems
to not be sensitive to context. We suggest that these and other context-dependent learning phenomena
can be explained by a normative statistical model in which the animal performs inference of latent causes
in an infinite mixture model. In contrast to classical associative theories, the infinite mixture model assumes
that each observation (trial) is generated by a single latent cause, selected from an unbounded set of latent
causes. This identifies the computational problem being solved by the animal as that of how to assign
observations to discrete latent causes (ie, clustering) and use these to predict reinforcement. The infinite

28

COSYNE 09

T-9
capacity of the model enables the animal to infer new clusters as it collects observations. Our simulations
show that the model predicts the context-dependence of latent inhibition and renewal. We also show how
restricting the model’s ability to infer new clusters explains developmental changes in learning as well as the
effects of hippocampal damage. The key difference between the infinite mixture model and previous models is the explanation of learning as inference over latent causes rather than the formation of associations
between observed stimuli, giving rise to novel predictions regarding the effects of training length on learning and context effects, and suggesting manipulations that could reverse behavioral phenomena that were
previously regarded as fixed constraints on learning. The model also has implications for understanding
context-dependent relaspe in drug addiction and anxiety disorders.
doi:10.3389/conf.neuro.06.2009.03.323

T-9. An ideal-observer model for optimal inference in the presence of
different types of uncertainty
Robert Wilson
Matthew Nassar
Joshua Gold

RCWILSON @ SEAS . UPENN . EDU
NASSARR @ MAIL . MED. UPENN . EDU
JIGOLD @ MAIL . MED. UPENN . EDU

University of Pennsylvania
Whether getting a new job or a new president, life is full of “changepoints” that cause the rules of the game
to change abruptly. Making predictions in such circumstances can be challenging because changepoints
can render much of the past irrelevant. For example, the outcome of this year’s presidential election suggests that policies from recent years will not necessarily be good predictors of policies in the coming years.
Here we examine how to identify changepoints in a noisy environment and then use that information to
adjust how to use past experiences to predict future events. We present an ideal-observer model of a dynamic probabilistic estimation task designed to quantify the extent to which human subjects use new pieces
of information in the presence of possible changepoints. At each time-step in this task, a bar appears on
screen at a random location, and subjects are asked to predict the location of the bar on the next time-step.
The subjects receive a cash reward based on the accuracy of their prediction. The location of the bar is
determined by a changepoint process, which is sampled from a Gaussian distribution whose mean and
variance change abruptly at changepoints but otherwise remain constant. Because the time of occurrence
of changepoints are unknown to the subjects, they must contend with two different kinds of uncertainty:
“expected” uncertainty corresponding to the variance of the Gaussian distribution and “unexpected” uncertainty corresponding to probability that a changepoint might occur [A.J.Yu and P.Dayan Neuron 46:681692,2005]. When the frequency of changepoints (known as the hazard rate), but not their exact times of
occurrence, is given, Adams and MacKay [Technical report, Cambridge University, 2007] and Fearnhead
and Liu [J.R.Statist.Soc.B 69(4):589–605,2007] have developed online Bayesian models capable of making
optimal predictions. However, in general, the hazard rate is unspecified and must be inferred from the data.
We solve this problem by developing an online Bayesian algorithm capable of making optimal predictions
in the face of expected and unexpected uncertainty and an unknown – and potentially variable – hazard
rate. In particular, we model the occurrence of changepoints as a Bernoulli process and infer a constant
hazard rate by keeping track of changepoints experienced over time. A hierarchical extension of the model
in which the hazard rate is itself generated by a changepoint process allows us to make inferences about
variable hazard rates. We compare performance of the model and human subjects on the dynamic probabilistic estimation task. We focus on the weight given to each datum in determining the next estimate as
the hazard rate changes. A key result is that this weighting differs substantially in a stable environment, in
which changepoints are rare and the past is a good predictor of the future, versus a volatile environment, in
which changepoints are common and past trends are typically not helpful. By quantifying these principles,

COSYNE 09

29

T-10 – T-11
the model establishes a benchmark both for comparing human performance on this kind of prediction task
and for analyzing a host of changepoint data.
doi:10.3389/conf.neuro.06.2009.03.044

T-10. Computational role of short-term synaptic plasticity in the neocortex
Misha Tsodyks

MISHA @ WEIZMANN . AC. IL

Weizmann Institute of Science
Neocortical circuits exhibit pronounced short-term synaptic plasticity (STP), but its computational role is
still unclear. Recent experimental observations indicate that the organization of STP varies across different
cortical areas, in particular synaptic transmission between pyramidal cells in the sensory cortices is characterized by activity-dependent synaptic depression, while in higher areas synaptic facilitation and depression
are mixed in different proportions. This observation may point to some degree of alignment between the
STP organization and the functional role of the corresponding area. It was proposed by us and others
that synaptic depression in the connections between pyramidal neurons in primary cortical areas shape the
response of these areas to sensory inputs. I illustrate this idea with a neural network model of the primary
auditory cortex that can generate brief epochs of localized synchronous firing, called ’population spikes’.
Analysis of the model suggests that population spikes could play an important role in encoding and processing complex acoustic inputs and grouping local responses to create distinct auditory percepts. In the
higher cortical areas, we propose that the interplay between synaptic depression and facilitation could hold
information about the past stimuli for a certain period of time, and thus contribute to working memory. This
mechanism has some important advantages compared to the usually assumed persistent spiking activity,
such as low metabolical costs, the robustness to distracting stimuli and the amenability of the duration of
the memory to attentional control. Finally, I introduce a recent suggestion that STP could underlie the representation of time in the brain. In particular, a synapse with a certain mix of depression and facilitation is
’tuned’ to a specific time interval, i.e. it exhibits a strongest postsynaptic response for a pair of spikes separated by this interval. We could also derive a learning rule that allow the selective training of a synapse to
a particular interval. This simple mechanism, when extended to populations of synapses, can also account
for the ’reset’ of the time representation with each incoming spike, and thus allow the system to extract
temporal information from a continuous stream of input spikes.
doi:10.3389/conf.neuro.06.2009.03.308

T-11. Reward enhances reactivation of experience in the hippocampus
Annabelle Singer1
Loren Frank2,1
1
2

ANNABELLE . SINGER @ UCSF. EDU
LOREN @ PHY. UCSF. EDU

University of California, San Francisco
W.M. Keck Center for Integrative Neuroscience

Experiences that lead to salient outcomes are more readily stored in memory. The hippocampus is required
for storing memories of the places and events that make up these experiences. Although we might expect

30

COSYNE 09

T-12
that salient outcomes would enhance memory storage, no neural mechanism has been found by which the
outcome of an event enhances hippocampal memory formation for that event. Previous studies examining
hippocampal responses to different outcomes have generally focused on the presence or absence of food
reward. These studies report responses to reward that are similar to responses to other sorts of cues.
Thus, there is currently no evidence for a change in hippocampal memory processing as a result of reward.
Previous studies focused on place field activity, where hippocampal excitatory cells (“place cells”) fire in
particular locations in space during active exploration. Place cells are also active during high frequency
network oscillations called sharp wave ripples (SWRs), in which sequences of cells active during running
are often reactivated. Reactivation is thought to be important for long term memory formation, so we asked
whether reward affects reactivation of experience in the hippocampus compared to no reward. We recorded
from principal neurons in hippocampal area CA3 while animals learned to switch between two spatial sequences in response to changing reward contingencies. Here we show that hippocampal CA3 principal
cells are much more active during SWRs at the end of rewarded as compared to unrewarded trajectories.
This increased activation is strongest among cells that have place fields related to the rewarded location.
At a population level, SWRs associated with reward preferentially activate pairs of CA3 place cells with
overlapping place fields. The result is patterned CA3 activity consistent with the ordered replay of past and
future trajectories seen in downstream hippocampal area CA1. These findings establish that information
about the presence or the absence of a reward sculpts subsequent memory reactivation in the hippocampus. We propose this enhanced reactivation could be a general mechanism to encode experiences related
to salient outcomes. More, generally this repetition of recent experience as a result of reward may help us
understand how the hippocampus contributes to reinforcement learning.
doi:10.3389/conf.neuro.06.2009.03.303

T-12. State-dependent cortical processing: Cholinergic modulation of
visual responses
Michael Goard
Yang Dan

MIKEG @ BERKELEY. EDU
YDAN @ BERKELEY. EDU

University of California, Berkeley
Neuromodulatory systems are capable of modifying cortical states by altering cell excitability and functional connectivity, but the consequences for cortical processing are not well understood. We address this
issue by using silicon polytrodes to record visual cortical responses of anesthetized rats in response to
stimulation of the basal forebrain cholinergic system, an important neuromodulatory system implicated in
arousal and attention. We find that basal forebrain stimulation causes an mAChR-dependent decrease
in between-neuron correlations across all layers of cortex, consistent with previous studies using EEG
recordings. Furthermore, basal forebrain stimulation results in an increase in cortical response reliability to
repeated presentations of natural movies through mechanism(s) independent of cortical mAChRs. Both the
desynchronization and the improved response reliability are likely to enhance sensory coding capacity in
cortex. Thus, these modifications of cortical activity by the basal forebrain may mediate changes in sensory
processing during different behavioral states.
doi:10.3389/conf.neuro.06.2009.03.224

COSYNE 09

31

T-13 – T-14

T-13. Half a wiring diagram is better than none
Cori Bargmann

CORI @ ROCKEFELLER . EDU

The Rockefeller University
An animal uses stable neural circuits to generate flexible behavioral responses to its environment. This talk
will address relationships between genes, the nervous system, and behaviors in the nematode Caenorhabditis elegans. The nervous system of C. elegans is very simple: it consists of just 302 neurons with reproducible functions, morphologies and synaptic connections. Despite this simplicity, many of the genes
and signaling mechanisms used in the nematode nervous system are similar to those in mammals. In C.
elegans, the ability to manipulate the activity of individual genes and neurons makes it possible to determine how neural circuits function, with high resolution. The synaptic connections outlined in the worm’s
”wiring diagram” are the foundation of C. elegans neurobiology, but it is still a challenge to understand the
relationship between these connections and specific behaviors. Using quantitative behavioral analysis, genetics, and calcium imaging, we are examining C. elegans’s responses to volatile odors and environmental
oxygen. We find that pathways in the wiring diagram can explain rapid behavioral responses. However,
there are many modulators that change the properties of circuits; most of these pathways are not in the
wiring diagram. We think it is hard to read the wiring diagram for two reasons. First, it is incomplete: it
represents the fast feedforward connections, but not the slow peptide and modulatory feedback. Second,
it is ambiguous: depending on modulatory state, the same wiring diagram can generate several different
behaviors.
doi:10.3389/conf.neuro.06.2009.03.214

T-14. How neural systems adjust to different environments: an intriguing role for gap junction coupling
Sheila Nirenberg
Chethan Pandarinath
Illya Bomash
Jonathan Victor
Wayne Tschetter

SHN 2010@ MED. CORNELL . EDU
CHP 2015@ MED. CORNELL . EDU
ILB 2005@ MED. CORNELL . EDU
JDVICTO @ MED. CORNELL . EDU
WWT 2001@ MED. CORNELL . EDU

Weill Medical College of Cornell
The brain has an impressive ability to self-adjust – that is, as it moves from one environment to another,
it can adjust its information-gathering properties to accommodate the new conditions. For example, as
it moves into an environment with new stimuli, it can shift its attention; if the stimuli are low contrast, it
can adjust its contrast sensitivity; if the signal-to-noise ratio is low, it can change its spatial and temporal
integration properties. These shifts are well described at the behavioral level – and are critical to our
functioning – but the neural mechanisms that underlie them are not understood. How is it that a network can
change its information-gathering properties on the fly? Here we describe a case in which it was possible
to obtain an answer. It’s a simple case, but one of the best-known examples of a behavioral shift – the
shift in visual integration time that occurs as an animal moves from a day to a night environment. During
the day, when photons are abundant, the animal’s visual system is biased toward short integration times.
As night approaches, and photons become limited, the system shifts toward long integration times. An
intriguing hypothesis has emerged for how the shift takes place – it involves gap-junction coupling among
the horizontal cells of the retina: It’s well known that the coupling of these cells is determined by the
light level in the environment. When the light level is high, the junctions close, and there’s little coupling.

32

COSYNE 09

T-15
When the light level drops, as it does at night, the junctions open, and extensive coupling ensues. Since
coupling shunts current, the idea is that the extensive coupling causes a strong shunting of horizontal cell
current, effectively taking the horizontal cells out of the system. Since horizontal cells are critical for shaping
integration time – they provide feedback to photoreceptors that keeps integration time short – taking these
cells out of the system makes integration time longer. What makes the hypothesis intriguing is that it raises
a new, and possibly generalizable idea – that a neural network can be shifted from one state to another by
changing the gap-junction coupling of one of its cell classes. The coupling serves as a means to take a cell
class out of the network and thus change the network’s structure. We tested this hypothesis for the case
above using transgenic mice that cannot undergo horizontal cell coupling. They lack the horizontal cell gapjunction gene, and, as a result, their horizontal cells get locked into the uncoupled state. If the hypothesis
is correct, these animals should not be able to undergo the shift to long integration times. The hypothesis
held: the shift in integration time was blocked completely at the behavioral level, and almost completely at
the physiological (i.e., ganglion cell) level. In sum, we tracked a behavioral change down to the network that
implements it. This revealed a new, simple, and possibly generalizable, mechanism for how networks can
rapidly adjust themselves to changing demands.
doi:10.3389/conf.neuro.06.2009.03.036

T-15. Bayesian reconstruction of perceptual experiences from human
brain activity
Jack Gallant

GALLANT @ BERKELEY. EDU

University of California, Berkeley
Sensory systems transform stimulus energy measured at the peripheral transducers into explicit representations of various abstract features. These transformations can be viewed as (nonlinear) mappings between
the stimuli and brain activity measurements [1]. Most work in sensory neuroscience has focused on development of quantitative encoding models posed in terms of nonlinear mechanisms that filter the stimuli
in order to produce appropriate responses. Recent interest in brain-machine interfaces has pushed development of decoding models that aim to classify, identify or reconstruct the stimulus directly from measured
brain activity. These models work in the opposite direction, transforming responses into stimuli features.
Most decoding models use non-parametric algorithms such as SVM, and are not based on explicit encoding models. We have pioneered an alternative approach in which the decoding algorithm is inferred from
one or more explicit encoding models. In a study published last year [2] we showed that this approach can
be used to extract far more information from functional MRI measurements than was generally believed
possible. In more recent work along these lines, we have developed a new Bayesian decoding model that
can reconstruct natural images seen by an observer from measured brain activity. The decoder combines
three elements: a structural encoding model that characterizes signals from early visual areas; a semantic
encoding model that characterizes signals from higher visual areas; and appropriate priors that incorporate
statistical information about the structure and semantics of natural scenes. By combining all these elements
the decoder produces reconstructions that accurately reflect the distribution, structure and semantic category of the objects contained in the original image. These results help clarify how distinct representations in
different parts of the brain can be combined to provided a coherent reconstruction of the visual world; they
also highlight a potentially important role for prior knowledge in visual perception. Our Bayesian decoding
model can be generalized directly to permit reconstruction of other perceptual dimensions, such as color
and motion. It might also be possible to use this framework to reconstruct subjective perceptual processes
such as visual imagery and dreaming. More generally, these results suggest that Bayesian decoding algorithms could form the basis of powerful new brain-reading technologies and brain-computer interfaces. [1]
Kay KN, Naselaris T, Prenger RJ, Gallant JL (2008) Identifying natural images from human brain activity.

COSYNE 09

33

T-16
Nature 452:352-355. [2] Wu MC, David SV, Gallant JL (2006) Complete functional characterization of sensory neurons by system identification. Annu Rev Neurosci 29:477-505. Acknowledgements. The functional
MRI scans were conducted with the support of the staff at the Brain Imaging Center at UC Berkeley and
at the Veterans Administration in Martinez, CA. This research was supported by the National Eye Institute
and by UC Berkeley intramural funds.
doi:10.3389/conf.neuro.06.2009.03.225

T-16. Attention reduces trial-to-trial and correlated variability in V4
neurons
Marlene Cohen
John Maunsell

MARLENE COHEN @ HMS . HARVARD. EDU
MAUNSELL @ HMS . HARVARD. EDU

Harvard Medical School
Howard Hughes Medical Institute
The responses of individual sensory neurons are variable, and laboratory studies typically deal with this
variability by averaging responses to many stimulus presentations. In the real world, however, animals and
humans must respond to a single occurrence of a stimulus, and the brain is thought to compensate for
the variability of neurons by encoding information about a stimulus in the responses of large populations
of neurons. Our goal is to understand how sensory information is encoded in populations of neurons and
how the population code relates to behavior on short timescales. One of the best ways to identify important
aspects of a population code is to look at differences between the way information about a sensory stimulus
is encoded when that stimulus is used to guide behavior and when it is behaviorally irrelevant. Tasks that
control attention provide a powerful way to manipulate task relevance. Attention allows observers to focus
on the important information in a complex scene, improving psychophysical thresholds for an attended
location or feature. Attention-related improvements in behavioral performance are associated with changes
in the responses of sensory neurons: visual neurons typically respond more strongly to attended than
unattended stimuli. However, the resulting improvement in signal-to-noise for individual neurons is typically
too small to account for the behavioral improvement. Therefore, attention is likely to enhance behavioral
performance by improving the stimulus selectivity of a population of neurons and/or changing how much
different neurons contribute to a behavior. We used visual attention to study how attention affects the
responses of a population of neurons in visual area V4, a mid-level area whose neurons are visual but
are also modulated by spatial and feature-based attention. While the monkey performed a visual task in
which spatial and feature-based attention was controlled, we recorded from dozens of neurons in both
hemispheres using implanted electrode arrays. With this approach, we could simultaneously monitor the
responses of populations of neurons representing an attended stimulus and an unattended stimulus, and
also the corresponding psychophysical effects of attention. We found that in addition to modulating mean
firing rates, attention independently affects the variability of V4 neurons. First, attending to a stimulus
decreases the trial-to-trial variability of single neurons encoding that stimulus. Second, the correlated trialto-trial variability among groups of neurons encoding a stimulus is greatly reduced by attention, making
the noise in the responses of neurons encoding the most behaviorally relevant stimulus in our task nearly
independent. The reductions in independent and especially correlated variability greatly reduce the trialto-trial variability in the average response of the population as a whole. Finally, both the reductions in
trial-to-trial variability and in correlated variability were greatest in neurons whose tuning was best suited for
the stimulus presented. We hope to use these data to understand the circuit-level mechanisms of attention
and to explore the way these changes in the responses in single neurons and in the interactions between
neurons affect the ability of the population to encode information about a stimulus and guide behavior.
doi:10.3389/conf.neuro.06.2009.03.068

34

COSYNE 09

T-17

T-17. Statistical decision theory and the allocation of cognitive resources in multiple object tracking
Edward Vul1
Michael Frank1
George Alvarez2
Joshua Tenenbaum1
1
2

EVUL @ MIT. EDU
MCFRANK @ MIT. EDU
ALVAREZ @ WJH . HARVARD. EDU
JBT @ MIT. EDU

Massachusetts Institute of Technology
Harvard University

Multiple object tracking (MOT) is a common task used to investigate the limitations on human visual attention. However, the nature of these limitations is still unclear. In the current study, we define an ideal
observer for the multiple object tracking task and test hypotheses about human limitations based on the
correspondence between human performance and the observer’s performance. Because the agent optimally allocates its resources, this approach allows us to estimate what resources limit human performance
in MOT without making additional heuristic assumptions. MOT is computationally identical to an “aircraft
tracking” problem. Our ideal observer model observes the initial locations and labels of some number
of objects, and then obtains unlabeled measurements for a series of time-steps. Its goal is to solve the
correspondence problem at each time-step, determining which object label generated which observation.
We implemented an online solution to this problem using a Rao-Blackwellized particle filter. Unmodified,
this ideal observer was able to solve MOT tasks substantially better than humans, but it still showed many
commonly observed phenomena in human MOT. For example, tracking performance suffered as object
speed increased, as the number of objects in the display increased, and as the unpredictability of the object
trajectories increased. However, the unmodified model did not match another phenomenon: the tradeoff
between the number of targets that human observers can track and the speed of the targets. Fitting this
phenomenon requires that some limit on the MOT system be allocated flexibly. We tested three potential
limits on our optimal algorithm (based loosely on the literature on human MOT): 1)Limited computation.
Perhaps a central processing bottleneck dictates that only some number of object state estimates may be
updated within a certain amount of time. A limit on the computational resources necessary for inference in
the model (e.g. by limiting the number of particles available in the particle filter) is analogous to this type
of limit on state updates. 2)Limited measurement fidelity. “Attention” is hypothesized to improve the precision of observation. Thus, a limit on measurement fidelity is analogous to hypothesizing a fixed amount of
“attention” which may be distributed across the visual field. 3)Limited memory fidelity. Most attention tasks
also require some component of working memory storage. Thus, limiting the precision with which state
estimates propagate through time amounts to imposing a storage limit in working memory. Our results suggest that limiting measurement fidelity or memory fidelity (but not limiting computation) produced a pattern
of performance in our ideal observer corresponding to the speed/number of objects tradeoff. However, the
potential increase in measurement precision due to “attention” required to match human data is far larger
than that observed in human experiments. Thus, only limitations on memory fidelity may be considered a
viable candidate for the restriction on human performance. We conclude that human MOT performance is
limited by the fidelity with which state estimates are stored and propagated through time: working memory.
Time permitting, we will also discuss new evidence reflecting the flexibility of human resource allocation.
doi:10.3389/conf.neuro.06.2009.03.232

COSYNE 09

35

T-18 – T-19

T-18. Unlimited-capacity, metabolically constrained visual short-term
memory in multiple object tracking
Wei Ji Ma
Wei Huang

WJMA @ BCM . EDU
WHUANG @ BCM . EDU

Baylor College of Medicine
The nature of the limitations of visual short-term memory (VSTM) has been subject of intense recent debate.
Leading models assert that memory can contain no more than a fixed number of 3 or 4 items. Alternatively,
resource models claim that VSTM capacity is very large and performance limitations mainly arise from
increased uncertainty per item as the number of items increases. We attempt to resolve this debate by
modeling several published data sets obtained from a multiple object tracking paradigm. Stimuli are dots
following linear trajectories. One or more dots change direction halfway, all by the same angle and all on
the vertical midline of the display. Subjects report whether the deviation was clockwise or counterclockwise.
Since only the current position of each dot is visible, their past positions have to be kept in VSTM. This
paradigm has produced several intriguing findings: 1) When all trajectories deviate, the deviation threshold
is nearly independent of the number of trajectories. 2) When only one trajectory deviates, the threshold
increases steeply with the number of trajectories, suggesting a VSTM capacity of about 1 item. 3) When
the deviation is suprathreshold and has fixed magnitude, the effective capacity depends strongly on the
magnitude of the deviation, but barely on the number of trajectories or the number of deviating trajectories.
Neither a model with a fixed, limited capacity nor a simple signal detection model can explain these results.
Instead, we propose that the brain performs Bayesian inference to solve this task, but under a fixed-energy
constraint. Bayesian inference means that the brain combines noisy trajectory observations in a way that
optimally takes into account their uncertainty, where uncertainty is manifested both in trajectory directions
and in spatial locations (causing mispairings of pre- and post-midpoint trajectories). The energy constraint
stems from the fact that action potentials are metabolically costly, a cost that is especially relevant if firing
is persistent in order to sustain a visual memory. We postulate that the total amount of energy available to
represent all trajectories is approximately fixed, and gets distributed approximately equally as the number of
trajectories increases. We further assume that the direction of each trajectory is encoded in a neural population and that neural variability belongs to the physiologically plausible family of Poisson-like distributions.
Then, the uncertainty per item increases approximately as the square root of the number of items. Using
this framework, we derive optimal decision rules for each experiment and simulate human performance using very few free parameters. We find that all data sets are well described by the metabolically constrained
Bayesian theory, but not by a limited-capacity model or an unconstrained Bayesian model. This not only
suggests that visual perception can be nearly optimal also for complex multi-stimulus tasks, but also that
the concept of a limited-capacity bottleneck in VSTM is dubious.
doi:10.3389/conf.neuro.06.2009.03.107

T-19. The fly lobula plate: a sensory network for ego-motion estimation based on optic flow
Alexander Borst

ABORST @ NEURO. MPG . DE

Max Planck Institute of Neurobiology
Navigation based on optic flow is a fundamental way of visual course control described in many different
species including man. In the fly, an essential part of optic flow analysis is performed in the lobula plate, a
retinotopic map of motion in the environment with the four cardinal directions (down, up, rightward, leftward)

36

COSYNE 09

T-20
represented in four different layers. There, a small a small network of identified neurons, the so-called
lobula plate tangential cells, extend their dendrites in different parts of the map and in different layers to
receive motion input. The lobula plate tangential cells possess large receptive fields with different preferred
directions in different parts of the receptive field, the structure of which resembles an optic flow as elicited
by certain flight maneuvers [1]. However, their dendritic fields within the lobula plate are much smaller and
confined, in most cases, to one layer only, suggesting a spatially restricted receptive field with a uniform
preferred direction. On the other hand, recent studies demonstrated an extensive connectivity between
different tangential cells, providing, in principle, the structural basis for their complex receptive fields [2]. I
will present a network simulation of the lobula plate tangential cells, comprising all the neurons studied so
far (22 on each hemisphere) with all the known connectivity based on gap junctions as well as excitatory
and inhibitory chemical synapses. The dendrite of each of these model neurons receives input from a
retinotopic array of Reichardt-type motion detectors according to the location of the dendrite within the
lobula plate. When probing the receptive fields of the model neurons, they exhibit flow fields similar to
their natural counterparts, demonstrating that the connectivity between the lobula plate tangential cells
indeed can quantitatively account for their complex receptive field structure. When tested with global motion
patterns corresponding to different types of ego-motion (e.g. yaw, roll, pitch), each of the model neurons
responds optimally to a particular flight maneuver. [1] Krapp HG, Hengstenberg R (1996) Estimation of
self-motion by optic flow processing in single visual interneurons. Nature 384: 463-466 [2] Borst A, Haag
J (2007) Optic flow processing in the cockpit of the fly. In: Invertebrate Neurobiology. Eds: G North, RJ
Greenspan. CSHL-Press, pp 101-122.
doi:10.3389/conf.neuro.06.2009.03.229

T-20. Efficient spike encoding for mapping visual receptive fields
Gordon Pipa1,2
Zhe Chen2,3
Sergio Neuenschwander1
Bruss Lima1
Emery Brown4

PIPA @ MPIH - FRANKFURT. MPG . DE
ZHECHEN @ MIT. EDU
NEUENSCHWAND @ MPIH - FRANKFURT. MPG . DE
BRUSS @ MPIH - FRANKFURT. MPG . DE
ENB @ NEUROSTAT. MIT. EDU

1

Max-Planck Institute for Brain Research
Massachusetts Institute of Technology
3
Havard Medical School
4
Massachusetts General Hospital
2

A standard way to understand spike-encoding principles by which neurons represent and process information in stimuli is through neural coding analysis. Three spike-encoding methods are commonly used to relate
quantitatively neural spiking activity to the features of putative stimuli: Wiener-Volterra kernel (WVK), the
spike-triggered average (STA), and the point process generalized linear model (GLM). In a visual receptive
field mapping study, we compared the performance of these three spike coding approaches in estimating
receptive field sizes and orientation tuning of 251 V1 neurons recorded from two adult monkeys during a fixation period in response to a moving bar. The GLM consisted of two formulations of the conditional intensity
function for a point process characterization of the neural spiking activity: one with a stimulus-only component and one with both stimulus and spike history components. The GLM fit is implemented efficiently in
MATLAB using a maximum likelihood principle. Goodness-of-fit of spike encoding methods was assessed
using cross-validation with Kolmogorov-Smirnov (KS) tests in light of the time-rescaling theorem. Spike
coding accuracy and reliability are evaluated for each model in predicting the individual neurons’ spiking activity for every moving bar direction (4016 models in total, for 251 neurons,16 different directions, and 9 trials
per direction). The GLM that considered spike history of up to 35 ms accurately predicted neuronal spiking
activity (95% confidence intervals for the one-sample KS test) with a performance of 97.0% (3895/4016) for

COSYNE 09

37

T-21
the training data, and 96.5% (3876/4016) for the test data. If the spike history was not considered, then the
performance dropped to 73.1% in the training and 71.3% in the testing data. In contrast, the WVK and the
STA predicted spiking activity poorly, achieving the accuracy rates of 24.2% and 44.5% for the test data,
respectively. In our findings, the receptive field size estimates obtained from the GLM (with and without
history), WVK, and STA were comparable using a standard 1/e criterion; however, using a new criterion
based on the 95% confidence bound of the rate estimate, the RF size estimates from WVK and STA are
greatly underestimated, whereas the estimation bias can be avoided if employing the new criterion from
the GLM estimate. We also found that in orientation tuning, compared to the GLM estimate, the results
from the WVK and the STA were underestimated on average by a factor of 0.45. Qualitative comparisons
between three spike-coding methods reveal their links and differences. The main reason for using the STA
and WVF approaches is their apparent simplicity. However, our experimental analyses suggest that more
accurate spike prediction as well as more reliable estimates of receptive field size and orientation tuning can
be computed using GLM. We believe these findings provide valuable insights in mapping visual receptive
fields using efficient spike encoding analysis. Acknowledgments: Research was funded by an European
Union grant EU-04330 (to G.P.) and US NIH grants DP1 OD003646-01, MH59733-07, and HL084501-01(to
E.N.B.).
doi:10.3389/conf.neuro.06.2009.03.086

T-21. Inferring functional connectivity in an ensemble of retinal ganglion cells sharing a common input
Michael Vidne1
Jayant Kulkarni2
Yashar Ahmadian1
Jonathan Pillow3,4
Jonathon Shlens5
EJ Chichilnisky5
Eero Simoncelli6
Liam Paninski1

MV 333@ COLUMBIA . EDU
JK 2619@ COLUMBIA . EDU
YASHAR @ STAT. COLUMBIA . EDU
PILLOW @ MAIL . UTEXAS . EDU
SHLENS @ SALK . EDU
EJ @ SALK . EDU
EERO. SIMONCELLI @ NYU. EDU
LIAM @ STAT. COLUMBIA . EDU

1

Columbia University
Cold Spring Harbor Laboratory
3
Center for Perceptual Systems
4
University of Texas at Austin
5
The Salk Institute
6
HHMI/NYU
2

Common inputs from unobserved neurons can have a significant effect on the dynamics and coding properties of neural populations [1]. In the retina, [2] found that neighboring parasol retinal ganglion cells (RGCs)
receive a significant amount of strongly correlated common input in the absence of modulated light stimuli.
They also found that the direct electrical coupling between RGCs is weak. Meanwhile, [3] introduced a
statistical model that was successful in capturing the spatiotemporal correlations between the spike trains
of large populations of RGCs by means of effective direct couplings between cells, but did not account for
common input to the cells. Here, we use a state-space model that explicitly accounts for common input,
in order to infer the functional connectivity in this neural population. Our state-space model is constructed
in conjunction with the generalized linear model framework introduced by [3], and includes spatiotemporal
stimulus filtering, history-dependent spiking, and interneuronal direct coupling effects which capture dependencies on the recent spiking of other cells, as well as the unobserved common input. The common input
is modeled as a multivariate autoregressive process whose correlation time is set to be consistent with the

38

COSYNE 09

T-22
results of [2]. To estimate all of the model parameters simultaneously, we need to maximize the marginal
likelihood of the observed spiking data, given the observed stimulus. The required marginalization over
the unobserved common input is difficult to perform exactly, but a Laplace approximation to the marginal
likelihood may be computed efficiently using fast block-tridiagonal matrix methods [4]. We performed the
likelihood optimization under the constraint that the direct coupling terms must have a delay (so that one
neuronś firing can not affect the others instantaneously); this constraint ensures the complete parameter
identifiability of this common input model. This framework extends the model developed by [3] significantly,
leading to several improvements. We observe that when the common input model is applied to the data
from [3], the inferred direct connectivity is weak, in agreement with the intracellular recordings described
in [2]. The model is still able to account quite well for the cross-correlation properties of this network. In
addition, the model allows us to estimate the sub-threshold common input effects on a single trial basis. We
are currently studying how this common noise affects the amount of information transmitted by this network
about the visual environment. [1] Kulkarni, J. & Paninski, L. (2007). Common-input models for multiple
neural spike train data. Network: Computation in Neural Systems 18: 375-407. [2] Trong, P.K. and Rieke, F.
(2008). Origin of correlated activity between parasol retinal ganglion cells. Nature Neuroscience. [3] Pillow,
J.W. and Shlens, J. and Paninski, L. and Sher, A. and Litke, A.M. and Chichilnisky, EJ and Simoncelli, E.P.
(2008). Spatio-temporal correlations and visual signalling in a complete neuronal population. Nature. [4]
Koyama and Paninski submitted (2008). Efficient computation of the maximum a posteriori path and parameter estimation in integrate-and-fire and more general state-space models. Journal of Computational
Neuroscience.
doi:10.3389/conf.neuro.06.2009.03.248

T-22. Different strategies for coding What and When in the archer fish
retina
Genadiy Vasserman
Maoz Shamir
Ronen Segev

VASSERMA @ BGU. AC. IL
SHMAOZ @ BGU. AC. IL
RONENSGV @ BGU. AC. IL

Ben Gurion University
How can the central nervous system estimate the value of a continuous external stimulus, such as changing
visual scene, from the noisy responses of nerve cell populations? The traditional approach for studying the
neural code has focused on characterizing the noisy neural responses to external stimuli relative to stimulus
onset time. However, stimulus onset time must also be estimated by the brain, making the utility of such an
approach questionable. We addressed this issue using the framework of colour coding by the archer fish
retinal ganglion cells, the only cells to project axons to the brain. Psychophysical measurements yield a
lower bound on the archer fish colour discrimination accuracy, in a continuous-time stimulus. Recording the
dynamic response of retinal ganglion cells to stimulus colour, we find that stimulus colour can be estimated
from single cell responses with accuracy comparable to psychophysical accuracy. However, to extract this
information, accurate estimate of stimulus onset is essential. We find that stimulus onset time can be
estimated using a linear nonlinear readout mechanism, albeit accurate estimate requires the use of the
responses of about two hundred cells. Thus, relatively large neural population are required to estimate
’When’ stimulus occurres, whereas the temporal structure of single cell responses is sufficient to estimate
’What’ stimulus.
doi:10.3389/conf.neuro.06.2009.03.021

COSYNE 09

39

T-23 – T-24

T-23. Learning, and learning to learn, with hierarchical Bayesian models
Joshua Tenenbaum

JBT @ MIT. EDU

Massachusetts Institute of Technology
In everyday learning tasks, human learners routinely make deep and correct generalizations from very limited evidence. Even young children can infer the meanings of words, the hidden properties of objects, or
the existence of causal relations from just one or a few relevant observations. These abilities far outstrip
what we expect from conventional statistical learning algorithms in machines, or typical animal learning
paradigms. In both those settings, we are used to providing much greater quantities of data to the learner
and seeing much weaker forms of generalization. What kinds of computational models could explain characteristically human forms of rapid learning and generalization, and what kinds of brain mechanisms could
implement these computations? I will argue that people’s everyday inductive leaps can be understood
as Bayesian inferences over hypothesis spaces generated by abstract knowledge representations – what
cognitive scientists have sometimes called ”intuitive theories” or ”schemas”. Hierarchical Bayesian models
(HBMs) can explain how abstract knowledge may itself be learned from experience, at the same time as
it functions to guide more specific generalizations from sparse data. I will discuss HBMs for problems of
generalizing and ”learning to learn” in the domains of categorization, word learning and causal learning.
A key challenge in these HBMs is to balance strong and highly constrained inductive biases – necessary
for rapid generalization – with the flexibility to adapt to the structure of new environments, learning new
inductive biases for which we could not have been pre-programmed. I will offer a few comments about how
these models relate to our current understanding of learning mechanisms in the brain, and the prospects
for using computational models to bridge the cognitive and neural levels of description for human learning.
doi:10.3389/conf.neuro.06.2009.03.307

T-24. Matching spontaneous and evoked activity in V1: a hallmark of
probabilistic inference
Pietro Berkes1
Gergo Orban1
Mate Lengyel2
Jozsef Fiser1
1
2

BERKES @ BRANDEIS . EDU
OGERGO @ BRANDEIS . EDU
M . LENGYEL @ ENG . CAM . AC. UK
FISER @ BRANDEIS . EDU

Brandeis University
University of Cambridge

Neural responses in the visual cortex of awake animals are highly variable, display substantial spontaneous
activity even when no visual stimuli are being processed, and the variability in both evoked activity (EA)
and spontaneous activity (SA) is strongly structured. However, most theories of visual cortical function
remain mute about the possible computational roles and consequences of such variability and treat it as
mere nuisance or, at best, as an epiphenomenon. Here, we propose that neural response variability in EA
and SA may be a hallmark of statistical inference carried out by the visual cortex and test a key prediction
of this normative theory in multiunit recordings from awake ferrets. Under our working hypothesis, neural
response variability represents uncertainty about stimuli: we treat cortical activity patterns as samples from
an internal, probabilistic model of the environment. Thus, given a stimulus, EA can be interpreted as representing samples from the posterior probability distribution of possible causes underlying visual input. In the
absence of a stimulus, this probability distribution reduces to the prior expectations assumed by the internal

40

COSYNE 09

T-25
model as reflected by SA. This interpretation of EA and SA directly leads to a critical prediction about their
relation: If they represent samples from the same, statistically optimal model of the environment, then the
distribution of spontaneous activity must be identical to that of evoked activity averaged over natural stimuli. In practice, a perfect identity may not be achieved, but crucially, the two sides of this equation should
become closer as the internal model of the environment implemented by the cortex is being matched to
the statistics of natural scenes. We analyzed multiunit data from 14 awake P29-151 ferrets recorded with a
linear array of 16 electrodes. Neural activity was recorded in two conditions: while the animal was watching
a movie (EA), and while the animal was in complete darkness (SA). Neural data was discretized in 2ms
bins and binarized. We constructed the joint distribution over possible states of the 16 channels in the two
conditions and computed the Kullback-Leibler (KL) divergence, a standard measure of statistical dissimilarity between these two distributions. We found that after visual development the distribution of EA was very
close to that of SA (less than 1.5% of the minimum coding cost), that this similarity significantly increased
with visual experience, and that it was brought about by a match between the spatial correlational structure
of the activity patterns, rather than merely by preserved firing rates across conditions. A similarly significant
increase in the match between the temporal correlational structure of EA and SA was also found. In addition, we found that classical theories of visual cortical function based on independence and sparseness
were not supported by our data. These results suggest that neural variability samples from a probabilistic
model of the environment that is gradually being tuned to natural scene statistics by sensory experience as
the visual system develops.
doi:10.3389/conf.neuro.06.2009.03.314

T-25. A walk through the woods explains the space variant oblique
effect
Thomas Weisswange
Constantin Rothkopf
Jochen Triesch

WEISSWANGE @ FIAS . UNI - FRANKFURT. DE
ROTHKOPF @ FIAS . UNI - FRANKFURT. DE
TRIESCH @ FIAS . UNI - FRANKFURT. DE

Frankfurt Institute for Advanced Studies
Learning of receptive fields from natural image ensembles has usually assumed stationary statistics across
the visual field and implicitly also independence from the active selection process due to eye movements
as well as the imaging geometry. Such models cannot explain that visual performance depends on the
position within the visual field. As an example, perceptual studies in humans have demonstrated reduced
discrimination ability for obliquely oriented patterns as compared to horizontal and vertical ones in the central area of the visual field [1]. This so called oblique effect varies with the position within the visual field in
that with increasing eccentricity sensitivity to meridionally oriented stimuli increases [2]. What is the origin
of this space variant oblique effect? While it has been proposed to understand some aspects of the oblique
effect through the second order statistics of natural images, as quantified by their power spectrum [3,4], we
investigate the role of higher order dependencies and task behavior on properties of receptive fields across
the visual field. Image sequences are obtained by simulating an agent navigating through wooded environments. Standard unsupervised learning algorithms are used to learn a sparse code of the images but
contrary to previous approaches, the learning is separately carried out for different regions of the visual field
and different gaze allocations during walking. The properties of the learned receptive fields are quantified
through the parameters of best fitting Gabor functions. This analysis shows that the receptive fields have
a preference for horizontal and vertical orientations at the center and more and more meridional directions
in the periphery. They also show systematic changes with increasing eccentricity. Furthermore, we relate
these results to the behavioral measurements through Fisher information, demonstrating qualitative agreement with the space variant properties of the oblique effect. Furthermore, it is shown that the distribution of

COSYNE 09

41

T-26 – T-27
preferred orientations significantly depends on where gaze is directed within the visual field during walking.
Artificial scenes using textures of natural images only up to second order, as well as textures sampled from
dead leaves models shown that the second order dependencies are not sufficient for observing the receptive fields anisotropies. The analysis was repeated on a dataset obtained from human subjects walking
through a wooded environment also demonstrating comparable results. We conclude that the properties
of model receptive fields not only depend on the statistics of visual scenes in the environment, but also on
the statistics imposed on the stimulus by the imaging geometry and the statistics of the interaction with the
environment during natural task execution. Taking all these determinants together, receptive fields can be
learned that explain the space variant oblique effect. Acknowledgements: This research was supported by
EC MEXT-project PLICON and the German Federal Ministry of Education and Research within the ”Bernstein Focus: Neurotechnology” research grant. References [1] Appelle, S., Psych. Bulletin 78 (1972),
266-278 [2] Rovamo, J., et al., Invest. Ophtalmol. Vis. Sci. 23 (1982), 666-670 [3] Wainwright, M. J., Vision
Research 39 (1999), 3960-3974 [4] Bruce, N.D., Tsotsos, J. , Neurocomp. 69 (2006), 1301-1304
doi:10.3389/conf.neuro.06.2009.03.283

T-26. Interpreting primary motor cortex based on optimal feedback
control
Stephen H. Scott

STEVE @ BIOMED. QUEENSU. CA

Queen’s University
Primary motor cortex (MI) is a key component of the voluntaryl motor system providing the largest contribution to the corticospinal tract and receiving input from many cortical and subcortical structures. The most
common approach for interpreting MI function has been based on the notion of sensorimotor transformations, focusing attention on experiments that identify which coordinate frames best describe neural activity
in MI. However, myriad coordinate frames or neural representations have been observed illustrating correlations with spatial goals, hand motion, joint motion, muscular torque, muscular power and EMG activity.
Are we learning anything from identifying what neuron’s ‘code’? The focus of my talk will be to provide
an alternate approach for interpreting MI function based on optimal feedback control. I will show how this
conceptual framework is consistent with many aspects of neural processing in MI including the importance
of sensory feedback, dramatic shifts in neural ‘coding’ across behaviours and the intimate link between MI
activity and the properties of the musculoskeletal system.
doi:10.3389/conf.neuro.06.2009.03.296

T-27. Representation of Motor Learning in the Smooth Eye Movement
Region of the Frontal Eye Fields
Jennifer Li
Stephen Lisberger

JENNIFER . LI @ UCSF. EDU
SGL @ PHY. UCSF. EDU

University of California San Francisco
Repeated exposure to a consistent change in target motion allows the smooth pursuit eye movement system
to learn rapidly and emit an eye movement that predicts the time and direction of a change in target motion
[1]. To understand how the neural signals for learning are represented and modified as they progress

42

COSYNE 09

T-28
through the pursuit circuit, we recorded from the smooth eye movement region of the frontal eye field
(FEFSEM). The FEFSEM is a motor cortex for pursuit that processes the visual motion input from area MT
to drive downstream structures. Pursuit neurons in the FEFSEM do not respond in a stereotyped manner
to pursuit. Rather, the coding of pursuit is distributed so that each neuron is most active during specific
epochs of the movement [2]. This raises the possibility that a neuron is selectively involved in learning
at its preferred times, or the times after the onset of target motion when the neuron is most active. We
recorded from 55 single units in the macaque FEFSEM to explore how these neurons alter their activity
during pursuit learning. The change in target direction always occurred at 250 ms after the onset of target
motion, but the direction change itself was chosen anew each day to be the cardinal direction closest to
the neuron’s preferred direction. Learning-related changes in firing rate varied widely across our neural
population. We found that the magnitude of neural learning could be predicted by the normalized size of
the neuron’s response in normal pre-learning pursuit, at the time of the instructive stimulus 250 ms after
the onset of target motion. Differences in neural learning could not be explained by other factors, such as
the day to day variability in the learned behavior. Based on our results for when the instructive stimulus
occurred at 250 ms, we postulated that FEFSEM neurons tend to express learning when the instructive
stimulus occurs around the time of the neuron’s maximum response. As a direct test, we exposed 11
neurons to an additional learning experiment which differed from the first learning experiment only in the
timing of the instructive stimulus, which was chosen to be 150 ms for some neurons and 350 ms for others.
For 8 of the 11 neurons, neural learning was larger when the instructive stimulus occurred at a time more
preferred by the neuron. Taken together, these findings indicate that one of the advantages of a temporally
distributed coding for pursuit in the FEF may be to allow the system to selectively modify portions of the
movement. Acknowledgements This work was supported by the Howard Hughes Medical Institute and NIH
grant MH77970. References [1] Medina JF, Carey MR, Lisberger SG (2005) Neuron 45(1):157-67. [2]
Schoppik D, Nagel KI, Lisberger SG (2008) Neuron 58(2):248-60.
doi:10.3389/conf.neuro.06.2009.03.039

T-28. The building blocks of cerebellum-dependent learning
Jennifer Raymond

JENNIFER . RAYMOND @ STANFORD. EDU

Stanford University School of Medicine
The relatively simple circuit architecture of the cerebellum suggests that this brain structure may be one
of the first to yield its computational principles. A key function of the cerebellum is motor learning. Results from my laboratory indicate that the cerebellum contains independent and molecularly-distinct building
blocks from which motor memories can be constructed in a combinatorial fashion. We are now working to
dissect cerebellum-dependent learning into its elemental components. To date, we have identified molecularly distinct mechanisms supporting: increases versus decreases in movement amplitude; changes in
movement dynamics versus amplitude; changes in dynamics in one direction (phase advance) versus the
other (phase lag), and the effects of low-frequency versus high-frequency training. Our results suggest that
a single training experience can engage more than one plasticity mechanism. Moreover, subtle changes in
the way the training is done can influence which molecular mechanisms are recruited or not recruited.
doi:10.3389/conf.neuro.06.2009.03.294

COSYNE 09

43

T-29 – T-30

T-29. Developing a working memory with reward-modulated STDP
Cristina Savin
Jochen Triesch

SAVIN @ FIAS . UNI - FRANKFURT. DE
TRIESCH @ FIAS . UNI - FRANKFURT. DE

Frankfurt Institute for Advanced Studies
It is commonly believed that the neural correlate of working memory is the selective persistent neuronal
activity during delay period. Such dynamics are traditionally modelled by recurrent networks exhibiting attractor dynamics, where a non-selective background activity attractor coexists with multiple stimulus-specific
attractors [1]. These models provide a good match to experimental data, but so far completely ignore how
working memory develops in the context of goal directed behavior. Given that working memory evolves significantly after birth, in both capacity and maintenance time [2, 3], it is interesting to ask to what extent this
development is driven by goal directed learning. To address this question, we investigate whether working
memory properties can emerge in a generic recurrent neural network due to reward-dependent learning.
Specifically, we consider the classical delayed response task requiring a stimulus to be stored in memory for
a variable time such that a specific response can be delivered after an external cue. The model consists of
a sparsely connected neural network receiving localized, stimulus specific inputs. This neuron population is
connected to a layer of motor neurons, which decide the action to be taken through a winner-take-all mechanism. Reward-modulated STDP [4] is used to shape the recurrent connectivity and the synapses to motor
neurons. We show that the recurrent neural network can reshape its connectivity to successfully perform the
task. Synaptic changes lead to a segregation of the network in stimulus-specific subpopulations. Moreover,
these subpopulations exhibit stimulus-specific activation during maintenance, as seen in neurophysiological studies. The final performance in the task is significantly higher compared to that of networks with fixed
connectivity or networks trained with STDP only. Our model demonstrates that working memory dynamics
may naturally emerge in a recurrent neural network with reward-modulated STDP, suggesting that reward
dependent learning may be a central aspect of the development of working memory. Acknowledgements
The authors are supported by EC MEXT project PLICON and the German Federal Ministry of Education
and Research (BMBF) within the ”Bernstein Focus: Neurotechnology” through research grant 01GQ0840.
References [1] N. Brunel and X.J. Wang. Effects of neuromodulation in a cortical model of object working
memory dominated by recurrent inhibition. Journal of Computational Neuroscience, 11:63–85, 2001. [2]
M. Luciana and C. Nelson. The functional emergence of prefrontally-guided working memory systems in
four-to eight-year-old children. Neuropsychologia, 36(3), 1998. [3] L.T. Singer and P.S. Zesking, editors.
Biobehavioral Assessment of the Infant. Guilford Publications, 2001. [4] E. Izhikevich. Solving the distal
reward problem through linkage of STDP and dopamine signaling. Cerebral Cortex, 17:2443–2452, 2007.
doi:10.3389/conf.neuro.06.2009.03.276

T-30. Robust learning of position invariant visual representations with
OFF responses
Henning Sprekeler1,2
Wulfram Gerstner1,2
1
2

HENNING . SPREKELER @ EPFL . CH
WULFRAM . GERSTNER @ EPFL . CH

Brain Mind Institute, EPFL
Computer and Communication Sciences, EPFL

Our ability to recognize objects in spite of variations in size or position is important for a successful interaction with the environment. One possible neural basis for this ability is the visual representations that have
been found in the temporal lobe of primates. The firing rate of neurons in these regions have been shown to

44

COSYNE 09

T-31
be object specific but largely invariant to the retinal position and scale of the object [1]. Recent experiments
by DiCarlo and colleagues suggest that the position invariance of visual representations in inferotemporal cortex (IT) of monkeys are learned by exploiting the temporal contiguity of our environment [2,3]. The
rational behind the learning paradigm DiCarlo tested is that the visual stimuli received before and after a
saccade are likely to contain the same objects, but at different retinal positions. Therefore, position invariant
representations can be learned by changing the sensory processing such that the firing rate of neurons in
IT in response to the visual stimuli before and after saccade should be the same. Any learning rule that
implements this idea has to compare the pre- and postsaccadic stimulus and therefore requires a memory
trace that stores either the identity of the presaccadic stimulus or the firing rate of the neuron in response to
that stimulus. Here, we propose that this memory trace could be represented by neuronal OFF responses.
OFF responses are short periods of increased activity after the offset of a stimulus, which are displayed
by many neurons in the visual system. Because these responses are stimulus specific while overlapping
temporally with the initial response to the next stimulus, they introduce correlations between responses to
stimuli that occur in temporal succession. Here, we show that a simple rate-based model that combines
OFF responses with Hebbian learning and homeostatic plasticity allows (a) the unsupervised learning of
position invariant representations, (b) the reproduction of the experimental results of Li and DiCarlo [2] and
(c) the development of neuronal selectivity with a qualitative match to experimental results on neuronal
selectivity in IT [4]. Our simulations show that learning with OFF responses implements a time scale invariance in that it is very robust to variations in the intersaccade interval. A comparison shows that the trace
rule [5], the classical online rule for learning invariances, is more sensitive to fixation time variations. Our
results suggest a new computational role for OFF responses in sensory learning and show that we may
lose interesting learning mechanisms by concentrating on mean firing rates and neglecting the temporal response pattern of sensory neurons. [1] Logothetis and Sheinberg, Annual Reviews in Neuroscience, 1996
[2] Li and DiCarlo, Science, 2008 [3] Cox, Meier, Oertelt and DiCarlo, Nature Neuroscience, 2005 [4] Franco
and Rolls, Biological Cybernetics, 2007 [5] Foldiak, Neural Computation, 1991
doi:10.3389/conf.neuro.06.2009.03.060

T-31. Sparse neural representations of odor and associative learning
Iori Ito1
Chik-ying Ong1,2
Baranidharan Raman3,4
Mark Stopfer1

ITOI @ MAIL . NIH . GOV
ONGCHIKY @ MAIL . NIH . GOV
RAMANBAR @ MAIL . NIH . GOV
STOPFERM @ MAIL . NIH . GOV

1

NICHD/NIH
Dept. of Biochemistry, CUHK, Hong Kong
3
National Institute of Health
4
National Institute of Standards & Technology
2

Sensory systems create neural representations of environmental stimuli; these representations can be
associated with other stimuli through learning. Are patterns of spikes the neural representations that get directly associated with reinforcement during conditioning? In the moth Manduca sexta, we intracellularly and
extracellularly recorded from Kenyon cells (KCs), a neural population long linked to learning and memory in
insects. Using a wide variety of odors, we found that odor pulses elicited only one or two spikes upon the
odor’s onset (and, if the pulse duration exceeded 750 ms, also, sometimes upon the odor’s offset) in each of
a small fraction of KCs. Guided by our physiological characterization of KC responses, we designed behavioral experiments (proboscis extension conditioning paradigm) such that the timing of sucrose reinforcement
was systematically varied relative to the onset of a brief odor pulse. We found the intervals between odor
pulses and reinforcement that produced associative conditioning included no temporal overlap between
spiking in KCs and sucrose presentation. Further, increasing the temporal overlap between spiking in the

COSYNE 09

45

T-32 – T-33
KCs and sucrose reinforcement actually reduced the learning efficacy. Moreover, selectively reinforcing
offset spikes induced by an extended long odor pulse did not support learning. Thus, spikes in KCs do not
constitute the odor representation that coincides with reinforcement, and spike-timing-dependent plasticity
in KCs alone cannot underlie this learning.
doi:10.3389/conf.neuro.06.2009.03.093

T-32. Synaptic mechanisms of whisker sensory perception
Carl Petersen1,2
1
2

CARL . PETERSEN @ EPFL . CH

Brain Mind Institute
Ecole Polytechnique Federale de Lausanne

A key goal of modern neuroscience is to understand the neural circuits and the synaptic mechanisms underlying simple forms of sensory perception and associative learning. Here, I will discuss our efforts to
characterise sensory processing in the mouse barrel cortex, a brain region known to process tactile information relating to the whiskers on the snout. Each whisker is individually represented in the primary
somatosensory neocortex by an anatomical unit termed a ”barrel”. The barrels are arranged in a stereotypical map, which allows recordings and manipulations to be targeted with remarkable precision. In this
cortical region it may therefore be feasible to gain a quantitative understanding of neocortical function. As a
mouse explores its environment, the whiskers are actively moved backwards and forwards as if searching
the space for tactile input. Sensory information in this sensory pathway is therefore actively acquired. It is
therefore crucial to obtain information on cortical function in awake behaving mice directly correlated with
whisker behaviour. We have begun this process using reward-based learning paradigms, viral manipulations, whole-cell recordings, voltage-sensitive dye imaging and two-photon microscopy. Highly-correlated
large-amplitude subthreshold membrane potential changes are accompanied by uncorrelated sparse action potential firing in layer 2/3 pyramidal neurons of the mouse barrel corex. Furthermore, we find strong
evidence for state-dependent processing of sensorimotor information. Together with precise genetic manipulations, we think that these approaches will lead to significant advances in our understanding of sensory
perception at the level of individual neurons and their synaptic connections.
doi:10.3389/conf.neuro.06.2009.03.289

T-33. Nonlinear receptive field mapping reveals two subpopulations
of orientation selective neurons in V2.
Anita Schmid
Jonathan Victor

AMS 2031@ MED. CORNELL . EDU
JDVICTO @ MED. CORNELL . EDU

Weill Medical College of Cornell
Primary visual cortex (V1) mainly extracts oriented luminance boundaries, while secondary visual cortex
(V2) detects more complex boundaries, including texture boundaries. Detection of texture boundaries requires that neurons respond in a nonlinear way to combinations of stimulus components. Such nonlinear
interactions have previously been investigated primarily in V1, using stimuli consisting of multiple spots or
bars. Here we investigate these interactions in V1 and V2, using stimuli consisting of multiple patches of
oriented gratings. Because V1 neurons provide the main input for V2 neurons, understanding these complex responses promises insight into how one cortical region transforms the output of another. We recorded

46

COSYNE 09

T-34
responses from single neurons in V1 and V2 of anesthetized monkeys. Stimuli consisted of a 4 by 5 or 6 by
6 grid of adjacent rectangular regions, covering the classical and non-classical receptive field. Each region
contained sinusoidal gratings with either the preferred orientation or the non-preferred orthogonal orientation controlled by m-sequences (frame rate 20 ms). The m-sequences were designed to enable extraction
of the first- and second-order response kernels in space as well as time. V1 neurons all have monophasic
first-order kernels, and their timing is very consistent across the population. In contrast, V2 neurons have
two distinct patterns of responses: some are biphasic, with an initial peak width narrower than the V1 responses (‘transient V2 neurons’); others are monophasic, but with a broader peak than the V1 responses
(‘sustained V2 neurons’). The biphasic response pattern indicates dynamic orientation tuning: there are
some time lags for which the response to the non-preferred orientation is larger than the response to the
preferred orientation. These responses predict that the optimal stimulus within a patch is the non-preferred
orientation followed by the preferred orientation. The spatial second-order kernels - how the response to a
pair of regions differs from the sum of the responses to the two regions presented independently – enforce
the distinctions between V1 and the two V2 subpopulations. Neurons in V1 have nonlinear interactions consistent with cross-orientation suppression. Transient V2 neurons exhibit the opposite: nonlinear interactions
that result in cross-orientation facilitation. Sustained V2 neurons show no measurable nonlinear spatial interaction. The temporal second-order kernels – the response to combinations of orientations in subsequent
frames – also distinguish theV1 and the two V2 subpopulations. Neurons in V1 mainly sum supralinearly
over time, leading to augmentation of responses for prolonged presentations of the same orientation. Transient V2 neurons on the other hand have a preference for changing orientation over time. Sustained V2
neurons show no temporal interaction. This study shows, firstly, how non-linear as well as linear responses
of neurons in V1 differ from V2 responses. Secondly, using nonlinear receptive field mapping, we identified
two different classes of orientation selective V2 neurons. These results suggest a conceptual model for
how V1 outputs are processed to make V2 responses. The transient V2 neurons differentiate the V1 input
in space and time and therefore respond well to changes in orientation. Sustained V2 neurons pool the V1
input and respond better to constant orientation signals.
doi:10.3389/conf.neuro.06.2009.03.193

T-34. Imperfect receptive fields can enhance performance of neural
populations
Yuan Liu
Tatyana Sharpee

YUANLIU @ SALK . EDU
SHARPEE @ SALK . EDU

The Salk Insitute for Biological Studies
Neural variability is present throughout the nervous system. Yet, in some tasks, motor output variability can
be almost exclusively attributed to sensory input variability [1], despite the many levels of neural circuitry
that are involved in sensory-motor transformations. This suggests that mechanisms for noise reduction
exist. Here, we find one such mechanism by studying the properties of retinal receptive fields (RFs). Recent studies indicate that while retinal RFs are approximately circular and tile the visual space by forming a
hexagonal lattice, there are also noticeable irregularities both in RF shapes [2, 3] and center positions [3].
Specifically, we questioned whether these two types of irregularities provide additive noise contributions, or
whether irregularities in RF shapes compensate, at least partially, for noise due to scattered RF centers.
We first searched for optimal RF configurations where both RF centers and outlines can be adjusted independently. As the criterion for optimality we used the mutual information available in the population of
neural responses about the location of a point light source. The maximal information was achieved when
RFs were arranged on a regular hexagonal lattice, with RF size (as measured by a standard deviation of
best fitting Gaussian) 0.47 of the lattice constant, in full agreement with measurements of average RF size

COSYNE 09

47

T-35
in the rabbit [4] and guinea pig [5] retinas, as well as for different cell types in the primate retina [3, 6]. This
answer was obtained by modeling RF profiles as a sigmoidal increase in the spike probability across the
RF boundary. However, the optimal value for RF size did not depend on any adjustable parameters, because it was independent of the width of the sigmoidal function (as long as this width was smaller than RF
size). This analysis yielded better agreement with experimental data than that obtained by modeling RFs
as Gaussians. Next, we examined whether information maximization could explain the local RF properties
given the RF center positions and firing rates. By modeling RF boundaries as ellipses, as in experimental
studies [3, 6], we found that maximizing the information provided by the retinal circuit resulted in values for
RF aspect ratios and orientations that matched values from experimental mosaics on a neuron-by-neuron
basis. The average correlation coefficient between predicted and experimental RFs was 0.89-0.91 for different mosaics, significantly higher than that for random mosaics (0.78-0.82, p<0.05). Our work demonstrates
that some RF irregularities are not contributing additional noise to neural coding, but rather serve as noisereducing mechanisms. [1] L. C. Osborne, S. G. Lisberger, and W. Bialek, Nature 437, 412 (2005). [2] C.
L. Passaglia, J. B. Troy, L. Ruttiger, et al., Vision Res 42, 683 (2002). [3] G. D. Field and E. J. Chichilnisky,
Annu Rev Neurosci 30, 1 (2007). [4] S. H. Devries and D. A. Baylor, J Neurophysiol 78, 2048 (1997). [5]
B. G. Borghuis, C. P. Ratliff, R. G. Smith, et al., J Neurosci 28, 3178 (2008). [6] G. D. Field, A. Sher, J. L.
Gauthier, et al., J Neurosci 27, 13261 (2007).
doi:10.3389/conf.neuro.06.2009.03.285

T-35. Cortical analysis of auditory scenes and speech
Shihab Shamma

SAS @ UMD. EDU

University of Maryland
Spectrotemporal modulations are critical in understanding the perception of complex sounds, especially in
conveying intelligibility in speech and quality in music. Neurons in the primary auditory cortex extract and
explicitly represent these modulations in their responses. In this talk, I shall review experimental methods
to study the representation of these modulations in the cortex and the way they reveal the acoustic features
of complex sounds such as phonemes. I shall also discuss various abstractions of this analysis to develop
algorithms for the analysis of complex auditory scenes. One example is the question of how spectrotemporal
modulation content of speech can be used to assess its intelligibility, to discriminate it from non-speech
signals, and to enhance it by performing Weiner-like filtering in the space of the cortical representation. In
music, these modulations underlie the perception of sound quality, and hence can be effectively employed
to construct a perceptually-meaningful metric of musical timbre. Finally, I shall review recent extensions of
these studies that explore the role of feedback and top-down attentional effects on the representation of
modulations, measurements that are carried out while animals are engaged in a variety of auditory tasks.
The resulting understanding of these rapid adaptive mechanisms has been formulated as algorithms for the
segregation of complex sound mixtures to resolve difficult examples of the ”cocktail-party problem”.
doi:10.3389/conf.neuro.06.2009.03.305

48

COSYNE 09

T-36 – T-37

T-36. Natural sound selectivity in the auditory forebrain is strongly
shaped by the acoustic environment
Noopur Amin1
Frédéric Theunissen2
1
2

NOOPUR @ BERKELEY. EDU
THEUNISSEN @ BERKELEY. EDU

Helen Wills Neuroscience Inst., UC Berkeley
University of California, Berkeley

Perceptual discrimination of vocal communication signals is crucial for the reproductive fitness of many animals in the wild, including birds. We have investigated and found support for the postulate that the avian
auditory system shows some degree of specialization for the processing of species-specific vocalizations.
Neural responses in the zebra finch auditory forebrain regions of field L and the lateral caudal mesopallium (CLM) have shown selectivity for conspecific song compared to statistically-matched synthetic sounds
(Grace et al., 2003). Here we investigate the role that structured acoustic stimulation during development
plays in shaping this neural selectivity and tuning for natural sounds: we raised zebra finches in isolation and
in continuous unstructured white noise until adulthood and then electrophysiologically recorded responses
in field L (primary auditory cortex analog) and CLM (higher/secondary auditory cortex analog) to natural
sounds and matched synthetic sounds. We then compared both the neural selectivity and spectro-temporal
receptive fields (STRFs) obtained from birds raised in noisy environments to the results obtained from normal, social adults. We find that the selectivity for conspecific song is significantly reduced when compared
to a subset of synthetic sounds and completely disappears when compared to more broadband type synthetic sounds. This is the first evidence that absence of patterned auditory stimulation during post-natal life
dramatically reduces the neural selectivity for natural sounds. Our study also reveals that tuning to power
spectra found in natural sounds is mostly unaffected, whereas the tuning to the fine structure of natural
sounds (i.e. temporal and spectral modulations found in song) is either acoustically and socially driven or at
the very least sensitive to environmental and social manipulation. We also find a significant reduction in the
diversity of functional classes of receptive fields in birds reared in white noise, and that more than half of
the observed receptive fields are tuned to low frequencies, which is vastly different from the receptive fields
obtained from normal adults. Our findings therefore imply that impoverished or noisy environments have a
powerful impact on the brain and could adversely affect important perceptual tasks.
doi:10.3389/conf.neuro.06.2009.03.179

T-37. Synaptic mechanisms underlying sustained responses in auditory cortical neurons.
Michael Wehr
Ben Scholl

WEHR @ UOREGON . EDU
SCHOLL . BEN @ GMAIL . COM

University of Oregon
Auditory cortical neurons can show sustained spiking responses to sounds in awake animals. The number
and timing of individual spikes in these sustained responses varies randomly across trials. How does this
randomness arise? One explanation is that neurons receive large numbers of uncorrelated synaptic inputs,
which produce a sustained depolarization of the membrane potential to a value near threshold. Individual
spikes are then triggered by random fluctuations around threshold. In this view, information is propagated
through the cortical network by the increased firing rates of pre- and postsynaptic neurons. An alternative view is that information is propagated through cortical networks by synchronously arriving volleys of
synaptic inputs. This view is supported by intracellular recordings from auditory and somatosensory cor-

COSYNE 09

49

T-38
tical neurons in anesthetized animals, which show that their membrane potentials remain near rest most
of the time, except for brief high-amplitude excursions (or ”bumps”) which reliably generate spikes. These
membrane potential dynamics imply that network activity is highly correlated. However, evoked responses
are transient in auditory and somatosensory cortical neurons in anesthetized animals. The synaptic mechanisms underlying sustained responses are therefore unclear. On one hand, neurons could show sustained,
“DC” depolarizations, implying temporally uncorrelated presynaptic inputs. On the other hand, neurons
could show a sustained increase in the rate of high-amplitude membrane potential excursions, i.e., an increased “AC” occurrence of bumps. This would imply that even sustained spiking responses, for which
spike times vary randomly across trials, are generated by synchronously arriving volleys of synaptic inputs.
Here we report that sustained responses in auditory cortical neurons in unanesthetized rats can consist of
sustained epochs of bumps. In many of these sustained responses, bumps were well-separated in time,
such that the membrane potential repeatedly returned nearly to rest during a response. In many other sustained responses, in contrast, the membrane potential showed large fluctuations but remained elevated well
above rest during the response. However, these fluctuations were highly correlated with fluctuations in the
simultaneously recorded local field potential. This suggests that these fluctuations reflect concerted, transient volleys of network activity. We conclude that all sustained membrane potential responses in auditory
cortical neurons result from sustained epochs of bumps, which, depending on their rate, may temporally
summate into sustained depolarizations.
doi:10.3389/conf.neuro.06.2009.03.324

T-38. Connecting brain to mind through computation: The birth of
computational psychiatry
P. Read Montague

READ @ BCM . TMC. EDU

Baylor College of Medicine
Over the last 50 years, we have witnessed an increased understanding of the biological underpinnings of
mental function. Our current ability to identify and study neurotransmitter systems, their receptors, and their
impact on important behaviors has provided neuroscience and modern psychiatry with new sets of tools.
However, between receptors, neurons and behavioral endpoints lie many layers of information processing
and its here that modern computational models are making inroads to understand normal and pathological
mental function in both neurobiological and computational terms. The current generation of computational
models provides new ways to understand normal and abnormal mental function and is taking advantage of
the continuing rise of computational sciences in all areas of science and medicine. This talk will review this
new area and give a perspective on its current accomplishments and future promise.
doi:10.3389/conf.neuro.06.2009.03.222

50

COSYNE 09

I-1

I-1. Network adaptation improves temporal representation of naturalistic stimuli in Drosophila eye
Mikko Juusola1,2
Anton Nikolaev
Trevor J. Wardill1
Cahir J. O’Kane
Gonzalo G. de Polavieja3
Lei Zheng

M . JUUSOLA @ SHEFFIELD. AC. UK
NIKOLAEV @ MRC - LMB . CAM . AC. UK
T. WARDILL @ SHEFFIELD. AC. UK
C. OKANE @ GEN . CAM . AC. UK
GONZALO. POLAVIEJA @ UAM . ES
LEI . ZHENG @ EYE . OX . AC. UK

1

University of Sheffield
Beijing Normal University
3
Cientifico Titular CSIC
2

Retinal networks must adapt constantly to best present the ever changing visual world to the brain. Although
such processing is often considered sequential, it is likely that adaptation occurs ubiquitously across the
whole retinal network. What then are the dynamics and mechanisms of network adaptation in retinal information processing? Drosophila, with its well-defined genetics, modular eye structure, fully characterized
synaptic layout of the first visual neuropil1 and accessibility for electrophysiology of single neurons2, 3 is
a powerful model for studying neural adaptation in vivo. The first visual neuropile, the lamina, contains a
system of neurons consisting of photoreceptors (R1-R6) and interneurons: large monopolar cells (LMCs;
L1-L5) and an amacrine cell (AC) that co-process visual information4. While photoreceptors depolarize and
LMCs hyperpolarize to light, owing to a web of feedforward and feedback synapses their graded voltage
responses are shaped together3. Here we exploit an in vivo Drosophila preparation to investigate how the
circuits in the fly eye route and process changing information toward the fly brain. We studied adaptation
dynamics at the first visual synaptic layer in Drosophila by intracellularly recording responses of photoreceptors (R1-R6) and their histaminergic output neurons, large monopolar cells (LMCs) to naturalistic light
contrast series. By analyzing the synaptic throughput in wild-type and mutant flies, we show that visual
information processing is dynamically enhanced by network adaptation. Network adaptation improves both
the frequency and amplitude representation of LMC output by enhancing sensitivity to under-represented
signals, and the signal-to-noise ratio of this transmission. It happens at different speeds for different brightness of stimulation, requiring a dynamic balance between the light-mediated conductance and feedbackmediated synaptic conductances. This equilibrium can be offset by tampering with either synaptic feedforward or feedback pathways. A faulty feedforward pathway in post-synaptic ort6 mutants speeds up LMC
output, mimicking extreme light adaptation; a faulty feedback pathway in L2-shiTS1 flies slows down LMC
output, mimicking dark adaptation. These results highlight the importance of network adaptation for efficient
neural coding and as a mechanism for selectively regulating the size and speed of signals in neurons. 1.
Meinertzhagen, I. A. & O’Neil, S. D. Synaptic organization of columnar elements in the lamina of the wild
type in Drosophila melanogaster. J Comp Neurol 305, 232-63 (1991). 2. Juusola, M. & Hardie, R. C. Light
adaptation in Drosophila photoreceptors: I. Response dynamics and signaling efficiency at 25 oC. J Gen
Physiol 117, 3-25 (2001). 3. Zheng, L. et al. Feedback network controls photoreceptor output at the layer
of first visual synapses in Drosophila. J Gen Physiol 127, 495-510 (2006). 4. Meinertzhagen, I. A. & Sorra,
K. E. Synaptic organization in the fly’s optic lamina: few cells, many synapses and divergent microcircuits.
Prog Brain Res 131, 53-69 (2001).
doi:10.3389/conf.neuro.06.2009.03.002

COSYNE 09

51

I-2 – I-3

I-2. Offset adaptation in the inner hair cell synapses enhances speech
coding
Huan Wang1,2
Marek Rudnicki2
Werner Hemmert2
1
2

HUAN . WANG . CN @ GMAIL . COM
MAREK . RUDNICKI @ TUM . DE
WERNER . HEMMERT @ TUM . DE

Infineon Technologies, AG
Technische Universitaet Muenchen

One of the most critical processing steps during encoding of sound signals for neuronal processing is
when the analog pressure wave is coded into discrete nerve-action potentials. This conversion induces
massive information loss - or to phrase it positively - information reduction. As any information lost during
this process is no longer available for neuronal processing, it is important to understand and quantitatively
model the underlying principles. We have developed a detailed model of auditory processing, which codes
sound signals into spike-trains of the auditory nerve. We have also developed Hodgkin-Huxley models of
cochlear nucleus neurons, which are driven by auditory nerve spike-trains. We analyze the quality of coding
with the framework of automatic speech recognition and the temporal information processing capabilities
with the methods of information theory. Our latest improvements in speech coding by introducing the effect
of offset-adaptation together with an improved matching of neuronal features to the speech recognizer using
an artificial neuronal network has lead to significant improvements of recognition scores, now reaching the
values of successful technical feature extraction methods. Offset adaptation is also required to drive onset
neurons in the cochlear nucleus. The importance of temporal information is highlighted by the fact that 50%
of the information in their spike trains is coded with a precision better than 1 ms. In summary, our results
provide quantitative insight into temporal processing strategies of neuronal processing and are relevant
improving automatic speech recognition systems and hearing aids, especially cochlear implants. This work
was funded within the Munich Bernstein Center for Computational Neuroscience by the German Federal
Ministry of Education and Research (reference numbers 01GQ0441 and 01GQ0443).
doi:10.3389/conf.neuro.06.2009.03.029

I-3. Phase Response Curve of Spike Response Model
Munenori IIDA1
Toshiaki Omori2
Toru Aonishi
Masato Okada
1
2

IIDA @ MNS . K . U - TOKYO. AC. JP
OMORI @ K . U - TOKYO. AC. JP
AONISHI @ DIS . TITECH . AC. JP
OKADA @ K . U - TOKYO. AC. JP

Grad. sch. of Front. Sci., The Univ. of Tokyo
The University of Tokyo, RIKEN

Phase response curves (PRCs) that measure how sensitively oscillatory neurons respond to external perturbation stimuli can capture the essence of non-equilibrium dynamics induced by small disturbances,
and it has been shown theoretically and experimentally that the collective dynamical behavior of a neural population—such as coherent oscillations and traveling waves—can be quantitatively reproduce by a
phase reduction system based on PRCs obtained from a single neuron. The PRC is thus one of the most
important representations bridging the gap between single-neuron dynamics and network dynamics. Since
the PRC characterizes the change of the spike initiating time due to a perturbation, it must substantially reflect the properties of the subthreshold membrane response. Previous studies, however, have not clarified
the clear relation between the PRC and the subthreshold membrane dynamics. In this study we analytically

52

COSYNE 09

I-4
derive the relation between the PRC and the dynamics of the subthreshold membrane potential by using
the spike response model (SRM). The SRM neuron is analytically tractable and more biologically plausible than the leaky integrate-and-fire (LIF) neuron. The SRM can reproduce a different neuronal response
property by setting kernels appropriately. When we clarify the relation between the linear and nonlinear
elements of the PRC and the kernel functions of the SRM, we are able to estimate the PRC accurately
from the subthreshold membrane potential dynamics of a single neuron. By analysis we have clarified the
relation between the PRC and the subthreshold membrane potential dynamics when a perturbation with
finite amplitude is applied to a neuron. It has been shown that linear and nonlinear elements of PRCs are
expressed by kernel functions of the SRM. Numerical simulations have shown that the theoretically derived
PRC that has both linear and nonlinear elements is more similar to the numerically obtained PRC than is
the theoretically derived PRC that has only the linear element. Since the amplitude of the perturbation stimulus in physiological experiments is finite, the PRCs observed in physiological experiments may be mixture
of a linear element and higher-order nonlinear elements. When a perturbation stimulus that is not negligibly small is added, the higher-order nonlinear elements are important for obtaining the PRC. One could
estimate the PRC accurately from the electrophysiological response property of a single neuron using the
theoretically derived PRC of the SRM.
doi:10.3389/conf.neuro.06.2009.03.255

I-4. Intrinsic membrane properties control gamma-frequency input integration
Stephani Otte1,2
Andrea Hasenstaub3
Terrence Sejnowski
Ed Callaway

OTTES 01@ GMAIL . COM
ANDREA @ SALK . EDU
TERRY @ SALK . EDU
CALLAWAY @ SALK . EDU

1

Salk Institute
University of California San Diego
3
Crick-Jacobs Center, Salk Institute
2

Synchronization of neuronal activity in the neocortex, particularly at gamma (30-80Hz) frequencies, is related to cognitive functions such as working memory and attention; disruptions of this synchrony are associated with cognitive and attentional deficits. The primary correlate of gamma oscillatory activity is the
synchronized activity of inhibitory interneurons, but the functional consequences of this synchrony remain
unclear. We therefore ask: do different types of neurons respond differentially to synchronized inhibition?
If so, what mechanisms account for these differences? Here we combine electrophysiology, modeling, and
dynamic clamp to examine the interaction between a neuron’s intrinsic properties, the degree of gamma
synchrony among its inhibitory inputs, and its integration of and responsiveness to excitatory inputs. We
used dynamic clamp to stimulate cortical neurons with constant artificial excitatory conductances, and with
fluctuating artificial inhibitory conductances that varied in their mean amplitude and in their degree of gamma
synchrony. Neurons varied considerably in the dependence of their response on inhibitory synchrony, both
in the degree to which their responsiveness depended on synchrony, and also in the dynamic range over
which changes in synchrony changed their responsiveness. Investigating a simple model suggested that
two factors, time constant and afterhyperpolarization duration, predicted the effect of gamma synchrony on
responsiveness: model neurons with longer time constants, that more strongly filtered gamma frequencies
in their inputs, had firing rates less affected by gamma-band inhibitory synchrony. In addition, AHP duration
determined dynamic range: model neurons with long AHPs could be gradually driven to more frequent firing
by increasing the amplitude of the gamma oscillation, while models with brief AHPs fired either on every
cycle of gamma or not at all. Both relationships were experimentally confirmed: neurons less modulated

COSYNE 09

53

I-5
by gamma synchrony (e.g. large layer V pyramids) had longer time constants, while those with steeper I-O
curves (e.g. layer V FS interneurons) had shorter AHPs. We further confirmed these findings by using dynamic clamp to artificially change the AHP duration or time constant of real neurons; neurons’ dependence
on gamma input synchrony increased when leak conductance was added (to reduce its time constant),
and neurons’ dynamic range increased when larger and slower AHPs were added. We conclude that a
neuron’s intrinsic physiology substantially affects its responsiveness to gamma-synchronized inhibitory inputs. We therefore predict that, in vivo, cortical neurons will exhibit layer- and type-specific variations in
the modulation of their responsiveness by synchronized inhibition, independent of laminar variations in the
strength of inhibitory synchrony. Further, we note that the relevant physiology is not static, but may be
altered by contextual or neuromodulatory factors; for instance, a neuron’s time constant decreases during
intense synaptic activity, while acetylcholine can alter AHP amplitude and duration, as well as membrane
time constant. We therefore suggest that the effect of gamma oscillations on responsiveness is not fixed,
but may be dynamically shaped (by factors such as neuromodulation or local background activity) to suit
the cortex’s computational requirements during attention or cognition.
doi:10.3389/conf.neuro.06.2009.03.181

I-5. The spatial extent of attentional facilitation and inhibition of return
in humans and monkeys
Aarlenne Khan
Naomi Takahashi
Stephen Heinen
Robert McPeek

AARLENNE @ SKI . ORG
NAOMI @ SKI . ORG
HEINEN @ SKI . ORG
RMM @ SKI . ORG

Smith-Kettlewell Eye Research Institute
When a salient cue is flashed just before a target, it reduces reaction times to the target presented at the
cued location compared to a target in the opposite hemi-field. This is known as attentional facilitation.
However, if the cue and target are separated further in time, reaction times to the target in the cued location
are increased, a phenomenon known as inhibition of return (IOR). Although these cueing effects are well
documented for cued and opposite hemi-field locations, it remains less clear how the cue and target interact
in other areas of the visual field over time. Here, we investigate the spatial extent of attentional facilitation
and inhibition of return (IOR) in response to an exogenous cue. We used saccade latencies as a behavioral
metric of attentional allocation. Two humans and two monkeys made saccades to visual targets at 136
locations spread across the visual field (9◦ up/down/left/right) in an 18 deg region centered on a central
fixation target. In 80% of the trials, a behaviorally irrelevant cue was flashed at one of 4 oblique locations
(7◦ eccentricity) either 50 or 200 ms before the target. In the remaining 20% of the trials, no cue was
presented. We calculated average saccade latency as a function of distance from the cue at each of the
four cue locations relative to uncued saccade latency. When the cue preceded the target by 50ms, we found
slightly shorter saccadic reaction times for targets located in the same quadrant as the cue. Interestingly,
we also found a strong inhibitory effect (longer latencies) for saccades to targets in all other quadrants,
with the strongest effect in the quadrant opposite to the cue. This pattern virtually reversed when the cue
preceded the target by 200ms; we found strong inhibition in the cued quadrant and slight facilitation of all
other quadrants. These results were similar for both humans and monkeys. Our findings suggest that the
spatial attention does not only affect the location of attentional allocation but rather extends to the entire
visual space. The spatial extent of the cue-related saccade latency modulations points toward a centersurround mechanism for both attentional facilitation and IOR. These results could be explained by a neural
model combining lateral inhibition with post-synaptic suppression in salience map.
doi:10.3389/conf.neuro.06.2009.03.004

54

COSYNE 09

I-6 – I-7

I-6. Saccade related phase resetting of theta and delta rhythms modulates cortical high gamma activity
Christopher Kovach1
Hiroto Kawasaki
Naotsugu Tsuchiya2,3
Matthew Howard
Ralph Adolphs

CHRISTOPHER - KOVACH @ UIOWA . EDU
HIROTO - KAWASAKI @ UIOWA . EDU
NAOTSU @ GMAIL . COM
MATTHEW- HOWARD @ UIOWA . EDU
RADOLPHS @ HSS . CALTECH . EDU

1

University of Iowa Hospitals and Clinics
California Institute of Technology
3
Japan Society for the Promotion of Science
2

The generation of saccadic eye movements and control of attention depend on common brain systems.
These systems are linked to neuromodulatory pathways of the brainstem and basal forebrain that are positioned to influence activity throughout cortex. The role of the latter pathways in gating of visual information
is largely unexplored. In order to look for an association between shifts of overt attention and the modulation of activity in extrastriate visual cortex we obtained electrocorticographic and eye-movement data from
six surgical epilepsy patients during a task requiring visual search. Patients attended to upright or inverted
faces among distractors of the opposite orientation. Recorded areas included parts of fusiform gyrus and
superior temporal gyrus implicated in face perception. We used high gamma (80 – 200 Hz) power as a
measure of local cortical activation. In all six patients, saccades were associated with a characteristic modulation of HGP in focal areas of cortex near the lateral and ventral occipitotemporal junction. After the onset
of a saccade, HGP first fell below the presaccadic baseline, implying relative suppression of locally synchronized activity, then rose above baseline, implying enhancement. At the same time, HGP was continuously
coupled to the ongoing phase in the delta (2-4Hz) or theta (4-10Hz) ranges, with peak coupling frequency
varying by recording location and subject. Across all contacts, saccade-related HGP modulation appeared
in the presence of both delta/theta coupling and phase resetting within the coupled low frequency, while
the absence of either coupling or resetting resulted in no HGP modulation. We therefore conclude that the
observed saccade related modulation of HGP is not extrinsic to the delta/theta cycle but is a consequence
of phase resetting within the coupled low frequency. These observations support three inferences: (1) they
imply an association between saccade generation and modulation of cortical states by way of the theta and
delta rhythms; (2) they suggest a mechanism for saccadic suppression; and (3) they support a model of
processing in which visual cortex assumes alternating transmissive and receptive states at frequencies in
the theta or delta range which are optimally regulated with respect to eye movements. We speculate that
projections from basal forebrain nuclei which modulate theta and delta activity and receive input from the
brainstem reticular formation provide a mechanism for saccade related phase resetting.
doi:10.3389/conf.neuro.06.2009.03.326

I-7. Causal role of auditory cortex in mediating attention to moments
in time
Santiago Jaramillo
Anthony Zador

JARA @ CSHL . EDU
ZADOR @ CSHL . EDU

Cold Spring Harbor Laboratory
Attention is the means by which the brain selects and highlights important sensory information for further
processing, ignoring or suppressing the rest. The earliest single unit studies revealed that neural activity

COSYNE 09

55

I-8
in the first stages of cortical sensory processing is modulated by attention (Hubel et al., 1959). To date,
most of what is known about the neural mechanisms underlying attention is based on correlations between
neural activity and behavior. Here we establish for the first time that not only is neural activity in the auditory
cortex correlated with attention, but that the auditory cortex is causally involved in mediating the behavioral
advantages conferred by attention. We first developed a rodent behavioral paradigm in which we could
demonstrate that attention enhanced performance on a sensory discrimination task. The task was an auditory two-alternative choice paradigm, modeled after a related visual paradigm in humans (Coull and Nobre,
1998), in which we manipulated attention to moments in time. Rats were required to detect a frequencymodulated target sound immersed in a train of pure tone distractors. Performance varied systematically
with task difficulty, manipulated by the varying modulation depth of the target. Behavioral analysis showed
a clear improvement in both reaction times and correct discrimination whenever the target appeared at an
expected moment, compared to the same target appearing unexpectedly. These results show that attention
improved performance. We next used tetrodes to record responses from single neurons in the primary
auditory cortex of rats performing this task. The majority of responsive neurons showed an increase evoked
response to tones immediately preceding the expected appearance of the target when compared against
responses to the same tones occurring long before the expected target location. These results show that attention to moments in time modulates changes in neural activity in the auditory cortex. Finally, we assessed
the effect on performance of pharmacological inactivation of the auditory cortex. We used the GABA-A
receptor agonist muscimol to reversibly inactivate the auditory cortex bilaterally. Muscimol inactivation impaired performance compared with control application of saline, implying that the auditory cortex plays an
important role in this task under normal conditions. Furthermore, inactivation of the auditory cortex significantly reduced the improvements in performance associated with attention. These results suggest that the
auditory cortex is required to mediate the behavioral advantage conferred by attention. In summary, we
have shown that: (1) attention to moments in time can lead to improved performance on a two-alternative
choice auditory detection/discrimination task in rats; (2) neural activity in the primary auditory cortex is
correlated with attention to moments in time; and (3) the auditory cortex plays a causal role in mediating
the behavioral advantage conferred by attention. Hubel DH, Henson CO, Rupert A, Galambos R (1959).
”Attention” units in the auditory cortex. Science 129:1279-1280. Coull JT, Nobre AC (1998). Where and
when to pay attention: the neural systems for directing attention to spatial locations and to time intervals as
revealed by both PET and fMRI. J Neurosci 18:7426-7435.
doi:10.3389/conf.neuro.06.2009.03.129

I-8. Computing the cost function in decision making
Jan Drugowitsch
Ruben Moreno-Bote
Alexandre Pouget

JDRUGOWITSCH @ BCS . ROCHESTER . EDU
RMORENO @ BCS . ROCHESTER . EDU
ALEX @ BCS . ROCHESTER . EDU

University of Rochester
Decision making under time constraints is hard, as it requires the decision maker to trade off between
making ill-informed decisions immediately, and continuing to gather more evidence under potentially high
costs. This trade off is described by the cost function, which is the decision maker’s cost of making additional observations to support the decision. This cost function is generally assumed to be constant, which
implies optimal decision making with the Sequential Probability Ratio Test (SPRT) and its continuous-time
counterpart - the diffusion-to-bound model. However, this assumption is never verified and it is therefore
unknown if the SPRT is indeed the best procedure to use. This work introduces a method for computing the
cost function from behavioral data that, for the first time, allows verification of the assumption of constant
cost, in addition to providing further insight into the decision maker’s decision procedure and the optimality of decision making under certain experimental conditions. The core assumption of the method is that

56

COSYNE 09

I-9
the behaviorally measured probability of correct choice corresponds to the subject’s belief (in the Bayesian
sense) that this choice is indeed correct. We show this premise to hold for a large class of rate-to-bound
models, an instance of which is the popular diffusion-to-bound model. The only required assumptions are
a symmetry property of the model and a symmetric prior on the evidence strength, both of which are easily
satisfied. Equating belief and performance makes it possible to compute from behavioral data the subject’s
belief over time when making a decision. Then, a dynamic programming formulation of the decision problem under the premise of reward maximization allows for deriving the cost for gathering additional evidence,
which results in the subject’s cost function. This method is applicable to both maximizing reward in a single
trial, and the more natural setting of maximizing the reward rate in repeated trials. Also, it reveals how the
decision boundary ought to change over time in the corresponding rate-to-bound model. We have applied
the outlined method to behavioral data from monkeys (Roitman and Shadlen, 2002) and humans (Green
and Bavelier, personal communication) in a 2AFC motion discrimination task. Both subject groups feature
similar performance over time that starts at close to 100% correct decisions and decays over time almost
linearly to chance performance. The resulting cost function is initially constant and then rises linearly, which
is at odds with what is commonly assumed, and might be explained by an increasing effort of maintaining
attention to the stimulus for longer time. This indicates that popular models based on the SPRT need to be
reevaluated to incorporate a non-constant cost function that accounts for our findings. Also, computing the
cost function from performance opens the door to comparing this function across subjects and tasks, such
that the source of performance improvements can be evaluated. This idea will be applied to investigating
the increased performance of frequent players of action video games in both visual and auditory tasks, as
observed by Green and Bavelier.
doi:10.3389/conf.neuro.06.2009.03.034

I-9. Estimation and reproduction of time intervals by LIP neurons
Mehrdad Jazayeri1,2
Michael Shadlen3,1

MJAZ @ U. WASHINGTON . EDU
SHADLEN @ U. WASHINGTON . EDU

1

University of Washington
HHWF, HHMI
3
HHMI
2

We use our internal sense of time to learn, anticipate and appropriately respond to the temporal regularities
in the environment. The neural computations that enable us to track time are however unknown. We
trained monkeys in a novel “Ready, Set, Go” paradigm to examine the behavioral and neural correlates of
time estimation and time production. Monkeys fixated a central spot, and were presented with two brief
peripheral flashes, a “Ready” cue followed by a “Set” cue. These cues demarcated a reference interval
(RI) that varied randomly from trial to trial. The distribution from which RI samples were drawn (i.e., the
prior) was Uniform and varied between 500 and 1000 ms. Monkeys were required to reproduce RI by
making a saccadic eye movement RI time units after “Set”. Production times (PT) accurately reproduced
RIs with a systematic bias towards the mean of the prior distribution over RI. The magnitude of the bias was
more pronounced for longer RIs. These observations conform to a Bayesian framework in which subjective
estimates of RIs are assumed to derive from probabilistic fusion of (1) the likelihood associated with a
sample RI, and (2) the prior distribution over RI. We placed the saccade target within the response field
of individual LIP neurons whose activity is believed to reflect various spatial and temporal contingencies of
impending saccades. The “Ready” and “Set” cues were presented outside the response field. Presentation
of the “Ready” cue caused a transient dip in LIP activity. Subsequently, the firing rate increased until the
“Set” cue was presented. The strength of LIP responses at the time of “Set” was correlated with RI, and
was predictive of the time of the impending saccade on a trial-by-trial basis. The onset of the “Set” cue
caused another transient in LIP response that was followed by a monotonic rise to a constant threshold at

COSYNE 09

57

I-10
the time when the saccade was initiated. The slope with which the pre-saccadic activity rose to threshold
was progressively steeper for shorter reference intervals, and was also predictive of the PT on a trial-bytrial basis. Importantly, the trial-to-trial relationship between LIP activity during the estimation phase (i.e.
“Ready-Set”) and the ensuing PT was specific to the behavioral demands of the task. In a control experiment
in which the relationship between RI and PT were reversed (i.e. longer PT for shorter RI and vice versa),
the relationship between neural activity and the ensuing production time was also reversed. LIP activity
during the “Ready-Set” interval suggests that neurons might track elapsed time through a monotonically
increasing firing rate. On the other hand, time production – as in the “Set-Go” phase of our task – seems
to rely on a bounded ramp-like activity whose slope is adjusted to the temporal demands of the task. The
data hint at a capacity to adjust the rate of rise in the production phase based on the relationship between
an external event (e.g., “Set”) and the representation of elapsed time.
doi:10.3389/conf.neuro.06.2009.03.052

I-10. Behavior-dependent responses in primate frontal cortex neurons during natural behavior
Cory Miller
Xiaoqin Wang

CORY. MILLER @ JHU. EDU
XIAOQIN . WANG @ JHU. EDU

Johns Hopkins University
Primates are typified by their complex social systems and the sophisticated behaviors used to navigate
them. Although relatively little research has been directly aimed at examining the neural substrates underlying primate social behaviors, it is clear that producing these behaviors requires an integration of both the
sensory and motor information. While data on sensory-motor interactions in the primate cortex are available, relatively little remains known about this process in species’ natural behaviors. Given the significance
of natural behaviors in the evolutionary history and daily lives of nonhuman primates, empirical study of
their underlying neural mechanisms may reveal unique insights into cortical functions. Here we recorded
the single-unit activity of frontal cortex neurons in freely-moving common marmoset monkeys (Callithrix jacchus) engaged in a natural, species-typical behavior known as antiphonal calling, a vocal behavior involving
the reciprocal exchange of vocalizations between conspecifics. Producing an antiphonal call is dependent
upon first hearing a particular vocalization, a phee call, and producing the same call type in response. Importantly, the sequence of sensory and motor events in this behavior occurs over a 10-15s period of time.
In our first set of analyses, we compared neural activity during each of the three elements of this behavior: sensory period (phee call stimulus), latency delay, and vocal-motor response (antiphonal call). The
aim here was to determine whether neurons across the population showed firing rate changes during any
of these three periods individually or in combination. Preliminary analyses revealed neuronal responses
during each of these behavioral periods, with a population showing behaviorally modulated activity over
the duration of the antiphonal calling behavior. This population of neurons typically showed one of two responses. The first class showed an increased firing rate during the phee stimulus and suppressed activity
during the antiphonal call. The second class showed the inverse response, exhibiting suppression during
phee stimulus and excitation during the antiphonal response. In our second set of analyses, we examined
the effects of behavioral context of the neural response observed during antiphonal calling. Specifically,
during test sessions, we presented subjects with phee call stimuli that did not elicit antiphonal callings and
subjects produced phee calls spontaneously. As such, we tested whether neurons responded similarly during either the sensory stimulus or motor response alone compared to the same sensory or motor period
during antiphonal calls. Our initial analysis suggests that the aforementioned ‘antiphonal calling’ neurons
show little response during the sensory stimulus and vocal-motor response independently. Together these
data suggest that it is the sensory-motor integration during the antiphonal calling behavior that is driving

58

COSYNE 09

I-11
the neural response. This study builds on our previous work examining the ethology and functional neuroanatomy of antiphonal calling in common marmosets. This work supported by grants from the NIH to
CTM (F32 DC007022, K99 DC009007) and XW (R01 DC005808).
doi:10.3389/conf.neuro.06.2009.03.067

I-11. Adaptive decision making in monkeys during a non-stationary
rock-paper-scissors game
Hiroshi Abe
Daeyeol Lee

HIROSHI . ABE @ YALE . EDU
DAEYEOL . LEE @ YALE . EDU

Yale University School of Medicine
When the outcomes from a particular action are uncertain, the value of an action can be given by the
average of alternative outcomes weighted by their probabilities. These probabilities are, however, often
unknown. Therefore, according to the reinforcement learning theory, action values are adjusted iteratively
according to the reward prediction errors. The reinforcement learning theory can also account for the
choices of humans and non-human primates in a social context. However, outcomes from the decision
maker’s action are more difficult to predict in a social setting, because other decision makers can change
their behaviors rapidly. According to the belief learning theory, decision makers in a social context estimate
the action values using the probabilities of various actions that might be taken by other decision makers.
Computationally, this is equivalent to updating the action values for all available actions based on real and
hypothetical payoffs expected from the actions of other players. To test these different learning algorithms,
we trained two rhesus monkeys in a computer-simulated rock-paper-scissors game. The animals began
each trial by fixating a small disk presented at the center of a computer screen. After a 0.5-s fore-period,
three peripheral targets, which were designated as rock, paper, and scissors, were presented on a circular
array, and the animal was required to shift its gaze towards one of the peripheral targets when the central
target was extinguished 0.5 s later. After a 0.5-s fixation period, all peripheral targets changed their colors
indicating their corresponding payoffs (0,1,2,3,4 drops of juice; 1 drop=0.2 ml) according to the payoff
matrix of a biased rock-paper-scissors game. The optimal (Nash equilibrium) strategy for this game was to
choose rock with p=0.5, and each of the other two targets with p=0.25. The positions of rock, paper, and
scissors targets were fixed in a block of 50 to 100 trials and then switched without any explicit cues. The
reinforcement learning model, belief learning model and a hybrid model (general learning model), which has
different learning rates for real and hypothetical payoffs, were tested with different types of utility functions
(linear, power, factor). Their performance was compared using the Bayesian information criterion (BIC).
The results showed that the choice behaviors of both monkeys were better accounted for by the general
learning model with factor utility functions. We have also recorded neural activity from the dorsolateral
prefrontal cortex (DLPFC) during the same task. Approximately 50% of neurons modulated their activity
according to the real payoff during the feedback period. A small number of neurons (9%) also changed their
activity according to hypothetical payoffs, suggesting that DLPFC might be involved in updating the action
values according to both real and hypothetical payoffs.
doi:10.3389/conf.neuro.06.2009.03.124

COSYNE 09

59

I-12

I-12. The Human Brain Computes Two Different Prediction Errors
Jan Glascher1
Nathaniel Daw2
Peter Dayan3
John O’Doherty

GLASCHER @ HSS . CALTECH . EDU
NATHANIEL . DAW @ NYU. EDU
DAYAN @ GATSBY. UCL . AC. UK
ODOHERJP @ TCD. IE

1

California Instititute of Technology
New York University
3
Gatsby Computational Neuroscience Unit, UCL
2

Reinforcement learning (RL) provides a framework involving two diverse approaches to reward-based decision making: model-free RL assesses candidate actions by directly learning their expected long-term
reward consequences using a reward prediction error (RPE), whereas model-based RL uses experience
with the sequential occurrence of situations (“states”) to build a model of the state transition and outcome
structure of the environment and then searches forward in it to evaluate actions. This latter, model-based
approach requires a state prediction error (SPE), which trains predictions about the transitions between
different states in the world rather than about sum future rewards. Eighteen human subjects performed a
probabilistic Markov decision task while being scanned with functional magnetic resonance imaging. The
task required subjects to make two sequential choices, the first leading them probabilistically into an intermediary state, and the second into one of three outcome states enjoying different reward magnitudes.
In an attempt to dissociate model-based from model-free RL completely, we exposed our subjects in the
first scanning session just to transitions in the state space in complete absence of rewards or free choices.
This permits a pure assessment of the model-building aspects of model-based RL, because model-free
RL cannot learn about future expected rewards in their absence, and the RPE is therefore nil. Prior to the
second, free-choice session subjects were exposed to the rewards that would be available at the outcome
state. Our subjects demonstrated the essential model-based RL competence of combining the information
about the structure of the state space with the reward information, by making more optimal choices at the
beginning of the free-choice session than would have been expected by chance (p<0.05, sign test, onetailed). In order to assess the neural signatures of these two error signals, we formalized the computational
approaches as trial-by-trial mathematical models and determined free parameters by fitting the model to
behavioral choices. An RPE and an SPE were derived from a model-free SARSA learner and a modelbased FORWARD (model) learner respectively. Choices from the FORWARD learner were computed using
dynamic programming. A combined model was derived by weighting the choice preferences of SARSA and
FORWARD; the relative influence of the latter was found to decrease over trials. The trial-by-trial reward
and state error signals derived from the two model components were included in the analysis of the imaging
data in order to seek their correlations with neural signals. We found evidence of a neural state prediction
error in addition to the previously well characterized RPE. The SPE was present bilaterally in intraparietal
sulcus (IPS) and lateral prefrontal cortex (latPFC), and was clearly dissociable from the RPE located predominantly in ventral striatum (all regions p<0.05, whole-brain correction). Importantly, the left latPFC and
right IPS also correlated with the SPE during the non-rewarded first session, underlining their importance in
pure state space learning. These findings provide evidence for the existence of two unique forms of learning
signals in humans, which may form the basis of distinct computational strategies for guiding behavior.
doi:10.3389/conf.neuro.06.2009.03.270

60

COSYNE 09

I-13 – I-14

I-13. A Computational and Neurobiological Account of Theory of Mind
Wako Yoshida
Ben Seymour
Karl Friston
Ray Dolan

W. YOSHIDA @ FIL . ION . UCL . AC. UK
BSEYMOUR @ FIL . ION . UCL . AC. UK
K . FRISTON @ FIL . ION . UCL . AC. UK
RDOLAN @ FIL . ION . UCL . AC. UK

University College London
Theory of mind describes our ability to represent the intentions and goals of others, and allows us to
optimise our interactions with them. Such strategic decision processes are central to achieving cooperative
and competitive equilibria in game theory, and humans are known to acquire the capacity make inferences
about others intentions at an early age. However, despite being widely studied, quite how the brain achieves
this from a computational perspective remains substantially unclear. We first present a neurobiologically
realistic model of theory of mind, based on optimal control theory and game theory. We consider the
representations of goals in terms of value-functions that are prescribed by utility or rewards. Critically, the
joint value-functions and ensuing behaviour are optimised recursively, under the assumption that I represent
your value-function, your representation of mine, your representation of my representation of yours, and so
on ad infinitum. However, if we assume the degree of recursion is bounded; players need the opponent’s
degree of recursion (or sophistication) to respond optimally. This induces a problem of inference on the
opponent’s sophistication, given behavioural exchanges. We show it is possible to deduce whether players
make inferences about each other and quantify their sophistication using choices in sequential games.
Next, we show that this model offers a good account of real subjects behaviour in an experimental ‘staghunt’ game. The stag-hunt game refers to a class of pro-cooperative game (prescribed by the pay-off
matrix) in which players choose to either cooperatively catch and share a valuable reward (a stag), or
to succumb to a competitive temptation to unilaterally catch a small reward (a hare). We embedded the
payoff matrix within a realistic, maze-based hunting game with one other player, and introduced a small
discount for the effort involved in catching the prey, parameterised in terms of time taken to make a catch
(with the stag being more difficult). Subjects played the game with a computerised opponent who changed
her sophistication level (cooperative or competitive) with time. Analysis of the behavioural data supports
the model, suggesting that subjects do indeed make inferences about the sophistication of the opponent,
and exploit it when making decisions. Lastly, we also show that the model predicts brain activity when
concurrently measured with fMRI. Several brain areas, notably the rostral medial prefrontal surface, have
been implicated in theory of mind based on previous exploratory studies, although quite what is represented
in their activity has been left to speculation. We show that rostral mPFC activity correlates with the entropy
of the inferred level of sophistication of the opponent. In contrast the left dorsolateral PFC correlates with
the inferred level of sophistication itself. In summary, these data offer support for an theory of mind model
that involves inferences about the bounded level of sophistication of other agents, and casts light on the
computational function of some of the hitherto enigmatic social brain areas.
doi:10.3389/conf.neuro.06.2009.03.161

I-14. Bayesian decision making predicts adaptive sensory weights
and decision threshold
Sophie Deneve

SOPHIE . DENEVE @ ENS . FR

Ecole Normale Supérieure, Paris
To make fast and accurate behavioural choices, we need to integrate the noisy sensory input, take into ac-

COSYNE 09

61

I-15
count prior knowledge, and adjust our decision criteria to maximize the expected outcome of actions. These
problems can be formalized in a Bayesian framework: Given the sensory input, what decision strategies
will result in maximizing the total reward? In two alternative forced choice tasks, Bayesian decision making is equivalent to a “diffusion” process: sensory inputs are integrated over time and a decision is made
when this integrated signal reaches ones of two bounds. However, this apparent analogy hides a “chicken
and eggs” problem: The weight of sensory signals (the diffusion rate), and, indirectly, the optimal decision
criteria (the threshold) depend on the reliability of the sensory input, i.e. the log likelihood ratio. In most
2AFC protocols, easy and hard decisions are randomly intermixed and sensory reliabilities are not known
in advance. We present a Bayesian decision model that infer both the choice probability and the reliability
of the sensory input, within a single trial, and based solely on the responses from a population of sensory
neurons with Poisson noise. This results in a “modified” non-linear diffusion model updating the impact of
a sensory spike and the decision threshold on-line. This algorithm performs a form of on-line “expectation
maximization” that infer the choices probability (E stage) and update of the sensory reliabilities (M stage) at
each time step. This adaptation of sensory weights has strong consequences on the decision mechanisms.
In hard decision tasks, the sensory input is weighted more strongly early during stimulus presentation. Its
influence later decays, implying that the choice is made based on prior knowledge and the first few sensory
observations, not the sensory input just preceding the decision. Moreover decision thresholds are not fixed
but evolve as a function of time and the sensory input: For hard tasks, this threshold collapses, resulting in
forcing a decision within a limited time frame. For easy tasks, this threshold increases, i.e. decisions are
made with higher accuracy at the cost of longer reaction times. Finally, this on-line update of confidence in
the sensory input regulates the contribution of prior beliefs. Thus, when the sensory input is not informative, the influence of the prior does not decay over time (as would be predicted by a diffusion model) and
thus dominates the decision, as it should. We implemented a motion discrimination task that have been
extensively studied in primates and compare the Bayesian decision maker with a diffusion model. While
both models predict similar trends for the mean reaction time and accuracy, the Bayesian model also predicts strong deviations from the diffusion model predictions that have been observed in behaving monkeys
trained at this task. Finally, we explore the possible implications of this adaptive decision making model for
the role of neural adaptation in cortical circuits.
doi:10.3389/conf.neuro.06.2009.03.204

I-15. Computational implications of a normalized value representation
in decision circuits
Kenway Louie
Lauren Grattan
Paul Glimcher

KLOUIE @ CNS . NYU. EDU
LG 1166@ NYU. EDU
GLIMCHER @ CNS . NYU. EDU

New York University
Value information is a critical component of decision-making, and a growing number of studies report valuerelated signals in decision-related neural circuits. Decision theorists have long hypothesized that choices
are guided by a relative reward representation, but the specific form in which the brain encodes value
remains unclear. Using two- and three-target choice tasks, our recent work suggests that visuomotor neurons in the monkey lateral intraparietal area (LIP) encode a relative measure of saccade value, normalized
across available options: activity is negatively correlated to target values outside the response field while
positively correlated to target value in the response field. Interestingly, this value normalization is welldescribed by a divisive normalization model that also characterizes nonlinear phenomena such as gain
control and cross-orientation suppression in visual cortex: FR = K*[B+V] / [S+Vt] where the firing rate of
an LIP neuron (FR) is a function of the value associated with a saccade to the target in the response field

62

COSYNE 09

I-16
(V), the total value of all available options (Vt), and three additional parameters (K,B,S). How might such a
value-coding scheme affect choice behavior? Here, we explore the computational implications of divisive
normalization for value-based decision-making. Specifically, we examine how relative reward encoding interacts with noisy population estimates of value and the resulting effects on choice. The motivating intuition
is straightforward: as the total value of available options increases, the separation between the distributions
of firing rates representing two differently valued options decreases; if variance doesn’t decrease appropriately, the options will be increasingly difficult to distinguish. To examine this computationally, we examine
the choice between two differently valued options as additional lower valued options are added to the choice
set. We simulate the decision process as the selection of the highest valued option, with the value of each
option drawn from a distribution of firing rates around the option-specific mean firing rate. The mean rate
for each option is determined using the value-based divisive normalization equation, with parameters fit
to the observed monkey physiological data. Noisy population coding is modeled by including two general
sources of noise: 1) a mean-rate dependent noise term, which models the considerable intrinsic response
variability observed in cortical neurons (response variance typically 1-1.5 times the mean response), and
2) a mean-rate independent additive noise term. We find that the presence of either form of noise leads
to increasing errors as the choice set size increases, and characterize this phenomenon as a function of
the number and value of additional alternatives as well as different degrees of noise. We conclude that
a relative representation of reward, mediated by divisive normalization, can lead to suboptimal choice behavior. These results suggest a computational explanation for why and how choice behavior responds to
multiple options and changing choice sets, and highlight the importance of understanding both the specific
mechanism of cortical representation and the nature of the population code.
doi:10.3389/conf.neuro.06.2009.03.218

I-16. Analysis of neural activity during error trials in decision-making
task
R. James Cotton
Andreas S. Tolias

RCOTTON @ CNS . BCM . EDU
ATOLIAS @ CNS . BCM . EDU

Baylor College of Medicine
Activity in the prefrontal cortex (PFC) is considered the major substrate of working memory, where single
units fire differentially to behaviorally relevant aspects of a stimulus that must be remembered through time.
This encoding has been demonstrated with a wide variety of stimuli and experimental paradigms and is
often interpreted as either a memory of stimulus or memory of an upcoming motor response. However,
much less is known about the firing properties of these neurons during incorrect trials – particularly how
stimulus selectivity on correct trials relates to the neural firing on error trials. Using a simple task that
dissociates working memory for class from motor responses, an adequate number of incorrect trials for error
analysis were collected while recording from neurons in the PFC of non-human primates. Our preliminary
analysis shows that neurons with class selectivity in correct trials reverse class preference on incorrect trials.
This finding suggests that neurons in PFC fire similarly on trials that the animal will indicate a particular
class, regardless of motor response and stimulus that prompts the decision. Therefore, while making an
incorrect decision, rather than encoding the working memory for the visual stimulus or not having any
stimulus selectivity, most neurons in PFC encode the upcoming decision from early in the memory period.
doi:10.3389/conf.neuro.06.2009.03.233

COSYNE 09

63

I-17

I-17. Evaluating signal detection models of perceptual decision confidence
Brian Maniscalco
Hakwan Lau

BRIAN @ PSYCH . COLUMBIA . EDU
HAKWAN @ GMAIL . COM

Columbia University
Introduction In making perceptual decisions, humans and animals are able to report the level of confidence
associated with the decision (1). In this work we investigate the mechanism underlying this confidence
reporting procedure. In particular, is all the information used for perceptual decision making available to
confidence reporting mechanisms? Some models of perception suggest that there are multiple channels
of information, and subjective reports such as confidence ratings can only tap into one of the channels
(e.g. cortical, but not subcortical channels). Is this popular view correct? We capitalize on an original
psychophysical finding (2) that subjective reports of perceptual confidence and perceptual performance (d’)
can dissociate, and apply formal model comparison techniques to identify the mechanism underlying confidence reporting. Methods and Results Signal detection theory (SDT) can be extended to characterize
sensitivity and response bias in the type 2 task, i.e. the task of discriminating between one’s own correct
and incorrect perceptual judgments using confidence ratings (3). We considered several such SDT models,
including: A simple SDT model where type 2 decisions are made by setting criteria on a transformation
of the primary decision axis; A decision noise model where type 2 sensitivity is affected by variation in
type 2 criterion setting; A late noise model where the noisy perceptual signal becomes even noisier when
making confidence judgments; A two-channel model where only one channel contributes to confidence
judgments. Psychophysical data from the metacontrast masking paradigm dissociates perceptual performance from confidence in a manner that cannot plausibly be accounted for by differences in type 2 criterion
setting (2). As such, this data presents a stringent test for models of type 2 performance. We compared
models by evaluating the likelihood of each model, given the metacontrast masking data, using the Akaike
information criterion. All models could account for perceptual performance, but only the late noise model
provided a close fit to the observed performance-confidence dissociation. Discussion Our results suggest
that traditional SDT models are not adequate to model type 2 performance, because they do not provide
a process that allows for the kind of performance-confidence dissociation observed in the metacontrast
masking paradigm. However, this extra process need not be an extra information processing channel. Our
best-fitting model was a hierarchical, single-channel model where noisy perceptual signals accrued further
noise when used for rating confidence. This suggests that confidence decisions may be made by mechanisms downstream from perceptual decision mechanisms. 1. Kepecs, A., Uchida, N., Zariwala, H. A., &
Mainen, Z. F. (2008). Neural correlates, computation and behavioural impact of decision confidence. Nature, 455(7210), 227-31. 2. Lau, H. C., & Passingham, R. E. (2006). Relative blindsight in normal observers
and the neural correlate of visual consciousness. Proceedings of the National Academy of Sciences of the
United States of America, 103(49), 18763-8. 3. Galvin, S. J., Podd, J. V., Drga, V., & Whitmore, J. (2003).
Type 2 tasks in the theory of signal detectability: discrimination between correct and incorrect decisions.
Psychonomic Bulletin & Review, 10(4), 843-76.
doi:10.3389/conf.neuro.06.2009.03.335

64

COSYNE 09

I-18

I-18. Dynamical state spaces of cortical networks representing various horizontal connectivities
Nicole Voges1
Laurent Perrinet2
1
2

NICOLE . VOGES @ INCM . CNRS - MRS . FR
LAURENT. PERRINET @ INCM . CNRS - MRS . FR

INCM / CNRS – Univ. Aix-Marseille
INCM / CNRS – Univ. de la Méditerranée

Most studies of cortical network dynamics are either based on purely random wiring or neighborhood couplings, e.g., [Kumar, Schrader, Aertsen, Rotter, 2008, Neural Computation 20, 1–43]. Neuronal connections
in the cortex, however, show a complex spatial pattern composed of local and long-range connections, the
latter featuring a so-called patchy projection pattern, i.e., spatially clustered synapses [Binzegger, Douglas,
Martin, 2007, J. Neurosci. 27(45), 12242–12254]. The idea of our project is to provide and to analyze
probabilistic network models that more adequately represent horizontal connectivity in the cortex. In particular, we investigate the effect of specific projection patterns on the dynamical state space of cortical
networks. Assuming an enlarged spatial scale we employ a distance dependent connectivity that reflects
the geometry of dendrites and axons. We simulate the network dynamics using a neuronal network simulator NEST/PyNN. Our models are composed of conductance based integrate-and-fire neurons, representing
fast spiking inhibitory and regular spiking excitatory cells. In order to compare the dynamical state spaces
of previous studies with our network models we consider the following connectivity assumptions: purely
random or purely local couplings, a combination of local and distant synapses, and connectivity structures
with patchy projections. Similar to previous studies, we also find different dynamical states depending on
the input parameters: the external input rate and the numerical relation between excitatory and inhibitory
synaptic weights. These states, e.g., synchronous regular (SR) or asynchronous irregular (AI) firing, are
characterized by measures like the mean firing rate, the correlation coefficient, the coefficient of variation
and so forth. On top of identified biologically realistic background states (AI), stimuli are applied in order
to analyze their stability. Comparing the results of our different network models we find that the parameter space necessary to describe all possible dynamical states of a network is much more concentrated
if local couplings are involved. The transition between different states is shifted (with respect to both input parameters) and sharpened in dependence of the relative amount of local couplings. Local couplings
strongly enhance the mean firing rate, and lead to smaller values of the correlation coefficient. In terms of
emergence of synchronous states, however, networks with local versus non-local or patchy versus random
remote connections exhibit a higher probability of synchronized spiking. Concerning stability, preliminary
results indicate that again networks with local or patchy connections show a higher probability of changing
from the AI to the SR state. We conclude that the combination of local and remote projections bears important consequences on the activity of network: The apparent differences we found for distinct connectivity
assumptions in the dynamical state spaces suggest that network dynamics strongly depend on the connectivity structure. This effect might be even stronger with respect to the spatio-temporal spread of signal
propagation. This work is supported by EC IP project FP6-015879 (FACETS).
doi:10.3389/conf.neuro.06.2009.03.207

COSYNE 09

65

I-19 – I-20

I-19. Typical behaviors in co-evolving recurrent network of oscillatory
neurons
Takaaki Aoki
Toshio Aoyagi

AOKI @ ACS . I . KYOTO - U. AC. JP
AOYAGI @ I . KYOTO - U. AC. JP

Kyoto Univesity
We investigate the typical behaviors that emerge in a co-evolving recurrent network in which neurons and
synaptic weights interact with each other. In general, the collective activities of neurons depend on the
network structure of the synaptic connections. The neural connectivity, however, is adaptively modulated by
plasticity in the synapses that depends on neural activity, and in turn the activity of neurons is affected by
the adaptive connectivity. This mutual co-evolution of the neurons and synapses is believed to be essential in providing a neuronal basis for higher brain functions such as learning and memory. Because of the
perceived importance of plasticity, the emergent behaviors under the synaptic plasticity have been studied
in several conditions by a number of researchers. In a neural network which has dense recurrent connections, however, it is difficult to understand how the co-evolving mechanism affects the collective behavior of
neurons, and it is still unclear what types of behavior the adaptive network can exhibit. To investigate the
fundamental properties possessed by the adaptive network, we present a simple mathematical model of
the co-evolving network in which oscillatory neurons are the network nodes. The plasticity is incorporated
by allowing the synaptic weights to develop in time according to the states of the pre- and post-synaptic
neurons. This model can be characterized by a few parameters. Specifically, when and particularly the
neurons are assumed to have the same firing rate, only two parameters are needed. As a result, we can
systematically investigate the possible behaviors of our model for all ranges of model parameters, and we
find three typical asymptotic behaviors, depending on the nature of the dynamics of the synaptic weights.
When the dynamics of the weights is qualitatively similar to the Hebbian rule, a two-cluster state emerges,
in which the activity of neurons converges to synchronized sub-groups. For another set of the model parameter values in which the synaptic dynamics become similar to spike-timing-dependent plasticity (temporal
asymmetric rule), a coherent state with a fixed spike pattern emerges. In contrast to the two-cluster state, a
sequential spike pattern of neurons is maintained with the synaptic connections organized through the coevolving dynamics. For yet another set of parameter values, a chaotic state is realized when the dynamics
of the synaptic weights and neurons are frustrated. In this case, the synaptic dynamics possesses an effect
opposite to that of a Hebbian-like rule. According to this anti-Hebbian-like rule, a reciprocal destabilization
of both the spike pattern and the network structure is observed. Moreover, we confirmed that Lyapunov
exponents take positive values, and then we refer to this as a chaotic state. In summary, we found that the
model of the co-evolving network under investigation possesses three distinct types of dynamical behaviors.
Because of its structural stability, our model will provide a framework for describing essential behaviors in
adaptive recurrent networks.
doi:10.3389/conf.neuro.06.2009.03.005

I-20. A constructive mean-field analysis of multi population neural
networks with random synaptic weights
Olivier Faugeras1,2
Jonathan Touboul3
Bruno Cessac
1
2

66

OLIVIER . FAUGERAS @ SOPHIA . INRIA . FR
JONATHAN . TOUBOUL @ SOPHIA . INRIA . FR
BRUNO. CESSAC @ SOPHIA . INRIA . FR

INRIA
ENS

COSYNE 09

I-21
3

INRIA Sophia Antipolis

We deal with the problem of bridging the gap between two scales in neuronal modeling. At the first (microscopic) scale, neurons are considered individually and their behavior described by stochastic differential
equations that govern the time variations of their membrane potentials. They are coupled by synaptic connections acting on their resulting activity, a nonlinear function of their membrane potential. At the second
(mesoscopic) scale, interacting populations of neurons are described individually by similar equations. The
equations describing the dynamical and the stationary mean-field behaviors are considered as functional
equations on a set of stochastic processes. Using this new point of view allows us to prove that these
equations are well-posed on any finite time interval and to provide, by a fixed point method, a constructive
method for effectively computing their unique solution. This method is proved to converge to the unique
solution and we characterize its complexity and convergence rate. We also provide partial results for the
stationary problem on infinite time intervals. These results shed some new light on such neural mass models as the one of Jansen and Rit: their dynamics appears as a coarse approximation of the much richer
dynamics that emerges from our analysis. Our numerical experiments confirm that the framework we propose and the numerical methods we derive from it provide a new and powerful tool for the exploration of
neural behaviors at different scales.
doi:10.3389/conf.neuro.06.2009.03.025

I-21. Insights into Parkinson’s disease from mean-field modeling of
brain electrical activity
Sacha van Albada1
Richard Gray1
Peter Drysdale1,2
Peter Robinson
1
2

ALBADA @ PHYSICS . USYD. EDU. AU
R . GRAY @ PHYSICS . USYD. EDU. AU
P. DRYSDALE @ PHYSICS . USYD. EDU. AU
ROBINSON @ PHYSICS . USYD. EDU. AU

The University of Sydney
Brain Dynamics Centre, Westmead

Parkinson’s disease is characterized by the gradual degeneration of the substantia nigra pars compacta
and associated areas, which decreases dopaminergic input mainly to the striatum. In addition to cognitive
and mood disturbances, this results in motor symptoms such as difficulty initiating movements, slowness
of movement, and resting tremor of the extremities. Electrophysiological correlates of Parkinson’s disease
include altered average neuronal firing rates and responses to transient stimuli, and synchronized oscillations around 5 Hz and 20 Hz in the basal ganglia, thalamus, and cortex. Furthermore, the alpha peak
in electroencephalographic (EEG) spectra shifts to lower frequencies, and relative low-frequency power
increases. There is a wealth of empirical data on neuronal activity in Parkinson’s disease, whereas quantitative models are relatively sparse. Existing models have not allowed a consensus to be reached on the
substrates and mechanisms responsible for parkinsonian oscillations and changes in average firing rates.
Moreover, as far as we are aware, no previous quantitative model has addressed changes in EEG spectra
in parkinsonian patients. We present a physiologically-based model of the basal ganglia-thalamocortical
system, which describes fluctuating average firing rates within each component. Key connections between
cortex, thalamus, and basal ganglia are taken into account, including direct and indirect pathways through
two populations of striatal neurons, primarily expressing either the D1 or D2 type of dopamine receptor,
and a hyperdirect pathway from cortex to the subthalamic nucleus (STN). Based on empirical findings, we
consider various possible effects of dopamine loss. Of these, two are mutually exclusive: (i) decreased
corticostriatal connection strengths and striatal firing thresholds to approximate a reduced signal-to-noise
ratio in the striatum; and (ii) increased cortico-D2 and decreased cortico-D1 connection strengths, repre-

COSYNE 09

67

I-22
senting differential modulation of the direct and indirect pathways. Either (i) or (ii) can be combined with (iii)
weakened lateral inhibition in the external pallidum (GPe); (iv) weaker intracortical excitation and especially
inhibition resulting from mesocortical dopamine loss; and/or (v) reduced GPe and STN firing thresholds, and
a stronger D2-GPe projection. We find that a combination of options (ii)-(v) produces changes in average
firing rates and responses to transient stimuli that accord with experiments, and increased synchronization throughout the basal ganglia-thalamocortical system. Furthermore, the strengthened indirect pathway
leads to enhanced oscillations around 5 Hz. Particularly strong subcircuits may sustain limit cycles, for
which phase relationships between nuclei are consistent with empirical observations. Approximately 20 Hz
oscillations are found to have a corticothalamic origin, but can spread to the basal ganglia when the indirect pathway becomes strong. Changes in model EEG spectra corresponding to the combination of (ii)-(v)
agree with empirical results, including increased relative theta power, decreased relative alpha power, and
lower dominant frequencies in the alpha range. The fact that a single set of parameter changes around the
healthy state produces all the above electrophysiological effects suggests that our model captures many
features of the parkinsonian brain in a realistic manner.
doi:10.3389/conf.neuro.06.2009.03.040

I-22. Neural correlations in a heterogeneous network model dominated by recurrent inhibition
Alberto Bernacchia
Xiao-Jing Wang

ALBERTO. BERNACCHIA @ YALE . EDU
XJWANG @ YALE . EDU

Yale University
Simultaneous recordings of the activity of multiple cortical neurons in vivo have revealed pairwise correlations with a fairly wide distribution and a small positive average. This positive correlation is believed to be
due to common excitatory inputs shared by nearby neurons, either projections from a separate cortical area
or local recurrent connections. Beyond this simple intuition, a quantitative understanding of the mechanisms
regulating the amount of correlations between cortical cells is still lacking. Even in simple cortical network
models, it is generally difficult to study correlations analytically and to make precise statements and predictions about their behavior. Previous models have been often designed with a highly sparse connectivity,
implying that different neurons share only a negligible fraction of their inputs. Alternatively, the opposite
case has been considered in which the network is fully connected and homogeneous, and all the neurons
receive the same input. In the former case, correlations between all neuron pairs are nearly zero, while in
the latter the distribution of correlations is narrow, at odds with the fairly wide distribution of correlations observed in physiological experiments. What are the network properties that determine neural correlations? In
the present study, we consider a model network driven by a strong and fluctuating input, and dominated by
recurrent inhibition. The output of neurons is described by their firing rate, and the synapses between neurons are highly heterogeneous: each synaptic strength is independent and normally distributed with a given
mean and variance. The random and “quenched” nature of the synaptic matrix poses a serious challenge
for theoretical analysis, because the expectation values of infinite series of products of that matrix must be
calculated. Using a novel approximation, we were able to solve this problem and to calculate analytically
various statistical quantities, including mean neural activities, pairwise correlations and the width of their
distributions. The main results are the following: 1) Local recurrent inhibition allows the network to display
a relatively small (nonzero) and noisy activity, mimicking the cortical spontaneous activity. 2) Heterogeneity
of synaptic strengths determines a wide distribution of activities among different neurons. 3) The mean
correlation between pairs of neurons decreases with the number of neurons, provided that the recurrent
connections are dominated by inhibition. 4) The width of the distribution of correlations also decreases with
the number of neurons and with increasing inhibition, but it reaches a finite nonzero value. Heterogeneity of

68

COSYNE 09

I-23 – I-24
synaptic strengths also plays an important role by decreasing the mean correlation but increasing the width
of its distribution. Besides the role played by local recurrent inhibition in balancing the activity of a cortical
module, that has been highlighted in numerous previous studies, our results suggest a new mechanism by
which inhibition reduces pairwise correlations between neurons. If our analysis could be generalized, as we
believe, to more complex and realistic architectures, including spiking neurons and synaptic filtering, then it
will provide a new framework for the interpretation and prediction of experimental physiological results.
doi:10.3389/conf.neuro.06.2009.03.069

I-23. Generalized Wilson-Cowan rate equations for correlated activity
in neural networks.
Michael Buice1
Jack Cowan2
Carson Chow
1
2

MABUICE @ GMAIL . COM
COWAN @ MATH . UCHICAGO. EDU
CARSONC @ MAIL . NIH . GOV

National Institutes of Health
University of Chicago

The Wilson-Cowan equations are the foundation of a common approach to modeling for neural networks.
These equations provide dynamics for the firing rate of neurons within a network given some connectivity. The shortcoming of these equations is that they take into account only the average firing rate while
leaving out higher order statistics like correlations between firing. Recently, Buice and Cowan formulated
a stochastic theory of neural networks which includes statistics at all orders. We describe how this theory
yields a systematic extension to the Wilson-Cowan equations by introducing equations for correlations and
appropriate coupling terms. Each level of the approximation yields closed equations, i.e. they depend only
upon the mean and specific correlations of interest, without an ad hoc criterion for doing so. In addition
to presenting the evolution equations for the mean activity and correlations, we solve them for the simple
case of a homogeneous all-to-all connected network and compare the results to simulations of a network
of integrate-and-fire neurons.
doi:10.3389/conf.neuro.06.2009.03.078

I-24. A role for symmetric head-angular-velocity cells: Tuning the
head-direction network.
Peter Stratton1,2
Gordon Wyeth
Janet Wiles1
1
2

STRATTON @ ITEE . UQ . EDU. AU
WYETH @ ITEE . UQ . EDU. AU
J. WILES @ ITEE . UQ . EDU. AU

The University of Queensland
Queensland Brain Institute and School of ITEE

Computational models of the head direction (HD) system of the rat usually assume that the connections
that maintain HD neuron activity are pre-wired and static. Ongoing activity in these models relies on precise
attractor dynamics. It is currently unknown how such connections could be so precisely wired, and how
accurate calibration is maintained in the face of ongoing noise and perturbation. A model of the HD system
that uses symmetric head-angular-velocity (HAV) cells as a training signal shows that the HD system can

COSYNE 09

69

I-25
learn to support stable firing patterns from poorly-performing, unstable starting conditions. The proposed
calibration mechanism explains why symmetric HAV cells in the rat outnumber their asymmetric counterparts. The mechanism also conjectures that the efficacy of one synapse onto a postsynaptic cell can be
controlled in part by activity received by that same cell on another synapse. If its existence in biological
networks is confirmed, this mechanism will add significantly to our understanding of synaptic plasticity.
doi:10.3389/conf.neuro.06.2009.03.300

I-25. Improved (I)CA-noise elimination of electrophysiological data
using band-pass filtered components
Kai Görgen1,2
Conrado Bosman2
Thilo Womelsdorf2,3
Robert Oostenveld2,3
Pascal Fries2

KAI . GOERGEN @ BCCN - BERLIN . DE
CONRADO. BOSMAN @ FCDONDERS . RU. NL
T. WOMELSDORF @ FCDONDERS . RU. NL
R . OOSTENVELD @ FCDONDERS . RU. NL
PASCAL . FRIES @ FCDONDERS . RU. NL

1

BCCN Berlin
Donders Inst for Brain, Cognition & Behavior
3
Radboud University Nijmegen
2

Removal of signal components unrelated to the signals of interest, so called “noise”, is a general problem
in many types of data. One method for identifying and removing noise (e.g. eye movement artifacts or
power-line noise) from electrophysiological data is independent component analysis (ICA) (e.g. Jung et al.,
1998). A shortcoming of this method is that it does not guarantee a complete separation of physiological
signals and noise sources into different components. Thus, those components containing obvious noise
contributions might also contain physiological data. This phenomenon is called “leakage”. Rejecting these
components might therefore not only clean the noise but may additionally and erroneously take out parts of
the physiological data. Here, we suggest that this leakage can be reduced using prior knowledge about the
spectral features of the noise. After identification of the noise components, we applied band-pass filtering to
these components at the frequencies of the noise before subtracting the back-projected components from
the data. This effectively prevents deletion of physiological signals in frequency bands beyond the a priori
specified noise frequency bands. To test this method, we applied it to simulated data and physiological
signals obtained from ECoG recordings in macaque monkey. Our results demonstrate that this method (a)
is as efficient as the standard ICA correction, (b) also works for data available only as short time segments
(“trials”), (c) results in a clearer noise time-course allowing the subtraction of less non-noise activity, and
(d) does not – in contrast to the standard unfiltered ICA method – change the power spectra of the cleared
signals in spectral regions outside the noise regions. Additionally, we present a simple and computationally
efficient method to find noise components with sharp frequency tunings (e.g. line noise), which can be used
independently of or prior to ICA to improve its performance. To achieve this, we band-pass filter the original signal prior to component identification (which can consequently be done using standard component
analysis techniques, e.g. PCA/ICA). We thereby reduce the influence of components whose main contribution lies in other frequency bands, thereby rendering the noise components more salient to the component
identification algorithm. This method is especially useful when no clear noise-components can be detected
using standard ICA on the unfiltered data. In summary, we show that ICA noise elimination is improved using prior knowledge about the spectral features of the noise, ultimately enhancing the signal-to-noise ratio
of neurophysiological data recordings.
doi:10.3389/conf.neuro.06.2009.03.257

70

COSYNE 09

I-26 – I-27

I-26. State Dependent Frequency Modulation of Hippocampal Theta
Activity
David Nguyen1,2
Matthew Wilson
Emery Brown1
Riccardo Barbieri1,3

DPNGUYEN @ MIT. EDU
MWILSON @ MIT. EDU
ENB @ NEUROSTAT. MIT. EDU
BARBIERI @ NEUROSTAT. MIT. EDU

1

Massachusetts General Hospital
Massachusetts Institute of Technology
3
Harvard Medical School
2

Functional information may be contained in the small timescale variations of local field potentials (LFPs),
however, commonly used spectral methods do not consider this possibility. Additional insight into the functional correlates of LFP activity may be obtained by examining how the frequency of brain rhythms are
modulated within sub-second timescales. Although it is a seemingly basic problem, estimation of frequency
modulation (FM) is a non-trivial task that requires fine resolution in both the temporal and spectral domains.
State-space methods represent a class of solutions that are known for their ability to model non-stationary
data. In particular, the autoregressive Kalman filter (AR-KF) framework is well-known for its ability to describe noisy and non-stationary sinusoids over continuous parameter spaces. The main issue with AR-KF
based algorithms, however, is the adaptation of model parameters are based on the ability to reconstruct the
time-domain signal rather than estimate the frequency domain components. In our application of the AR-KF
framework to simulated noisy and harmonic data, we used a frequency domain residual that compared the
true center oscillation frequency with the estimated center oscillation frequency over time, rather than scoring performance using a time-domain residual. Based on these analyses, we identified several issues that
would likely lead to erroneous estimates of instantaneous frequency and FM. These issues are particularly
important if the goal is to obtain reliable, single trial neural correlates of function based on LFP activity. We
have developed methods for overcoming these issues as well as a novel method for KF variance estimation
based on frequency domain measures. We have applied our modified AR-KF method to the analysis of LFP
activity recorded from the rodent hippocampus during behavior on a linear track. In contrast to previous reports, we found that the theta rhythm is indeed modulated according to running speed. In addition, we find
that theta frequency modulation and theta amplitude modulation can occur independently of each other.
These results suggest that our methodology may be used to extract additional bits of information from LFP
data for the purposes of encoding and decoding behavorial and/or cognitive state on a single-trial basis.
doi:10.3389/conf.neuro.06.2009.03.110

I-27. Complex Bayesian Inference in Neural Circuits using Divisive
Normalization
Jeffrey Beck
Peter Latham
Alexandre Pouget

JEFFBECK @ GATSBY. UCL . AC. UK
PEL @ GATSBY. UCL . AC. UK
ALEX @ BCS . ROCHESTER . EDU

Gatsby Computational Neuroscience Unit, UCL
A very wide range of computations performed by the nervous system involves a type of probabilistic inference known as marginalization. The goal of marginalization is to recover a distribution, p(x), over a variable
x, given a joint distribution over several variables, say x, y, and z, p(x,y,z). This operation comes up in
seemingly unrelated computations such as causal reasoning, odor recognition, motor control, visual track-

COSYNE 09

71

I-28
ing, coordinate transforms, visual search, decision making, and object recognition, to name just a few. In
all cases, marginalization is required to compute the marginal probability, of any particular cause, p(x—o),
given a set of observations (o) based on knowledge of the joint distribution, p(x,y,z—o), over all causes given
the observations (e.g. what is the probability of smelling coffee given a set of odorants and knowledge of the
joint distribution over all possible smells—such as coffee, orange juice, bacon, etc— given the odorants).
The question we address here is: how do neural circuits implement such marginalizations? The answer
depends on how neurons represent probability distributions. Given the Poisson-like statistics of spike trains,
we have recently argued that neurons use what we call linear probabilistic population codes. These codes
have the advantage of reducing probabilistic inference such as evidence integration or maximum likelihood
estimation to simple linear operations over neural activity. This greatly simplifies both learning and computations in tasks such as multisensory integration, action selection, and accumulation of evidence over
time in decision making. Given these computational properties, it would seem important to understand
how networks could both perform marginalization and keep the marginal distribution encoded with a linear
probabilistic population codes. When all distributions are Gaussian, we can show analytically that perfect
marginalization can be achieved with a type of lateral inhibition known as divisive normalization. Moreover,
the same nonlinearity works for marginalization over time (as in a Kalman filter) and provides a near optimal
solution for inference over discrete classes. We tested our analytical result with simulations of networks of
spiking neurons using four types of computations: coordinate transforms for sensory motor transformation,
Kalman filters for motor control, explaining away in infants (a.k.a backward masking) and odor recognition.
We show that in all cases the use of divisive normalization can lead to near perfect performance, i.e., in
all cases the network recovers a posterior distribution encoded by a linear probabilistic population code
with only negligible information (Shannon) loss. By contrast, networks which cannot implement either the
quadratic non-linearities or the divisive normalization operation perform quite poorly and/or do not yield
linear probabilistic population codes. This is a particularly intriguing result because divisive normalization
has been reported in numerous neural circuits, from insects to mammals. This normalization had been
implicated in gain control, attention and redundancy reduction; our results suggest a much wider role, as a
general solution to marginalization with probabilistic population codes.
doi:10.3389/conf.neuro.06.2009.03.109

I-28. Neural model of action-selective neurons in STS and area F5
Martin Giese1,2
Antonino Casile
Falk Fleischer1,2
1
2

MARTIN . GIESE @ TUEBINGEN . MPG . DE
ANTONINO. CASILE @ UNI - TUEBINGEN . DE
FALK . FLEISCHER @ MEDIZIN . UNI - TUEBINGEN . DE

Hertie Institute for Clinical Brain Research
Center for Integrative Neuroscience

The visual recognition of goal-directed movements is crucial for the understanding of intentions and goals
of others as well as for imitation learning. So far, it is largely unknown how visual information about effectors
and goal objects of actions is integrated in the brain. Specifically, it is unclear whether a robust recognition
of goal-directed actions can be accomplished by purely visual processing or if it requires a reconstruction of the three-dimensional structure of object and effector geometry. We present a neurophysiologically
inspired model for the recognition of goal-directed grasping movements from real video sequences. The
model integrates several physiologically plausible mechanisms in order to realize the integration of information about goal objects and the effector and its movement: (1) A hierarchical neural architecture for the
recognition of hand and object shapes, which realizes position and scale-invariant recognition by subsequent increase of feature complexity and invariance along the hierarchy based on learned example views
[1,2,3]. However, in contrast to standard models for visual object recognition this invariance is incomplete,
so that the retinal positions of goal object and effector can be extracted by population codes. (2) Simple

72

COSYNE 09

I-29
recurrent neural circuits for the realization of temporal sequence selectivity [4,5,6]. (3) A novel mechanism
combines information about object shape and affordance and about effector (hand) posture and position
in an object-centered frame of reference. This mechanism exploits gain fields in order to implement the
relevant coordinate transformation [7,8]. The model shows that a robust integration of effector and object
information can be accomplished by well-established physiologically plausible principles. Specifically, the
proposed model does not contain explicit 3D representations of objects and the effector movement. Instead, it realizes predictions over time based on learned view-dependent representation of the visual input.
Our results complement those of existing models of action recognition [9, 10] and motivate a more detailed
analysis of the complementary contributions of visual pattern analysis and motor representations on the
visual recognition of imitable actions. Acknowledgments Supported by DFG (SFB 550), EC, and Hermann
und Lilly Schilling Foundation. References [1] Riesenhuber, M. and Poggio, T. (1999): Nat. Neurosci. 2,
1019-1025. [2] Giese, A.M. and Poggio, T. (2003): Nat. Rev. Neurosci. 4, 179-192. [3] Serre, T. et al.
(2007): IEEE Pattern Anal. Mach. Int. 29, 411-426. [4] Zhang, K. (1996): J. Neurosci. 16, 2112-2126.
[5] Hopfield, J. and Brody, D. (2000): Proc Natl Acad Sci USA 97, 13919-13924. [6] Xie, X. and Giese,
M. (2002): Phys Rev E Stat Nonlin Soft Matter Phys 65, 051904. [7] Salinas, E. and Abbott, L. (1995): J.
Neurosci. 75, 6461-6474. [8] Pouget, A. and Sejnowski, T. (1997): J. Cogn. Neurosci. 9, 222-237. [9]
Oztop, E. et al. (2006): Neural Netw. 19, 254-271. [10] Prevete, E. et al. (2008) Brain Res. 1225, 133-45.
doi:10.3389/conf.neuro.06.2009.03.136

I-29. Deciding without remembering
Surya Ganguli1
Robert Guetig2,3
Haim Sompolinsky3,4

SURYA @ PHY. UCSF. EDU
GUETIG @ CC. HUJI . AC. IL
HAIM @ FIZ . HUJI . AC. IL

1

UCSF
Racah Institute of Pysics
3
Center for Neural Computation
4
Center for Brain Science
2

In many decision making processes [1,2], evidence for and against multiple choices is distributed across
time, imposing stringent memory requirements [3] on any neural circuitry that optimally implements such
decisions. Furthermore, in many sensory circuits [4,5,6], stimulus information is stored in extended spatiotemporal spike patterns, requiring perceptual decision making circuits to integrate and remember not
only spike counts, but also spike times. How well can neural circuits with limited memory make decisions
about stimuli encoded in temporally extended, multineuronal spike time patterns? What are the limits of
performance under a memory constraint, and what kinds of neuronal computations can achieve these performance limits? We address these questions by developing a new statistical decision framework, which
we term “forgetful” ideal observer models. These models perform optimal decision making given a limited
memory of incoming spike times across a pool of sensory neurons. The simplest such model has access to
only the most recent spike times within a short temporal window. We analyze the performance of this model
in a sensory discrimination task in which two stimuli are encoded in two different, noisy spike time patterns which last much longer than the temporal window and are impossible to discriminate using only spike
counts. The forgetful ideal observer computes a running log likelihood ratio of the most recent spike times
and compares this to an optimized threshold. We find that performance degrades as the integration window
shortens, but more slowly than naively expected. Even though the integration time decreases, leading to a
smaller SNR in the log likelihood ratio between the two stimuli at any given time, the number of independent
samples of this ratio increases, partially compensating for the reduced memory. For example, we find that
forgetful ideal observers with a window of 30ms can, remarkably, discriminate between patterns lasting 1
second with spike timing jitters as large as 300ms. We further analyze forgetful observer models that only

COSYNE 09

73

I-30
have access to the relative timing between incoming spikes, and elapsed time relative to any given spike,
again within a short temporal window. The performance of such models, although degraded, is comparable to that of the forgetful ideal observer despite the restriction to relative timing information. We analyze
the computations performed by these models and compare them to that of biologically plausible neuronal
models for spike timing computation, including the tempotron model which has shown prior success in discriminating temporally extended spike time patterns [7]. Finally, in addition to performance, forgetful ideal
observer models make specific predictions about reaction time distributions, and we test these predictions
on neural and behavioral data from the olfactory system of rodents trained to perform odor discrimination
tasks [2]. [1] J.Roitman, M.Shadlen, J.Neurosci 2002. [2] N.Uchida, Z.Mainen, Nat.Neurosci 2003. [3]
S.Ganguli, D.Huh, H.Sompolinsky, PNAS 2008. [4] T.Gollish, M.Meister, Science 2008. [5] R.S. Johansson, I. Birznieks, Nat.Neurosci 2004. [6] O.Mazor, G.Laurent, Neuron 2005. [7] R.Guetig, H.Sompolinsky,
Nat.Neuroci 2006.
doi:10.3389/conf.neuro.06.2009.03.168

I-30. Anthropic correction for mutual information and its application
to neural redundancy estimations.
Frédéric Theunissen1
Michael Gastpar
Patrick Gill2
Sarah Munro3

THEUNISSEN @ BERKELEY. EDU
GASTPAR @ EECS . BERKELEY. EDU
PRG 56@ CORNELL . EDU
SARAH . MUNRO @ BERKELEY. EDU

1

University of California, Berkeley
Cornell University
3
Helen Wills Neuroscience Inst., UC Berkeley
2

In sensory systems, neural discrimination measures the ability of single or ensemble of neurons to classify
stimulus features. Measuring neural discrimination is useful because it provides a model free assessment
of the neural coding occurring in a given processing stage. Neural discrimination measures can thus be
used to investigate the nature of code (eg. rate vs temporal code), to validate models (eg. linear vs. nonlinear) and to assess the relevant stimulus space (eg. natural vs synthetic; phoneme vs. syllable). Neural
discrimination for ensembles of neurons can also be compared to neural discrimination for single neurons
thus measuring the degree of redundancy or synergy in ensemble codes. Finally, neural discrimination can
be assessed in different processing stages to investigate changes in the neural representation. In a previous
study, we showed the advantages of estimating mutual information (MI) from the confusion matrix obtained
from ideal observer decoding of neural spike trains. We will call these estimates the IOMI estimates (Ideal
Observer Mutual Information estimates). The two principal advantages of the IOMI estimates are that:
first, they gave maximum values at time resolutions in the decoding algorithm that corresponded to those
found from a direct estimate of the MI, suggesting that the Ideal Observer method is able to capture the
information in the temporal patterns of spike trains; second, the IOMI estimates could easily be computed
for an ensemble of neurons with little additional computational cost. This second advantage makes is
a appealing approach for using MI to estimate neural redundancy. However, the use of IOMI does not
completely resolve the data limitation problem (there is no free lunch). A specific issue concerns the fact that
the IOMI estimate is upper bounded by the logarithm of the number of different stimuli that were used in the
experiment (unless stationarity assumptions are made). This can be a severe limitation particularly when
populations of many neurons are considered because IOMI measures will saturate to this upper bound. To
address this issue, we proposed a corrected estimate that is inspired from the weak anthropic principle: this
principle states that one must take into account our special place on earth when making observations about
the universe by effectively removing earth from the analysis. For our mutual information correction, we show

74

COSYNE 09

I-31
that one can apply a similar principle by replacing the MI by the KL-divergence between the distribution of
response (predicted stimulus in this case) given a stimulus and the distribution of responses given all other
stimuli except the one used in the conditional probability. We show that the anthropic correction of the MI is
not bounded and that it gives absolute values of MI that are more in line with the direct estimates. Finally,
we apply this approach to estimate neural redundancy measures in the auditory midbrain, thalamus and
forebrain of songbirds.
doi:10.3389/conf.neuro.06.2009.03.212

I-31. Separation of single neurons from optical recordings in Tritonia
diomedea using ICA
Caroline Moore-Kochlacs1
Evan Hill2
William N Frost
Jean Wang
Terrence Sejnowski
1
2

CAROLMK @ SALK . EDU
EVAN . HILL @ ROSALINDFRANKLIN . EDU
WILLIAM . FROST @ ROSALINDFRANKLIN . EDU
JEAN . WANG @ ROSALINDFRANKLIN . EDU
TERRY @ SALK . EDU

Salk Institute for Biological Studies
Rosalind Franklin University

Optical recordings obtained with photodiode arrays and voltage-sensitive dyes allow firing of large numbers of neurons to be recorded during behaviorally-relevant motor programs. Although this technique has
tremendous potential for network studies, the resulting datasets can be challenging to interpret; one detector may record multiple neurons and one neuron may appear on many detectors. Earlier studies applied
independent component analysis (ICA), a blind source separation technique, to optical traces from Tritonia
diomedea’s escape swim behavior, to recover components from individual neurons [1] and identify unreported neurons involved in the behavior [2]. Using intracellular and optical methods [3], we evaluate the
accuracy of neuronal activity that ICA returns and demonstrate an application of these methods, measuring
the change in individual neurons’ activity following stimulus adaptation. To evaluate accuracy, we recorded
from neurons in Tritonia with intracellular electrodes while imaging with a 464-element photodiode array using the fast voltage-sensitive absorbance dye RH-155. ICA was run on the optical traces, transforming them
into statistically independent components. ICA returned one component for neurons detected by multiple
diodes and separated neurons mixed on a single diode. Intracellular recordings confirmed the accuracy of
this technique. For each intracellular recording, we found a component that corresponded exactly. Also,
the location returned by ICA matched that of the electrode. Additionally, these methods allow us to drive
an identified neuron and image its functional connectivity to large numbers of other individual neurons,
before and after a treatment of interest. We imaged the responses of several pedal ganglion neurons to
a test train of action potentials in CPG neuron C2 driven with an intracellular electrode. Then we applied
a sensitizing nerve shock stimulus. Two minutes later we imaged the responses of the same neurons to
a second test train. Combining the records and applying ICA, we observed how responses of individual
neurons changed after the sensitizing stimulus. Optical recordings in combination with ICA allow quick
identification of functional synaptic connections onto previously unknown neurons. Using location maps
from ICA, those neurons can be penetrated with intracellular electrodes within the same experiment, enabling new experiments to probe the Tritonia brain over a wide range of conditions and time scales. Given
our intracellular validation of ICA’s unmixing, these methods promise to be a powerful tool for discovery of
unreported neurons and studies of functional connectivity and network plasticity. Supported by the Dart
Foundation, Snite Foundation, RFUMS, and HHMI [1] Brown GD, Yamada S, Sejnowski TJ. (2001) Independent components analysis at the neural cocktail party. Trends Neurosci., 24. [2] Brown GD, Yamada
S, Nakashima M, Moore-Kochlacs C, Sejnowski TJ, Shiono S. (2008) Independent Component Analysis of

COSYNE 09

75

I-32
Optical Recordings from Tritonia Swimming Neurons. INC-08-001-tritonia, Institute for Neural Computation
Technical Report, UCSD. [3] Frost WN, Wang J, and Brandon CJ. (2007) A stereo-compound hybrid microscope for combined intracellular and optical recording of invertebrate neural network activity. J. Neurosci.
Methods. 162:148-154.
doi:10.3389/conf.neuro.06.2009.03.231

I-32. Towards linking blood flow and neural activity : Petri net-based
energetics model for neurons
Venkateswaran Nagarajan
Ashutosh Mohan

WARAN @ WARFTINDIA . ORG
ASHUTOSH @ WARFTINDIA . ORG

WAran Research FoundaTion
The MMINi-DASS project at WARFT[1] is towards predicting an interconnectivity structure against a functional specification. fMRI is an important form of this specification and hence a computational method that
links neuronal energetics with neuronal signalling is required. Our initial approach[1] was to model cerebral
blood flow and link it with neuronal activity by conventional methods.Currently the main approach is to measure fMRI response and neuronal dynamics to arrive at a relationship between the two[2,3]. The authors
believe that while such experimental methods are most valuable, a computational approach to solve this
problem is a pressing need. Thus, we propose an Petri Net-based Energetics model for single-neurons in
which both signaling and metabolism are considered. This is in contrast to current models which consider
only signalling. The importance of metabolism is largely overlooked. A robust computational model to study
both signaling and metabolism in a unified manner is required. Petri nets consist of places(states), transitions and arcs connecting a place and a transition. Thus, it is a natural model to represent the complex
chemical reactions of the neuron that characterize its energetics. Glycolysis, Kreb’s cycle, the electron
transport system and distribution of ATP to various cellular and signaling processes are modeled as a
stochastic petri nets with tokens. Specifying all probabilistic state transitions is difficult due to a lack of
experimental data. Thus, an optimization technique is used to fix up various transition values within their
biologically realistic limits. For effective optimization when the number of parameters are large, a methodology based on Simulated Annealing and Game Theory has been developed at WARFT. This single-neuron
model can then be used to perform fault simulation or study effects of various specific metabolic processes
on neuronal signalling. While drawing significantly from experimental data for computational research, it
will also drive experimental research. This approach essentially provides only a framework using which
we can rigorously approach problems relating to the link between blood flow and neural dynamics. Current work at WARFT is towards developing energetics-based soma and synapse models, fixing up petri-net
transition probabilities using available data[4] and verifying their biological realism Interconnectivity prediction against a functional specification like fMRI, electrophysiological recordings etc is a first of its kind
effort undertaken at WARFT. To accomplish this, a novel approach of energetics-based neuron models is
required. This requires a global effort and significant synergy between computational and experimental
research. REFERENCES: 1.Nagarajan V, Srinivasan K, Mohan A, Daniel V, R V, Chandran H and J V
(2008). MMINi-DASS - Large-scale brain circuit construction and simulation for interconnectivity prediction.
Frontiers in Neuroinformatics. Conference Abstract: Neuroinformatics 2008 2.David J. Heeger and David
Ress, “What does fMRI tell us about neuronal activity?,” Nature Reviews Neuroscience 3, no. 2 (February
2002): 142-151, doi:10.1038/nrn730. 3.John Martindale et al., “The Hemodynamic Impulse Response to a
Single Neural Event,” J Cereb Blood Flow Metab 23, no. 5 (May 2003): 546-555. 4.RobertG. Shulman and
Douglas L. Rothman, Brain Energetics and Neuronal Activity: Applications to fMRI and Medicine, 1st ed.
(Wiley, 2004).
doi:10.3389/conf.neuro.06.2009.03.337

76

COSYNE 09

I-33 – I-34

I-33. Task-driven Saliency Using Natural Statistics (SUN)
Matthew Tong
Christopher Kanan
Lingyun Zhang
Garrison Cottrell

MHTONG @ CS . UCSD. EDU
CKANAN @ CS . UCSD. EDU
LINGYUN @ CS . UCSD. EDU
GARY @ CS . UCSD. EDU

University of California, San Diego
One important task of the visual attention system is to focus attentional resources on important objects
in a scene. Starting with a probabilistic definition of this goal, we derive a salience model that directs
attention to locations likely to contain objects of interest. The Saliency Using Natural Statistics model (SUN)
utilizes three kinds of statistical knowledge about the world in choosing which areas of a scene should be
fixated: what features are rare, the visual appearance of particular objects of interest, and the locations in
a scene likely to contain such objects. The components that emerge all have been proposed individually
before. Novelty of features has been argued to attract attention (see Wolfe, 2001 for a discussion). SUN’s
appearance model is reminiscent of Guided Search (Wolfe, 1994) and Iconic Search (Rao et al., 1995) and
location-based guidance has also been argued to play an important role (e.g. Turano, 2003). Unlike other
models of saliency, SUN learns its statistics from natural image statistics in advance from a collection of
images of natural scenes. Previous work with SUN defined the self-information of features (their novelty)
as a form of bottom-up, task-independent saliency and demonstrated its state-of-the-art ability to predict
human fixations when free viewing images (Zhang et al., in press) and video. SUN’s use of natural statistics
learned through experience also explains search asymmetries that models driven solely by the statistics of
the current image cannot. However, when viewing the world we do so with a purpose, and understanding
the role of the current task is essential. Here we go beyond the previous work, implementing the remaining
portions of SUN and applying the complete model to a real world task. Torralba and colleagues collected
data from subjects counting people, paintings, and cups in indoor and outdoor scenes and assess the
performance of their contextual guidance model on this data (2006). We compare SUN’s performance
with contextual guidance, finding that SUN’s use of learned statistics matches or exceeds the performance
provided by contextual guidance. This comparison is of particular interest as the two probabilistic models
share many surface similarities, with contextual guidance being strongly influenced by the statistics of the
current scene and SUN relying more heavily on previous experience. The results support the claim that
learned knowledge of the rareness of features, object appearance, and object location play a key role in
determining where the eyes look.
doi:10.3389/conf.neuro.06.2009.03.334

I-34. Predictors of successful memory encoding in the human hippocampus and amygdala
Ueli Rutishauser1
Adam N. Mamelak2
Ian B. Ross3
Erin Schuman1

URUT @ CALTECH . EDU
ADAM . MAMELAK @ CSHS . ORG
IANROSSMD @ AOL . COM
SCHUMANE @ CALTECH . EDU

1

California Institute of Technology
Cedars-Sinai Medical Center
3
Huntington Memorial Hospital
2

In daily life, humans witness a large number of events. We remember some of these past events in great

COSYNE 09

77

I-35
detail whereas others are forgotten within minutes. What determines what we remember and what we
forget? Behaviorally, we tend to remember best those events that were high in salience- that is, rewarding,
threatening, surprising or novel. At the level of single neurons, however, it is unclear what sequence of
events leads to the storage of a new memory. In particular it is unclear whether mechanisms that induce
synaptic plasticity (e.g. LTP/LTD) in vitro and in vivo are utilized at the level of populations of neurons to
encode a new memory. Whether a sequence of action potentials induces plasticity depends on when the
pre-synaptic neuron fires relative to the activity of its neighbors. In the hippocampus, stimulation leads to
LTP only if stimulated at the peak of the theta (4-8 Hz) oscillations. Whether this mechanism is also at work
for natural stimuli is unknown. We recorded extracellular neural activity from the MTL of epileptic patients,
who had electrodes implanted for the purpose of localizing seizures. We simultaneously recorded singleunit activity and the local field potential (LFP) from the amygdala and hippocampus. Patients (n=5) viewed
a sequence of unique images. In a later recognition memory test, patients indicated whether they had seen
the image before or not on a 6-point confidence scale. Patients had good memory (average d’=1.22) and a
good sense of confidence. Here, we focus on neural activity during learning. First, we compared the LFP
power between stimuli that were later remembered and stimuli that were forgotten. We found that power
differences in several distinct frequency bands were predictive of memory formation: delta (<3 Hz), theta
(4-8 Hz), alpha (8-12Hz) as well as gamma (>25 Hz). Also, we trained a decoder on learning trials for which
the memory test followed shortly after learning. We then used this decoder to predict whether the patient
will remember a separate set of stimuli 24 h later. The decoder performed significantly above chance. Thus,
LFP power differences can be used to predict successful memory formation. We further observed that a
substantial fraction of single units fired action potentials that were phase-locked to the theta as well as
the gamma band. Can differential phase locking explain why LFP power differences are a good indicator
of memory formation? We computed the spike-field coherence (SFC) for each unit and found significant
peaks in the SFC for theta as well as gamma frequencies. Comparing the SFC between subsequently
remembered and forgotten stimuli shows a larger SFC for later remembered stimuli. This indicates that
tight coordination between stimulus-evoked neuronal firing and the activity of the local population is crucial
for successful memory formation. Since the SFC is normalized for both the number of spikes as well as local
power changes, this further indicates that coordination beyond oscillation power increases is a prerequisite
for memory formation.
doi:10.3389/conf.neuro.06.2009.03.092

I-35. Topological stability of the hippocampal spatial map
Yuri Dabaghian1
Facundo Memoli2
Gurjeet Singh2
Loren Frank1
Gunnar Carlsson2
1
2

YURA @ PHY. UCSF. EDU
MEMOLI @ MATH . STANFORD. EDU
GURJEET @ STANFORD. EDU
LOREN @ PHY. UCSF. EDU
GUNNAR @ MATH . STANFORD. EDU

Keck Center for Integrative Neuroscience, UCSF
Stanford University

The crucial role of the hippocampus in creating a spatial representation of the environment and in forming
spatial memories is well known. Rodent hippocampal neurons are generally referred to as “place cells,” a
name derived from the fact that each active neuron tends to fire in a restricted region of the animal’s environment. This property has led to the general statement that the rodent hippocampus codes for “space” but
it is not clear exactly what is meant by this claim. In particular, space can be thought of in terms of two types
of representations: topological (e.g. connectivity of locations) and geometric (e.g. distances and angles).
Current theories suggest that the hippocampus explicitly represents geometric elements of space derived
from a path integration process that takes into account distances and angles of self motion information.

78

COSYNE 09

I-36
This hypothesis has difficultly explaining the results of several experimental studies that indicate that the
hippocampal spatial map is invariant with respect to a significant range of geometrical transformations of
the environment [1]. This invariance suggests an alternative framework where hippocampal neural activity
is best understood as representing the topology of the animal’s environment. We therefore suggest that the
actual role of the hippocampus is to encode topological memory maps, where the patterns of ongoing neural
activity represent the connectivity of locations in the environment or the connectivity of elements of a memory trace. From a computational perspective, this hypothesis suggests a specific approach to interpreting
the temporal activity patterns of place cells where the temporal ordering of spiking from hippocampal neural
ensembles is the key determinant of the spatial information communicated to downstream structures. If so,
then the variation seen in hippocampal firing rates should be limited to a range that preserves the global
topological information encoded in the ensemble spike trains. More generally, if the overall approach to spatial information analysis is correct, the experimentally observed parameters of firing activity must guarantee
the topological stability the hippocampal map. We therefore investigate the robustness of the hippocampal
topological map with respect to independent variations of various place cell activity parameters, such as
the firing rate and the distribution of sizes of place fields. We used the Persistent Homology method [2],
applied to simulated data. Using the simulated data is important in our approach because it allows us to
probe the complete stability range for each parameter independently and hence to theoretically establish
the range of spiking parameters that lead to topological stability. After establishing the theoretical range
of topological stability of the hippocampal map, we compare the results with the values of the parameters
that were observed experimentally and find that our theoretical framework is consistent with experimental
data. We believe that this analysis can provide fundamental insights into the parameter ranges seen in
experimental studies.
doi:10.3389/conf.neuro.06.2009.03.272

I-36. Dynamic hippocampal remapping using recurrent inhibition on
realigning grid cell inputs
Joe Monaco
Larry Abbott

JOE @ NEUROTHEORY. COLUMBIA . EDU
LFABBOTT @ COLUMBIA . EDU

Center for Theoretical Neuroscience, Columbia
Can heterogeneity of entorhinal grid metrics enable fast global remapping in the hippocampus? The realignment of MEC grid-cell responses in a remapping condition occurs concurrently with associated hippocampal
place code changes [1]. This implies that the initial formation of a spatial map in a novel environment is
a real-time dynamic readout of MEC grid structure. Further, this indicates that complicated input selection
processes, such as associative learning on afferent synapses or contextual gating to reconfigure inputs,
may not be salient to the initial recoding of spatial maps. There are many ways to derive place fields
from grids, but most models assume constraints on the orientation and/or spatial phase of MEC inputs.
While recorded colocalized ensembles of grid-cells exhibit similar spacing and orientation [2], the afferent
input structure to dorsal hippocampal place cells is still largely unknown. Here, we demonstrate a dynamic
competitive rate model of homogenous place-units. Each output unit has sparse random weights from a
set of simulated grid-cell response maps computed for a 1m square environment. The grid maps correspond to the spatial frequency range observed in the dorsal 1mm of dorsocaudal MEC [2] and have random
phases and orientations. Output competition is mediated by recurrent inhibition provided by a single global
interneuron. Each output integrates its input currents and the recurrent inhibition within a saturating nonlinearity. This enables the more strongly excited outputs, at a given location in the environment, to shut
down the activity of weaker units. The position of the nonlinearity determines how many units are receiving
enough excitation to effectively compete. When the input excitation is balanced with the inhibitory gain, the

COSYNE 09

79

I-37
resulting population-based spatial maps demonstrate qualitatively relevant levels of sparsity: about 50-60%
of place units have active place fields, and active units have on average 1.4 place fields. This is better
spatial specificity than demonstrated by some models with associative learning. The positional code for
these maps is available immediately, while the rate code converges within several time constants. Since
these spatial maps depend on the competitive balance at each location in the environment, the place code
is more highly sensitive to input changes. That is, small non-coherent shifts in the phase and orientation
of MEC grids can have an anisotropic effect that results in the sort of recoding inherent to global remapping. Adding just 0.1 radians of non-coherent noise to input grid orientations or 3-4 cm of noise to spatial
phases produces a 75% positional remapping (measured with a pair-wise correlation). Experimental data
suggests moderate realignment variance within colocalized ensembles [1]. This flexibility could be balanced
by structural coherence in realignment and the stability of learned maps for familiar environments. We suggest that heterogeneous MEC inputs combined with dynamic nonlinear competition can quickly provide an
initial remapping of novel environments. [1] M. Fyhn, T. Hafting, A. Treves, E. I. Moser, and M.-B. Moser.
2007. Nature, 446(7132):190–194. [2] T. Hafting, M. Fyhn, S. Molden, M.-B. Moser, and E. I. Moser. 2005.
Nature, 436(7052):801–806.
doi:10.3389/conf.neuro.06.2009.03.263

I-37. Synaptic decision making: flipping switch-like synapses with
cubic autocatalysis.
Gayle Wittenberg

GAYLE . WITTENBERG @ SIEMENS . COM

Siemens Corporate Research
In the brain, the process of learning is a noisy decision making process. At the level of a single synapse,
when two connected neurons are active, each generating a different noisy, but perhaps correlated, sequence of action potentials, the synapse connecting them faces a decision – how or whether to modify
the strength of that connection. Recent work suggests that this change in synapse strength might come in
the form of one of two binary decisions (O’Connor et al., PNAS102:9679-9684, 2005). For a synapse in a
low-state, the decision is whether to increase in strength (long-term potentiation; LTP), or remain the same.
For a synapse in a high-state, the decision is whether to decrease in strength (long-term depression; LTD)
or remain the same. When plasticity is measured in a population of synapses, graded learning rules can
be measured which reflect the statistics of the single synapse decisions. One form of learning rule which
has generated a great deal of interest over the last decade is spike-timing dependent plasticity (STDP), in
which the amount and direction of plasticity is a function of the timing difference between presynaptic and
postsynaptic action potentials. In hippocampal synapses however, the story is more complicated: both the
magnitude and direction of plasticity are influenced by other activity parameters such as spiking frequency,
the number of pairings delivered (Wittenberg and Wang, J Neurosci 26:6610-6617, 2006). The biophysical
mechanism by which neural activity changes the strength and state of a synapse is well-studied. Both increases in synapse strength and decreases in synapse strength are initiated by increases in postsynaptic
[Ca2+] at the synapse above baseline [Ca2+] levels. Large calcium transients drive synapses to potentiate,
whereas moderate and prolonged calcium transient drive synapses to depress. The goal of this work is
to synthesize and summarize these experimental findings with a simple biophysical model for a switch-like
synapse. We construct a simple model based on the bistability arising from cubic autocatalysis, which is
reminiscent of the Activator/Inhibitor models for spatial pattern formation in animal coats. With this approach
we are able to naturally capture the features of the learning rule determined at the CA3-CA1 synapse described above.
doi:10.3389/conf.neuro.06.2009.03.273

80

COSYNE 09

I-38 – I-39

I-38. Projection Neurons in Medial and Lateral Striatum Show Different Ensemble Patterns during Learning
Catherine Thorn
Ann Graybiel

C THORN @ MIT. EDU
GRAYBIEL @ MIT. EDU

Massachusetts Institute of Technology
Existing anatomical evidence suggests that distinct parallel basal ganglia loops exist for the control of motor
and cognitive functions during procedural learning and habit formation. Further behavioral studies show that
dorsomedial (associative loop) and dorsolateral (sensorimotor loop) striatal regions make distinct functional
contributions to procedural task acquisition, habit formation and motor performance. These studies suggest
that the medial striatum is essential for action-outcome association formation and goal directed/exploratory
behavior early in training, and that the lateral striatum is necessary for stimulus-response association formation and habitual/exploitative behavior later in training. To obtain more direct evidence of the hypothesized
medial/lateral dichotomies, we recorded single unit activity simultaneously in the medial and lateral regions
of the dorsal caudoputamen of 9 rats during acquisition and overtraining on two differently-cued (auditory
or tactile) versions of an associative T-maze task presented alternately within single daily sessions. Here
we show that medium spiny projection neurons in the dorsomedial and dorsolateral regions of the striatum
respond differently during task performance. As an ensemble, lateral task-responsive neurons developed
responses to the start and end of the task, while neurons classified as non-task-responsive reduced their
firing with training compared to a pre-trial baseline period. By contrast, medial striatal task-responsive neurons remained active throughout the middle of the task and non-task-responsive neurons did not reduce
their activity with training. These results are consistent with a functional differentiation between the two
striatal regions. The same ensemble patterns were found during auditory and tactile trial blocks, suggesting
that projection neurons in the two regions are not sensitive to cue modality. This was true both in the group
of rats that performed well on both cue types and in the group that did not, suggesting that insensitivity to
specific stimuli was not a function of acquiring the task-relevant associations. Single-unit analysis suggests
that individual neurons have highly-correlated firing patterns during auditory and tactile blocks, that only a
small percentage in each region (<10%) fire differentially to the two modality types, and that individual neurons both medially and laterally are more likely to differentiate between right and left turn responses ( 25%)
without regard to the stimulus. These findings demonstrate that striatal neurons in distinct cortico-basal
loops are (1) simultaneously active, and (2) exhibit some commonality of spiking responses, but (3) have
strikingly different ensemble patterns of activity during procedural learning and performance. Supported by
NIH/NIMH grant MH60379, ONR grant N00014-04-1-0208, and the Friends of the McGovern Institute.
doi:10.3389/conf.neuro.06.2009.03.162

I-39. Frequency selectivity using spike-timing-dependent plasticity
Matthieu Gilson1,2
Moritz Buerck3
Anthony Burkitt1,2
J. Leo van Hemmen

MGILSON @ BIONICEAR . ORG
MBUERCK @ PH . TUM . DE
ABURKITT @ UNIMELB . EDU. AU
LVH @ PH . TUM . DE

1

The University of Melbourne
The Bionic Ear Institute
3
Technische Universitaet Muenchen
2

Learning in neuronal networks is believed to rely on mechanisms at the molecular level that modify the

COSYNE 09

81

I-40
strength (or weight) of synaptic connections between neurons. Spike-timing-dependent plasticity (STDP) is
a candidate synaptic mechanism, which determines the evolution of the weights according to the precise
timing of pre- and post-synaptic spikes. In turn, this weight evolution slowly modifies the neuronal activity,
which can lead to the emergence of functional pathways in neuronal networks. In the present paper we
investigate a special case where STDP fine-tunes through synaptic competition the input selectivity for a
single neuron stimulated by a periodic input signal. We show how STDP can potentiate or depress synapses
in a pool depending on their post-synaptic response parameters (e.g. time constants and delays) and on the
input frequency. Moreover, the quality of the resulting filter can be enhanced when the neuron also receives
inhibition from the same input signal; in this case STDP can lead, at the same time, to a homeostatic
equilibrium in the average weight, which results in excitation balancing the inhibition at low frequencies. This
unsupervised learning can be performed by training either excitatory or inhibitory synapses; we constrain
the present study to the case of plasticity on only the excitatory synapses. We use a theoretical framework
based on additive STDP and Poisson neuron models in order to analyze the neuronal response to periodic
inputs and the resulting evolution of the synaptic weights over time. Our analysis focuses on “steady”
oscillatory inputs that have fixed frequency and amplitude. The weight change can be exactly calculated for
each synapse in this case, which predicts the pattern of potentiation versus depression for the whole pool
of synapses. Our results suggest that among a large pool of synapses with sufficient diversity of parameter
values, the application of a periodic input signal can select the synapses that have appropriate post-synaptic
response parameters. Consequently the response of the neuron will be strengthened for oscillatory synaptic
inputs in the range of the stimulating frequency and weakened for other frequencies. These results are
supported by numerical simulation. The mechanism presented in this paper can be considered as a first
step in the larger process whereby STDP can give rise in a neuronal network to a frequency map encoding
sensory periodic signals, such as arise in auditory processing. In order to obtain such a map, neurons are
trained by presenting many frequencies and each neuron becomes selective to a portion of the stimulus
spectrum. A second necessary step consists in obtaining a continuous representation of the frequencies in
the map, such that neighboring neurons are selective to similar frequencies. In this way, the sensory map
behaves as an estimator for the frequency component of its stimulating inputs. This second step will be the
subject of a subsequent study.
doi:10.3389/conf.neuro.06.2009.03.330

I-40. Biologically plausible model of synapse formation in the neocortex
Gina Escobar
Armen Stepanyants

ESCOBAR . G @ NEU. EDU
A . STEPANYANTS @ NEU. EDU

Northeastern University
Changes in synaptic connectivity in the adult cerebral cortex can be mediated by growth and retraction of
dendritic spines accompanied with the formation of new synapses and elimination of the existing ones. New
synaptic contacts appear in locations in the neuropil where the gaps between axonal and dendritic branches
can be bridged by dendritic spines. As the relative distances between pre- and post-synaptic branches vary,
they must be bridged by spines of different lengths. Longer spines can sample from a larger population of
available pre-synaptic axons and, hence, make a larger contribution to the potential of structural synaptic
plasticity. Spines, however, do not extend beyond few micrometers in length because longer spines must
be more costly to the organism. Though the exact nature of the dendritic spine cost is not known, we were
able to deduce its dependence on the spine length [1] by assuming that the distribution of spine lengths,
which may change in development, has reached statistical equilibrium in the adult. Our results are based on
the experimental measurements of spine lengths in human temporal cortex and macaque monkey cortical

82

COSYNE 09

I-41
areas V1, V2, V4, 7a, TE, and 46. We showed that, the cost of dendritic spines is a universal function of
the spine length, i.e. it is the same in all the considered systems. In the present study, we built a model of
synaptic connectivity that takes into account the cost of dendritic spines, as well as the details of neuron
morphology. Our model is based on the following three steps. First, to establish a synaptic connection,
the pre-synaptic axon has to be within the spine reach from the post-synaptic dendrite (potential synapse).
Second, a dendritic spine or a filopodium has to find and establish an initial contact with the axon. Finally,
based on the cost of the dendritic spine and functional properties of the two neurons, the connection is
either stabilized and transformed into a synapse or is eliminated. Based on axonal and dendritic arbors
reconstructed in 3D from different cortical layers of cat V1, we generated the potential connectivity matrix
for the artificial cortical network. This matrix contains information about the numbers of potential synapses
between any pair of neurons in the network, as well as the spine lengths required to transform individual
potential synapses into actual. Because the potential connectivity matrix in the adult cerebral cortex does
not generally change in time, it serves as a constraint for the model of synapse formation. Next, spines were
randomly placed in potential synaptic sites in a Monte Carlo simulation and either stabilized or retracted
by using the Metropolis algorithm. The latter procedure accounted for the cost of dendritic spines and
differences in the functional properties of neurons. Finally, the numbers of synapses between synaptically
coupled neurons and the probability of finding such neurons in the artificial networks were compared with
experimental data. [1]. Escobar, G. and Stepanyants, A., Statistical theory of structural synaptic plasticity,
Society for Neuroscience Annual Meeting, program number 636.21, Washington DC, 2008
doi:10.3389/conf.neuro.06.2009.03.073

I-41. Associative representations in lateral intraparietal (LIP) area
Jamie Fitzgerald1
John Assad1
David Freedman2
1
2

JKFITZG @ FAS . HARVARD. EDU
JASSAD @ HMS . HARVARD. EDU
DFREEDMAN @ UCHICAGO. EDU

Harvard Medical School
The University of Chicago

The primate brain is adept at rapidly making and breaking associations. Increasing evidence suggests
that parietal cortex plays a role in encoding learned associations between visual stimuli. Here we tested
the hypothesis that the lateral intraparietal area (LIP) can reflect learned stimulus-stimulus associations
between multiple types of visual stimuli. We trained two monkeys to group six shapes into three pairs and
to perform a delayed pair association (DPA) task in which they viewed a sample shape (650 ms), delay
(1500 ms), and test shape (650 ms). During the test period, the monkeys had to release a lever if the
test shape was the pair-associate of the sample shape for a juice reward. We recorded from 112 LIP
neurons in two animals during the DPA task and found that a majority was shape selective (Kruskal-Wallis
test, p < 0.01). Moreover, many neurons (permutation test, p < 0.05) had activity that robustly reflected the
learned associations: the spike rate evoked by a particular shape was most similar to that evoked by its pairassociate. Our lab previously showed that LIP neurons can reflect the learned significance of visual motion
during a direction-categorization task (Freedman & Assad, 2006). To test whether the same population of
LIP neurons reflects associations between both motion and shape stimuli, we recorded from 45 neurons as
a monkey switched between tasks. We computed an index to quantify the influence of learned associations
on neuronal activity. In both tasks, the index revealed a significant tendency for neurons to respond similarly
to stimuli from the same category or pair (t-test, p < 0.01 for each task). Furthermore, many cells reflected
both the shape pairs and motion categories (permutation test, p < 0.05), and indices for the two groupings
were positively correlated (r = 0.46, p < 0.01). These results suggest that LIP neurons can convey nonspatial signals about the learned associations of visual stimuli within their receptive fields (RFs). LIP cells
are also selective for the location of visual stimuli or saccadic targets. We tested whether the same neurons

COSYNE 09

83

I-42
encode spatial and non-spatial information by having a monkey switch between a memory delayed saccade
task, to reveal spatial signals, and shape DPA task with stimuli confined to RFs, to reveal non-spatial
signals. Many cells were robustly modulated by both tasks (permutation and Kruskal-Wallis tests, p <
0.05), indicating that single LIP neurons can reflect both spatial and non-spatial signals. In addition to its
role in spatial processing, our results suggest that LIP can also reflect non-spatial information about the
learned significance of, or relationship between, visual stimuli.
doi:10.3389/conf.neuro.06.2009.03.145

I-42. Specificity versus associativity in models of paired-stimulus learning
Mark Bourjaily1,2
Paul Miller3

MARKBOUR @ BRANDEIS . EDU
PMILLER @ BRANDEIS . EDU

1

Volen Center for Complex Systems
Department Neuroscience, Brandeis University
3
Brandeis University
2

Many cognitive tasks require association of two stimuli to produce a response that differs from the response
to either stimulus alone. For example, in the first phase of an associative transitive inference task [1], rats
are presented with containers identifiable by a single cue odor (A or X) then a choice of two containers with
odors (B or Y). To obtain reward, rats must learn that A predicts B, and X predicts Y. Thus neurons responsive to specific associations (e.g. A then B) must arise. In this study we investigate the requirements on
network structure and plasticity rules for the formation of an associative response, namely neural firing to the
combination of stimuli A and B that differs from the response A and Y or X and B. We begin with an initially
randomly recurrently connected network of spiking neurons with structured inputs. We use progressively
less structured inputs to determine how much randomness can be tolerated in a network that still generates
a correct response. We find that spike-timing dependent plasticity (STDP) tends to “over-associate” so cells
initially most responsive to a particular combination of inputs (A and B) also become more responsive to
a single input (A alone) or a different combination (A and Y) after many trials. Long-term potentiation of
inhibition [2] (LTPi) solves this problem by generating strong cross-inhibition that is essential to produce and
maintain specificity in the circuitry. Triplet (BCM-like) STDP [3] combined with LTPi also produces specificity
and has the advantage of generating strong self-excitatory connections necessary in the network circuitry,
which are necessary for persistent activity (memory). In all studies, homeostasis by multiplicative synaptic
scaling [4] is necessary to produce stability in the network. Finally, we find that progressively more random
networks can still generate correct responses with the same learning rules. In conclusion, we find that
biophysical learning rules can produce networks with circuitry capable of performing associative learning
tasks, provided that strong cross-inhibition arises to maintain selectivity in the neural response. We discuss
how these different plasticity mechanisms can be distinguished in the development of neural activity patterns during electrophysiological recordings during training [1] M. Bunsey and H. Eichenbaum, Nature 1996
[2] A. Maffei, K. Nataraj, S.B. Nelson and G.G. Turrigiano, Nature 2005. [3] J.P. Pfister and W. Gerstner, J.
Neuroscience 2006. [4] G.G.Turrigiano, Cell 2008.
doi:10.3389/conf.neuro.06.2009.03.154

84

COSYNE 09

I-43 – I-44

I-43. Sensory input balances excitation and inhibition to close the auditory cortical critical period
Anja Dorrn
Christoph Schreiner
Robert Froemke

ANJA . DORRN @ UNI - ULM . DE
CHRIS @ PHY. UCSF. EDU
RFROEMKE @ PHY. UCSF. EDU

University of California, San Francisco
Early in life, neural circuits are highly susceptible to outside influences. The organization of the primary
auditory cortex (AI) in particular is driven by acoustic experience primarily during the ‘critical period’, a
period of time early in postnatal development during which AI is especially plastic. This neonatal sensitivity
to the structure of sensory inputs is believed to be essential for constructing stable representations of the
auditory world and for the acquisition of language skills by children. Previous studies have shown that
the critical period for development of rodent AI occurs during the second and third weeks of postnatal
life. During this time, the tonotopic organization of AI can be extensively altered by passive exposure to
auditory stimuli such as pure tones (de Villers-Sidani et al. 2007). Additionally, the synaptic properties of
cortical neurons reach maturity at the end of this period (Oswald and Reyes 2008). However, it is unclear
how these processes at the cellular and network levels are coordinated to control the organization and
plasticity of AI receptive fields over development. Here we use in vivo whole-cell recording to measure AI
synaptic receptive fields in the intact brains of anesthetized young rats. We first asked how excitatory and
inhibitory frequency tuning curves are organized from P12 to P30. While tone-evoked synaptic currents
could be evoked at all ages, excitatory and inhibitory frequency tuning profiles were uncorrelated early in
life (P12-P16), indicating that excitation and inhibition are locally imbalanced at the onset of the critical
period. By P21, however, excitatory and inhibitory inputs were highly correlated and indistinguishable from
tuning curves measured in older animals (Wehr and Zador 2003; Froemke et al. 2007). Finally, we found
that repetitive sensory stimulation increased the strength of tone-evoked excitation and inhibition together,
leading to an increase in excitatory-inhibitory coupling across the receptive field.
doi:10.3389/conf.neuro.06.2009.03.247

I-44. The attention-gated reinforcement learning model: performance
and predictions.
Lawrence Watling
Pieter Roelfsema
Arjen Van Ooyen

LAWRENCE . WATLING @ CNCR . VU. NL
P. ROELFSEMA @ NIN . KNAW. NL
ARJEN . VAN . OOYEN @ FALW. VU. NL

CNCR, VU University Amsterdam
A problem with many traditional neural network learning algorithms is that they lack a biologically plausible
method of assigning credit to the cells in the earlier levels of the network that play a decisive role in the
stimulus response mapping. In the proposed Attention Gated Reinforcement Learning Model (AGREL) this
so-called credit assignment problem is overcome by the focal influence of attention during learning. Each
trial a global reward value (conveyed by neuromodulators) is calculated which serves to increase the likelihood that a successful, and decrease the likelihood that an unsuccessful, behavior be repeated. The activity
in the output level of the network is then channeled back through the network via the reciprocal feedback
connections, which therefore selectively target the lower level nodes that were important for generating the
current output. The learning in the network depends on the interaction of the reward and attention factors,
so only the lower level nodes that receive feedback will have their weights modified. This attentional gating

COSYNE 09

85

I-45
of the reward signal offers a biologically realistic solution to the credit assignment problem and in so doing
provides a coherent and unifying framework for learning, with attentional selection at its core. Model neurons in AGREL are shown to exhibit similar changes in tuning properties following categorization training,
as have been observed in primate cortical neurons. AGREL also generates several predictions concerning
the extent of the learning impairments caused by distortions in the reward signal, or the partial removal or
loss of the cortical feedback connections.
doi:10.3389/conf.neuro.06.2009.03.010

I-45. Neural activity in nigrostriatal circuits can signal action value
and action sequence
Xin Jin
Rui Costa

JINX @ MAIL . NIH . GOV
COSTARUI @ MAIL . NIH . GOV

Lab for Integrative Neuroscience, NIAAA/NIH
The basal ganglia are known to be involved in the initiation of complex actions which often are composed of
a sequence of movements, and for the establishment of the relation between complex actions and specific
outcomes during instrumental conditioning. We established a behavioral paradigm to study the relationships
between action sequences and appetitive outcomes in mice. Animals were trained to press levers for a
sucrose solution as reward on a fixed-ratio schedule (FR8 - 8 presses deliver a reward). Every day animals
had two single-lever training sessions, and in each of the sessions a different lever was paired with a
different reward magnitude (large or small). The order of the sessions was counterbalanced. After six
days of regular training, animals were given a test in extinction with both levers present to examine choice.
Subsequently, the lever-reward magnitude correspondence was reversed to confirm that the animal’s action
choice is contingent upon the expected value of the reward. We simultaneously implanted microelectrode
arrays in the dorsal striatum (both dorsomedial and dorsolateral), the substantia nigra pars compacta (SNc)
and pars reticulata (SNr), and investigated the changes in neural activity while the mice performed the
task. Putative cell types were clarified based on spike waveform, baseline firing rate, and drug response
(e.g. quinpirole). We observed that putative dopaminergic neurons fired phasically prior to lever pressing,
which was different from the well-known cue-elicited phasic increase in dopaminergic cell firing observed
in Pavlovian learning. Furthermore, the magnitude of this phasic firing increase in dopaminergic neuron
firing was found to be modulated by the expected value of the action. Similar modulation by the expected
magnitude of the outcome was observed in putative GABAergic neurons in the substantia nigra. In striatum,
putative cholinergic interneurons paused its firing during the phasic increases in dopaminergic firing and
throughout the lever pressing. Interestingly, as the animals behave more repetitively under the fixed-ratio
schedule with training, neural activity related to the initiation and termination of lever-press sequences
emerged. Sequence initiation and termination signals were widely found among putative dopaminergic
neurons, putative GABA neurons in nigra, and also putative projection neurons in striatum. Furthermore,
some of these sequence-boundary selective signals were modulated by the expected value of the action
sequence. These data suggest that nigrostriatal circuits can encode the expected value of actions, and may
play a crucial role in forming and expressing action sequences.
doi:10.3389/conf.neuro.06.2009.03.061

86

COSYNE 09

I-46 – I-47

I-46. Value function uncertainty as a cognitive map for reinforcement
learning
Erick Chastain1
Nathaniel Daw2
1
2

EJC @ CS . WASHINGTON . EDU
NATHANIEL . DAW @ NYU. EDU

University of Washington
New York University

Temporal-difference (TD) reinforcement learning (RL) methods underlie prominent accounts of dopamine
neuron spiking. However, it has long been known that these theories are not, by themselves, an adequate
account of animal conditioning behavior. A key challenge for such theories is Tolman’s (1932) demonstration of ”latent learning” in spatial tasks: rats are faster at learning to traverse a maze to obtain food in a
particular location, if they have previously been exposed to the maze without reward. This phenomenon is
normally understood to suggest that rats learn a representation of the spatial configuration of the maze (a
”cognitive map”) during preexposure and use it to plan actions toward a subsequently discovered goal. This
is consistent with ”model-based” RL methods, but not with standard TD algorithms, which are ”model-free”
in the sense that they do not represent any information about task contingencies, such as a spatial map, but
instead learn only a value function measuring the proximity of states (e.g., maze locations) to reward. These
methods, accordingly, learn nothing during maze preexposure, and exhibit no latent learning. Because of
these and similar experiments, it has been proposed that the purported model-free dopaminergic RL system
is accompanied by a separate, more cognitive model-based RL planning system (Daw et al 2005). Here
we reconsider these issues in the context of Bayesian versions of TD, which instead of maintaining a point
estimate of the value function, use Bayes’ theorem to maintain a distribution over values. In particular, we
consider a theory based on Gaussian Process TD (Engel et al 2003), which represents uncertainty about
states’ values not just for each state separately, but instead jointly using a full state-state covariance matrix.
We show that with learning, the structure of the posterior covariance captures the transition dynamics of
the task (eg, states’ spatial proximity), like a cognitive map, and that this information facilitates subsequent
learning. In simulations, the covariance learned during preexposure allows the model to reproduce the
latent learning effect because it enables a single subsequent experience with reward at the goal to update
the value estimates for all states in the maze. These findings forge an unexpected connection between
research on how uncertainty modulates learning in conditioning (extending Kakade & Dayan’s, 2000, account of retrospective revaluation) and other work on accelerating learning in RL using basis functions that
allow experience to generalize between ”nearby” states. In particular, we demonstrate a formal relationship
between the posterior value covariance and the ”successor representation” basis for generalization in TD
(Dayan 1993). Similarly, the results suggest that cognitive maps (and neural systems thought to subserve
them, like the hippocampus) may be interpreted in terms of uncertainty as well as spatial representation,
and may allow knowledge about task structure to be integrated with value estimates in a way that combines
the strengths of both model-free and model-based RL approaches.
doi:10.3389/conf.neuro.06.2009.03.105

I-47. Effects of learning on the motor gestures of birdsong
Jorge Mendez
Analı́a Dall’Asén
Brenton Cooper
Franz Goller

JORGEMANUEL . MENDEZ @ UTAH . EDU
YANIL . DALLASEN @ UTAH . EDU
BRENTON . COOPER @ TCU. EDU
GOLLER @ BIOLOGY. UTAH . EDU

University of Utah

COSYNE 09

87

I-48
Young zebra finches (Taeniopygia guttata) require exposure to a tutor song model early in development
to form an acoustic template. Based on this template birds shape their own song through sensori-motor
learning until it becomes stereotyped (crystallization). If zebra finches are not exposed to tutor song during a sensitive period, they produce abnormal songs that must reflect genetically coded features (innate
template). Whereas the acoustic effects of learning and isolation have been studied, very little is known
about how vocal learning is manifested in the syringeal motor gestures of song production. To address this
issue, we compared the syringeal motor commands (bilateral electromyographic activity of ventral syringeal
and dorsal tracheobronchial muscles) of crystallized song between socially tutored and untutored (isolate)
zebra finches. Differences between tutored and isolate birds are most evident in the temporal pattern of
activity within the motif and lateralization of activation between the two sides of the syrinx. Although the
songs of isolate zebra finches are acoustically distinct from those of tutored birds, mean syringeal muscle
activity during the motif does not differ between the groups. Periods of high muscle activation are more
concentrated during discrete periods of the motif in isolates compared to what is observed in tutored birds.
The deviation of muscle activity from mean muscle activity, integrated over the whole motif, is higher in the
songs of isolates than in tutored birds. The more clustered activation pattern of isolates is also evident in
a greater width of the autocorrelation functions at half maximum. To assess lateralization of muscle activation, cross-correlation analysis between electromyographic (EMG) recordings from the left and right sides
of the syrinx were conducted. Cross-correlation functions show peak values near zero lag time and these
peaks are wider for the isolate birds. This result is not explained by the wider autocorrelation functions in
isolate birds, as cross-correlation analysis for surrogates with the same autocorrelations as those found in
the original time series resulted in lower cross-correlation functions than found for bilateral EMG data. This
analysis indicates therefore that isolate birds have more symmetrical activation of the syringeal musculature compared to the tutored zebra finches. In conclusion, learning affects the motor commands of song
production by refining an innately developing activation pattern. This refinement includes a more distributed
activation of syringeal muscles across the motif and increased lateralization of syringeal muscles. The results of this experiment allow quantification of learned and innate components of the motor commands of
song production and therefore allow inference about the modification of central control by vocal learning.
Support: NIH DC04390 and DC06876
doi:10.3389/conf.neuro.06.2009.03.127

I-48. Context dependent movement encoding and variability in the
motor cortex
Martin Nawrot1,2
Jörn Rickert
Alexa Riehle3
Ad Aertsen
Stefan Rotter4

NAWROT @ NEUROBIOLOGIE . FU - BERLIN . DE
RICKERT @ BIOLOGIE . UNI - FREIBURG . DE
ALEXA . RIEHLE @ INCM . CNRS - MRS . FR
AERTSEN @ BIOLOGIE . UNI - FREIBURG . DE
STEFAN . ROTTER @ BIOLOGIE . UNI - FREIBURG . DE

1

Freie Universitat Berlin
Bernstein Center Computational Neuroscience
3
INCM - CNRS
4
BCCN Freiburg
2

When we want to perform a skilled movement such as reaching for an object, we can make use of prior contextual information during movement preparation in order to increase speed and accuracy of our movement.
In this study, we asked how the degree of prior information about a movement target affects the encoding
of movement direction during preparation and execution, and whether we observe an effect of contextual
information on neuronal variability? We recorded single unit activity from the motor cortex of two monkeys

88

COSYNE 09

I-49
that had been trained to perform a delayed center-out reaching task to one out of six targets. In three
different task conditions we varied the degree of prior information about target location that was presented
with the initial preparatory stimulus (PS), indicating either one (complete information), or 2/3 adjacent targets (incomplete information) at the beginning of the hold period. Target ambiguity was resolved with the
response stimulus (RS) after 1s. We used Bayesian inference for the time-resolved single-trial decoding of
movement direction from single unit and population activities. We estimated single-trial firing rates by kernel
convolution and computed for each point in time the tuning probabilities P(r—d) of observing rate r given
direction d. Using Bayes’ rule we computed the single trial probabilities P(d—r) to correctly decode direction
d given the empirical rate estimate r. Average single-unit Bayesian probabilities were initially at chance level
but increased rapidly about 150-250ms after PS. They reached highest values for the 1-target condition, but
lower values for incomplete prior information. On the population level (50-100 neurons), the degree of prior
information was accurately reflected throughout the preparatory period in the Bayesian probabilities that
reached 1/3rd, 2/3rd and 1 for 3, 2, and 1 target cues, respectively. Individual neurons typically contributed
only during short episodes. From this, we conclude that accurate movement representation is maintained
through time by multiplexing the contributions from neuronal ensembles of time-varying composition. Next,
we performed a time-resolved analysis of the trial-by-trial count variability (Nawrot et al. 2008). The Fano
factor was highest before PS and sharply dropped after PS. This general behavior has been repeatedly
reported (Nawrot et al. 2000; Nawrot et al. 2003; Churchland et al. 2006; Nawrot et al. 2008). During
spontaneous condition single neuron variability was highest in the one target condition, and lower for the
2-target and 3-target condition. After PS the variability dropped and reached the lowest levels in case of
complete prior information (1 target), but higher levels for incomplete prior information. We conclude that
single neuron variability can depend on contextual information. In our case, variability during movement
preparation was the lower, the more information was available to the monkey. We believe that the degree of
neural variability reflects the accuracy with which the upcoming movement could be prepared. Churchland
et al. (2006) J.Neurosci. 26:3697 Nawrot et al. (2000) Eur.J.Neurosci. 12(Supl.11):506 Nawrot et al. (2003)
Biol.Cybern 88:321 Nawrot et al. (2008) J.Neurosci.Meth 169:374
doi:10.3389/conf.neuro.06.2009.03.280

I-49. Can divergent connectivity generate reliable sparse activity patterns?
Thomas Nowotny
Ramon Huerta

T. NOWOTNY @ SUSSEX . AC. UK
RHUERTA @ UCSD. EDU

University of Sussex
The connectivity in the brain can be highly convergent and highly divergent. In the olfactory system of insects, for example, the connections from olfactory receptor neurons to projection (PNs) and local neurons
(LNs) in the antennal lobe, the primary center for olfactory information processing, are highly convergent.
The connections from the PNs to the Kenyon cells (KCs) in the secondary center of processing, the mushroom bodies, are, on the other hand, highly divergent. While the role of convergence and its properties
in improving signal to noise ratios are if not fully understood at least generally accepted, the role of divergent connectivities is less clear. It has been suggested by us and others that divergence may serve to
generate well separated, sparse activity patterns which are known to be beneficial for recognition (classification) and memory formation. In this work we analyze general implications of divergent connectivity in the
framework of unspecific (randomly connected) feedforward networks of generic (McCulloch-Pitts) neurons.
We find that the distribution for the number of active neurons in the target area is very skewed with large
probabilities for no response as well as a fat tail of non-zero probabilities for overwhelming responses of
many neurons. The results imply that in the example of the locust olfactory system, divergent connections

COSYNE 09

89

I-50
by themselves, cannot generate the desirable, and experimentally observed, sparse activity patterns if the
observed high density of PN-KC connections is correct. This apparent contradiction can be resolved by
feedforward gain-control, which may be implemented by the antennal lobe - lateral horn - mushroom body
pathway. The remainder of the work is dedicated to elaborating a hypothesis why nature may have chosen
a much more costly solution - dense PN-KC connections and feedforward gain control - over a much simpler
solution - sparse PN-KC connections. Our results are applicable to many questions in neuroscience as divergent connections are ubiquitous, another prominent example being the connections from the entorhinal
cortex to the dentate gyrus in the mammalian hippocampal formation. This work was partially funded by
the Biotechnology and Biological Sciences Research Council (grant number BB/F005113/1).
doi:10.3389/conf.neuro.06.2009.03.008

I-50. Bifurcation analysis of neural mass equations
Romain Veltz1
Olivier Faugeras2,3

ROMAIN . VELTZ @ SOPHIA . INRIA . FR
OLIVIER . FAUGERAS @ SOPHIA . INRIA . FR

1

INRIA Sophia Antipolis, NeuroMathComp Team
INRIA
3
ENS
2

The neural mass (NM) equations, also called the Wilson and Cowan or Hammerstein equations, are important because they provide models of the activity of several neural populations in a macroscopic patch of the
cortex. They are rate models and feature a spatio-temporal connectivity matrix function relating different
populations at different places in a continuum and a nonlinearity to convert average membrane potentials
into firing rates. Previous theoretical studies have described several time-varying solutions consistent with
optical imaging experiments. The steady states solutions are also interesting because they may account for
the memory holding tasks which have been demonstrated by experimentalists in primates. We study the dependency of the NM solutions with respect to the stiffness of the nonlinearity. This choice is motivated by two
reasons. First, it shows many differences with the well-studied, less biologically plausible, infinite stiffness
case. Second, the study is generic and the dependency of the solutions with respect to any other parameter
can be studied with similar tools. We assume that the cortex patch is finite although its dimension may be
arbitrary. Beside of its biological relevance this is crucial for our analysis because it always guarantees the
existence of a solution to the NM equations. The bifurcation analysis of the NM equations uses mainly two
tools: Turing patterns and reduction to Ordinary Differential Equations (ODEs) and Partial Differential Equations (PDEs). The first method is used in many papers (Attay-Hutt 05, Blomquist 07, Venkov-Coombes 07)
and leads to the description of a lot of behaviors: traveling waves, breathers, persistent states. The second
method is to reduce the NM equations to find a homoclinic orbit to ODEs/PDEs (Laing 02, 03) allowing the
use of finite-dimensional tools or PDE methods for the bifurcation analysis. We use infinite dimensional
methods recently developed for fluid dynamics (Iooss 08 to appear, Kielhoffer 04) which encompass the
previous methods. Coupled to a numerical continuation method, they allow to compute numerically the
steady states independently of their stability and provide a certified description of the local dynamics. This
is unique to our method. We consider a very general type of connectivity matrix function which reduces
exactly the dynamics to a system of ODEs. One thrust of the paper is that we use infinite dimensional tools
to make general statements on the behaviour of solutions of the NM equations (boundedness, number of
solutions, types of bifurcations) and use finite dimensional tools to do the actual computation, in a certified
manner. This is achieved through a new method of approximation of the connectivity matrix functions that
uses a combination of the Picherle-Goursat kernels and the Gegenbauer polynomials. Because our new
method allows to exhaustively explore the set of solutions of the NM equations when varying the parameters, and to characterize their behaviours qualitatively as well as quantitatively, it can be very useful to
relate a “cortical behaviour” observed, e.g., in optical imaging, to ranges of parameters values in the NM

90

COSYNE 09

I-51
equations which are known, through the bifurcation study, to feature similar behaviours.
doi:10.3389/conf.neuro.06.2009.03.064

I-51. Selective in vivo activation of fast- or regular-spiking barrel cortex neurons with Channelrhodopsin
Jessica Cardin1,2
Marie Carlen3
Konstantinos Meletis3
Ulf Knoblich1,4
Feng Zhang5
Karl Deisseroth5
Li-Huei Tsai
Christopher Moore4

CARDIN @ MAIL . MED. UPENN . EDU
MCARLEN @ MIT. EDU
MELETIS @ MIT. EDU
KNOBLICH @ CSAIL . MIT. EDU
ZHANGF @ STANFORD. EDU
DEISSERO @ STANFORD. EDU
LHTSAI @ MIT. EDU
CIM @ MIT. EDU

1

McGovern Institute, MIT
Univ. of Pennsylvania
3
Picower Institute for Learning and Memory, MIT
4
Massachusetts Institute of Technology
5
Stanford University
2

Cortical network states are believed to play crucial roles in brain function, and have been a topic of active investigation for 80 years. Several theories have proposed distinct roles for specific neuron types
in these cortical dynamics. Through the advent of optogenetic techniques, causal induction or modulation of a given network state by stimulation of specific neuron types in vivo is possible for the first time.
As a tool for examining cortical dynamics and their impact on sensory information processing, we have
optimized the cell type-specific expression in vivo of light-activated channels in either fast-spiking (FS) inhibitory interneurons or regular spiking (RS) excitatory neurons in barrel (SI) cortex. We directly tested
the influence of different neural cell types on local network activity in vivo by using the light-sensitive bacteriorhodopsin Chlamydomonas reinhardtii Channelrhodopsin-2 (ChR2). To target expression of the lightactivated channels specifically to parvalbumin-positive fast-spiking (FS-PV+) interneurons, we injected an
adeno-associated viral vector with Cre-dependent expression of ChR2 into PV-Cre knock-in mice. To confer
cell type-specific transduction and restrict expression of light-activated channels to cells with Cre expression, we used the vector AAV Double-floxed Inverted Open reading frame-ChR2-mCherry (AAV DIO ChR2mCherry). Immunohistochemistry demonstrated that ChR2-mCherry+ cells in the PV-Cre mice expressed
the interneuron-specific neurotransmitter GABA and 96.7 ± 1.0% (n = 4234 ChR2-mCherry+ neurons in 4
animals) of the ChR2-mCherry+ neurons expressed detectable levels of PV. To target expression of ChR2
to RS, putative excitatory cells, we injected the same viral vector into &#945;CamKII-Cre mice. Immunohistochemical analysis revealed that 100 ± 0% (n = 4024 ChR2-mCherry+ neurons in 4 animals) of the ChR2mCherry expressing neurons in the &#945;CamKII-Cre mice were immuno-negative for PV, and 100 ± 0%
expressed the neuronal marker NeuN. Cell type-specific transduction by the viral vector was successful in
all animals assessed, as confirmed by single-unit recordings in PV-Cre (n = 15/15) and &#945;CamKII-Cre
(n = 7/7) mice. Recordings from layers 2/3 and 4 of barrel cortex were made with stereotrodes and tetrodes
under isoflurane anesthesia. In agreement with the immunohistological results, sorting action potentials
based on their wave forms indicated that light-activated FS were exclusively observed in PV-Cre mice (n =
64/64 neurons in 15 animals), and light-activated RS in &#945;CamKII-Cre mice (n = 56/56 neurons in 7
animals; p < 0.01). The impact of RS and FS firing on local network activity was revealed in the local field
potential (LFP) during light activation (n = 14 sites in 6 PV-Cre animals; 13 sites in 5 &#945;CamKII-Cre
animals). Using this approach, we significantly modulated responses to sensory stimulation based on the

COSYNE 09

91

I-52
timing of sensory stimulation relative to pulsed light activation. For example, vibrissa evoked responses in
RS were suppressed by simultaneous or preceding stimulation of FS by light (n = 26; p < 0.01).
doi:10.3389/conf.neuro.06.2009.03.080

I-52. Stimulus onset quenches neural variability: a widespread cortical phenomenon
Mark Churchland1
Byron Yu1,7
John Cunningham1
Leo Sugrue1
Marlene Cohen1
Greg Corrado1
William Newsome1
Andrew Clark2
Paymon Hosseini2
Benjamin Scott2
David Bradley2
Matthew Smith3
Adam Kohn4,5
J. Anthony Movshon5
Katherine Armstrong1
Tirin Moore1
Steven Chang6
Lawrence Snyder6
Stephen Ryu1
Gopal Santhanam1
Maneesh Sahani7
Krishna Shenoy1

CHURCH @ STANFORD. EDU
BYRONYU @ STANFORD. EDU
JCUNNIN @ STANFORD. EDU
LEO @ MONKEYBIZ . STANFORD. EDU
MARLENE COHEN @ HMS . HARVARD. EDU
GREGC @ MONKEYBIZ . STANFORD. EDU
BNEWSOME @ STANFORD. EDU
CLARKAM @ UCHICAGO. EDU
PAYMON @ MIT. EDU
BBSCOTT @ MIT. EDU
THESTRANGEBLUE @ GMAIL . COM
MASMITH @ CNBC. CMU. EDU
AKOHN @ AECOM . YU. EDU
MOVSHON @ NYU. EDU
KATHERINE . ARMSTRONG @ GMAIL . COM
TIRIN @ STANFORD. EDU
STEVE @ EYE - HAND. WUSTL . EDU
LARRY @ EYE - HAND. WUSTL . EDU
SEOULMAN @ STANFORD. EDU
GOPAL @ NERUR . COM
MANEESH @ GATSBY. UCL . AC. UK
SHENOY @ STANFORD. EDU

1

Stanford University
University of Chicago
3
Carnegie Mellon University
4
Albert Einstein College of Medicine
5
New York University
6
Washington University
7
Gatsby Computational Neuroscience Unit, UCL
2

A major goal of neuroscience is system identification: the brain is probed with repeated ‘trials’, and its dynamics are inferred from recorded responses (typically spikes). Most often, the mean across-trial response
is all that is considered. By averaging, the experimenter hopes to combat the apparent noisiness of spiking,
and estimate ‘the underlying firing rate’ of the neuron. However, one must presume that the underlying rate
may differ across supposedly-identical trials, as indicated by optical-imaging studies (Arieli et al., 1996). Yet
such variability usually remains a hidden dimension of the data, its presence inferred only when it correlates
with behavior. Rarely is it considered that the rate variance might contain temporal structure comparable
to that of the mean rate. However, such structure is expected for a variety of dynamics. Attractor networks
may show declining variability as the network overcome initial variability. Integrator networks may show increasing variability as noisy signals are integrated. In fact, when distinguishing between candidate classes
of dynamics, rate variability can be more informative than mean rate. We investigated rate variance in seven

92

COSYNE 09

I-53
cortical areas. We employed classical and newer mathematical methods for assessing variance, including
an adaptation of the Fano factor and two variants of factor analysis. Critically, these methods can determine when an experimentally-observed change in variance is due to changes in underlying-rate variance,
rather than changes in spiking regularity. This is essential, as spiking regularity can change with firing rate
(e.g., due to refractoriness). Across all seven cortical areas, we found one remarkably consistent effect:
stimulus onset drove a decline in underlying-rate variability. The decline in variability occurred over a wide
range of circumstances: during responses to simple sensory stimuli (sine-wave gratings for V1), during
operantly-conditioned responses (reach targets for PRR and PMd), during reward-driven responses (OFC),
under anesthesia (V1), during passive viewing (V4), and during active task performance (PRR, PMd, LIP,
and MT). This decline may indicate that attractor dynamics are a prevalent feature of cortex. Alternately,
it has recently been argued that a general feature of large recurrent networks is a tendency to produce
chaotic spontaneous activity that is suppressed by an input (Rajan, Abbott & Sompolinsky, unpublished).
Consistent with both interpretations, neural variability declined even when there was little change in mean
rate. This observation underscores that a ‘non-responsive’ average rate can obscure robust responses on
individual trials. We further investigated trial-by-trial variability using an extension of factor analysis (GPFA,
Yu et al., NIPS 2009) to reconstruct single-trial ‘neural trajectories’. Trajectories were initially variable, then
converged to a common trajectory following stimulus onset. Taken together, our results indicate that a common property of cortical circuitry is that it is stabilized by an input. That said, we expect cortex to exhibit
a variety of dynamics – yielding a variety of variability ‘signatures’ – under different circumstances (e.g.,
Churchland, Kiani & Shadlen, SFN 2006). Measurements of variability, and the reconstruction of single-trial
responses, should be central to investigating those dynamics.
doi:10.3389/conf.neuro.06.2009.03.295

I-53. Columnar coding of neuronal populations in primary visual cortex
Roger Herikstad
Jonathan Baker
Charles Gray
Shih-Cheng Yen

G 0600089@ NUS . EDU. SG
JBAKER @ CNS . MONTANA . EDU
CMGRAY @ NERVANA . MONTANA . EDU
SHIHCHENG @ NUS . EDU. SG

National University of Singapore
In this study, we used a 54-channel columnar probe to record from populations of neurons within a cortical
column in the primary visual cortex of the anesthetized cat. To efficiently sort the data, we used the specific
geometry of the recording probe to assign compound signals to one or more spike trains. We applied this
method to recordings in which natural movies and drifting grating stimuli were presented. The first step in
our algorithm was to separate a signal into channel complexes. We defined a channel complex as a continuous range of channels for which the activity exceeded the background noise level. Through simulations,
we found that complexes originating from a single cell exhibited a distinct alternating and decaying pattern
of spike amplitudes across channels caused by the specific arrangement of the channels on the probe.
This allowed us to label complexes as single-cell or multi-cell complexes based on the channel profile. Similar single-cell complexes were then merged together based on the number of common channels, as well
as the similarity between mean waveforms, to form template clusters. The multi-cell complexes were then
matched against individual templates or pairs of templates using the correlation coefficient computed across
the active channels of the multi-cell complex. We also made sure that adding the multi-cell complex did not
create inter-spike intervals smaller than 1 ms. The above approach allowed us to identify a total of 393 cells
across five sites in one experimental animal, with a median of 91 cells per site. Using the responses to
drifting gratings, we defined a cell as tuned if the response distribution at the stimulus direction containing

COSYNE 09

93

I-54
the highest firing rate was significantly different from the response distribution at the perpendicular orientation. By this criteria, 149 cells were defined as tuned. We also computed the maximum firing rate for all
cells across 40 ms bins, which was the duration of a movie frame in our movie stimuli. The grating stimuli
evoked significantly stronger responses (2-sample KS-test (KS-test2), p < 0.01), with median and quartiles
of 17.5, 10.7 and 30.9 Hz for the movie responses, and 40.0, 30.0 and 61.2 Hz for the grating responses.
We also calculated the lifetime and population sparseness of the cells. Surprisingly, we found that the grating stimuli evoked higher lifetime and population sparseness (KS-test2, p<0.01 for both), with median and
quartiles of 67.8, 44.9, 98.7 (lifetime) and 0.49, 0.35 and 0.66 (population) for the movie responses, and
99.8, 99.8 and 99.9 (lifetime) and 0.93, 0.87 and 0.97 (population) for the grating responses. An analysis
of the correlation between the PSTH of all possible pairs of cells revealed that the responses to movie and
grating were quite heterogenous, with median and quartiles of 0.016, 0.0017 and 0.069 for movies, and 0.0,
0.0, and 0.0038 for gratings. The grating responses were again significantly more heterogenous (KS-test2,
p < 0.001). Our preliminary results suggest that the responses of neuronal populations in a cortical column
exhibit significantly different responses under grating and movie stimulation.
doi:10.3389/conf.neuro.06.2009.03.293

I-54. A multivariate phase distribution and its estimation
Charles Cadieu
Kilian Koepsell

CADIEU @ BERKELEY. EDU
KILIAN @ BERKELEY. EDU

University of California, Berkeley
Oscillations have received considerable attention within the neuroscience community. Early theories of
large-scale brain dynamics focused on oscillations [1], and oscillatory dynamics have recently received
widespread interest [2, 3, 4]. Neural oscillations are hypothesized to be functionally involved in a wide
range of tasks, such as representing sensory information, regulating the flow of information, learning and
recalling of information, and binding of distributed information. In each of these contexts, the phase of neural oscillations is of central scientific interest. For example, the pair-wise relationships of phase variables
recovered from ECoG recordings (electrical potentials recorded simultaneously from a grid of 64 electrodes)
show strong statistical dependencies [5]. The coupling of these oscillations may indicate common sources
of input or task based cortico-cortical communication. High-dimensional neural measurements of oscillatory dynamics (such as ECoG, EEG, LFP, or fMRI) pose two interconnected problems: how do we model
the full multivariate distribution of phase measurements, and how do we recover the parameters of such
a model from phase measurements? In order to address these questions, and ultimately to provide tools
for the scientific investigation of neural oscillations, we have developed models and analytical techniques
that can capture the observed statistical dependencies among multiple phase variables and recover the
parameters of these models from measurements. In this work we introduce a distribution that captures empirically observed pair-wise phase relationships: the distribution produces uniformly distributed marginals
of the individual phase variables and dependencies in the differences of pairs of phases. Importantly, we
have developed a computationally efficient and accurate technique for estimating the parameters of this
distribution from data using the Score Matching technique [6]. We show that the algorithm performs well
in high-dimensions (d=100), and in cases with limited data (as few as 10 samples per dimension). This
distribution and estimation technique can be broadly applied to any setting that produces multiple circular
variables and is useful for localizing behaviorally dependent functional networks [7]. [1] W.J. Freeman. Mass
Action in the Nervous System. Academic Press, New York, 1975. [2] P. Fries. A mechanism for cognitive
dynamics: neuronal communication through neuronal coherence. Trends in Cognitive Sciences, 9(10):474480, 2005. [3] T.J. Sejnowski and O. Paulsen. Network Oscillations: Emerging Computational Principles.
Journal of Neuroscience, 26(6):1673, 2006. [4] R. Canolty and K. Miller. Large scale brain dynamics. NIPS
Workshop, 2007. [5] C.F. Cadieu and K. Koepsell. A multivariate phase distribution and its estimation.

94

COSYNE 09

I-55
arXiv:0809.4291v1 [q-bio.NC], September 2008. [6] A. Hyvarinen. Estimation of Non-Normalized Statistical Models by Score Matching. The Journal of Machine Learning Research, 6:695-709, 2005. [7] A.
Huth, C.F. Cadieu, C.L. Dale, G.V. Simpson, K. Koepsell. Detecting functional connectivity in networks of
phase-coupled neural oscillators. Cosyne 2009.
doi:10.3389/conf.neuro.06.2009.03.260

I-55. Towards a physiological exploration of sensorimotor processing
in behaving Drosophila
Johannes Seelig1,2
Eugenia Chiappe*1,2
Gus Lott1,2
Thomas Adelman
Michael Reiser1,2
Vivek Jayaraman1,2
1
2

SEELIGJ @ JANELIA . HHMI . ORG
CHIAPPEE @ JANELIA . HHMI . ORG
LOTTG @ JANELIA . HHMI . ORG
THOMAS . ADELMAN @ GMAIL . COM
REISERM @ JANELIA . HHMI . ORG
VIVEK @ JANELIA . HHMI . ORG

Janelia Farm Research Campus
Howard Hughes Medical Institute

We are interested in understanding the cellular and circuit computations that underlie sensorimotor transformations. The model organism we have chosen, Drosophila melanogaster, has long been a mainstay
of molecular, developmental and behavioral research. It offers many experimental advantages, including
the ability to genetically label and manipulate the activity of specific sub-populations of neurons. More recently, it has become possible to perform in vivo electrophysiology and two-photon calcium imaging from
genetically identified neurons in the fly’s central brain [1, 2]. These advances make this powerful genetic
model organism attractive for investigations in systems neuroscience. In order to explore sensorimotor
processing, we think it is essential to simultaneously record both neural activity and behavior. Accordingly,
we have developed a setup for optical and electrophysiological recording of neural activity while the fly is
walking on an air-supported ball [3, 4]. The ball’s movements are recorded at better than single millisecond
resolution using an optical mouse sensor, and this readout is treated as a proxy for the fly’s movements
[5]. This preparation allows us to present controlled sensory stimuli (e.g., visual stimuli displayed using an
LED arena [6]) during visually guided cell-attached and whole-cell patch clamp recordings and two-photon
and wide-field fluorescence calcium imaging. Tracking ball motion with high temporal precision allows us
to perform both open- and closed-loop experiments, assess the relevance of particular sensory stimuli for
the animal, and correlate brain activity with behavior. We will describe the setup and present results from
preliminary experiments using this combination of techniques to explore sensory-driven orienting behavior
in the fly. 1. Wilson, Turner and Laurent (Science, 2004). 2. Jayaraman and Laurent (Frontiers in Neural
Circuits, 2007). 3. Gotz and Wenking (J Comp Physiol, 1973). 4. Bohm, Schildberger and Huber (JEB,
1991). 5. Lott, Rosen and Hoy (J Neurosci Meth, 2007). 6. Reiser and Dickinson (J Neurosci Meth, 2008).
* J.D. Seelig & E. Chiappe contributed equally to this work
doi:10.3389/conf.neuro.06.2009.03.130

COSYNE 09

95

I-56

I-56. In what regimes do regular-spiking excitatory neurons drive gamma
oscillations?
Dorea Vierling-Claassen1,2
Jessica Cardin3,4
Christopher Moore2
Stephanie Jones1,5

DOREA @ NMR . MGH . HARVARD. EDU
CARDIN @ MAIL . MED. UPENN . EDU
CIM @ MIT. EDU
SRJONES @ NMR . MGH . HARVARD. EDU

1

MGH-MIT-HMS Martinos Cntr for Biomed. Imaging
Massachusetts Institute of Technology
3
McGovern Institute, MIT
4
Univ. of Pennsylvania
5
Harvard Med. School
2

Gamma rhythmicity in neural networks is thought to be important for sensory processing. Computational
models of gamma emphasize the critical role of fast-spiking (FS) interneurons, and the time constant of
GABAA synaptic inhibition. The role of oscillatory activity in pyramidal, regular spiking (RS) excitatory neurons is more ambiguous: While sufficient spiking activity in RS is essential to maintain gamma oscillations
in a network, the temporal structure of excitatory drive is currently thought to be less important than parameters controlling inhibition. Decisively identifying the distinct contributions of RS and FS in network gamma
oscillations has been difficult, given the historical inability to selectively drive distinct cell populations in vivo.
We have recently developed techniques for doing so by using the light-sensitive bacteriorhodopsin Chlamydomonas reinhardtii Channelrhodopsin-2 (ChR2) (see Cardin et al. this meeting). To compliment these
experimental studies, our current modeling efforts explore the impact of selective rhythmic activation of either RS or FS on oscillations expressed in the local field potential (LFP) and/or magnetoencephalography
(MEG) signal. In modeling of human auditory cortex, we developed a network in which rhythmic drive to RS
excitatory cells generates 40 Hz resonance. This model, which accurately simulated the gamma range Auditory Steady State Response (ASSR) measured with MEG (Vierling-Claassen, Siekemeier, Stufflebeam,
Kopell, 2008), used a simplified single-compartment cellular model with reciprocally connected excitatory
and inhibitory populations. Each population received background excitatory noise and excitatory input in
the gamma range. When the synchronous input was stronger to the excitatory than the inhibitory population, the system was able to robustly resonate to 40 Hz. We have recently constructed a more biophysically
based cortical network, using multi-compartment FS and RS, including active somatic and dendritic ionic
currents appropriate to replicate cell anatomy and physiology described in whole cell in vitro recordings.
In contrast to the simplified model, this system failed to generate 40 Hz resonance in the modeled LFP
signal when RS were driven synchronously, but did generate 40 Hz resonance when FS were driven, as is
consistent with previous theoretical work regarding gamma generation. Using this second model, we are
examining the conditions under which pyramidal drive will induce gamma. By parametrically varying the
number of RS neurons responsive to gamma frequency drive, the level of synchrony among responsive
cells, and the degree of network connectivity between excitatory and inhibitory cell populations, we will define the parameter regimes in which pyramidal cells driven in the gamma range do and do not elicit gamma
frequency resonance, in a manner consistent with the ongoing experimental work (see Cardin et al. this
meeting).
doi:10.3389/conf.neuro.06.2009.03.155

96

COSYNE 09

I-57

I-57. Perception as Modeling: A neural network that extracts and models predictable elements from input.
David Sussillo
Larry Abbott

SUSSILLO @ NEUROTHEORY. COLUMBIA . EDU
LFABBOTT @ COLUMBIA . EDU

Columbia University
Perception is not simply the passive analysis of input data, it involves modeling the external world, making
inferences about predictable events and noting when something unexpected happens. In most models of
sensory processing, activity generated by external stimuli in peripheral sensors is used to drive networks
that perform computations such as object recognition. We consider a different scheme in which stimulus
driven activity acts as a training signal for a spontaneously active network. This “predictor” network continually attempts to model the training data in the sense of being able to generate it spontaneously, even if the
sensory input is cut off. Of course, only certain aspects of the sensory data may be predictable. To address
this, an “extractor” circuit that is guided by feedback from the predictor produces the training signal for the
predictor network. The extractor pulls out from the sensory stream aspects of the data that the predictor
network can reproduce. This automatically divides the input data into three categories: 1) noise, which is
the part of the input stream ignored by the extractor, 2) a predictable signal that is isolated by the extractor
circuit and internally reproduced by the predictor, and 3) surprising events, which are elements of the input
stream generated by the extractor that do not match the output of the predictor. The predictor is a recurrent
network of firing-rate model neurons with a linear readout that provides both the output of and feedback to
the network. As in the work of Jaeger and Haas (Science, 2004), the learning algorithm only modifies the
weights connecting the network to the output unit, but unlike their approach, we leave all feedback intact
during learning. The key element is a novel learning rule we call FORCE learning that restricts errors in
the output to small values throughout training. This was originally a supervised learning scheme, but in the
extractor-predictor approach, the extractor circuit acts as the supervisor for the predictor network, so the
scheme is unsupervised. Due to the nature of FORCE learning, the modeling network always generates
a close match to the target and thus does not generate “hallucinations” unrelated to external reality. The
rate of change of the readout weights provides a measure of whether the target function can be generated autonomously by the predictor. We use this measure as a supervisory signal for the extractor, which
is modified if the target signal it is extracting from the input data cannot be autonomously modeled. The
combined recurrent predictor network and linear-filter extractor is successful at finding and modeling predictable structure (if it exists) in high-dimensional time series data even when polluted by extremely complex
noise. This network can be viewed as a general method for extracting patterns from complicated time series
or, from a systems neuroscience perspective, as a model of perception where the way to understand the
world is through internal prediction, with further processing used only when sensory input fails to match
expectations.
doi:10.3389/conf.neuro.06.2009.03.253

COSYNE 09

97

I-58

I-58. Modelling Adaptive Coding of Sound Localization Cues in the
Inferior Colliculus.
Phillipp Hehrmann1
Julia Maier2
Nicol Harper3,4
David McAlpine2
Maneesh Sahani1

HEHRMANN @ GATSBY. UCL . AC. UK
JKMAIER @ GOOGLEMAIL . COM
NICOL . HARPER @ UCL . AC. UK
D. MCALPINE @ UCL . AC. UK
MANEESH @ GATSBY. UCL . AC. UK

1

Gatsby Computational Neuroscience Unit, UCL
UCL Ear Institute
3
UCL
4
University of California, Berkeley
2

Interaural time differences (ITDs), small differences in the timing of a sound at the two ears, provide an
important sensory cue about the azimuthal location of a sound source. Like sensory neurons in many
other modalities and species, ITD sensitive neurons in the inferior colliculus (IC) of guinea pigs adapt their
responses depending on the statistical distribution of ITDs in the stimulus. As the position of a narrow high
probability region (HPR) of ITDs changes within the physiological range, gain changes and small shifts
can be observed in the tuning curves of many IC neurons (Maier et al., Assoc. Res. Otolaryngol. Abs.
432, 2007). Specifically, we consistently observe an decrease in neural gain as the HPR center is moved
from the ipsilateral to the contralateral side. In addition, about half the neurons shift their tuning curves
towards the HPR center for distributions with ipsilateral-leading HPRs. These gain changes and shifts are
accompanied by corresponding changes in the Fisher Information curves. No adaptation occurs when
the HPR width (i.e. the variance of the ITD distribution) is manipulated while keeping its center position
fixed. Studying ITD stimulus adaptation in the IC is particularly interesting, because binaural convergence
and thus ITD sensitivity first occurs in the medial superior olive (MSO), one synapse upstream of the IC.
Furthermore, there is evidence suggesting that neural reponses in the MSO itself encode the instantaneous
ITD of a stimulus largely independent of its context (Spitzer & Semple, J. Neurophysiol. 80:3062-3076,
1998). Hence, adaptation may be assumed to arise at a site presynaptic to or within the IC itself, potentially
allowing us to identify the underlying mechanism. Here, we present a model of midbrain ITD processing that
can account for the essential aspects of the observed ITD stimulus adaptation. One important feature of
this model is synaptic depression (Markram & Tsodyks, Nature 382:807-810, 1996) at excitatory synapses
between statically ITD-tuned MSO neurons and the IC. Synaptic depression by itself replicates the observed
gain changes, but does not account for the shifts occurring in a significant subset of neurons. Our modelling
suggests that the shifts may be caused by slowly acting, shunting inhibition driven by an indirect pathway
from the contralateral, oppositely tuned MSO via the dorsal nucleus of the lateral lemniscus. We conclude
that systems-level adaptive effects – which often have functionally important consequences (e.g. Dean et
al., Nat. Neurosci. 8:1684–1689, 2005) – may depend on a combination of biophysical mechanisms, even
when expressed across a single stage of processing. Supported by the Gatsby Charitable Foundation
doi:10.3389/conf.neuro.06.2009.03.171

98

COSYNE 09

I-59

I-59. Quantitative analysis of the learning dynamics of a two-phase
neural model of classical conditioning
Ivan Herreros-Alonso
Andrea Giovannucci
Riccardo Zucca
Matti Mintz
Paul F. M. J. Verschure

IVANHERREROS @ GMAIL . COM
ANDREA . GIOVANNUCCI @ GMAIL . COM
RZUCCA @ IUA . UPF. EDU
MINTZ @ FREUD. TAU. AC. IL
PAUL . VERSCHURE @ IUA . UPF. EDU

SPECS. Universitat Pompeu Fabra
The Polish psychologist Konorski proposed in the early 1960ies that the associative processes underlying classical conditioning can be separated in a fast non-speci&#64257;c or preparatory learning systems
(NLS) and a slow speci&#64257;c or consummatory one (SLS). In earlier work we have successfully developed biologically based computational models of the roles of the amygdala, basal forebrain and auditory
cortex (AC) in the NLS, and the cerebellum (CE) as the SLS [1,2]. This has led us to propose that the
NLS should be seen as a stage of stimulus identification that identifies which specific event in the environment is predictive of an upcoming salient event such as an unconditioned stimulus. We have tested this
hypothesis in our robot-based cognitive architecture Distributed Adaptive Control [5]. Thus far, however, it
is not clear whether the same holds for the biological system. Here our aim is twofold. On the one hand
we develop a theoretical framework able to analytically describe the learning dynamics of the SLS in a
classical conditioning task. Based on such a novel framework, we identify some mismatches between the
performances of our SLS model and behavioral observations. In particular, behavioral data show that the
number of trials required to acquire a conditioned response critically depends on the inter-stimulus interval
[3]. Based on our theoretical model, we are able to consistently extend the SLS model in order to more
robustly and reliably match the learning dynamics observed in behavioral experiments. This is achieved by
introducing spike-timing dependent plasticity at the parallel fibers to Purkinje cell synapses (pf-PU). This is
in complete agreement with in vitro studies of LTD at the pf-PU synapses [4], where it has been show that
LTD is greatest when pf activation precedes the climbing fiber activation within 50-200 ms [4]. Moreover,
we evaluate the learning performances of the extended composite model at the circuit level in simulated
conditioning experiments, as well as at the behavioral level using a mobile robot. We demonstrate that the
model provide a complete account of Konorski’s proposal by displaying both a NLS and a SLS under realworld conditions, and that the learning dynamics of the two subsystems consistently match behavioral data.
References [1] Learning sensory maps with real-world stimuli in real time using a biophysically realistic
learning rule. M.A. Sanchez-Montanes, P. Koenig and P.F.M.J. Verschure, IEEE Trans. on Neural Networks
13:619-632, May 2002. [2] The cerebellum in action: a simulation and a robotics study. C. Hofstoffer, M.
Mintz and P. Verschure. European Journal of Neuroscience 16:1361-1376, July 2002. [3] Conditioning and
Associative Learning: NJ Mackintosh. Oxford University Press, USA. 1983. [4] Order-Dependent Coincidence Detection in Cerebellar Purkinje Neurons at the Inositol Trisphosphate Receptor. Dmitry V. Sarkisov
and Samuel S.-H. Wang. The Journal of Neuroscience, 2008, 28(1):133-142. [5] Environmentally-mediated
synergy between behaviour and perception in mobile robots,Verschure, P. and Voegtlin, T. and Douglas, R.
Nature, 425. 620-624. 2003.
doi:10.3389/conf.neuro.06.2009.03.206

COSYNE 09

99

I-60

I-60. Possible differential sensitivity to the matrix vs core thalamocortical systems by MEG vs EEG
Nima Dehghani1,2
Eric Halgren
Syd Cash
1
2

NIMA @ UCSD. EDU
EHALGEN @ UCSD. EDU
SCASH @ PARTNERS . ORG

MGH, Neurology Dep. Harvard
Multimodal Imaging Lab, UCSD

Nima Dehghani 1,2, Eric Halgren 1, Syd S. Cash 2 (1) Dept. Neurosciences and Radiology, University
of California San Diego (2) Dept. Neurology, Massachusetts General Hospital, Harvard Medical School.
Sleep spindles are bursts of rhythmic 7-15Hz activity, lasting 0.5-2s, that occur during normal stage 2
sleep. In intact animals, sleep spindles appear to be synchronous across the cortex and thalamus [1].
Scalp EEG studies in humans support this as well. We recorded 306 channels of MEG simultaneously
with 60 channels of EEG during naturally-occurring spindles in 7 healthy subjects. EEG recordings were
highly coherent across the scalp but the MEG recordings clearly were not. In some cases, spindles were
apparent in MEG but not in EEG. These discrepancies were most apparent in comparisons of referential
EEG to MEG gradiometers; bipolar EEG and MEG magnetometers presented intermediate characteristics.
MEG spindles varied in frequency and phase across locations, spindles, and early vs late segments of
spindles. In contrast, EEG spindles were much more uniform but showed an overall tendency to be higher
frequencyin posterior sites at the beginning of spindles. Source localizations were estimated with a noisenormalized cortically-constrained minimum norm. The location and timing of cortical activity inferred from
EEG had a low correlation with that inferred from MEG. These results were confirmed and extended with
invasive recordings, obtained in 5 patients during presurgical diagnosis of partial epilepsy. In subdural grid
recordings, widespread channels often displayed quasi-synchronous activity during spindles. However, at
the same time, other channels, especially those showing the largest spindles, were clearly out of phase
with the other channels. Transcortical bipolar depth recordings of focal spindle activity showed that strong
dissociations in phase and amplitude between simultaneously recorded sites is the rule rather than the
exception during spindles. Population trans-membrane neuronal currents (current-source density) were estimated in each cortical layer using laminar multi-microelectrode arrays. These recordings indicated two
patterns during spindles. One, with sinks in the middle layer, suggests activation of the core thalamocortical system, which carries focal activity. The other pattern, with sinks in the uppermost cortical layer(s),
suggests activation of the matrix thalamocortical system, which carries diffuse activity. Biophysical considerations suggest that the EEG would be mainly sensitive to activation of the Matrix system [2], with each
sensor summating activity generated in the crowns of multiple gyri. In contrast, the MEG should be mainly
sensitive to activation of the Core system [2], with each sensor recording only asynchronous and thus noncancelling activity in nearby sulci. This suggests that the widespread synchronous spindles recorded by
EEG may reflect actual widespread synchronous generators that are poorly recorded by MEG due to spatiotemporal cancellation. Conversely, the wide spatial summation of EEG renders it relatively insensitive to
the asynchronous focal generators detected by MEG. It appears that EEG and MEG are differentially sensitive to distinct thalamocortical spindle generating subsystems. [1] Contreras D, Destexhe A, Sejnowski
TJ, Steriade M. Spatiotemporal patterns of spindle oscillations in cortex and thalamus. J Neurosci. 1997
Feb 1;17(3):1179-96.Click here to read Links [2] Jones EG. The core and matrix of thalamic organization.
Neuroscience. 1998 Jul;85(2):331-45.
doi:10.3389/conf.neuro.06.2009.03.279

100

COSYNE 09

I-61 – I-62

I-61. High-performance halorhodopsin variants for improved geneticallytargetable optical neural silencing
Brian Chow1,2
Xue Han
Xiaofeng Qian1
Mingjie Li
Edward Boyden3,1

BCHOW @ MEDIA . MIT. EDU
XUEHAN 1@ GMAIL . COM
QXIAOFENG 2000@ YAHOO. COM
MINGJIECN @ GMAIL . COM
ESB @ MEDIA . MIT. EDU

1

MIT Media Lab
MIT Biological Engineering & Brain Cog Sci
3
Massachusetts Institute of Technology
2

Recently developed optogenetic tools to modulate transmembrane potential - such as the blue light activated cation channel, channelrhodopsin-2 (ChR2), and yellow light-activated chloride pump, N. pharaonis
halorhodopsin (Halo/NpHR) - offer many advantages over physical electrodes for brain circuit mapping,
neural prostheses, and therapeutic applications. For example, they can be genetically-targeted to specific
cell types, conduct specific ions, and enable large populations of cells to be stimulated or silenced in a
spatio-temporally complex manner. We here present a novel set of halorhodopsin variants with improved
currents and kinetics. Halo2 has currents that are 200% higher than wild-type Halo and 33% higher than
the targeting-improved molecule eNpHR (Gradinaru et al., Brain Cell Biol. 2008 August; 36(1-4): 129–139).
Halo3 has currents that are 165% higher than wild-type Halo, and additionally, unlike other silencing opsins
described to date, can effectively recover from light-induced inactivation in the dark (unlike N. pharaonis
halorhodopsin, which requires blue light exposure for full recovery, as shown in Han and Boyden, PLoS
ONE, 2007. 2(3): p. e299). Implications of the new expanded class of silencing opsins will be discussed. *
The first three authors contributed equally to this work.
doi:10.3389/conf.neuro.06.2009.03.347

I-62. Adaptation in simple neurons: dependence of feature selectivity
on stimulus statistics
Michael Famulare
Adrienne Fairhall

FAMULARE @ U. WASHINGTON . EDU
FAIRHALL @ U. WASHINGTON . EDU

University of Washington
The relationship between a neuron’s complex inputs and its spiking output defines the neuron’s coding strategy. A linear/nonlinear cascade model—a linear filter or set of filters that extracts the features of the stimulus
that are relevant for triggering spikes and a nonlinear function that relates stimulus to firing probability—
captures much of the computation of single neurons. In many sensory systems, these two components of
the coding strategy adapt to changes in the statistics of the inputs, in such a way as to improve information
transmission. Here we explore analytically the contributions to adaptation to stimulus statistics that are due
to neuronal dynamics, without change in the underlying neuronal parameters. Following the pioneering work
of Hodgkin and Huxley, it is known that the space-clamped behavior of a neuron can be accurately modeled
by a nonlinear dynamical system. Thus, to understand adaptation in the phenomenological LN model, we
want to understand how an LN model arises from the underlying dynamical system. Here, we consider the
simplest intrinsically-spiking model neuron: the Quadratic Integrate-and-Fire model, or QIF. First, we use
reverse correlation methods to find the elements of the LN model corresponding to the QIF. The simplest
correlation measure is the Spike-triggered Average stimulus, or STA, which is the mean stimulus preceding

COSYNE 09

101

I-63
a spike; the STA is the optimal linear filter for firing rate estimation and is a good choice for the filter in an LN
model. For the QIF, for Gaussian white noise stimuli with constant mean, the STA is found empirically to be
a function of the variance—the LN model of the QIF adapts to stimulus statistics. To gain an understanding
of how the STA adapts, we present methods to analytically derive the outcome of the reverse correlation
analysis. As the low variance limit has been explored previously by others, we focus here on applying a
technique known as Stochastic Linearization to calculate the STA in the high variance limit. This technique
captures the affects on the sampled features due to how driving the neuron with different stimulus statistics
changes the exploration of the subthreshold regime in the approach to a spike. This method provides us
with explicit expressions for how the STA depends on the spike-generating mechanism, the spike history,
and the variance of the stimulus. While we focus here on a very simple model, the techniques and the
conceptual picture they embody generalize to more complex models. Further, our results underscore the
difficulty of inferring underlying biophysical parameters from the output of reverse correlation, independent
of a consideration of the stimulus properties.
doi:10.3389/conf.neuro.06.2009.03.015

I-63. Decoding of stimulus velocity using a model of ganglion cell
populations in primate retina.
Edmund Lalor1
Yashar Ahmadian2
Jonathan Pillow3,4
Eero Simoncelli5
Liam Paninski2

EDLALOR @ TCD. IE
YASHAR @ STAT. COLUMBIA . EDU
PILLOW @ MAIL . UTEXAS . EDU
EERO. SIMONCELLI @ NYU. EDU
LIAM @ STAT. COLUMBIA . EDU

1

Trinity College Dublin
Columbia University
3
Center for Perceptual Systems
4
University of Texas at Austin
5
HHMI/NYU
2

The encoding of visual stimuli in the spike trains of retinal ganglion cells (RGCs) places limitations on subsequent visual processing. Here, we examine such limitations in the context of visual motion estimation.
We describe the encoding of visual stimuli in spike trains using a recently developed statistical model for
RGC populations (Pillow et al., Nature 454(7206): 995–999, 2008). The model includes spike-history effects and cross-coupling between cells of the same kind and of different kinds and, accurately captures the
stimulus dependence and spatio-temporal correlation structure of population responses. It also provides a
relatively tractable expression for the probability of observing a given spike train, conditioned on the stimulus. Based on this model-based likelihood function, we construct an optimal (Bayesian) estimator for image
velocity given the population spike train response. We implement two variants of this decoder. In the first,
we assume the visual input is formed by translation of a known spatial intensity image. In the second, we
assume only the (naturalistic) correlation structure of the intensity image is known, but do not know the
image explicitly. Finally we explore a biologically-plausible “motion energy” method for decoding the velocity and show that, as with estimators based on spatio-temporal gradients, there is a close mathematical
connection between this energy method and the optimal Bayesian decoder in the case that the image is not
known. Through simulations, we show that the performance of the Bayesian decoder is less accurate with
decreasing prior image information. Simulations across several different speeds and contrasts of a moving
bar stimulus reveal that the Bayesian decoder with full image information achieves an average speed estimation precision of 2.7%, while the motion energy method results in an average speed estimation precision
of only 7.8% across the same set of conditions. For both methods, estimation precision is shown to be bet-

102

COSYNE 09

I-64
ter for more slowly moving stimuli and for stimuli with higher contrast. Human psychophysical performance
on short duration velocity estimation tasks seems to be much better represented by the performance of the
model in the case that the image is not known exactly a priori. Finally, we show that estimation performance
is shown to be rather insensitive to the details of the precise receptive field location, correlated activity
between cells, and spike timing.
doi:10.3389/conf.neuro.06.2009.03.022

I-64. Decoding dynamic patterns of neural activity using a ‘biologically plausible’ fixed set of weights
Ethan Meyers1
David Freedman2
Gabriel Kreiman3,4
Earl Miller1
Tomaso Poggio1

EMEYERS @ MIT. EDU
DFREEDMA @ BSD. UCHICAGO. EDU
GABRIEL . KREIMAN @ CHILDRENS . HARVARD. EDU
EKMILLER @ MIT. EDU
TP @ AI . MIT. EDU

1

Massachusetts Institute of Technology
The University of Chicago
3
Children’s Hospital Boston, Harvard Med.Sch.
4
Center for Brain Science, Harvard University
2

Recent work using population decoding methods has shown that visual information in the inferior temporal
and prefrontal cortex of macaque monkeys is contained in changing patterns of activity across a population
of neurons. These changing patterns are due to many neurons having short time windows of selectivity
relative to the length of a trial (Meyers et al. 2008). Optimally decoding these changing patterns of activity
over short time periods (e.g., 150ms time periods) using a linear decoder requires that different weights
are used at different time periods (Meyers 2008, Nikolic 2007). If these weights are interpreted as synaptic strengths (as is commonly assumed in neural network interpretations of decoding algorithms) then this
creates a biologically unlikely model whereby the synaptic connections between neurons change within the
timecourse of a single trial, consistently across trials of a given type, in a way that is time locked to the
stimulus onset. In this work, we examine how decoding accuracy is affected by using a more biologically
realistic ‘fixed’ set of weights, and by using different temporal binning schemes. We show that decoding
accuracy does not decrease when longer time bins (up to 600 ms) are used for decoding. Furthermore,
when decoding over these relatively long time intervals, adding separate weights for different sub-bins only
marginally increases the decoding accuracy compared to using one fixed set of weights. This suggests
that including firing activity outside of many of the neuron’s time windows of selectivity does not drastically
decrease the information that can be decoded within that time window. Additionally, we examine the consequence of using a fixed set of weights based on using the average firing rate over the whole trial, and
find that the results at most time points are only moderately lower than when we use dynamically changing
weights. Thus, a high degree of accuracy can still be achieved using a set of weights that is fixed over the
course of a trial.
doi:10.3389/conf.neuro.06.2009.03.354

COSYNE 09

103

I-65

I-65. A decoder-based spike train metric for analyzing the neural code
in the retina
Yashar Ahmadian1
Jonathan Pillow2,3
Jonathon Shlens4
Eero Simoncelli5
EJ Chichilnisky6
Liam Paninski1

YASHAR @ STAT. COLUMBIA . EDU
PILLOW @ MAIL . UTEXAS . EDU
SHLENS @ SALK . EDU
EERO. SIMONCELLI @ NYU. EDU
EJ @ SALK . EDU
LIAM @ STAT. COLUMBIA . EDU

1

Columbia University
Center for Perceptual Systems
3
University of Texas at Austin
4
Salk Institute
5
HHMI/NYU
6
The Salk Institute
2

Spike trains of retinal ganglion cells exhibit variability in response to repeated presentations of the same
stimulus. Thus the exact timing and configuration of spikes is not enitrely relevant for visual signaling. Here,
based on a Bayesian decoder, we devise a spike train metric [1] which measures the distinguishability of two
given sets of spike trains, and allows for quantifying the importance of different spikes and spike patterns in
encoding stimuli. The decoder is based on a generalized linear model (GLM) which accurately predicts how
a group of neurons transform stimuli (spatiotemporal contrast fields) into spikes, and accounts for history
dependencies and interactions between cells. The model has been fit to multi-electrode recordings from
macaque retina [2]. Given the observed spike trains, the decoded stimulus may be obtained by maximizing
the posterior probability arising from the GLM. We define the distance between two spike trains to be
the metric distance between their associated decoded stimuli. This metric is entirely determined by the
properties of the GLM, the stimulus ensemble, and the stimulus metric, and has no free parameters of its
own. Direct calculation of this metric would seem to require computation time that scales quadratically with
stimulus duration, rendering it essentially unusable. But by exploiting the likelihood concavity and temporal
quasi-locality of the GLM, and properties of banded matrices, we have devised a novel method for finding
the posterior maximum in a computational time that scales linearly with the stimulus duration. Because
the Bayesian decoder is nonlinear, the stimulus information encoded by a spike is not fixed and depends
on its context. We use our metric to define two types of costs associated with spike-train variability, in the
spirit of [1]. For each spike, we define its addition/removal cost to be the distance between two spike trains
differing only in the presence of that spike, and define its jitter sensitivity to be the distance accrued per
unit time shift of that spike, in the limit of small shifts. We studied the statistics of these quantities and
the factors influencing them (the local firing rate, synchrony with spikes in neighboring cells, etc.) in the
recorded data. The decoder nonlinearity results in large variations in these quantities across spikes; by
contrast, a linear decoder would yield constant values. Moreover, we found that the relative cost of spike
shifts vs. removals and additions exhibits less variability; on average jittering a spike time by 10 ± 2ms
was equivalent to removing it. Finally, we show that small lossy compressions of spike trains, which coarse
grain the (collective or single spike) degrees of freedom optimally according to relevance, are dictated by
the local limit of our metric. As an example, for nearly synchronous spikes in neighboring cells, the optimal
compression retains the relative timing information with higher resolution than the average (absolute) timing.
Interestingly, compression based on a linear decoder would coarse grain both degrees of freedom equally.
[1] J.D. Victor, Curr. Op. Neurobiol. 15, 585 (2005). [2] J.W. Pillow et al., Nature 454, 995 (2008).
doi:10.3389/conf.neuro.06.2009.03.318

104

COSYNE 09

I-66

I-66. Precise spike synchronization in the gamma band increases information gain in awake monkey V1
Thilo Womelsdorf1,2
Bruss Lima3
Martin Vinck
Robert Oostenveld1,2
Wolf Singer
Sergio Neuenschwander3
Pascal Fries

T. WOMELSDORF @ FCDONDERS . RU. NL
BRUSS @ MPIH - FRANKFURT. MPG . DE
MARTIN . VINCK @ FCDONDERS . RU. NL
R . OOSTENVELD @ FCDONDERS . RU. NL
SINGER @ MPIH - FRANKFURT. MPG . DE
NEUENSCHWAND @ MPIH - FRANKFURT. MPG . DE
PASCAL . FRIES @ FCDONDERS . RU. NL

1

Donders Inst for Brain, Cognition & Behaviour
Radboud University Nijmegen
3
Max-Planck Institute for Brain Research
2

Cortical information processing relies on efficient communication among those neurons coding for the veridical information about sensory inputs. This entails two fundamental functions subserved by neuronal circuitry, including (i) the robust encoding of sensory information, and (ii) a mechanistically efficient transfer
and communication of encoded information between neuronal groups. Approaches so far have considered
both aspects as largely independent, but here, we propose that encoding and transfer are tightly linked and
subserved by rhythmic neuronal synchronization in the gamma frequency band. Previous work has shown
that neuronal communication is rendered particularly efficient during periods of precise gamma-band synchronization [1]. It is unclear, however, whether synchronization of neuronal spike times to the gamma band
rhythm of a local neuronal group is also modulating the information gain of the spiking response and thereby
contributes also to a robust and reliable encoding of sensory information [2, 3]. We therefore hypothesized
that the local gamma band rhythm provides a temporal reference frame to render those spikes particularly
informative about sensory inputs, which are also the most efficient spikes in affecting postsynaptic target
neurons. To test this hypothesis, we simultaneously recorded the spiking activity of single isolated units and
the local field potentials (LFPs) in area V1 of three awake macaque monkeys performing a fixation task.
During fixation, neuronal sites were stimulated with moving gratings of eight different orientations. For each
orientation, we calculated the neuronal spiking responses as a function of the LFP gamma-band phase
based on standard Hanning windowed Fourier transforms of the LFP. This allowed us to quantify neuronal
information about stimulus orientation for spikes occurring in different time periods within the gamma cycle.
Similar to previous studies, we find that spike rates and spike-LFP synchronization are both tuned to orientation. However, stimulus information varied across the gamma cycle with maximal information gain for
spikes at the preferred gamma phase of a neuron. Surprisingly, the gamma cycle allowed to obtain maximal
information about stimulus orientation with less then 20% of all spike occurrences of a particular neuron:
More information was gained from the most gamma-locked spikes than from all spikes taken together, i.e.
from the spike count. These findings strongly suggest that rhythmic gamma band activity provides a critical
temporal reference which renders some spikes to be informative about sensory inputs, while other spikes
are rendered redundant and non-informative. Taken together, the study demonstrates a synergy between
neuronal encoding mechanisms and neuronal communication mechanisms, revealing that those spikes that
are mechanistically most effective in affecting postsynaptic target neurons, are also functionally the most
informative. Acknowledgments This research was supported by The Netherlands Organization for Scientific
Research, grant 452-03-344 (P.F.) and grant 016-071-079 (T.W.) References [1] Womelsdorf T, Schoffelen
JM, Oostenveld R, Singer W, Desimone R, Engel AK, Fries P (2007) Modulation of neuronal interactions
through neuronal synchronization. Science 316:1609-1612. [2] Sejnowski TJ, Paulsen O (2006) Network
oscillations: emerging computational principles. J Neurosci 26:1673-1676. [3] Fries P, Nikolic D, Singer W
(2007) The gamma cycle. Trends Neurosci 30:309-316. 7.
doi:10.3389/conf.neuro.06.2009.03.101

COSYNE 09

105

I-67

I-67. Temporal processing with plastic short term synaptic dynamics
Robert Guetig1,2
Haim Sompolinsky2,3
Misha Tsodyks4

GUETIG @ CC. HUJI . AC. IL
HAIM @ FIZ . HUJI . AC. IL
MISHA @ WEIZMANN . AC. IL

1

Racah Institute of Pysics
Center for Neural Computation
3
Center for Brain Science
4
Weizmann Institute of Science
2

Sensory processing such as speech and music perception, time interval estimation, or motion processing
requires the central nervous system to extract and process information encoded in the temporal domain.
Psychophysical studies of interval discrimination tasks have highlighted the brain’s ability to decode temporal features of incoming stimuli over timescales ranging from tens-of-milliseconds to seconds. However,
the neural mechanisms that underlie such processing on the basis of the spiking activity of sensory neural
populations are unknown. Previous models have suggested that short-term synaptic plasticity is a key element in spike-based temporal computations[1]. These models have been limited by the lack of an efficient
spike-based learning rule that is capable of tuning the dynamics of synaptic connections to the requirements
of a given processing task. To overcome this limitation, we have derived a novel supervised spike-based
learning rule for dynamical synapses[2], extending our recently proposed tempotron model[3]. By jointly
acting on pre-synaptic release probabilities and post-synaptic response amplitudes as well as on synaptic
recovery and facilitation time constants, this modification can change the dynamical properties of a given
synaptic connection on the behaviorally important time scale of several hundred milliseconds. An ‘integrateand-fire’ model neuron with plastic dynamical synapses exhibits high capacity to decode spike-timing based
information in the temporal domain. Moreover, by decoupling the processing of temporal information from
the somatic voltage integration, plastic synaptic dynamics allow neurons to multiplex temporal features that
arrive at different synaptic sites with high robustness to interference and neural noise. While the non-linear
dependence of dynamic synaptic transmission on the input spike history allows the post-synaptic cell to
process information in the temporal domain, the same history dependence limits a neuron’s capability to
process continuous streams of incoming spikes that prevent synapses from relaxing to their resting states.
We demonstrate that this ”reset” problem[1] can be overcome within populations of dynamical synapses.
By appropriately aligning their dynamics, the effects of previous spikes can be effectively canceled. For
instance a neuron can learn to detect a certain inter-spike interval independently of the preceding spikes.
This active spike ”forgetting” mechanism requires only one additional synapse per spike canceled. Importantly, this spike cancellation mechanism operates for arbitrary times of the previous spikes. As a result
post-synaptic neurons can learn to process temporal information embedded within a continuous stream of
inputs. A comparison between our model and the psychophysics of interval discrimination is carried out.
Experimental studies have suggested a plethora of putative interaction sites between synaptic dynamics
and long-term synaptic changes. Our new learning rule offers a computational rationale for such interactions. These results uncover powerful temporal computational capabilities of dynamical synapses within
spiking neural networks. We suggest that training the synaptic dynamics could be mediated by long-term
modifications in short-term synaptic plasticity in neocortex[4]. [1] Karmarkar UR, Buonomano DV (2007),
Neuron 53: 427-438. [2] Tsodyks M, Pawelzik K, Markram H (1998), Neural Comput 10: 821-835. [3]
Guetig R, Sompolinsky H (2006), Nat Neurosci 9: 420-428. [4] Markram H, Tsodyks M (1996), Nature 382:
807-810.
doi:10.3389/conf.neuro.06.2009.03.111

106

COSYNE 09

I-68 – I-69

I-68. Which model can properly describe dynamics and smoothness
of firing rate?
Ken Takiyama1
Kentaro Katahira1,2
Masato Okada
1
2

TAKIYAMA @ MNS . K . U - TOKYO. AC. JP
KATAHIRA @ MNS . K . U - TOKYO. AC. JP
OKADA @ K . U - TOKYO. AC. JP

The University of Tokyo
RIKEN Brain Science Institute

Firing rate, which is assumed to be inherent in the observed spike trains, plays an important role in decoding.
Though estimating the true firing rate requires sufficiently many spike trains, we have to estimate the firing
rate using limited spike trains that are obtained from a neurophysiological experiment. In such a situation,
estimating firing rate with probabilistic model is effective. Selecting the proper probabilistic model that can
describe both the dynamics and the appropriate smoothness of the firing rate hence is one of the issues
in neuroscience. We construct the algorithm that can simultaneously estimate the firing rate, selects the
proper model, and determine the proper smoothness depending on the functional form of the firing rate
using belief propagation (BP). Since the BP can calculate the marginal likelihood, we can select the proper
model and determine the proper smoothness based on model selection framework and empirical Bayes
method, respectively. The prior distributions are Line process model, Gaussian model, and Cauchy model,
which corresponds to the special case of Student’st model. These models can describe a priori knowledge
that the firing rate evolves smoothly. Both the Line process model and the Cauchy process model can also
express the discontinuous variation. We estimate two kinds of the firing rates: The first firing rate shows
temporally smooth fluctuation, but the second firing rate includes discontinuous variation. We discuss which
model is the best in each firing rate estimation experiment. Estimating the firing rate using a limited number
of spike trains involve the assumption that the firing rate includes smoothness of which both the form and the
degree are unknown. The Line process model, in the first experiment, gives the largest marginal likelihood
value of the three models, which implies that the Line process model can appropriately express both the
form and the degree of the firing rate smoothness. On the other hand, stimulus added to the neuron causes
the discontinuous variation to the firing rate. Since many neurophysiological experiments observe the neural
responses to stimuli, the algorithm that can estimate the firing rate involving discontinuity is needed. But
there has yet to be the algorithm if the stimulus timings are entirely-unknown. We show, in the second
experiment, the Line process model can also describe the discontinuous firing rate the best. The Line
process model being able to estimate the unknown stimulus timings, we show the effectiveness of the Line
process model on estimating the firing rate involving the discontinuous variation.
doi:10.3389/conf.neuro.06.2009.03.256

I-69. How far the decoding process in the brain can be simplified?
Masafumi Oizumi
Toshiyuki Ishii
Kazuya Ishibashi
Toshihiko Hosoya
Masato Okada

OIZUMI @ MNS . K . U - TOKYO. AC. JP
TISHII @ BRAIN . RIKEN . JP
KAZUYA @ MNS . K . U - TOKYO. AC. JP
HOSOYA @ BRAIN . RIKEN . JP
OKADA @ K . U - TOKYO. AC. JP

The University of Tokyo
How information about stimuli is decoded from noisy neural activities is an important but poorly understood
question in neuroscience. Because of the lack of knowledge about neural decoding, we often assume an

COSYNE 09

107

I-70
optimal decoder and evaluate the accuracy of a neural code when the optimal decoder is used. The mutual
information is a widely used quantity which evaluates how much information about stimuli can be extracted
from neural responses by the optimal decoder. However, taking account of the complexity of the optimal
decoding, the assumption that information is optimally decoded in the brain is doubtful. It is more natural to
think that information is decoded in a suboptimal manner by a simplified decoder. In this study, we evaluate the amount of information that can be obtained by simplified decoders in population activities of retinal
ganglion cells. By comparing the mutual information and the amount of information that can be extracted
by simplified decoders, we investigate how far the decoding process can be simplified in the retina. To
address this issue, we need to consider two problems. One is how the amount of information that can be
extracted by simplified decoders is quantified. The other one is how the simplified decoders are constructed.
To evaluate the amount of information for simplified decoders, we use an information theoretically correct
quantity which was derived by Merhav et al. (Merhav et al., 1994). This quantity is an extension of the
mutual information in the context of error correcting code. Regarding the method of constructing simplified
decoders, we employ the maximum entropy method. When population activities are analyzed, we have to
deal with not only second-order correlations but also higher-order correlations in general. Therefore, we hierarchically construct simplified decoders that account of up to Kth-order correlations, where K=1,2,...,N, by
the maximum entropy method. By computing how much information is obtained by the simplified decoders,
we investigate how many orders of correlation should be taken into account to extract enough information.
We apply the theoretical framework described above to the spike data obtained from retinal ganglion cells
responding to a natural movie. We find that more than 90% of the information can be extracted from the
population activities of ganglion cells even if all orders of correlations are ignored in decoding. In this analysis, we regard a 100-ms-long natural movie as one stimulus. If we consider stimuli that are long compared
with the speed of the change in the firing rates as one stimulus, we find that correlation seems to carry a
large portion of information. This result shows the problem of the previous study (Schneidman et al., 2006)
in which the stationarity of neural responses is assumed for a duration that is too long. When the length
of the stimulus is appropriately set, the information loss caused by ignoring correlations is negligibly small.
Our results imply that the brain uses a simplified decoding strategy in which correlation is ignored.
doi:10.3389/conf.neuro.06.2009.03.264

I-70. Extensions to copula based modeling of spike counts
Arno Onken1,2
Steffen Grünewälder1
Klaus Obermayer1,2
1
2

AONKEN @ CS . TU - BERLIN . DE
GRUENEW @ CS . TU - BERLIN . DE
OBY @ CS . TU - BERLIN . DE

Berlin Institute of Technology
Bernstein Center for Comp Neurosci Berlin

Recently, it has been shown that copula based models can be used to model a rich set of dependence
structures together with appropriate distributions for single neuron variability [1, 2]. In this study, we extend
the copula approach in three ways: Firstly, we present a novel copula transformation with interpretations
for the underlying neural connectivity. The so-called Flashlight transformation is a generalization of the
copula survival transformation and makes it possible to move the tail dependence of a copula into arbitrary
corners of the distribution. We discuss several interpretations with respect to inhibitory and excitatory
connections of projecting populations and demonstrate their validity on integrate and fire population models.
Secondly, we present a mixture approach that enables us to combine different dependence structures and
thereby it allows us to test for different driving processes simultaneously. For example, we combine the
advantages of the Flashlight transformation with the Farlie-Gumbel-Morgenstern family. Furthermore, we
derive an expectation maximization inference method for the mixture model. Thirdly, we address problems
associated with the current approach: 1) The computational complexity restricts the number of neurons that

108

COSYNE 09

I-71
can be analyzed. 2) Typically, not many samples are available for model inference – hence overfitting is
an issue. We consider a second class of probability models: the exponential families of distributions which
allow efficient inference in terms of computation time and sample size. Furthermore, it has been shown that
a subfamily of the exponential families allows Bayes-optimal cue integration in easy manner [3]. Combining
both approaches allows to make efficient inference, while maintaining the interpretability of the spike count
distributions. The tool we use for combining the approaches is the Kullback-Leibler divergence between both
families of distributions. The approaches are applied to data from macaque prefrontal cortex. This work
was supported by BMBF grant 01GQ0410. [1] Onken, A., Grünewälder, S., Munk, M., and Obermayer, K.
(2009), Modeling short-term noise dependence of spike counts in macaque prefrontal cortex. In Advances
in Neural Information Processing Systems 21. MIT Press, 2009. in press. [2] Berkes, P., Wood, F., and
Pillow J. (2009), Characterizing neural dependencies with copula models. In Advances in Neural Information
Processing Systems 21. MIT Press, 2009. in press. [3] Ma W. J., Beck J. M., Latham P. E., and Pouget A.
(2006), Bayesian inference with probabilistic population codes, Nature Neuroscience 9:1432-1438.
doi:10.3389/conf.neuro.06.2009.03.173

I-71. Decoding of regular Purkinje cell spiking based on synaptic depression in a model of a DCN neuron
Johannes Luthman
Reinoud Maex
Rod Adams
Neil Davey
Volker Steuber

J. LUTHMAN @ HERTS . AC. UK
R . MAEX 1@ HERTS . AC. UK
R . G . ADAMS @ HERTS . AC. UK
N . DAVEY @ HERTS . AC. UK
V. STEUBER @ HERTS . AC. UK

University of Hertfordshire
Cerebellar Purkinje neurons have recently been shown to fire more regularly than previously assumed(1).
These regular spike patterns could affect the behaviour of neurons in the deep cerebellar nuclei (DCN),
the main targets of the Purkinje cells and the primary output neurons of the cerebellum. We investigated
the effect of regular versus irregular Purkinje neuron firing in a conductance based model of an excitatory
DCN projection neuron(2). The model was implemented in NEURON and contained 517 compartments,
each with up to ten ion channel mechanisms and GABA-A, AMPA, and NMDA receptor synapses. The
GABAergic synapses between Purkinje cells and DCN neurons undergo short-term depression (STD)(3).
Our model included this STD of the inhibitory synapses, based on a recent model by Shin et al. (2007)(1).
We presented the DCN model with synaptic input at constant mean rates and with varying degrees of
regularity, ranging from completely regular spike patterns to trains with interspike intervals (ISIs) that were
drawn randomly from a third degree gamma distribution. We found that the degree of regularity of the
Purkinje cell input affected DCN spiking, with increased firing rates for irregular Purkinje cell input trains. The
noise-dependent increase of DCN neuron firing rates occurred to a statistically significant extent for synaptic
input rates of 5-140Hz. Larger input irregularities resulted in larger increases in firing rate. Moreover, the
maximal increase in DCN neuron spiking of 20.5% (from 31.0Hz to 37.3Hz) was reached for Purkinje cell
input at 80Hz, which is close to mean spontaneous Purkinje cell firing rates in awake rats(4). The increase
in DCN spike rate depended on synaptic depression; the effect disappeared in the absence of STD at the
GABAergic synapses. The predicted effect of irregular Purkinje cell spiking has medical implications: an
increased irregularity of Purkinje cell firing has been found in ataxic mouse models, in which the induction
of more regular firing led to amelioration of the symptoms(5, 6). 1. Shin, S. L. et al. PLoS ONE 2, e485
(2007). 2. Jaeger, D. et al. Soc Neurosci Abstr 179.11. (2005). 3. Telgkamp, P. & Raman, I. M. J Neurosci
22, 8447-57 (2002). 4. Savio, T. & Tempia, F. Exp Brain Res 57, 456-63 (1985). 5. Hoebeek, F. E. et al.
Neuron 45, 953-65 (2005). 6. Walter, J. T. et al. Nat Neurosci 9, 389-97 (2006).

COSYNE 09

109

I-72
doi:10.3389/conf.neuro.06.2009.03.240

I-72. Overcoding-and-paring: a bufferless neural chunking model
Gerard Rinkus

GRINKUS @ BRANDEIS . EDU

Brandeis University
Chunking is the process by which a single, purely spatial code, comes to represent a sequence of other
purely spatial codes, the items. To assign unique chunk codes to sequences having common first items
or prefixes, e.g., ‘car’ and ‘cat’, the brain must wait at least until a sequence’s first distinguishing item is
presented. However, for the chunk code to be associated with all sequence items via accepted neural
learning principles, which require pre- and post-synaptic coactivity, the chunk code must activate during the
first item and remain active throughout the sequence. One way to reconcile these opposing constraints is to
store items in a first-in-first-out queue—a short-term memory (STM) buffer—consisting of several disjoint,
functionally-equivalent (apart from ordinal position represented) fields, or slots. This allows all items to be
co-active, allowing the chunk code to be chosen after all items are present and thus be a function of the
entire sequence. Most existing chunking models follow this approach [1-4]. However, there is no direct
neurophysiological evidence for such an STM buffer, and there is mounting evidence against this view [5].
I therefore propose a radically different, bufferless, chunking/STM model, called, overcoding-and-paring
(OP). Implicit in the following model description are two assumptions: i) chunks/items are represented by
sparse distributed codes, and ii) cells at higher levels, e.g., in the chunking field, have a longer intrinsic
activation duration—or, persistence—than lower levels cells. Suppose a two-item sequence, ‘AB’, is to be
chunked. At t=1, item A is activated in an input field and gives rise to an oversized sparse distributed code—
an overcode—in a chunking field. A is then bidirectionally and transiently associated with the overcode.
When B presents at t=2: a) it causes some of the overcode’s cells to deactivate, or, be pared out, leaving
behind the final chunk code; b) the transient synaptic increases between A and the pared-out cells are
depotentiated; c) the transient increases between A and the surviving cells are made permanent; and d)
B is bidirectionally associated with the surviving cells. This leaves behind a chunk code that depends on
the whole sequence and yet was activated on its first item, reconciling the opposing constraints without the
need for an STM buffer. In essence, the progressive persistence property transfers the STM functionality
needed for chunking from the item level to the chunking level, essentially turning the standard conception
of STM on its head. Studies of neural activity during learning of new associations, reviewed in [6], are
supportive of this scheme. 1. de Groot & Gobet, in Heuristics of the Professional Eye. 1996, Van Gorcum.
2. McClelland & Rumelhart, Psych. Rev., 1981. 88(5) 3. Grossberg, S. in Pattern recognition by humans
& machines... 1986, Academic 4. Agam, Bullock, Sekuler, J Neurophys, 2005. 94(4) 5. Warden & Miller,
Cereb. Cortex, 2007. 17(suppl 1) 6. Suzuki & Brown, Behav Cognitive Neurosci Rev, 2005. 4(2)
doi:10.3389/conf.neuro.06.2009.03.292

110

COSYNE 09

I-73

I-73. Computational models of millisecond level neuronal timing mechanisms
Brandon Aubie
Suzanna Becker
Paul Faure

AUBIEBN @ MCMASTER . CA
BECKER @ MCMASTER . CA
PAUL 4@ MCMASTER . CA

McMaster University
Discrimination of stimulus duration on the order of milliseconds has been observed in behavioural and neurophysiological studies across a variety of species and taxa. Mammalian studies have found neurons in
the auditory midbrain (inferior colliculus) selective for signal duration that are referred to as duration tuned
neurons (DTNs). This study formulates three computational models that amalgamate conceptual models
with physiological responses in the bat auditory midbrain to evaluate the biological plausibility of the proposed neural mechanisms. The computational models employ the adaptive exponential integrate-and-fire
neuron model [1] and Poisson spiking processes to reproduce several responses observed in vivo including DTN response classes, spike counts, first-spike latencies, level tolerance, best duration tuning, and
neuropharmacological effects of inhibitory neurotransmitter antagonists applied to DTNs in vivo. Coincidence Detection Model First proposed by [2], this model produces shortpass or bandpass DTNs via an
onset-evoked excitatory post-synaptic potential (EPSP) and an offset-evoked EPSP that each evoke subthreshold depolarization in the DTN. Delaying the arrival of the onset-evoked EPSP results in both EPSPs
coinciding for stimulus durations within a specific duration window to evoke action potentials in the DTN. For
example, if the onset-evoked EPSP is delayed for 6 ms from stimulus onset and the offset-evoked EPSP
occurs 4 ms from stimulus offset, then both EPSPs will coincide after a 2 ms stimulus. Anti-coincidence
Model Suggested by [3], this model produces shortpass DTNs. The anti-coincidence model has a delayed
onset-evoked EPSP that produces supra-threshold depolarization in the DTN. An onset-evoked IPSP lasting the duration of the stimulus is also present. For short stimulus durations, the IPSP arriving at the DTN
ends before the arrival of the delayed, onset-evoked EPSP (i.e. the two events do not coincide), thus allowing the DTN to produce action potentials. For longer durations, the IPSP and the EPSP overlap to suppress
action potentials in the DTN. Longpass Model Suggested by [4], this model produces DTNs that respond
only to stimuli that are longer than a specific duration. The model incorporates a supra-threshold EPSP at
the DTN lasting for the duration of the stimulus and an adapting IPSP that suppresses activity in the DTN
for only the first several milliseconds to prevent action potentials in the DTN for short duration stimuli. Our
computational model also replicates the increased first-spike latency shift that occurs at higher stimulus
amplitudes (i.e. a paradoxical latency shift) seen in vivo by hypothesizing that the IPSP input activity has
a steeper rate-level function than the EPSP input activity. Each model is supported by neurophysiological
data and is specific to a particular class of DTNs. This study provides biological grounding to previously only
conceptual models to further our understanding of millisecond level neuronal timing mechanisms. [1] Brette
& Gerstner (2005) J. Neurophysiology 94:3637-3642. [2] Narins & Capranica (1980) Brain, Behavior and
Evolution 17(1):48-66 [3] Fuzessery & Hall (1999) Hearing Research 137:137-154 [4] Faure, et al. (2003)
J. Neuroscience 23(7):3052-3065
doi:10.3389/conf.neuro.06.2009.03.244

COSYNE 09

111

I-74

I-74. Temporal response properties in auditory cortex are layer-dependent
G. Bjorn Christianson1
Maneesh Sahani2
Jennifer Linden1
1
2

G . CHRISTIANSON @ UCL . AC. UK
MANEESH @ GATSBY. UCL . AC. UK
J. LINDEN @ UCL . AC. UK

UCL Ear Institute
Gatsby Computational Neuroscience Unit, UCL

In the visual and somatosensory cortices, many neuronal response properties vary systematically across
layers within a cortical column. These laminar differences have provided the foundation for theories about
how cortical columns transform visual and somatosensory information. In the auditory cortex, however,
few clear laminar differences in neuronal response properties have been found, complicating attempts to
understand intracolumnar cortical processing of auditory information. It has been suggested that temporal processing is a major function of the auditory cortex. We hypothesised that layer-dependence might
therefore be most obvious for temporal response properties. To test this hypothesis, we recorded neuronal
responses to short trains of pulsed noise in the mouse primary auditory cortex and the anterior auditory
field. Rates of noise pulse trains varied from 1 to 20 pulses per second (PPS). Responses of neuronal
clusters were recorded simultaneously at different cortical depths, using a single-shank silicon multielectrode with 16 recording sites spaced 100 microns apart. Over 400 cluster recordings were obtained from
a total of 60 penetrations in six mice. Characteristic frequencies were similar for different recording sites
within the same penetration, confirming that the multielectrode array was placed approximately orthogonal
to the cortical surface. Cortical depth estimates for recordings from different penetrations were aligned
relative to the depth at which a reversal of polarity was observed in the early wave of local field potentials (LFPs) recorded during noise pulses. This reference depth likely corresponded to layer II (Kral et
al., Cerebral Cortex 10:714, 2000). Temporal precision of the responses to individual noise bursts was
depth-dependent (p<0.001, Kruskal-Wallis test). The peak precision across PPS conditions, as quantified
by vector strength and related measures of time-locked firing, was highest at the most superficial depths
and decreased with depth. The probability of a response to second and later noise pulses in the pulse
trains was also depth-dependent (p<0.001). Peak response probability across PPS was maximal about
500 microns below the LFP reversal point, a depth presumed to correspond to layer V (Anderson et al.,
Brain Research, in press). The same depth dependencies in temporal precision and response probability
were observed when these measures were computed for a fixed slow PPS, rather than for the preferred
PPS for each recording and response measure. The PPS at which measures of temporal precision were
maximised did not vary significantly with cortical depth, while the PPS at which response probability was
largest showed weak depth-dependence. These results demonstrate that temporal precision of cluster responses is highest in superficial layers, while response reliability is maximal at a depth likely to correspond
to layer V. Thus, there is systematic depth-dependence in the temporal response properties of neuronal
clusters within auditory cortical columns. It remains to be seen whether this arises from layer-dependent
temporal response properties in single units, or layer-dependent changes in the composition of neuronal
clusters.
doi:10.3389/conf.neuro.06.2009.03.137

112

COSYNE 09

I-75 – I-76

I-75. Targeting auditory cortex: combining physiology and anatomy
to identify higher auditory regions
Poppy Crum1
Elias Issa1
Troy Hackett
Xiaoqin Wang2
1
2

POPPY @ JHMI . EDU
ISSA @ MIT. EDU
TROY. A . HACKETT @ VANDERBILT. EDU
XIAOQIN . WANG @ JHU. EDU

Johns Hopkins School of Medicine
Johns Hopkins University

Anatomical studies of human and non-human primates have suggested that the auditory cortex is divided
into hierarchically connected core, belt, and parabelt regions distinguished by both thalamocortical and
corticocortical connectivity (Hackett et al, 1998). However, to date there exists little physiological evidence
for a similarly defined parabelt region. The paucity of physiological data within this region largely results
from lack of reliable in vivo indicators of its boundaries. Previously, we reported a new method to address
this difficulty by measuring the Current-Source-Density (CSD) to register a single electrode penetration as
belonging within a given cortical region. Post-mortem histological staining (PV, vGlut2, AChE, and Nissl)
confirmed a tight relationship between shifting anatomical patterns of lemniscal input and the presence of
a defined layer IV current sink in the CSD. Here, a linear discriminant analysis (LDA) exploiting defined
spectral and temporal features of the CSD was used to determine two planes of maximum separation which
resulted in a highly probable assignment of neural location to a particular anatomically defined region. Additionally, we report our first single neuron and local-field-potential (LFP) recordings from three areas, core,
belt, and parabelt identified with our new approach that links both anatomical and physiological classification of these regions. Using a single tungsten electrode the LFP was measured at 100µ steps orthogonal
to the laminar striations and used to compute the CSD for an individual recording track. These recordings
were made across the medial-to-lateral surface of the awake marmoset auditory cortex. Single-units and
LFPs recorded within a penetration were grouped using boundaries determined with our new method and
histologically as originating from the core, belt, or parabelt. Across these boundaries physiological differentiation was observed for both single-units and LFP responses. These included marked changes in latency,
bandwidth, and synchronization to modulated sounds. Specifically, both latency and spectral tuning width
increased from the core to belt and parabelt whereas the frequency at which cells could synchronize to
modulated sounds decreased along the same dimension. These findings provide evidence for a physiologically defined parabelt region, in addition to sharpening existing core and belt distinctions, and suggest
that CSD-based electrode registration offers a useful measure for identifying the location of recorded neural
responses in the primate auditory cortex across the medial-to-lateral dimension.
doi:10.3389/conf.neuro.06.2009.03.192

I-76. Linking stimulus response properties and functional circuitry in
the mouse auditory cortex
Hysell Oviedo
Ingrid Bureau
Karel Svoboda
Anthony Zador

OVIEDO @ CSHL . EDU
INGRID. BUREAU @ INMED. UNIV- MRS . FR
SVOBODAK @ JANELIA . HHMI . ORG
ZADOR @ CSHL . EDU

Cold Spring Harbor Laboratory
Auditory cortical neurons respond to different stimulus features that are organized across several cortical

COSYNE 09

113

I-77
axes. For example, frequency and sweep selectivity are represented rostro-caudally. More recently, wholecell recordings in vivo have revealed how conductances shape auditory responses. However, there is a
gap in our understanding of the link between stimulus representation and local circuitry. To make a direct
connection, we combined targeted in vivo recordings with functional circuit mapping in vitro. We began
by targeting layer 2/3 (L2/3) excitatory neurons in primary auditory cortex (A1) of juvenile mice using cell
attached recordings and juxtacellular filling for post-hoc identification of individual neurons. Circuit models
from various cortical areas suggest that L2/3 is one of the first relays where cortical computations are
performed and dispersed. We found marked differences in the responses of cells recovered in the most
superficial layer compared to cells recovered 250&#956;m below the surface. Putative L2 cells showed
strong tuning to pure tones of different frequencies and sweep direction, whereas putative L3 cells did not.
To assess whether these differences in stimulus responses translate into functional circuitry differences,
we used laser-scanning photostimulation (LSPS) to map presynaptic connections to L2/3 pyramidal cells in
vitro. For the in vitro study, slices were cut horizontally to preserve circuit organization across the rostrocaudal axis. Brief flashes of ultraviolet light uncage glutamate, causing cells near the flash (<100 &#956;m)
to fire action potentials. If they are synaptically coupled to a cell patched, then this region of cortex is a
source of input. Surprisingly, we found that, unlike other sensory cortices, L4 is not the main source of input
to L2/3 cells, but that instead L5 and 6 provide most of the input. L2 mainly received input from L5 cells
located directly beneath the patched cell (the lateral cortical distance between the pre and postsynaptic
cells was <150 &#956;m), whereas L3 received off-center L6 input (lateral distance increased to 200-300
&#956;m), biased mostly to the higher frequency side of the L3 cell. These distinct input patterns into L2/3
cells were translational, with a separate group of L2/3 cells located roughly 200 &#956;m away from the
first group exhibiting a similar organization. L2/3 cells in auditory cortex also function more independently
than in other cortical areas. The correlation of input maps between nearby cells (within 50 &#956;m of one
another) was roughly half of what was previously determined for neighboring barrel cortex neurons. Our
findings at the circuit level show that the flow of information within the auditory cortex differs from that in
other widely studied sensory cortices (e.g. visual and somatosensory), and provide insight into the stimulus
response findings. L2 shows strong tuning to various stimuli and receives columnar input, whereas L3 is
not responsive to these stimuli and receives off-center input. This combination of in vivo and in vitro circuit
mapping approach is a powerful combination to uncover functional distinctions between the layers and cell
types that execute auditory computations.
doi:10.3389/conf.neuro.06.2009.03.328

I-77. Temporal coding in the olfactory bulb of awake behaving rats
during active sampling.
Kevin Cury
Naoshige Uchida

KCURY @ FAS . HARVARD. EDU
UCHIDA @ MCB . HARVARD. EDU

Harvard University
Many sensory systems, including vision and audition, represent stimuli with temporally precise trains of
action potentials. Might this also be the case in olfaction? Experiments in anesthetized animals have
demonstrated that many Mitral and Tufted cells (MTCs) – the exclusive output neurons of the olfactory bulb
– are entrained by respiration, and that odorant stimulation produces shifts in the timing of spikes within
single breathing cycles. Olfaction in rodents, however, is an active sense much like vision: rats, for example,
exhibit a rapid and stereotyped mode of breathing ( 7-8 Hz), called sniffing, as they explore their olfactory
environment. It is unclear, though, how olfactory processing relates to their rich and dynamic sampling
behavior. Psychophysical experiments suggest a close relationship, as rats are capable of performing
highly accurate odorant discriminations with just a single sniff. Our general goal is to determine the odor

114

COSYNE 09

I-78
coding principles of MTCs during exploratory sniffing in a behaving rodent. Towards this, we monitored
the respiration behavior of rats while simultaneously recording the activity of multiple single MTCs while
the animal engaged in an odor discrimination task in which they exhibit high frequency sniffing during
odorant sampling. One aspect of our analysis explored the timing of spikes relative to their phase within the
respiration cycle. Through the generation of respiration-phase histograms, we were able to identify phaseshifted responses in 21% of our data set (odor-neuron pairs). These responses are comprised of odorand neuron-specific epochs of excitation and inhibition precisely locked to the respiration cycle. Importantly,
these “phase” responses often occur (83% of the cases) in the absence of a significant change in overall
spike count. The above analysis suggests that odorant information is contained in the spike trains of MTCs
on timescales less than the duration of an individual sniffing bout ( 140 ms at 7 Hz). To more explicitly
determine the temporal resolution of response, we employed a linear discrimination analysis on the spike
trains of individual MTCs to identify the optimal integration window for maximum classification of stimuli. The
results of this analysis suggest that optimal discrimination occurs for windows of less than 20 ms. Overall,
these findings demonstrate an intricate relationship between sniffing and the activity of MTCs in awake,
behaving rodents. The identification of responses on fine time scales that do not accompany changes in
firing rate suggests that responses in the olfactory bulb are not as sparse as previously reported. Lastly,
our finding on the optimal discrimination window strengthens the notion that the olfactory system exhibits
temporal coding in the early stages, and provides insight into the putative decoding mechanisms utilized by
downstream processing stages.
doi:10.3389/conf.neuro.06.2009.03.006

I-78. Receptive field size and spike threshold control decoding of information from synchronous afference
Jason Middleton1,2
André Longtin3
Jan Benda4
Leonard Maler3

JMIDDLET @ PITT. EDU
ALONGTIN @ UOTTAWA . CA
BENDA @ BIO. LMU. DE
LMALER @ UOTTAWA . CA

1

Department of Neurobiology
University of Pittsburgh
3
University of Ottawa
4
Ludwig-Maximilians University Munich
2

Parallel sensory streams carrying distinct information about various stimulus properties have been observed in several sensory systems, including the visual system. What remains unclear is why some of
these streams differ in the size of their receptive fields (RFs). Theoretical studies have concluded that,
for a two-dimensional input space, estimation of location parameters is independent of RF size. Optimal
estimation of parameters such as stimulus intensity and spread are dependent on RF size and this might be
related to experimentally reported variations of RF size (or tuning curve width) in various sensory systems.
These studies do not, however, take into account temporal response properties that also appear to correlate
with RF size. In the electrosensory system, of weakly electric fish, pyramidal neurons in the electrosensory
lateral line lobe (ELL), a brainstem nucleus, with large RFs have short latency responses and are tuned
to high frequency inputs. Conversely, neurons with small RFs are low frequency tuned and exhibit longer
latency responses. What principle underlies this organization? We show experimentally that synchronous
electroreceptor afferent (P-units) spike trains selectively encode high frequency stimulus information from
broadband signals. This finding relies on a comparison of stimulus-spike output coherence using output
trains obtained by either summing pairs of recorded afferent spike trains, or selecting synchronous spike
trains based on coincidence within a small time window. We propose a physiologically realistic decoding

COSYNE 09

115

I-79
mechanism, based on postsynaptic RF size and postsynaptic output rate normalization that tunes target
pyramidal cells in different electrosensory maps to low or high frequency signal components. We model
pyramidal neurons in either a simple way, as a current threshold element, or in a biophysically realistic way,
as a conductance-based spiking neural model, constrained by experimentally measured parameters. By
driving these models with our experimentally obtained P-unit spike trains we show that a small postsynaptic
RF is matched with an integration regime leading to responses over a broad range of frequencies, and
a large RF is matched with a fluctuation-driven regime that requires synchronous presynaptic input and
therefore selectively encodes higher frequencies, confirming recent experimental data. The high frequency
information decoded by neurons with large RFs and high output thresholds qualitatively matches the high
frequency information encoded by synchronous afferent spike activity. Thus our work reveals that the frequency content of the broadband stimulus decoded by pyramidal cells depends on the amount of afferent
convergence they receive as well as their output threshold. This principle may operate in other system,
for example the visual system, where the covariance of temporal response properties and RF sizes is also
observed.
doi:10.3389/conf.neuro.06.2009.03.075

I-79. Receptive field maps depend on high order stimulus structure:
evidence for nonlinear feedback
Jonathan Victor
Ferenc Mechler
Anita Schmid
Ifije Ohiorhenuan
Keith Purpura

JDVICTO @ MED. CORNELL . EDU
FMECHLER @ MED. CORNELL . EDU
AMS 2031@ MED. CORNELL . EDU
IFO 2001@ MED. CORNELL . EDU
KPURPURA @ MED. CORNELL . EDU

Weill Medical College of Cornell
The dominant view of primary visual cortex is that typical neuronal responses can be modeled reasonably
well as a simple feedforward cascade of a linear filter followed by a static nonlinearity. Deviations from this
“LN” picture are well recognized, but it is generally thought that they amount to combining multiple such
subunits in parallel, and adding modulatory gain controls. On the other hand, cortical anatomy is characterized by strong feedback, and computational modeling has indicated that feedback is important for shaping
tuning curves and determining the simple-vs.-complex character of responses to gratings. Thus, it is unclear whether the shortcomings of the LN picture are fundamental and qualitative, or merely quantitative.
The direct approach to making this distinction is to create a model of a V1 neuron’s receptive field, complete
with gain controls; to collect sufficient data from individual neurons to determine model parameters; and to
test it with out-of-sample stimuli. Since this is impractical, we took an indirect approach. We created two
basis sets that were matched for luminance, contrast, and spatial frequency content, so that both basis sets
would engage gain controls to the same degree. With the confounding effects of gain controls thus removed,
LN-like neurons should yield identical characterizations with the two basis sets; conversely, deviations from
this prediction would suggest alternative architectures. We implemented this strategy with two-dimensional
Hermite (TDH) functions. TDH functions can be organized into two basis sets: one with Cartesian symmetry, and one with polar symmetry, which are matched for the requisite first- and second-order statistics,
but not for high-order statistics (phase correlations). However, receptive fields determined from these two
basis sets were significantly different in shape or sensitivity in the majority (74/104) of V1 neurons, thus indicating the presence of nonlinear processes inconsistent with LN models. Moreover, a population analysis
indicated that oriented subunits were responsible for these changes (Sharpee & Victor 2008) –implying an
intracortical origin. Finally, although these changes were more prominent in extragranular layers, they were
present in a majority (21/36) of input-layer neurons as well – thus implying a feedback process driven by

116

COSYNE 09

I-80 – I-81
high-order statistics. To isolate the effect of different kinds of high-order statistics, we recently implemented
the matched-basis-set strategy in a more general way. Via methods originally developed to study human
texture perception, we generate binary checkerboard stimuli in which specific third- and fourth-order correlations are present, but second-order correlations are absent. Receptive field maps obtained with these
stimuli show a substantial influence of high-order correlation in most (13/16) V1 neurons – including some
stimuli for which the high-order correlations are not perceptually apparent. This observation challenges
models in which dedicated feedforward circuits extract specific nonlinearities, since it would be surprising
if circuitry had been dedicated to extract correlations that have little relevance for perception. However, a
recurrent nonlinear network could lead to this kind of behavior, since the recurrence effectively leads to
crosstalk among nonlinear elements, thus generating sensitivity to a wide range of spatial correlations.
doi:10.3389/conf.neuro.06.2009.03.219

I-80. Learning Natural Image Structure with a Horizontal Product Model
Urs Köster
Jussi Lindgren
Aapo Hyvärinen

KOSTER @ CS . HELSINKI . FI
JTLINDGR @ CS . HELSINKI . FI
AAPO. HYVARINEN @ HELSINKI . FI

University of Helsinki
We present a novel extension to Independent Component Analysis (ICA), where the data is generated
as the product of two submodels, each of which follow an ICA model, and which combine in a horizontal
fashion. This is in contrast to previous nonlinear extensions to ICA which were based on a hierarchy of
layers. We apply the product model to natural image patches and report the emergence of localized masks
in the additional network layer, while the Gabor features that are obtained in the primary layer change their
tuning properties and become less localized. As an interpretation we suggest that the model learns to
separate the localization of image features from other properties, since identity and position of a feature
are plausibly independent. We also show that the horizontal model can be interpreted as an overcomplete
model where the features are no longer independent.
doi:10.3389/conf.neuro.06.2009.03.012

I-81. A point process model for the parabigeminal nucleus as a recursive estimator
Rui Ma1,2
Todd Coleman2
Joseph Malpeli
1
2

RUIMA 2@ ILLINOIS . EDU
COLEMANT @ ILLINOIS . EDU
JMALPELI @ CYRUS . PSYCH . UIUC. EDU

Coordinated Science Laboratory
University of Illinois Urbana-Champaign

The parabigemial nucleus (PBN) is a satellite of the superior colliculus (SC), the midbrain saccadic oculomotor control center. PBN neurons fire action potentials at a rate proportional to the retinal position error
(RPE) of attended visual targets, whether stationary or moving. The tuning functions of PBN neurons are
usually not Gaussian, but are better described by an exponentially decaying sigmoid function. Recently,
using periodically blinking moving targets, it has been discovered that PBN activity continues to predict
the unseen (virtual) target’s trajectory, as inferred from kinetic information before the target disappears.

COSYNE 09

117

I-82
The sigmoidal tuning function is maintained during target-off phases, except that its dynamic range shrinks
somewhat. Such predictive neural events are critical for behavior, but are not well understood, either from
the physiological or computational standpoint. This is study is focused on the encoding problem, attempting to understand the computational mechanism for the predictive neuronal activity in the PBN by building
statistical models that can be tested against real spike train data. The point process theoretical framework that we used treats the spike train as a non-homogeneous poisson process, enabling us to relate
various possible factors, such as auto-regressive self-firing history and predicted virtual target positions, to
the logarithm of conditional intensity of spiking probability at each discrete time bin via a linear model. We
assume that the PBN has full knowledge of the dynamics of the visual target and the eye movements made
by the animal. As a consequence, when the target is momentarily turned off, the animal will assume its
continual existence and predict the virtual target’s position using a Kalman filter. The mean of the virtual
target positions predicted by the Kalman filter lies very close to the “true” target position, whether visible
or not. The variance of the prediction is near zero when the target is on, grows linearly with time after the
target is turned off, and suddenly drops back to near zero when the target is turned back on. We have also
incorporated the temporal evolution of the sigmoid tuning function dynamic range during a complete duty
cycle by modulating its parameters as functions of the variance of the Kalman filter prediction. The results
of this modeling were evaluated with a K-S plot constructed according to the time rescaling theorem, as well
as an auto-correlation scatter plot of the consecutive rescaled inter-spike intervals. The curve falls mainly
within the 95% confidence intervals, and the auto-correlation scatter plot is spread basically evenly on the
unit square area. These observations suggest the model provides a satisfactory fit for the data presented.
Our model has successfully captured the characteristics of PBN spike trains related to the encoding of extrapolated positions of virtual targets, and thus provided a plausible explanation for the origin of predictive
activity, and how this internal representation of the external world is reflected in neural spike trains.
doi:10.3389/conf.neuro.06.2009.03.014

I-82. Inference of object attributes from local image features caused
by occlusion
Xaq Pitkow

XAQ @ NEUROTHEORY. COLUMBIA . EDU

Columbia University
The natural visual environment has considerable structure, but to date the characterization of this structure
has been restricted to fairly simple statistical regularities. Here I describe a novel method to calculate the
joint statistics of complex image features exactly, for a generative model of images known as the Dead
Leaves model. The simplified world described by this model is composed of independent, textured objects
which occlude each other. This simplification enables us to isolate certain image properties that are important for object segmentation. It also allows us to calculate the joint probability distribution of image values
sampled at many arbitrarily located points, in some cases without approximation. By choosing the samples
appropriately, one can then convert this result into joint probabilities of edges, T-junctions, and other salient
image features. These features can be related to the underlying generative model, providing us a way to
relate local intensity patterns to higher-level attributes like object size, shape, depth, and border ownership.
We use this method to demonstrate how such psychophysical phenomena as contour facilitation, phantom
edges, and the cornsweet illusion can be interpreted as probabilistic inference.
doi:10.3389/conf.neuro.06.2009.03.017

118

COSYNE 09

I-83

I-83. Sensory input statistics and network mechanisms in primate primary visual cortex
Philipp Berens1,2
Jakob Macke1
Alexander Ecker1,2
R. James Cotton2
Matthias Bethge1
Andreas Tolias2
1
2

BERENS @ TUEBINGEN . MPG . DE
JAKOB @ TUEBINGEN . MPG . DE
ALEXANDER . ECKER @ TUEBINGEN . MPG . DE
RCOTTON @ CNS . BCM . EDU
MBETHGE @ TUEBINGEN . MPG . DE
ASTOLIAS @ BCM . EDU

MPI for Biological Cybernetics
Baylor College of Medicine

Understanding the structure of multi-neuronal firing patterns in ensembles of cortical neurons is a major
challenge for systems neuroscience. The dependence of network properties on the statistics of the sensory
input can provide important insights into the computations performed by neural ensembles. Here, we study
the functional properties of neural populations in the primary visual cortex of awake, behaving macaques
by varying visual input statistics in a controlled way. Using arrays of chronically implanted tetrodes, we
record simultaneously from up to thirty well-isolated neurons while presenting sets of images with three
different correlation structures: spatially uncorrelated white noise (whn), images matching the second-order
correlations of natural images (phs) and natural images including higher-order correlations (nat). We find
that groups of six nearby cortical neurons show little redundancy in their firing patterns (represented as
binary vectors, 10ms bins) but rather act almost independently (mean multi-information 0.85 bits/s, range
0.16 – 1.90 bits/s, mean fraction of marginal entropy 0.34 %, N=46). Although network correlations are
weak, they are statistically significant. While relatively few groups showed significant redundancies under
stimulation with white noise (67.4 ± 3.2%; mean fraction of groups ± S.E.M.), many more did so in the
other two conditions (phs: 95.7 ± 0.6%; nat: 89.1 ± 1.4%). Additional higher-order correlations in natural
images compared to phase scrambled images did not increase but rather decrease the redundancy in the
cortical representation: Network correlations are significantly higher in phs than in nat, as is the number
of significantly correlated groups. Multi-information measures the reduction in entropy due to any form
of correlation. By using second order maximum entropy modeling, we find that a large fraction of multiinformation is accounted for by pairwise correlations (whn: 75.0 ± 3.3%; phs: 82.8 ± 2.1%; nat: 80.8 ±
2.4%; groups with significant redundancy). Importantly, stimulation with natural images containing higherorder correlations only lead to a slight increase in the fraction of redundancy due to higher-order correlations
in the cortical representation (mean difference 2.26 %, p=0.054, Sign test). While our results suggest that
population activity in V1 may be modeled well using pairwise correlations only, they leave roughly 20-25
% of the multi-information unexplained. Therefore, choosing a particular form of higher-order interactions
may improve model quality. Thus, in addition to the independent model, we evaluated the quality of three
different models: (a) The second-order maximum entropy model, which minimizes higher-order correlations,
(b) a model which assumes that correlations are a product of common inputs (Dichotomized Gaussian) and
(c) a mixture model in which correlations are induced by a discrete number of latent states. We find that
an independent model is sufficient for the white noise condition but neither for phs or nat. In contrast,
all of the correlation models (a-c) perform similarly well for the conditions with correlated stimuli. Our
results suggest that under natural stimulation redundancies in cortical neurons are relatively weak. Higherorder correlations in natural images do not increase but rather decrease the redundancies in the cortical
representation.
doi:10.3389/conf.neuro.06.2009.03.298

COSYNE 09

119

I-84

I-84. Modelling of light responses of Drosophila Photoreceptor
Zhuoyi Song1
Daniel Coca1
Stephen Billings1
Mikko Juusola1,2
1
2

ZHUOYI . SONG @ SHEFFIELD. AC. UK
D. COCA @ SHEFFIELD. AC. UK
S . BILLINGS @ SHEFFIELD. AC. UK
M . JUUSOLA @ SHEFFIELD. AC. UK

University of Sheffield
Beijing Normal University

It remains unclear how a network of photoreceptors and interneurons, whose responses are shaped together through feedforward and feedback synapses, co-process visual information. We began examining
this question in a relatively simple eye of Drosophila by constructing a mathematical model, which describes
voltage responses of R1-R6 photoreceptors to light stimuli. This model is proposed as an early input stage
for later network processing in this supposedly visual ‘motion’-pathway. Photoreceptors transform images
projected on the eyes into voltage responses that are transmitted toward the brain for further processing and
to update the neural representation of the visual world. Drosophila photoreceptors also receive feedbacks
from higher order neurons that shape their responses further. However, relatively little is known how these
dynamic interactions function. By constructing a model of a Drosophila photoreceptor, based on experimentally measured parameters, we aim to learn more about feedback interactions within photoreceptors
and between photoreceptors and higher-order neurons. A fly photoreceptor consists of a photo-sensitive
part (rhabdomere) and a photo-insensitive part (body). It is believed that photo-transduction cascade, which
translates light-quanta into light current, happens in the photo-sensitive part of photoreceptors, whereas the
photo-insensitive membrane converts light current into a voltage response. Accordingly, there are two parts
in our model. The first part of the model simulates the photo-transduction cascade based on known biochemical interactions of major proteins (rhodopsin, metarhodopsin, G-protein, PLC, PIP2, DAG, Na+/Ca2+exchanger, CAM), several of which are feedback targets for Ca2+ fluxing in via light-gated channels [1].
These trans-membrane currents then drive the second part of the model: the photo-insensitive body, which
uses Hodgkin-Huxley-formalism to approximate the dynamics of the known voltage-gated ion-channels [3].
The rhabdomere part of the model is simulated for bright and dim inputs separately to produce macroscopic light-induced currents; integrated by run down currents from 30,000 microvilli. For dim impulses,
assuming that photons are absorbed by different microvilli and integrated in a linear way, then macroscopic
light induced current is obtained by convolution of the latency distribution (Gillespie algorithm [5]) with a
quantum bump (single photon response, provided by a quantum bump model [2,4]). For bright impulses,
light adaptation is involved, thus microvilli are divided into several adaptation categories, and macroscopic
light induced current is then integrated from responses from each category of microvilli. The model is validated by performing intracellular measurements from Drosophila photoreceptors to light impulses in vivo
and by comparing these to the model output for the same light inputs. Even in this relatively basic form, our
model can predict well the waveforms of voltage responses. From a practical and systemic point of view,
this model can serve as a foundation to a preprocessing module for higher order models of the Drosophila
visual system that we intend to build in due course. 1. Hardie, R.C., Raghu, P, Nature 413, 186-193 (2001)
2. Luo C.H., Rudy Y., Cir. Res. 74, 1071- 1096 (1994) 3. Vähäsöyrinki, M. Thesis, University of Oulu(2004)
4. Pumir, A. et al. PNAS 10354-10359 (2008). 5. Gillespie D.T., J. Comp. Phys. A. 22, 403-434 (1976)
doi:10.3389/conf.neuro.06.2009.03.030

120

COSYNE 09

I-85

I-85. Adaptation-induced changes in orientation tuning and tilt aftereffect in network models of V1
Klaus Wimmer1,2
Klaus Obermayer1,2
1
2

KLAUS @ CS . TU - BERLIN . DE
OBY @ CS . TU - BERLIN . DE

Berlin Institute of Technology
Bernstein Center for Comp Neurosci Berlin

The perception of a sensory stimulus depends on its temporal context. A well studied psychophysical
phenomenon is the tilt aftereffect in which an adapting context stimulus causes a vertically orientated test
stimulus to appear repulsed away from the context orientation. Physiological studies have identified the accompanying changes in orientation tuning of individual neurons in primary visual cortex: (1) tuning curves
shift repulsively away from the adapting stimulus, (2) responses close to the adapting stimulus are suppressed. Previous modeling studies have been purely conceptional or have focused either exclusively on
the perceptual (tilt aftereffect) or the physiological (tuning curve changes) effects. Here, we investigate
whether synaptic depression in firing rate models can produce behavior compatible with both perceptual
and physiological findings. We first parameterize simple ring models, so that model cells receive strong
recurrent excitation and inhibition, both dominating the feed-forward input. We have shown previously [1]
that only such an operating regime is compatible with experimental data on orientation tuning in primary
visual cortex. In this ring model, we then systematically vary the strength of the synaptic depression parameters for the four types of recurrent connections (excitatory -> excitatory, excitatory -> inhibitory, inhibitory
-> inhibitory, and inhibitory -> excitatory). Each parameterization gives rise to a different model instance,
which is then used to simulate adaptation experiments: first orientation tuning curves of model cells are
measured in the unadapted condition (control), then the model is adapted to a certain orientation, and tuning curves are measured again (test). We find that, depending on the relative strength of the depression
parameters, tuning curves of individual cells as well as the population responses can either show attractive
or repulsive shifts, and adaption can lead to both, response suppression or facilitation. The adaptationinduced changes in tuning curves and population responses are then compared to experimental data. The
data provides strong evidence for those network models where inhibitory synapses depress less than excitatory synapses, and excitatory to excitatory connections have strongest depression. Only a relatively
small range of the model space gives rise to adaptation-induced changes that are compatible with the experimental data. We use these results to construct two-dimensional fire rate models that incorporate a
biologically plausible topographic orientation preference map. With these models, we can investigate how
the tuning curve shifts of model neurons depend on the local intercortical interactions. It has been found
experimentally, that adaptation induced changes are more pronounced close to pinwheel centers. Indeed,
in the model adaptation has a stronger effect on the broadly tuned recurrent inputs near pinwheels, which
in turn also leads to larger tuning curve shifts. We do not observe this difference between pinwheel and
orientation domain cells in models with weak recurrent interactions. Thus, the pronounced adaptation near
pinwheel centers is a consequence of the enhanced sensitivity to modulations of connection strengths in the
strong recurrent network regime. [1] Stimberg, Wimmer, Martin, Schwabe, Mariño, Schummers, Lyon, Sur
& Obermayer. The Operating Regime of Local Computations in Primary Visual Cortex. Cerebral Cortex, in
press.
doi:10.3389/conf.neuro.06.2009.03.033

COSYNE 09

121

I-86 – I-87

I-86. Roles of feedforward, recurrent and feedback connections in visual processing
Samat Moldakarimov
Maxim Bazhenov
Terrence Sejnowski

SAMAT @ SALK . EDU
BAZHENOV @ SALK . EDU
TERRY @ SALK . EDU

The Salk Institute for Biological Studies
The primate visual cortex has a complex organization of connectivity both between visual areas and within
areas. Feedforward connections from lower visual areas to higher ones are augmented by recurrent lateral
connections within each area. In turn, layers standing later in the processing pathway project back to
the previous layers and these feedback connections massively outnumber feedforward projections. We
built a neural network model to study roles of feedback and recurrent connections in processing of stimuli
in multilayer network. We found that processing in pure feedforward network is not reliable. Spikes can
propagate or fail to propagate depending on how strong and synchronous spikes are in the previous layer.
We have hypothesized that there are direct projections from V1 to higher visual areas such as IT, and
there are feedback connections from IT to the previous areas such as V1, V2, V4. We built a multilayer
neural network also including feedback and recurrent connections and showed that the model can reliably
process a stimulus during the entire stimulation time. Based on the simulations we also argue that the role
of feedback connections not only support reliable processing but selectively enhance neurons’ activity that
encode appropriate features. We also discuss that feedback connections participate in modulation of neural
activity by attention.
doi:10.3389/conf.neuro.06.2009.03.056

I-87. Motion discrimination unimpaired during silencing of binocular
disparity information in macaque MT
Alexandra Smolyanskaya
Richard Born

ASMOLYAN @ FAS . HARVARD. EDU
RICHARD BORN @ HMS . HARVARD. EDU

Harvard Medical School
Individual neurons in macaque middle temporal area (MT) can be selective for multiple stimulus features,
including binocular disparity and direction of motion (Maunsell and Van Essen 1983). Importantly, the
activity of these same neurons correlates with an animal’s behavioral reports in direction of motion (Britten,
et al. 1992) and absolute depth (Uka et al. 2003) discrimination tasks. While we know that there is some
interaction between the modalities of motion and depth in MT, it is not clear to what degree their respective
encoding is co-dependent. For example, it has been proposed that disparity acts as a kind of gate-keeper
to determine how conflicting motion signals are combined in MT (Bradley et al. 1995). By cooling V2
and V3 to reversibly inactivate these areas, we can specifically impair the selectivity of MT neurons for
binocular disparity while sparing their selectivity for direction of motion (Ponce et al. 2008). This allowed us
to ask whether a monkey’s behavioral performance on tasks thought to depend on representation of these
features in MT (Britten et al. 1992; Salzman and Newsome 1992; DeAngelis et al. 1998; Uka et al. 2003)
can be dissociated as well. Thus far we have found that the monkey’s performance on a direction of motion
discrimination task is unaffected by inactivation of V2/V3. The monkey’s grating acuity appears unaffected
as well. We are currently training the monkey on an absolute depth discrimination task. Our preliminary
data thus suggest that the direct pathway from V1 to MT is sufficient to support direction discrimination and
that the indirect pathway is not necessary. The data further argue that disrupting tuning for one modality

122

COSYNE 09

I-88 – I-89
(binocular disparity) in MT does not impair the representation of another (direction of motion).
doi:10.3389/conf.neuro.06.2009.03.065

I-88. Compensating fixational eye movements: A network model
Yoram Burak1
Uri Rokni
Haim Sompolinsky2,3
Markus Meister1

YBURAK @ FAS . HARVARD. EDU
ROKNIU @ MIT. EDU
HAIM @ FIZ . HUJI . AC. IL
MEISTER @ FAS . HARVARD. EDU

1

Harvard University
Center for Neural Computation
3
Center for Brain Science
2

Even when we fixate our gaze, our eyes are not entirely stable. In fact, the eye’s fixational movements are
large compared to the resolution acuity of the fovea, as measured whenever we read the bottom line of an
eye chart. This poses a difficult problem if one considers how the output of the retina could be decoded by
downstream visual areas. To illustrate the difficulty, a foveal ganglion cell firing at 100 Hz emits at most
one spike before the image on the retina shifts by a distance comparable to the cell’s receptive field size.
How, then, does our visual system achieve its high acuity? Our working hypothesis is that a visual area
downstream from the retina converts the moving, blurred retinal image to a static, high-resolution neural
representation. Furthermore, we assume that this computation is based solely on the visual inputs, rather
than on an efferent copy of eye movements in the visual coding areas. We propose a network model that
performs this computation, based on an approximation to the ideal Bayesian estimator for the image. The
proposed mean-field calculation builds up separate estimates for the eye’s trajectory and for the identity of
each pixel in the image. We characterize the performance of this mean field estimator, and compare it to
the performance of the optimal Bayesian one. We also present a model neural network that implements the
mean field computation, and propose experimental predictions.
doi:10.3389/conf.neuro.06.2009.03.125

I-89. Probing the early visual system with naturalistic, synthetic images
Ruben Coen-Cagli
Stephanie Wissig
Adam Kohn
Odelia Schwartz

RCAGLI @ AECOM . YU. EDU
SWISSIG @ AECOM . YU. EDU
AKOHN @ AECOM . YU. EDU
OSCHWART @ AECOM . YU. EDU

Albert Einstein College of Medicine
The physiology of early visual cortex has traditionally been studied with simple stimuli such as bars and
gratings. While this approach has revealed much of what we know, it is also likely to have provided an
incomplete view of early visual processing in natural vision. There has thus been a growing interest in using natural images, namely photographic depictions of natural environments, to study cortical processing.
A frequent criticism of this approach, however, is that natural images are so complex that it is difficult to
understand precisely which of their features drive the observed physiological effects. Our aim is to develop
a new class of stimuli that captures much of the statistical richness of natural images while still allowing full

COSYNE 09

123

I-90
parametric manipulation. We have combined the development of this method with its application to neurophysiological experiments in primary visual cortex and with computational modeling of the experimental
data. Our approach is based on characterizing the statistics of natural scenes in the context of an algorithm
that generates synthetic images [1]. The algorithm is based on a steerable pyramid of oriented filters, which
has a clear biological correlate in the receptive field properties of neurons in primary visual cortex (V1). The
resulting parameter space is quite high dimensional. Here we focused on the joint statistics of the magnitude of filter activations, and defined two perceptually relevant parameters: H, the entropy of filter activity
across orientations, which measures how well defined the average orientation of the image is; and L, the
spatial extent of correlation among parallel filters, capturing the extent of edges and lines in the image.
We analyzed an ensemble of natural scenes to find the parameter ranges typically encountered in such
scenes and then generated artificial images by choosing values from each of the distributions. Interestingly,
we found that natural images tend to cluster in a corner of the space defined by our parameters, that has
little overlap with sinusoidal gratings. We synthesized images, varying H and L independently to obtain a
parametric set of synthetic, naturalistic images. To test the applicability of our approach, we recorded the responses of single units in V1 of anesthetized macaque monkeys to synthetic stimuli presented either to the
classical receptive field or extending into the non-classical receptive field (nCRF). The nCRF experiments
showed a rich variety of nonlinear effects, including: a) clear dependency of nCRF surround suppression
on the parameter H, and, to a less extent, on L; and b) reduced suppression, and in some cases facilitation,
when the center and the surround of the cells’ receptive field were stimulated with different statistics. Gain
control models of cortical processing have been effective in explaining some V1 nonlinearities in response
to grating stimuli; we are testing the ability of Gaussian Scale Mixtures, a class of generative model of natural image statistics related to divisive gain control, to account for our data. [1] J. Portilla and E.P. Simoncelli
(2000) International Journal of Computer Vision. 40(1):49-71.
doi:10.3389/conf.neuro.06.2009.03.081

I-90. Experience-dependent changes in perceptual capacity: behavior
and brain states
Michael Wenger1
Leslie Blaha2
James Townsend
Rebecca Von Der Heide1
1
2

MJW 19@ PSU. EDU
LBLAHA @ INDIANA . EDU
JTOWNSEN @ INDIANA . EDU
RJV 151@ PSU. EDU

The Pennsylvania State University
Indiana University

Studies of visual perceptual learning have documented the extent to which basic perceptual abilities, such
as contrast detection, can improve given systematic practice. The majority of these studies have focused
on reductions in detection and identification threshold. Recently, Blaha and Townsend demonstrated perceptual practice can produce large improvements in perceptual capacity, measured as the total work per
unit time, quantified at the level of the integrated hazard function of the response time (RT) distribution. In
addition, their results strongly suggested the increase in capacity was indicative of a strong perceptual organization. The present effort had three goals: (a) replicate the large improvements in capacity documented
by Blaha and Townsend using measures based on RTs, (b) relate those improvements to improvements
in measures based on response frequencies (specifically, detection thresholds), and (c) relate both types
of improvements to changes in measures based on scalp-level EEG. Six observers began by performing a
detection task with a contrast-defined pattern. Contrast levels were supra-threshold and the stimulus was
split vertically into two halves. Each half of the stimulus could be present or absent. Half of the observers
were instructed to give a positive response if they detected one or both halves present. Remaining ob-

124

COSYNE 09

I-91
servers were instructed to give a positive response only if they detected both halves present. EEG, RT and
response choice were recorded. Following the initial session, observers completed 10 days of perceptual
practice with the stimulus pattern and threshold changes were recorded. Finally, observers completed an
additional set of detection blocks in which EEG, RT, and response choices were again recorded. Critical results were (a) large and reliable reductions in detection thresholds, (b) large and reliable increases
in capacity, and (c) large shifts in ERP amplitudes and latencies. Results are interpreted with respect to
implications for cortical efficiency in perceptual learning.
doi:10.3389/conf.neuro.06.2009.03.057

I-91. What is the ”contrast” in contrast adaptation?
Kristina Simmons
Gasper Tkacik
Jason Prentice
Vijay Balasubramanian

SIMMK @ MAIL . MED. UPENN . EDU
GTKACIK @ SAS . UPENN . EDU
JPRENTIC @ SAS . UPENN . EDU
VIJAY @ PHYSICS . UPENN . EDU

University of Pennsylvania
During natural vision, retinal ganglion cells encode enormous variations of light intensity. In response, they
adapt to the luminance distribution’s mean and width, the latter usually quantified by contrast (C = standard
deviation / mean). This contrast adaptation compresses the large dynamic range of stimuli into a narrow
range of firing rates. Here we describe ganglion cell responses via a linear-nonlinear model and ask, how
should the nonlinearity adapt to different stimulus ensembles to efficiently fill the cells’ response bandwidth?
To study this, we passed each stimulus through a fixed linear filter followed by rectification and a nonlinearity,
the output of which is taken to model an OFF ganglion cell’s firing rate. We then sought the nonlinearity that
maximized information transfer subject to a constraint on the peak firing rate. The predicted nonlinearity
maps the distribution of filter outputs to a uniform distribution of firing rates. An added mean rate constraint
gives a truncated exponential rate distribution. First, we considered a Gaussian stimulus ensemble. For
this ensemble, the model predicts a sigmoidal nonlinearity with a gain (slope at inflection point) proportional
to 1/C. To test this, we made patch-clamp recordings of guinea pig retinal ganglion cells subject to full-field
white noise stimulation at several contrasts. We found very close agreement between the predicted and
measured nonlinearities at each contrast level. Natural light fluctuations are strongly non-Gaussian: they
have temporal correlations and a skewed distribution. To study how this affects the optimal nonlinearity, we
sorted a natural time series (van Hateren, 1997) into segments of fixed contrast. For all segment lengths
we found that the distribution of contrasts was wide and skewed (peak C 0.5 and 10% of segments with C
> 1.5). In addition, for any given contrast, the light intensity distribution was also highly skewed. As above,
we filtered and rectified segments of a given contrast; the resulting optimal gain was proportional to 1/Cˆ0.5.
This behavior differs from the Gaussian results in that it predicts less adaptation to increasing contrast. This
is because skewed stimuli feature large but rare intensity fluctuations that strongly affect C, but excessive
adaptation to these outliers would render the cell insensitive to the much more frequent small intensities.
These considerations led us to ask whether a measure of contrast which is robust to outliers would be more
natural for skewed intensity distributions. One possibility is the q-width, namely the range around the mean
containing q% of intensity values. For Gaussian stimuli the q-width is proportional to the standard deviation.
We found that the optimal gain predicted from the natural time series went approximately as the reciprocal
of the 60-width. Thus, using this measure, the predicted gains have the same functional form for both
naturally skewed and Gaussian distributions. Experiments in preparation will measure contrast adaptation
in response to naturalistic stimuli, allowing us to test the prediction that contrast gain control is insensitive
to outliers. Reference: van Hateren, J.H. (1997). Vision Res. 37, 03407-3416.
doi:10.3389/conf.neuro.06.2009.03.144

COSYNE 09

125

I-92 – I-93

I-92. Adaptation in MT neurons shifts speed tuning laterally, facilitating change discrimination
Nicholas Price
Richard Born

NICHOLAS PRICE @ HMS . HARVARD. EDU
RICHARD BORN @ HMS . HARVARD. EDU

Harvard Medical School
There is rich information content in the spatial and temporal changes in natural scenes, suggesting that the
detection and discrimination of these changes may be critical for generating appropriate behavior. Adaptation improves perceptual sensitivity to stimulus changes and modifies neural responses to sustained stimuli,
but it is not clear if and how adaptation improves neural sensitivity to stimulus changes. To address this,
we recorded from neurons in area MT of two monkeys while they performed a speed change discrimination
task. A circular window of random dots moving in the neuron’s preferred direction was presented within the
neuron’s receptive field. The dots moved at a “reference” speed and then abruptly changed to a “test” speed
with change times distributed from 500-5000 ms following a flat hazard function. Monkeys were trained to
report the sign of the speed change by saccading to one of two targets within 150-750 ms of the change.
Because the task requires both detection and discrimination, trials could end in one of four ways: (1) false
detection; (2) correct detection and discrimination; (3) correct detection but incorrect discrimination; and
(4) failed detection. We used reference speeds of 8 or 12◦ /s on each trial, with test speeds of 4-18/s.
Psychophysical performance declined with increasing task difficulty: smaller speed changes were associated with more failed detections and longer reaction times. For many neurons, responses to test motion
depended strikingly on the reference speed. Speed tuning curves during the task were steeper in slope
and laterally shifted relative to those measured during fixation. The shifts were typically “attractive”, moving
the flank of the tuning curve towards the adapting (”reference”) speed, thus optimizing neural sensitivity
for speed discrimination. The size of these shifts correlated with the slope of the speed tuning measured
during fixation but not with the preferred speed. This suggests that the neurons most suited to contribute to
task performance are those showing the strongest effects of adaptation and with the steepest part of their
speed tuning curve near the reference speed. We calculated neurometric performance as the percentage
of trials in which the sign of the change in firing rate following a speed change correctly identified the sign
of the change in speed. This measure also correlated with the slope of the speed tuning curve, but not
the preferred speed, as was found for the speed tuning shifts during the task. Finally, choice probabilities
were calculated from false detection trials by comparing firing rate distributions from trials with faster and
slower choices. CPs peaked 200 ms prior to the behavioral response and were largest for cells with the
best neurometric performance indicating that the responses of the most informative neurons are those that
best predict the behavior of the monkey. These results are consistent with a causal role of MT neurons in
detecting and discriminating changes in speed.
doi:10.3389/conf.neuro.06.2009.03.142

I-93. Color Constancy of V1 Double Opponent Cells to Natural Images
Dimitri Fisher1
Bevil Conway2
Mark Goldman1
1
2

DFISHER @ UCDAVIS . EDU
BCONWAY @ WELLESLEY. EDU
MSGOLDMAN @ UCDAVIS . EDU

University of California, Davis
Wellesley College

The human visual system perceives colors of objects as largely independent of the lighting conditions even

126

COSYNE 09

I-94
though the spectral composition of the incident light, and thus of the light reflected off objects and reaching
the eye, can be very different under different types of lighting (such as midday sun, sunset, fluorescent or
incandescent light). The ability to maintain constant perception of object color is called color constancy, and
its neural basis remains unknown. Neurons in the retina and lateral geniculate nucleus (LGN) do not show
response properties consistent with color constancy, indicating that this computation is performed at a subsequent stage of neural processing. The critical computation underlying color constancy is a comparison of
the relative cone activations across visual space. Recent studies have confirmed the existence of doubleopponent cells in V1 that, in principle, have the appropriate receptive field structure to contribute to color
constancy: they are sensitive not to absolute cone responses but rather to differences in cone responses
both across cone types (cone opponency) and across space (spatial opponency). To test the hypothesis
that double-opponent cells contribute to color constancy we compare the color constancy of experimentally
characterized V1 neurons to that of model LGN neurons and model V1 neurons constructed on the basis
of physiological measurements. We use experimentally measured receptive fields of V1 double-opponent
cells in alert macaque monkeys (Conway, 2001; Conway and Livingstone, 2006). To model the responses
of a neuron population tiling a portion of the visual field, each measured receptive field was spatially convolved with natural images both before and after a simulated change in lighting conditions (modeled by a
von Kries transformation). Natural images were taken from Olmos and Kingdom (2004). We show that,
due to their double-opponent structure, both the physiologically characterized and model V1 cells show
stronger color constancy in response to natural images than the model LGN neurons. We quantify the
improvement across a range of different recorded cells and natural images. We further quantify the effects of receptive-field shape and cone-type balance upon the color constancy of V1 double-opponent cells
by comparing the recorded V1 cells to model V1 cells with different surround shapes and balances of L,
M, and S cone contributions. Finally, we consider the effect of contrast normalization by the responses
of neighboring neurons and generate experimental predictions for the influence of contrast normalization
on color constancy. Acknowledgments: This work was supported by the Whitehall Foundation (BC); and
the Sloan Foundation, UC Davis, and a UC Davis Ophthalmology Research to Prevent Blindness grant
(MG,DF). Conway BR (2001). Journal of Neuroscience 21(8):2768-2783. Conway BR and Livingstone MS
(2006). Journal of Neuroscience 26(42):10826-10846. Olmos A and Kingdom F (2004) McGill Calibrated
Color Image Database http://tabby.vision.mcgill.ca .
doi:10.3389/conf.neuro.06.2009.03.146

I-94. Distinct functional populations within macaque area MT as revealed by waveform analysis
Frederic Roemschied1,2
Frank Bremmer
Bart Krekelberg1
1
2

FREDERIC. ROEMSCHIED @ PHYSIK . UNI - MARBURG . DE
FRANK . BREMMER @ PHYSIK . UNI - MARBURG . DE
BART @ RUTGERS . EDU

Rutgers University
Philipps-Universitaet Marburg

Following a recent study [1], we asked whether an analysis of spike waveforms could reveal distinct neural
populations with different functional properties in the middle temporal area (MT) of the macaque. To identify
distinctive waveform features, we used an unsupervised clustering algorithm based on a sorting and classification technique that combines the wavelet transform with superparamagnetic clustering [2]. We applied
the waveform analysis to neuronal responses recorded extracellularly from area MT of five awake rhesus
monkeys that fixated a central fixation point while a whole-field dot pattern moved on a circular motion trajectory in the frontoparallel plane. Over time this stimulus maps out all directions of translational motion
and provides a quantitative measure of direction preference, and the sharpness of direction tuning [3]. We

COSYNE 09

127

I-95
defined spike waveform duration as the time in microseconds between trough and peak, and found a clear
bimodal distribution of this waveform duration among the MT neurons. This allowed us to identify a narrowand a broad-spiking subpopulation of neurons on the basis of the two modes of this distribution. Once these
subpopulations had been identified we investigated their spiking statistics to corroborate their identification
with morphologically distinct neuron classes. The inter-spike interval (ISI) statistics of the narrow-spiking
population resembled those of a Poisson process, whereas the ISI statistics of the broad-spiking population showed clear burstiness. Moreover, the peak firing rate in the narrow-spiking subpopulation was
significantly higher than that in the broad-spiking population. These characteristics suggest that the subpopulations indeed correspond to interneurons (narrow-spiking) and pyramidal cells (broad-spiking). Next,
we investigated the functional properties of these populations and found clear differences in the direction
tuning properties. First, direction-tuning was significantly sharper in the narrow-spiking population than
in the broad-spiking population. Second, while the broad-spiking population had a uniform distribution of
preferred directions, the narrow-spiking population had a clear preference for cardinal directions of motion.
We conclude, first, that MT neurons can be classified based on their spike-waveform and this classification
likely corresponds to anatomically distinct classes of neurons. Second, the putative interneurons in MT
are more sharply tuned for direction and the cardinal bias of direction preference reported previously [4]
is a property that is unique to these interneurons. These distinct functional properties shed new light on
motion processing in MT and need to be integrated into biologically plausible models of motion analysis.
References: [1] Mitchell et al. (2007). Neuron 55 , 131-141. [2] Quiroga et al. (2004). Neural Comput. 16 ,
1661-1687. [3] Schoppmann, A., & Hoffmann, K. (1976). Neurosci. Lett. 2 , 177-181. [4] Xu et al. (2006).
Proc Natl Acad Sci USA 103(46) , 17490-17495.
doi:10.3389/conf.neuro.06.2009.03.172

I-95. Divisive normalization provides summation and competition of
population responses in visual cortex
Laura Busse
Steffen Katzner
Andrea Benucci
Matteo Carandini

L . BUSSE @ UCL . AC. UK
S . KATZNER @ UCL . AC. UK
ANDREA @ CARANDINILAB . NET
MATTEO @ CARANDINILAB . NET

University College London
In sensory cortex, stimulus attributes are encoded by the concerted activity of neuronal populations. How
do populations represent the presence of more than one stimulus? Do the responses to individual stimuli
summate or compete in determining the overall population response? We measured population responses
in primary visual cortex (V1) of anesthetized cats to single gratings and plaids obtained by summing two
component gratings. We recorded multiunit activity across a 10x10 electrode array, and computed the
population activity by averaging responses of sites with similar orientation preference. As expected, single
gratings elicited a population response profile that peaked at the neurons whose preference matched the
stimulus orientation and grew with grating contrast according to a saturating function. These response profiles could be well described by a separable model of orientation and contrast, indicating contrast-invariance
of population responses. The population responses to plaids reflected both summation and competition between the responses to the two components, depending on their relative contrast. A summation model, in
which the response to a plaid is the scaled sum of the responses to the individual components, could account for responses to only those plaids with similar component contrasts. A winner-take-all model in turn,
in which the response to a plaid is given by the scaled response to the component of higher contrast, could
capture the responses to only those plaids with different component contrasts. None of these models could
predict responses in both regimes of summation and competition. A third model, in which the strongest of

128

COSYNE 09

I-96
the responses to the components is selected for each neuron (max model), succeeded in predicting population responses to plaids with similar component contrast, but only if the angle between the components
was large. It failed to capture the data when component contrasts were different. To account for the full
body of observations we turned to the normalization model, which has been shown to capture responses
of individual neurons to plaids including a preferred component. In the model, the response to the plaid is
the sum of the responses to the components weighted by the component contrasts, and divided by a term
proportional to root-mean-square contrast. If the two contrasts are similar, the model approximates a sum;
if they differ considerably, it approximates a winner-take-all operation. Thus, the normalization model could
account for both regimes of summation and competition, fitting the population responses to all of the stimuli
with a single set of parameters. We conclude that population activity to multiple stimuli can exhibit both
summation and competition, and that the functional mechanism that determines the regime of operation is
well described by divisive normalization. The success of the normalization model in capturing all regimes
of operation indicates that divisive normalization is a fundamental operation shaping population responses
to multiple stimuli. Support: Leopoldina Fellowship Program BMBF-LPD 9901/8-165 (LB), NIH EY017396
(MC)
doi:10.3389/conf.neuro.06.2009.03.319

I-96. Area MT pattern motion selectivity by integrating 1D and 2D motion features from V1 – a neural model
Cornelia Beck
Heiko Neumann

CORNELIA . BECK @ UNI - ULM . DE
HEIKO. NEUMANN @ UNI - ULM . DE

University of Ulm
Problem. The neural mechanisms of detecting, integrating, and disambiguating visual motion still remain
largely a puzzle. While many neurons in area V1 show coarse direction and speed selectivity signaling
component motion direction, evidence suggests that end-stop cells in V1 compute the true motion direction for 2D image features (Pack et al., Neuron 39, 2003). In area MT, neurons have been found that are
selective to speed and respond to pattern motion direction in the case of plaids from combined differently
oriented gratings (Movshon et al., Pattern Recognition Mechanisms, 1985). For elongated bars, MT neurons resolve the so-called aperture problem to signal the correct object motion after a temporal course of
disambiguation (Pack&Born, Nature 409, 2001). The construction of selectivity to pattern motion from component selective input is a topic of ongoing research. Different experiments suggest that the visual system
might use computational strategies of integrating different motion directions or selecting localized features
(Pack&Born, The Senses: A Comprehensive Reference, 2008). Methods. We propose a neurodynamical
model of motion integration in areas V1, MT, and MSTv which unifies the integrationist and selectionist
concepts. Normal flow responses are initially detected by model V1 simple/complex cells while inhibitory
interactions between cells at different V1 laminae generate local end-stop responses after a temporal delay.
These responses are integrated for a short temporal episode and enhanced to generate strong direction
signals. MT neurons then receive input from V1 complex and end-stop neurons (size ratio 1:5, direction
tuning +/-40◦ ). The integration of component selective responses leads to weak pattern direction selectivity
in MT, further sharpened by the input from localized end-stop neurons. Feedback from MT to V1 enhances
neurons with feature selectivity and slightly reduces normal flow responses near localized features. Feedforward and feedback connections between MT and MSTv (size ratio 1:1.25) disambiguate MT responses
along extended outline boundaries. Results and Conclusion. To investigate the aperture problem, we tested
the model with a vertically aligned bar moving in diagonal direction. Consistent with neurophysiological findings (Pack&Born, Nature 409, 2001), after few iterations MT neurons showed sharp speed and direction
tuning for the correct velocity, while the tuning of V1 neurons only slightly changed. Originating from 2D

COSYNE 09

129

I-97
signals at corners, the correct flow was further propagated in MT along the stimulus by each iteration while
propagation time increased with bar length (Born et al., Prog. Brain Res. 140, 2002). For a plaid stimulus,
model MT neurons were tuned to pattern motion after some iterations, while V1 neurons indicated normal
flow responses along the plaid components, except for end-stop neurons located at crossings. Hence, V1
neurons already provided some of the correct 2D cues, their overall tuning nevertheless remained coarse
in speed and direction. To conclude, the integration of 1D and 2D V1 motion features in MT as proposed
by our model suggests the generation of MT cell pattern selectivity based on component integration and
feature selection. The model explains findings of various neurophysiological experiments and unifies different modeling approaches into one framework. Acknowledgements: Supported by EU-project 027198
(Decisions-in-Motion)
doi:10.3389/conf.neuro.06.2009.03.163

I-97. Rats’ Detection of Oriented Visual Target is Impaired by Collinear
Flankers
Philip Meier
Erik Flister
Pamela Reinagel

PMEIER @ UCSD. EDU
EFLISTER @ UCSD. EDU
PREINAGEL @ UCSD. EDU

UCSD
Studies of human psychophysics demonstrate that detection of an oriented grating is influenced by the
proximity and relative orientation of nearby ”flanker” gratings (ref.1,2,3). Here we ask whether flankers influence rats similarly. Rats were trained in an automated operant chamber. In each trial, a CRT monitor
presented a single image composed of multiple square-wave gratings in non-overlapping gaussian masks.
Subjects were rewarded with water for correctly reporting the presence or absence of a centrally positioned
target grating by licking ports to the left or right. Incorrect responses were penalized with a timeout. Two
flanking stimuli were always present, one on either side of the target location. The target was one of two orientations. Flankers were either aligned with the target orientation (”collinear”), or alignment was disrupted
by a mismatch in either orientation, angular position, or both. Target presence, target orientation, and
flanker configuration were chosen independently for each trial. Flanking stimuli could influence detection
due to their luminance, contrast, distance, orientation, collinearity or other geometric configuration. Luminance, contrast and distance were not factors in our study, because the flankers varied only in orientation
and angular position. Orientation and angular position alone did not influence detection, but only impaired
detection when combined so that flankers were collinear with the target (6 of 7 rats showed a significant
decrease in correct responses). Together these data implicate visual processing specific to continuous
edge contours. An illusory contour caused by collinear flankers should prima facie impair performance
by increasing false positives. However, a system that suffers from illusory contours can avoid such false
positives with a compensatory increase in its decision threshold for reporting target presence whenever
collinear flankers are present. We observed a modest increase in false positives (7 of 7 rats, 1 of 7 individually significant), and an increase in misses (7 of 7 rats, 5 of 7 individually significant) which might reflect
a compensatory bias to minimize false positives. Under comparable stimulus and experimental conditions
(yes/no detection, collinear compared to diagonal flankers, randomly interleaved trials, comparable flanker
distance) humans showed no change in sensitivity for collinear stimuli, but did show a reduction in decision
criteria for collinear responses. (ref.1, fig 6c) Authors argue that this is consistent with a visual system that
”fills in” illusory contours. On the other hand we show a reduction in sensitivity for collinear trials, and only
a small increase in decision criterion. We note that the stimulus typically used in the human studies has
a single target orientation, and thus could be solved by detecting total contrast at that orientation. This
could explain biases in human studies. Our stimulus controls for this confound by presenting randomly

130

COSYNE 09

I-98 – I-99
interleaved target orientations, which may explain the difference in results. (1) Polat & Sagi (2007) The
relationship between the subjective and objective aspects of visual filling-in. Vision Research. (2) Chen
& Tyler (2002) Lateral modulation of contrast discrimination: flanker orientation effects. Journal of Vision.
(3) Zenger-Landolt & Koch (2001) Flanker effects in peripheral contrast discrimination—psychophysics and
modeling. Vision Research.
doi:10.3389/conf.neuro.06.2009.03.199

I-98. Suboptimal selection of initial saccade in a visual search task
Camille Morvan1,2
Laurence Maloney1
1
2

CAMILLE . MORVAN @ GMAIL . COM
LTM 1@ NYU. EDU

New York University
Collège de France, Paris, France

Purpose. We investigated how the visuo-motor system plans saccades in an economic task analogous to
visual search. Methods. The subject saw two visual squares (tokens) displayed along a horizontal “base”
line through the bottom of a screen 57 cm from the subject perpendicular to his line of sight. On each trial,
the subject could freely saccade to any point along the base line. During the saccade, one of the two tokens
would change slightly. The subjects then judged how the token changed, responding by keypress. If the
subject’s response were correct he received a reward. Prior to the main experiment, we mapped subjects’
visual sensitivity to token change at different eccentricities. 4 subjects participated in the task. Analyses.
The key independent variable was the spacing between the two tokens. The token change was chosen to
be difficult to see outside of the foveal region. If the tokens were near each other the subject could reliably
identify token change from a fixation point midway between the tokens. If the tokens were far apart, then
this middle strategy would lead to little reward; a stochastic strategy where the subject could saccade to one
target would perform better. The key prediction of the experiment is the critical separation where the ideal
subject should switch from the midpoint strategy to the stochastic strategy in order to maximize expected
gain. Results. We found that subjects were suboptimal in this task. With increasing separation of tokens,
subjects switched to the stochastic strategy too soon, consistent with the possibility that they underestimate
their visual acuity in periphery. As a result subjects earned only 83% of the expected gain had they switched
at the correct point. Conclusion. Even in simple displays with only two tokens and after training subjects do
not plan saccades that maximize expected gain.
doi:10.3389/conf.neuro.06.2009.03.251

I-99. Changes of visual response properties in area MT due to eye
movements
Till Hartmann
Thomas Albright
Frank Bremmer
Bart Krekelberg

TILL @ RUTGERS . EDU
TOM @ SALK . EDU
FRANK . BREMMER @ PHYSIK . UNI - MARBURG . DE
BART @ RUTGERS . EDU

Rutgers University
Humans perform about three eye movements per second. Each of these eye movements changes the
image on the retina, but despite this we perceive the visual world as perceptually stable. Researchers have

COSYNE 09

131

I-100
shown that the visibility and perceived location of targets presented in the temporal vicinity of fast and slow
eye movements are altered. Our goal is to understand the neural computations underlying these effects
and thereby obtain a more general understanding of perceptual stability. We created a stimulus to map
the receptive field (RF) properties of cells in the middle temporal area (MT) of the macaque. The animal
observed randomly positioned, flickering bars and either kept its eyes stationary on a central fixation point
or performed optokinetic nystagmus (OKN). The OKN was induced by a random dot pattern that moved to
the left or to the right and filled the entire screen. In the analysis we corrected for eye position and used
reverse correlation to determine RF kernels in retinal coordinates. First, we found that during the slow
phase of OKN a significant subset of MT cells shifted their RFs in the direction of the eye movement. To
gain a better understanding of the underlying circuitry we investigated whether this shift occurred for both
putative interneurons and projection neurons. We calculated the width of the spike waveforms and found
that cells with narrow spikes - likely to be interneurons - showed either small shifts or did not shift at all,
while cells with broad spikes - probably projection neurons - showed strong shifts. Second, we found that
the firing rates of the neurons were strongly modulated around the time of the fast phase of OKN. While
some neurons’ firing rate was decreased, others’ was increased. We hypothesize that these changes may
play a role in saccadic suppression. For many neurons these changes started at saccade onset or even
slightly before, which rules out that the retinal signal change induced by the eye movement caused the
rate changes. We could not find any consistent relationship between the changes in firing and the width
of the spike waveform. This suggests that these firing rate changes may be independent of cell type. Our
finding that putative interneurons have constant RFs provides new constraints for models of RF shifting.
One possibility is that the shifts occur due to feedback from other areas, without the involvement of local
interneuron circuitry in MT. The alternative that we are pursuing is that a reduction in the response of a
subset of interneurons can lead to the unmasking of inputs to projection neurons and thereby shift their
receptive fields.
doi:10.3389/conf.neuro.06.2009.03.322

I-100. High-speed imaging of local population activity in mouse visual
cortex
Vincent Bonin
Mark Histed
R. Clay Reid

VINCENT BONIN @ HMS . HARVARD. EDU
MARK HISTED @ HMS . HARVARD. EDU
CLAY REID @ HMS . HARVARD. EDU

Harvard Medical School
A fundamental goal in systems neuroscience is to understand how local populations of neurons coordinate
their firing to encode external events and to generate internal states. Much of what is known on the subject
has been inferred from multi-electrode recordings which can only sparsely sample neural populations. Recently, two-photon calcium imaging has made it possible to measure spiking from a large number of neurons
in a local region. Two-photon calcium imaging is an optical microscopic technique that allows visualization
and simultaneous measurement of the activity of nearly every neuron in a local plane or volume of the living
brain. Spiking activity induces large calcium influxes into the cell body which can be measured by loading a fluorescent calcium indicator into the cell and exciting the indicator with high intensity infrared laser
pulses. While two-photon calcium imaging routinely yields high-resolution activity maps, it has remained a
challenge to use it to study how neural responses evolve as a function of time, or how they vary from one
trial to the next. In theory, its temporal resolution should only depend on the time course of the calcium
influx, limited by diffusion, which has a rise time of a few tens of milliseconds. In practice, two-photon
imaging is limited by both the ability to scan the sample quickly and also by signal-to-noise. Scan speed
is affected by the technology used to deflect the laser. Signal-to-noise is primarily affected by the number

132

COSYNE 09

II-1
of collected photons since this type of imaging works at low light levels where shot noise dominates. To
address these problems we have developed a two-photon microscope which uses a high-speed resonant
galvanometer and an optimized excitation and collection light path which increases photon flux. We can
continuously image large populations (up to several hundreds of neurons) in the living brain at rates of >30
Hz with high signal-to-noise. We find that we can reliably detect the presence or absence spontaneous and
stimulus-evoked calcium transients, and that we can measure their onset of with a temporal resolution of
30 milliseconds. This yields the ability to analyze trial-to-trial responses of a large population of neurons.
We are examining how populations of neurons in visual cortex respond to repeated presentation of visual
stimuli.
doi:10.3389/conf.neuro.06.2009.03.239

II-1. Predicting spike times of any cortical neuron
Ryota Kobayashi1
Yasuhiro Tsubo2
Shigeru Shinomoto1
1
2

KOBAYASHI @ TON . SCPHYS . KYOTO - U. AC. JP
TSUBO @ BRAIN . RIKEN . JP
SHINOMOTO @ SCPHYS . KYOTO - U. AC. JP

Kyoto University
Neural Circuit Theory, RIKEN BSI

There has been a growing need for simulating a massively connected network of neurons, to an extent of
a column, a cortical area and ultimately the entire brain. The information coding in the brain is critically
attributed to the manner in which individual neurons integrate incoming signals and fire the action potentials
called spikes to send signal to others. A neuronal model should be able to accurately reproduce a variety
of responses of cortical neurons. The Hodgkin-Huxley model has been the standard, and continually revised by including ionic channels to account for some typical neuronal firing phenomena qualitatively and
it became possible to use simulation platforms, such as NEURON and GENESIS. However, it turned out
that complicated models are weak in quantitative reproduction as well as prediction of new phenomena. In
addition, they require the high computational cost, which hinders performing the simulation of a massively
interconnected network. It had been considered that a neuron performs a leaky integration of input signals
until the membrane potential reaches a certain fixed threshold, and it fires and swiftly returns the potential
to a baseline. Conventional mathematical models including the Hodgkin-Huxley model constructed with this
view had been unable to represent the responses of a variety of neurons present in the central nervous system, and were unsatisfactory in mirroring the spike responses of neurons under greatly fluctuating currents
that are typical under behavioral conditions. Here we propose a spike predictor that may reproduce and
accurately predict a rich variety of spike responses. The key features of this new model are a non-resetting
leaky integrator and an adaptive threshold predictor equipped with fast (10 ms) and slow (200 ms) time
constants. The model can easily be tailored to any cortical neuron, including regular spiking (RS), intrinsic
bursting (IB), and fast spiking (FS) neurons, by simply adjusting three parameters, and furthermore, may
also generate the characteristic firing pattern of chattering (CH) neurons. This fact implies that the hidden
threshold dynamics are more important than the visible voltage integration in reproducing a variety of firing
patterns. It is notable that the model can express in the three dimensional parameter space a continuous
variety of firing characteristics of biological neurons rather than just those identified in the conventional discrete categorization. Both the high flexibility and the low computational cost would help to model the real
brain faithfully and examine how network properties may be influenced by the distributed characteristics of
component neurons.
doi:10.3389/conf.neuro.06.2009.03.196

COSYNE 09

133

II-2 – II-3

II-2. Power-law distributions of inter-spike intervals in in vivo cortical
neurons
Yasuhiro Tsubo
Yoshikazu Isomura
Tomoki Fukai

TSUBO @ BRAIN . RIKEN . JP
ISOMURA @ BRAIN . RIKEN . JP
TFUKAI @ BRAIN . RIKEN . JP

Neural Circuit Theory, RIKEN BSI
Spike sequences recorded from cortical neurons in an awake animal are known to be highly irregular. The
irregular spike sequences carry crucial information for understanding complicated balance between excitatory and inhibitory inputs and intrinsic spike-generating mechanisms of neurons. Irregular spike sequences
in vitro have been explained by various stochastic processes, especially the gamma process. However, it
is still unclear how cortical neurons in vivo generate the spike sequence. To clarify this, we recorded the
spike sequence of identified neurons in the rat motor cortex by juxtacellular recording. We found that the
inter-spike interval (ISI) distribution of neurons recorded in this in vivo experiment exhibited a power-law
decay rather than an exponential decay of the gamma distribution. This power-law was commonly found
in pyramidal and fast-spiking neurons in different cortical layers, although the power-law exponents varied
from neuron to neuron. We found that the experimentally observed ISI distributions can be well explained
by spike generation through doubly stochastic gamma process (DSGP) model. This model is based on two
hypotheses. First, an observed ISI is determined by a gamma distribution with regularity parameter (kappa)
and instantaneous firing rate (xi). Second, (xi) is determined by another gamma distribution with regularity
parameter (alpha) and mean firing rate R. The latter gamma distribution describes the distribution of timevarying firing rate of in vivo neurons. In this DSGP model, the observed ISIs obey a beta distribution of the
second kind (Beta-2) with power-law decay if (xi) varies on time, while the observed ISIs obey a gamma
distribution with exponential decay if (xi)&#61472;is constant. This model is consistent with the previous result of in vitro experiments where instantaneous firing rate (xi) was constant and the observed ISIs obeyed
a gamma distribution. We found that the regularity (kappa) of the intrinsic spike-generating mechanism and
the regularity (alpha) of the time-varying instantaneous firing rate were different in different neurons. The
recorded neurons were classified into three types by the regularity parameters (kappa) and (alpha): the
promptly spike-generating type (large (kappa)), the stationary rate type (large (alpha)), and irregular firing
type (small (kappa) and (alpha)). The neurons of former two types were found mostly in deep layers, while
those of the last type were found in both deep and superficial layers. These results imply that information
coding scheme of cortical networks is layer dependent.
doi:10.3389/conf.neuro.06.2009.03.320

II-3. Modeling the Electrical Function of Dendritic Spines
Tim Vogels
Roberto Araya
Rafael Yuste

TIMVOGELS @ COLUMBIA . EDU
RA 2174@ COLUMBIA . EDU
RMY 5@ COLUMBIA . EDU

Columbia University
Howard Hughes Medical Institute
Dendritic spines have traditionally been thought to serve primarily for biochemical compartmentalization but
recent findings also attribute electrical compartmentalization to their morphology and possibly active ionic
conductances (1 & 2). The spine neck filters membrane potentials and such electrical isolation of dendritic
spines from one another has been suggested to be the basis for linear integration of excitatory inputs (3).

134

COSYNE 09

II-4
Using NEURON we investigate the effect of different morphological features of spines in an unbranched,
cylindrical cable model as well as a morphologically realistic multi-compartmental model based on the reconstruction of an L5 pyramidal cell. We use morphological features described in recent microscopy studies
(4, 5) to constrain the parameters of synaptic spines and systematically test the impact of different spine
features on dendritic and somatic excitatory postsynaptic potentials. Voltage filtering across the spine neck
depends largely on the diameter and length of the spine neck as well as on the amplitude and duration of
the synaptic event, but the experimentally observed effects cannot be explained easily from passive properties of the spine. In our model we explore the effect of different active conductances placed in the spine
head and neck. We conduct exhaustive, multi-dimensional parameter searches to predict the range of physiological parameters that can allow for the electrical properties and discuss our findings in the framework
of known channel types and densities. Our results show the necessity of a hyperpolarizing current such
as a potassium current combined with a minimum spine neck resistance of 1 G&#937; to accommodate
the experimentally observed effect. We also explore the possibility of boosting this effect through sodium
currents that may facilitate the activation of potassium channels in the spine head as suggested by experimental data. 1 Araya R, Jiang J, Eisenthal KB, Yuste R (2006) The spine neck filters membrane potentials.
PNAS. 103:17961-6 2 Araya R, Nikolenko V, Eisenthal KB, Yuste R. (2007) Sodium channels amplify spine
potentials. PNAS 104(30):12347-52 3 Araya R, Eisenthal KB, Yuste R (2006b) Dendritic spines linearize the
summation of excitatory potentials. PNAS. 103:18799-804 4 Arellano J, Benavides-Piccione R, DeFelipe
J, Yuste R (2007) Ultrastructure of dendritic spines: correlation between synaptic and spine morphologies.
Front. Neurosci. 1,1:131-143 5 Nägerl UV, Willig KI, Hein B, Hell SW, Bonhoeffer T. (2008) Live-cell imaging
of dendritic spines by STED microscopy. Proc Natl Acad Sci Funded by the Patterson Trust and the NEI
Keywords: spines, two-photon, modeling, active conductances, potassium channels, dendritc integration
doi:10.3389/conf.neuro.06.2009.03.119

II-4. Unequal partitioning of AMPA and NMDA conductances leads to
robust temporal order sensitivity
Yichun Wei
Bartlett Mel

YICHUNWE @ USC. EDU
MEL @ USC. EDU

University of Southern California
NMDA and AMPA receptors coexist at most glutamatergic synapses in the CNS, and are frequently described as having a synergistic interaction: AMPA channels provide the major depolarization needed to
relieve NMDA channels of their voltage-dependent Mg2+ block, which can lead under certain conditions to
highly nonlinear regenerative events such as NMDA spikes. With compartmental modeling, we systematically studied the conditions required to achieve two-input summation that is at once (1) strongly superlinear
(i.e. ”synergistic”), (2) temporal order-dependent (i.e. ”causal”), and (3) stable despite fluctuations in synaptic parameters (i.e. ”robust”). We found that all three desirable properties can be simultaneously achieved,
but only with (1) an unequal allocation of NMDA and AMPA conductances between the two input pathways,
and (2) fast-spiking voltage-dependent sodium channels in the dendritic membrane. Our results could serve
as a basis for understanding time-dependent contextual modulation effects.
doi:10.3389/conf.neuro.06.2009.03.235

COSYNE 09

135

II-5

II-5. Reconciling inter-areal gamma-range synchrony with neural irregular activity in selective attention
Salva Ardid1,2
Xiao-Jing Wang
Albert Compte3

SALVA . ARDID @ YALE . EDU
XJWANG @ YALE . EDU
ACOMPTE @ CLINIC. UB . ES

1

Yale University School of Medicine
Kavli Institute for Neuroscience
3
IDIBAPS
2

Selective attention modulates firing rates and local gamma-range synchronization of neurons in the visual
cortex. Synchronization is believed to be an important mechanism for enhancing attentional processing, but
its functional consequences remain unknown. However, spike firing is highly irregular, and attentional modulations do not have an appreciable effect on such variability. An understanding of how these results can
be reconciled is a prerequisite to evaluate the functional role of synchrony related to attention. These attentional modulations have been hypothesized to be under control of top-down inputs coming from working
memory circuits in PFC/PPC. Neurons in such circuits present sustained selective firing, which maintains
behaviorally relevant information during the delay-period between cue and response. These neurons also
provide top-down input to sensory circuits. Thus, attentional modulations, including synchrony modulations,
might emerge from the interaction of a sensory circuit and a downstream area in PFC/PPC. We integrate this
phenomenology in a biophysical model of two cortical areas. Both interacting networks, MT and PFC/PPC,
share the same 1-D ring architecture to code the stimulus feature (direction of motion), and are composed
of 4:1 spiking excitatory to inhibitory neurons connected through conductance-based synaptic inputs (mediated by AMPARs, NMDARs and GABAARs). Apart from the influence of the top-down signal onto rates,
(biased competition, multiplicative scaling and selectivity enhancement, which we considered before: Ardid
et al. J Neurosci 2007), the model matches synchronization and irregular firing. These measures offered
conflicting views on the impact of temporal dynamics modulations in attentional processing. Our model provides a new perspective integrating these results and reconciling the dichotomy. We show that significant
attentional modulation of local and long-range coherence is compatible with irregular Poisson-like statistics. Our model produces specific predictions: Gamma-range coherence between sensory and executive
areas occurs only between neurons of similar selectivity if the attended and test feature coincide with their
neuronal preference. The footprint of top-down synaptic projections underlies this specificity. In addition,
using a manipulation in the model, we assess the functional impact of synchronization by comparing attentional modulations of sensory neuron activity in the presence versus absence of synchronization across
the sensory and executive circuits. We demonstrate that the temporal reorganization of top-down incoming
spikes into MT is responsible of around 10-15% of the attentional modulations of firing rates. In conclusion,
our model presents a quantitative evaluation of the inter-areal synchronization role for selective attention,
reconciling local field potential oscillations with irregular spiking. Such constraint is important in evaluating
the functional impact of attentional synchronization on visual processing. Our findings reveal a moderate
but significant increase of activity modulations by synchronization, suggesting a role in the efficient control
of sensory processing during attention.
doi:10.3389/conf.neuro.06.2009.03.043

136

COSYNE 09

II-6

II-6. Spatial attention modulates steady state VEPs in retinotopic human visual cortex
Thomas Lauritzen1,2
Alex Wade2,3

TZL @ BERKELEY. EDU
WADE @ WADELAB . NET

1

University of California, Berkeley
Smith-Kettlewell Eye Research Institute
3
UCSF
2

Introduction: In primates, sustained allocation of spatial attention causes significant increases in single-unit
firing rates in higher visual areas such as V4 but similar increases are hard to measure in V1. Paradoxically,
fMRI measurements of spatial attentional show strong responses in all visual areas, including V1. Recently,
two groups have suggested that these fMRI changes are either purely additive (Buracas and Boynton 2007)
or a mixture of additive and multiplicative mechanisms (Li et al, 2008). Buracas and Boynton suggested that
spatial attention acts to marginally increase the baseline firing rates or presynaptic activity of all neurons
in the attended region of cortex irrespective of their tuning. Such a change might be undetectable at the
level of a single unit, yet could generate a large DC increase in metabolic demand that would influence
the fMRI BOLD signal. To test this hypothesis, this we measured the effect of spatial attention on steady
state visually-evoked potentials (SSVEPs) in four retinotopically-defined visual areas using source-imaged
EEG. Methods: We studied visual spatial attention in 7 subjects. SSVEP stimuli consisted of two randomlyoriented 3cpd, 3 degree Gabor gratings at 50% contrast located 3 degrees to the left and right of a fixation
point. There were three attentional conditions. In condition 1 subjects were cued to detect small contrast
modulations in the grating on the left. In condition 2 subjects performed the same task on the right. In
condition 3, subjects ignored both gratings and performed a demanding letter discrimination task at fixation.
Performance was approximately 75% correct on all conditions. EEG data were collected with a whole-head,
128-channel EGI Netstation system, and the locations of all electrodes were recorded using a 3D digitizer.
Minimum-norm inverses were computed using anatomically-correct headmodels, and the timecourse of
the mean cortical current density was extracted from fMRI-defined visual areas V1, V3a, V4 and MT+.
Spectral analysis was used to separate the responses to the two stimulus gratings in all four visual areas.
Results: In conditions 1 and 2 we found that attending to a target increases the amplitude of the frequency
component associated with that target in all the studied areas, including V1. Conclusion: Steady state
VEPs are modulated by attention in all stages of cortical visual processing. The modulations that we
measure have no DC components and are best modeled by a multiplicative, rather than an additive gain
function. Interestingly, signal levels of the ‘ignored’ gratings in conditions 1 and 2 were lower than those
in condition 3 suggesting that attentional selection is more effective for well-separated targets or, perhaps,
more necessary when those targets share common spatial features such as shape and spatial frequency.
Buracas, G. T. and G. M. Boynton (2007). ”The effect of spatial attention on contrast response functions
in human visual cortex.” J Neurosci 27(1): 93-7 Li, X., Z. L. Lu, et al. (2008). ”Blood oxygenation leveldependent contrast response functions identify mechanisms of covert attention in early visual areas.” Proc
Natl Acad Sci U S A 105(16): 6202-7.
doi:10.3389/conf.neuro.06.2009.03.252

COSYNE 09

137

II-7 – II-8

II-7. Estimates of spike-LFP coherence based on finite spiking data
vary with mean firing rate
John Curtis
Jude Mitchell
John Reynolds

JCURTIS @ SALK . EDU
JUDE @ SALK . EDU
REYNOLDS @ SALK . EDU

The Salk Institute
A growing number of neurophysiological studies have found that spike-LFP coherence (SFC) varies with behavioral state. SFC quantifies the extent to which spike activity is phase-locked to the LFP across frequency.
Changes in SFC within a particular frequency band are typically taken as evidence for task-dependent
changes in neuronal synchrony in that band, independent of firing rate. Here we show that within the finite
time interval of a single trial, spike count influences the variability of estimates of the relative amplitudes
and phases of spike and LFP activity, i.e. lower spike counts result in higher variability. As a result of
this increased variability, the trial-average coherence magnitude increases with increases in firing rate. We
illustrate this issue by analyzing spiking and local field potential data recorded in macaque area V4 while
animals performed an attention-demanding task. We compute coherence values and show that variability
in estimating relative phases and amplitudes of spike and LFP is contingent on the number of spikes per
trial, i.e. more variable estimates with lower firing rate results in reduced magnitudes of coherence. We
explore alternative methods of estimating neuronal synchrony that do not vary according to the number of
spikes per trial.
doi:10.3389/conf.neuro.06.2009.03.333

II-8. Where to look? Dissociating the effect of reward, salience and
attention
Vidhya Navalpakkam
Christof Koch
Antonio Rangel
Pietro Perona

VIDHYA @ CALTECH . EDU
KOCH @ KLAB . CALTECH . EDU
RANGEL @ HSS . CALTECH . EDU
PERONA @ CALTECH . EDU

California Institute of Technology
How do distinct sensory and economic attributes like salience and reward combine to guide where we
look? Are saccadic decisions dominated by visual salience or reward? Can saccadic decisions, like purely
economic decisions, fail to maximize expected reward? To study this, we designed a treasure hunt experiment in which subjects were presented with a brief display containing two targets (horizontal and vertical
bar) differing in reward and salience, embedded among 6 tilted distractors. Subjects were instructed to
maximize the treasure earned in 0.5 sec, by sequentially fixating on as many stimuli and earning their corresponding reward values. Across 28 blocks, we systematically varied target reward and salience values.
We analyzed the first saccade by testing four different hypotheses: 1) the observer searches for the most
rewarding item, 2) most salient item, 3) item with highest expected reward, 4) the observer saccades to
the location of maximum expected reward. Data from 6 subjects shows that instead of searching for a
predefined target, humans optimize reward trial-by-trial, by saccading to the location of maximum expected
reward. These findings generalize to other feature dimensions like intensity The optimal model predicts,
and we empirically validate in humans, that for low internal noise in stimulus representation, intermediate reward values increase the effective stimulus salience multiplicatively, suggesting an underlying neural
mechanism of gain control. Attention has been postulated to operate through a gain control mechanism,

138

COSYNE 09

II-9
which raises an interesting question: are the effects of reward mediated through attention? According to
this hypothesis, the rewarding target’s features receive greater attention, hence drawing more saccades. In
a second experiment, we find that top-down attention to a stimulus feature makes the stimulus appear more
salient, however, rewarding a stimulus does not alter its appearance or perceived salience. Thus, reward
and attention interact with salience through distinct mechanisms.
doi:10.3389/conf.neuro.06.2009.03.038

II-9. Category learning and decision making: a cortical circuit model
Tatiana Engel
Xiao-Jing Wang

TATIANA . ENGEL @ YALE . EDU
XJWANG @ YALE . EDU

Yale University
How does the brain recognize “meaning” of sensory stimuli? Through experience we learn to group stimuli into arbitrary categories (such as “animal” or “car”) according to certain shared characteristics. Recent
neurophysiological studies have begun to investigate the neural mechanisms of categorization. Freedman
and Assad (2006) recorded single-cell activity in monkeys classifying 360 of visual motion directions into
two discrete categories. On each trial, the monkey had to indicate whether categories of two successively
presented stimuli (sample and test) were the same (match) or different (nonmatch). Firing rates of neurons
in the lateral intraparietal (LIP) area were found to encode the category membership of the currently visible
stimulus and did not differentiate between physical features of the stimuli. In contrast, neurons in the middle
temporal (MT) area were more involved in visual feature processing and did not carry explicit information
about category. A similar categorization study (Freedman et al. 2003) revealed three neural populations in
the prefrontal cortex (PFC): the first encoded category information, the second encoded match/nonmatch
status of the test stimulus regardless of its category, and the third showed match/nonmatch effects that
were limited to one of the categories. The physiological data suggest that two main computational stages
are involved in this task: (i) extract the category of the sample; and (ii) perform match vs. nonmatch comparison with the test. To identify general mechanisms underlying these basic computations, we propose
a biophysically plausible network model of categorization and match vs. nonmatch comparison for motion
directions. The model comprises two interconnected brain areas: a sensory (MT) and a cognitive-type
area (LIP/PFC). Both areas are strongly recurrent circuits with dynamics governed by slow recurrent excitation and feedback inhibition. Importantly, heterogeneity of recurrent connections within the cognitive-type
circuit leads to diversity of neural responses and a continuos spectrum of neural selectivities (e.g. mixed
selectivity). We hypothesize that the bottom-up synapses from MT to LIP/PFC are plastic and implement
the reward-dependent stochastic Hebbian learning rule to acquire new categories. The plastic synapses
shape the direction-selective input from MT to produce category-tuned responses in LIP/PFC. Match vs.
nonmatch decision in the model results from the winner-take-all competition between neural pools preferring matches and nonmatches. Along with the bottom-up input from MT, these neurons receive a weak
modulatory input from the category selective neurons, which biases the competition and leads to enhancement/suppression of responses to the test stimulus, depending on whether it matches the sample category.
Our model accounts for the single-cell data and provides insight into general mechanisms of categorization
and comparison of perceptual stimuli. The model predicts that (i) categories can be learned by synaptic
plasticity in the input from sensory to cognitive-type areas; (ii) the mechanism of the match vs. nonmatch
comparison is the biased winner-take-all competition. D.J. Freedman and J. A. Assad, Nature 443, 85
(2006). D.J. Freedman et al., J. Neurosci. 23, 5235 (2003).
doi:10.3389/conf.neuro.06.2009.03.055

COSYNE 09

139

II-10

II-10. Serotonin modulates choice stickiness through an outcomeindependent striatal mechanism.
Ben Seymour1
Nathaniel Daw2
Peter Dayan3
Jonathan Roiser4
Ray Dolan1

BJ. SEYMOUR @ GMAIL . COM
NATHANIEL . DAW @ NYU. EDU
DAYAN @ GATSBY. UCL . AC. UK
J. ROISER @ UCL . AC. UK
RDOLAN @ FIL . ION . UCL . AC. UK

1

Wellcome Trust Centre for Neuroimaging, UCL
New York University
3
Gatsby Computational Neuroscience Unit, UCL
4
University College London
2

Introduction. The neuromodulator serotonin (5HT) has consistently been implicated in the control of decisionmaking, although the precise nature of its role, or roles, remains widely debated. Current theories of serotonin function span reward processing, temporal discounting, punishment learning, behavioural flexibility,
and behavioural inhibition. It has been difficult to identify any computationally precise mechanism or mode
of action. Methods We present data from a pharmacological fMRI study of decision-making in humans,
using a task designed to probe precise subcomponents of instrumental learning at both behavioural and
neural levels. We manipulated central serotoninergic signaling in a between-subject, double-blind design,
using the relatively selective method of acute dietary trypotophan depletion. Subjects selected one of four
”bandits” on each trial, with each bandit associated with a nonstationary chance of reward (20 pence) and
also a separate chance of punishment (a painful electric shock). Given the choice of a bandit, reward
and punishment were delivered independently, allowing their effects to be assessed separately. We characterized choice behavior and associated fMRI signals using reinforcement learning models, and further
assessed the serotonergic modulation of behavioral and neural measurements by studying how they covaried, across subjects, with blood tryptophan:LNAA ratios (a marker of the degree of central 5HT depletion).
Results Behaviorally, tryptophan status did not significantly influence action learning for either rewards or
punishments, or the trade-off between the two (ie, the financial value of the pain implied by the choices).
However, serotonin depletion substantially amplified the tendency of subjects to repeat previously chosen
actions (”choice stickiness”) regardless of the reward or punishment received. This behavior was not explicable as being mediated by any serotonergic effect on uncertainty-driven exploratory chocies. Using
concurrent fMRI, we showed that the serotonergic level also predicted the modulation of stickiness-related
activity in the medial head of caudate. We further identified both reward- and avoidance-related prediction
errors, which though analyzed independently, were found to converge in the same area of caudate, and
appeared to constitute a single error signal, with positive BOLD excursions indicating unexpected reward or
nonpunishment and negative excursions for unexpected punishment or nonreward. As with the behavior,
these reward- and punishment-related signals were not significantly related to serotonin. Discussion Our
results support a computational and neurobiological account of serotonin function in which it suppresses
the simple, outcome-independent, choice perseveration that characterises repeated decisions. Such choice
stickiness may enhance sampling efficiency in trial-and-error learning (by reducing switching due to local
fluctuation in payoffs). Suppressing it would be appropriate in the case of increased optimism about outcomes elsewhere in the environment, and the consequent promotion of exploration may reflect a basic
heuristic mechanism of behavioural flexibility that appears to neglect the individual choice uncertainties that
would direct an optimal sampling solution. The neural data closely follow the behaviour, and suggest a
central role for the head of caudate in integrating distinct components of reinforcement learning and choice.
doi:10.3389/conf.neuro.06.2009.03.082

140

COSYNE 09

II-11 – II-12

II-11. Computation of value functions based on gains and losses in
the cortico-striatal network
Hyojung Seo
Xinying Cai
Daeyeol Lee

HYOJUNG . SEO @ YALE . EDU
XINYING . CAI @ YALE . EDU
DAEYEOL . LEE @ YALE . EDU

Yale University School of Medicine
Human behaviors are not always rewarded or punished by primary reinforcers, but instead modified by the
gains and losses of conditioned reinforcers, such as social praise and money. Neuroimaging studies in
human subjects have identified a network of brain areas involved in the evaluation of monetary feedback.
In contrast, little is known about how action-outcome associations are encoded by the individual neurons in
the brain when the animal’s actions lead to the gains and losses of tokens or conditioned reinforcements. In
this study, we trained rhesus monkeys to perform a stochastic decision-making task under token economy,
and investigated how information about the animal’s choice and its outcome is processed in the medial and
lateral prefrontal areas as well as the dorsal and ventral striatum. At the beginning of each trial, a circular
array consisting of 0 to 5 red tokens was displayed at the center of a computer screen. These tokens
were exchanged with juice reward, when the animal accumulated 6 of them. During a 0.5-s fore-period, the
animal fixated a square at the center of a computer screen, and was required to shift its gaze towards one of
the two peripheral targets after a 0.5 delay. After the animal fixated its chosen target for 0.5 s, the outcome
of the animal’s choice was revealed by a feedback ring. The color of the feedback ring signaled an increase
(gain), a decrease (loss), or no change in the number of tokens, and this was determined according to the
payoff matrix of a biased matching-pennies game. A logistic regression analysis showed that the gain and
loss of a token exerted a robust reinforcing and punishing effect on the animal’s subsequent choices. We
tested how the neural activity is influenced by the animal’s choices and their outcomes, using a multiple
linear regression model. The model incorporated a set of interaction terms between choices and gains or
losses in the current as well as previous trial and was applied to spike rates measured for the delay period
(0.5 s after target onset) and feedback period (0.5 s after feedback onset). We found that the conjunctions of
choice and the outcome was more frequently encoded in the dorsomedial frontal cortex, compared to dorsal
anterior cingulate cortex or dorsolateral prefrontal cortex. Within the striatum, the conjunctions were more
frequently encoded by the caudate neurons compared to neurons in the ventral striatum. Furthermore, the
contingency analysis revealed that neural activity during delay period signaling the conjunction of choice
and gain or loss in the previous trial tends to be correlated with the difference in the action value functions
between two alternative actions. These results suggest that the cortico-striatal network, in particular the
dorsomedial frontal cortex and the caudate nucleus, might play a key role in adjusting animal’s behaviors
based on conditioned reinforcers and their losses.
doi:10.3389/conf.neuro.06.2009.03.132

II-12. An optimality framework for understanding inhibitory control in
countermanding tasks
Angela Yu

AJYU @ UCSD. EDU

University of California, San Diego
The flexible and timely ability to stop or alter a planned course of action in the face of changing demands is
a hallmark of intelligent behavior. The stop-signal or countermanding task, in which an initial ”go” signal is
later countered by a ”stop” signal inhibiting the response, is a classic paradigm used to examine the behav-

COSYNE 09

141

II-13
ioral strategy and neural processing underlying inhibitory control across normal and psychiatric populations
(notably ADHD patients), and across species. The prevalent theoretical model of inhibitory control in this
task posits a race between two noisy processes, those associated with ”go” and ”stop” respectively (Logan &
Cowan, 1984). This model (Hanes & Schall, 1995) and its extensions (Boucher et al, 2007) have been used
to account for some key behavioral measures and neurophysiological data. However, more recent behavioral experimental results, indicating behavioral adjustments at both short- and long-term timescales, pose
a serious challenge for the race model (Emeric et al, 2007). Although the race model can be augmented
mechanistically to incorporate these adjustments (Emeric et al, 2007), the computational provenance and
import of such an ad-hoc extension would be largely missing. In this work, we present an alternative theory
of inhibitory control in countermanding tasks, encouched in an optimality framework. Using a combination
of Bayesian probability theory and stochastic control theory, we demonstrate that provably optimal action
policies, based on clearly specified assumptions about task demands, neural noise, and behavioral objectives, not only account for the basic behavioral and neurophysiological data in the countermanding task,
but also the more challenging ones related to history dependence. In particular, our work provides a common framework that reconciles apparently contradictory experimental data, revealing important behavioral
consequences of subtle differences in experimental design or task instructions. Our model also makes
quantitative predictions regarding behavior and neurobiology in novel variations of the task. In summary,
this alternative theory provides a rigorous framework for explaining not only ”what” the behavioral strategy
is and ”how” the underlying neural machinery implements the necessary computations, but a normative account of ”why” the behavioral strategy and neural responses are what they appear to be for countermanding
tasks.
doi:10.3389/conf.neuro.06.2009.03.151

II-13. A computational theory of prefrontal executive control
Anne Collins1,2
Etienne Koechlin3,2

ANNE . COLLINS @ POLYTECHNIQUE . ORG
ETIENNE . KOECHLIN @ UPMC. FR

1

Institut National Santé Et Recherche Médicale
Ecole Normale Supérieure
3
INSERM

2

Prefrontal executive control is based on building, maintaining and switching between multiple task-sets (TS,
i.e. sensorimotor mappings) according to external cues and feedbacks. Little is known, however, about the
computational mechanisms underlying prefrontal executive function for optimal adaptive behavior in varying
noisy environments. Basic reinforcement learning can learn sensorimotor associations and adjust them to
changes of external contingencies but fail to store and subsequently switch to previously learned associations when appropriate. Multiple model-based reinforcement learning (MBRL) can learn, store, monitor
and select between multiple TSi according to responsibility signals (&#955;i) that are continuously updated
through bayesian inference for coding the reliability of each TSi inferred from past events and feedbacks.
However, MBRL models fail to capture fundamental distinctions like automatic vs. controlled behavior, exploitation vs. exploration and are inefficient with large spaces of TS. We propose a new model that learns,
stores, monitors and reuses TS based on computation of responsibility signals as in MBRL. In contrast to
MBRL, however, our model defines the notion of default TS, exploitation vs. exploration and provides a
mechanism for creating an arbitrary space of TS as need arises. Only one TS acts as the actor and critic
at one time: this default TSd is adjusted and remains responsible for behavior as long as it remains more
reliable than all other options collectively (&#955;d > 1/2). When it is no longer the case, the model switches
and learns a novel TS forming the new actor-critic, which governs exploration until the responsibility signal
of one TSi (possibly the novel one) increases above 1/2. The corresponding TS then becomes the default
TSd governing exploitation again. Default TSd are systematically stored, so that the space of TS enlarges

142

COSYNE 09

II-14
when a novel TS becomes a default TSd. Responsibility signals as described above only reveal the need to
switch ex-post in reaction to surprising negative feedbacks. We generalized the model in order to account
for task-switching in response to external cues preceding action. Accordingly, we introduced the notion
of ex-ante responsibility measuring the reliability of each TS given current external cues and past history.
Using Bayesian computations, we derived updating rules combining ex-ante and ex-post responsibility. In
this extended model, default TS are selected according to ex-ante responsibility signals, while associations
between external cues and every TS are learned with ex-post responsibility signals serving as reinforcers.
Such associations define an extended notion of TS, referred to as episodic-sets (ES). The described model
acting on TS can therefore be replicated at a higher level corresponding to ES and including similar reinforcers, responsibility signals associated with ES and updating rules derived from Bayesian computations.
This generalized model accounts for the hierarchical organization of prefrontal executive control. As confirmed by computer simulations, the model builds and learns different TS, as well as associations between
external cues and TS. It flexibly switches between them in response to external cues and past events or in
reaction to feedbacks. The model makes specific predictions that can be empirically tested in behavioral
and neuroimaging experiments.
doi:10.3389/conf.neuro.06.2009.03.265

II-14. Active updating of decision boundaries in rats can be explained
using bayesian classifiers
Pradeep Shenoy1
Erick Chastain1
Adam Kepecs2
Rajesh Rao1
1
2

PSHENOY @ CS . WASHINGTON . EDU
EJC @ CS . WASHINGTON . EDU
KEPECS @ CSHL . EDU
RAO @ CS . WASHINGTON . EDU

University of Washington
Cold Spring Harbor Laboratory

It is well established that categorization tasks can be learned through reinforcement, based on reward
feedback about the successes and failures of past decisions. Statistical learning theory proposes that
active learners use not only reinforcements but also their current estimates of decision uncertainty to set
the size of updates. Recent behavioral data from an olfactory categorization task demonstrates that rats
show systematic decision biases in trials subsequent to rewarded trials, consistent with “active updating”
(Kepecs et al., Cosyne 2008). Moreover, neural data from the same tasks show that some orbitofrontal
cortex neurons encode the likelhood of success for categorization decisions, in a manner consistent with
representing decision uncertainty (Kepecs et al., 2008). We apply a recent probabilistic interpretation of
Support Vector Machine (SVM) classifers to explain the trial-by-trial updating of the decision boundary in
a normative fashion (Tong & Koller, 2000). In Bayesian SVMs (Sollich, 2002), the size of the margin for
a sample (distance of the separating hyperplane to the sample) is proportional to the likelihood of that
point belonging to a class given the classifier (separating hyperplane). After appropriate normalization,
this yields a measure of the posterior variance of the belief state given the current model, i.e., an estimate
of confidence about which odor mixture component predominates. Points with high posterior variance
are the most informative for updating the decision boundary (Dasgupta et al., 05), and thus the classifier
must be preferentially updated with these samples. We used online stochastic gradient descent with an
active learning rule to continually update the classifier on a trial-by-trial basis. We simulated the odor
categorization experiments and evaluated the performance of our model on synthetic data. The Bayesian
SVM model was able to reproduce several key features of the neural and behavioral data. First, the margins
of samples from different mixtures show the same characteristic patterns as the firing rates of OFC neurons.
Second, our model reproduces the behaviorally observed choice biases shown by rats following a rewarded

COSYNE 09

143

II-15
trial. Finally, the efficacy of the active learning rule is demonstrated in the degree of bias observed following
easy and difficult trials– there is very little change to the boundary following an easy trial. Our results provide
a new interpretation of active learning in categorization tasks in terms of margins in Bayesian classifiers.
doi:10.3389/conf.neuro.06.2009.03.041

II-15. Competitive acceleration: A surprising consequence of neural
decision-making
Michael Wojnowicz1
Michael Spivey2,1
Melissa Ferguson
1
2

MTW 28@ CORNELL . EDU
SPIVEY @ UCMERCED. EDU
MJF 44@ CORNELL . EDU

Cornell University
University of California, Merced

Mental representation often requires the formation of categorical judgments. Common psychological tasks
include forming a syntactic representation of a main clause rather than a reduced relative, the phonemic
representation of /b/ear rather than /p/ear, or the social evaluation of liking rather than disliking a social
group [1,2]. These categorical judgments are represented by neural population codes, which dynamically
evolve during real-time mental processing – that is on the time-scale of milliseconds [1]. The mind progressively accumulates evidence for multiple candidate mental representations, yet self-organizes into a
selected decision due to competitive dynamic inhibition between conflicting population codes. Computational models from seemingly disparate domains of psychology all rely on this basic neural decision-making
framework: the Normalized Recurrence Model (for language, visual attention, semantic categorization) [1],
the Leaking Competing Accumulator Model (for perceptual choice) [3], and Dynamical Field Theory (for
motor movement, spatial cognition, and working memory) [4]. The commonality is likely due to the fact that
dynamically self-organizing decisions are guaranteed by four core principles for biologically-based computation [1,2,5]: distributed representations, continuous cascading of partial information, competitive inhibition,
and recurrent feedback. Based on our simulations, neural decision-making dynamics possess an interesting property: stronger competition between candidate decisions produces greater acceleration into the
eventual decision (or, more precisely, into the population code representing the eventual decision state).
Intuitively, these results are surprising, as an uninformed intuition might predict that greater acceleration
into a decision would come from relatively clear support for decision A over decision B. Our simulations
reveal precisely the reverse: raising the probabilistic conflict distributed across evidential sources distorts
the temporal dynamics of neural decision-making, producing abnormally slow velocities in earlier moments
of processing time (due to stronger competitive inhibition), yet abnormally fast velocities in later moments
of processing time (as this competitive inhibition is released). The result is unusually strong acceleration
into the final attractor state. We illustrate the phenomenon of competitive acceleration by simulating high
vs. low conflict in each of the three major neural decision-making algorithms, and plotting activation levels
of the winning mental representation over normalized time. Thus, competitive acceleration is a surprising
property of dynamic self-organization in neural decision-making; it is basic enough to generally hold across
a variety of domains; and it provides a potential neuro-computational explanation for behavioral phenomena
that appear under psychological conflict. References: [1] Spivey, MJ (2007). The continuity of mind. Oxford: Oxford University Press. [2] Wojnowicz, M, Ferguson M, Dale D, & Spivey M. The self-organization of
deliberate attitudes. Invited resubmission, Psychological Science. [3] Usher, M., & McClelland, J. L .(2001).
On the time course of perceptual choice: The leaky competing accumulator model. Psychological Review,
108, 550-592. [4] Erhlagen, W & Schoner, G (2002). Dynamic field theory of movement preparation. Psychological Review, 109, 545-572. [5] O’Reilly, R (1998). Six principles for biologically-based computational
models of cortical cognition. Trends in Cognitive Sciences, 2, 455-462.

144

COSYNE 09

II-16
doi:10.3389/conf.neuro.06.2009.03.243

II-16. Probabilistic population coding of action selection in the basal
ganglia
Eyal Kimchi1
Nandakumar Narayanan2
Mark Laubach3

EYAL . KIMCHI @ YALE . EDU
KUMAR . NARAYANAN @ YALE . EDU
MARK . LAUBACH @ GMAIL . COM

1

Yale University School of Medicine
The John B Pierce Laboratory
3
Yale University
2

Successful foragers are flexible. They are able to find food in dynamic and unpredictable environments.
The basal ganglia have been implicated in behavioral flexibility (Wise and Murray, 2000). Recording studies
in primates have shown that neurons in the striatum are modulated following changes in stimulus-response
and stimulus-reward contingencies (e.g., Tremblay et al., 1998; Brasted and Wise, 2004; Pasupathy and
Miller, 2005). Here, we studied how neurons in the rat striatum are sensitive to changes in stimulus-reward
contingencies using a novel go/no-go reaction-time task. Rats made nosepoke responses to produce stimuli
and then either attempted to collect rewards (Go response) or initiate new trials (No-Go response). Two
stimuli had fixed values (S+ and S-). A third stimulus had flexible value (SW) and switched from being
unrewarded to rewarded within single sessions. The task was run with two of the stimuli presented in a block.
If the block contained S+ and SW, Go responses to SW were not rewarded. If the block contained S- and
SW, Go responses to SW were rewarded. Changes of the stimulus block were not signaled. We implanted
arrays of electrodes into the medial and ventral striatum of 14 rats and recorded simultaneously in both
areas during the task. The response properties of neurons were heterogeneous; however, many neurons
were modulated during the reaction time period (from 0-0.6 sec following the stimulus). To determine if
these neurons were sensitive to changes in the value of the SW stimulus, we used decoding methods to
compare activity from trials before and after the switches in the value of the SW stimulus. Firing rate was
measured in a 0.6 sec bin, starting at the stimulus, and Go and No-Go responses were decoded using
a naı̈ve Bayes classifier. We observed that neurons in the medial, but not the ventral, striatum changed
following switches in stimulus-reward contingencies. The time-course of changes in neuronal activity were
compared to the time-course of changes in behavioral responding using change-point analysis and showed
that neurons changed before behavior. That is, neurons predicted Go responding before animals made
Go responses. There was a considerable amount of trial-to-trial variability across groups of simultaneously
recorded neurons. Such variability may indicate that individual striatal neurons formulate unique hypotheses
about the availability of reward given the current stimulus-reward contingencies. To examine this issue, we
carried out three analyses. First, we plotted the posterior probabilities of simultaneously recorded groups
of neurons. These resembled probability distributions. Second, we shuffled trial orders for pairs of neurons
and decoded action selection. Shuffling did not increase predictions of Go responding. Third, we measured
functional interactions between neurons using information theoretic methods from Narayanan et al. (2005).
Most neurons (>60-75%) contributed independent information to the pairwise decoding of Go responding.
Together, these results suggest that the striatum may employ a probabilistic population coding scheme to
represent the action that should be made given the current stimulus-reward contingency.
doi:10.3389/conf.neuro.06.2009.03.246

COSYNE 09

145

II-17 – II-18

II-17. Structure learning in human sequential decision-making
Daniel Acuna
Paul Schrater

ACUNA 002@ UMN . EDU
SCHRATER @ UMN . EDU

University of Minnesota
Humans daily perform sequential decision-making under uncertainty to choose products, services, careers,
and jobs. Studies of sequential decision-making in humans frequently find suboptimal performance relative
to an ideal actor that knows the task that generates reward in the environment. This has led to conclusions about how we explore new courses of actions and exploit what we have learned. We argue, however,
that humans have uncertainty about both the task and environmental structure, and that task and structure
learning can potentially explain much better how people schedule actions, including behaviors previously
deemed sub-optimal. We illustrate the task structure learning problem with an important special case that
controls optimal exploration/exploitation. In particular, we formulate the structure learning problem using
mixtures of two reward models—two-arm and one-arm bandit models—and solve the optimal action selection using Bayesian Reinforcement Learning. These two reward models represent extremes in both the
exploration–exploitation tradeoff and computational difficulty—one model needs to balance exploration–
exploitation and use long future horizons to compute actions, while the other needs no look-ahead and the
action selection is greedy. In simulations, we show that optimal learning with uncertainty about the task
structure can produce a range of qualitative behaviors deemed suboptimal in previous studies on sequential binary choice. In our experiments, each of 16 subjects (8 females) ran on 32 bandit tasks, a block
of 16 in a two-arm bandits and a block of 16 one-arm bandits. Within blocks, the presentation order was
randomized, and the order of the one-arm bandits was randomized across subjects. On average, each task
required 48 choices. For two-arm bandits, the subjects made 1194 choices across the 16 tasks, and 925
for the one-arm bandits. Our results show that humans rapidly learn and exploit new reward structure—
human behavior tracks the behavior of our structure learning model but is not explained by models that
assume the task is known. Other kinds of reward structure learning may account for a broad variety of
human decision-making performance. In particular, allowing dependence between the probability of reward
at a site and previous actions can produce large changes in decision-making behavior. For instance, in a
”foraging” model where reward is collected from a site and probabilistically replenished, optimal strategies
will produce choice sequences that alternate between reward sites. Thus, uncertainty about the independence of reward on previous actions can produce a continuum of behavior, from maximization to probability
matching. Instead of explaining behavior in terms of the idiosyncrasies of a learning rule, structure learning
constitutes a fully rational response to uncertainty about the causal structure of rewards in the environment.
Our hope is that, by expanding the range of normative hypotheses for human decision-making, we can
begin to develop more principled accounts of human sequential decision-making behavior.
doi:10.3389/conf.neuro.06.2009.03.238

II-18. Towards inferring neural circuits from population calcium imaging
Joshua Vogelstein1
Adam Packer2,3
Rafael Yuste2,3
Liam Paninski2

JV. WORK @ JHU. EDU
ADAMPACKER @ COLUMBIA . EDU
RMY 5@ COLUMBIA . EDU
LIAM @ STAT. COLUMBIA . EDU

1

Johns Hopkins University
Columbia University
3
HHMI
2

146

COSYNE 09

II-19

Simultaneously imaging small populations of neurons using calcium sensors is now becoming routine in
labs across the world. We have developed analytical tools to maximally utilize these beautiful data sets.
In particular, the following neurobiological questions are of interest: (i) what are the response properties
of populations of neurons that are in close physical proximity, (ii) what is the functional circuit underlying
these response properties. Previously, we built a forward model characterizing the relationship between
stimuli, spike trains, intracellular calcium concentration, and fluorescence time series observations; and
then inverted that model using non-linear state-space methods (i.e., a particle-filter-smoother adapted for
this model, embedded in an expectation-maximization algorithm) [1]. Here, that forward model is generalized to allow for dependencies between neurons (i.e., cross-coupling terms), and the inference and learning
algorithm is appropriately modified as well. We show that given only a short movie (< 10 min), and reasonable assumptions on noisiness, spike rates, and coupling strengths, we can accurately reconstruct circuits
governing small populations (e.g., 10 neurons). As the number of neurons increases, or the data quality decreases, we can impose experimentally justifiable priors on the distribution of coupling weights, to improve
our estimates. We are currently pursuing confirming our parameter estimates using in vitro preparations.
In particular, by simultaneously imaging a population of neurons, and recording electrophysiologically from
at least one, we can validate our inferred connection strengths. Our hope is that they algorithms developed
here will be useful in a wide array of experiments and preparations, especially in vivo calcium imaging. [1]
Vogelstein, JT and Watson, BO and Packer AM and Yuste, R and Jedynak B and Paninski, L. Spike inference from calcium imaging using sequential Monte Carlo methods. Biophysical Journal, in press. Available
for download at: http://www.stat.columbia.edu/ liam/research/pubs/vogelstein-bj08.pdf
doi:10.3389/conf.neuro.06.2009.03.209

II-19. A neuronal network model for the detection of binary odor mixtures
Andrei Zavada
Thomas Nowotny

A . ZAVADA @ SUSSEX . AC. UK
T. NOWOTNY @ SUSSEX . AC. UK

University of Sussex
In many lepidopteran species, males are attracted to females by pheromones, which typically are specific
blends of two or more components. We constructed a minimal model, consistent with the known electrophysiology and structure of the Macroglomerular Complex (the site of odor processing specific to the
male antennal lobe), that selectively responds to a blend of pheromone components in a fixed ratio over a
wide range of concentrations. In the model the olfactory receptor neurons (ORNs) are represented by two
groups of 200 Poisson neurons, each group responding specifically to one pheromone component. The
concentration of the sensed component is encoded as the firing rate of the Poisson neurons. The ORN
populations project to three groups of primary local neurons (LNs). These inhibit both each other and two
intrinsically active secondary LNs, which in turn inhibit an intrinsically active projection neuron (PN). The underlying principle is the competition of the three primary LNs: given appropriate component concentrations
and inter-LN synaptic strengths, the generalist LN becomes active and suppresses the two specialist LNs,
both secondary LNs are inhibited and the PN is disinhibited to signal the presence of the correct blend. All
LNs and the PN are implemented as Hodgkin-Huxley neurons. We tested the model with stimuli including
all combinations of component concentrations, where the frequency of ORNs was in the range of 10 to
100 Hz in 10 Hz increments. Calculating the spike density function for the PN at midpoint of each stimulus
presentation, we obtained a 10×10 response matrix for all combinations. An optimal response corresponds
to a matrix with maximal values on the diagonal and 0 off-diagonal. We designed a “success functional”
by convolution of the response matrix with such a target response profile and, to optimize it, adjusted the

COSYNE 09

147

II-20
synaptic strength on the following connections: ORNs to primary specialist LNs, ORNs to primary generalist LNs, and on all interconnections between primary LNs. We observed that (a) the relative strength of
inputs from ORNs to specialist and generalist primary LNs must be specific (55-65% specialist compared
to generalist strength for 1:1 pheromone ratio); (b) similarly, generalist-specialist, specialist-generalist and
inter-specialist connections must have balanced synaptic strengths (in a ratio close to 0.94:1.20:1.10 for
1:1 component ratio). Modulation of ORN-LN connections ensures proper ‘engagement’ of the generalist
LN, while LN interconnections appear essential in narrowing the generalist LN response, failing which the
responses lose specificity to the component ratio. Finally, the number of ORNs needs to be large enough
(hundreds) to avoid instances of ‘false starts’, where the generalist LNs starts spiking at the onset of a stimulus with an inappropriate pheromone component ratio simply due to the irregularity of ORN input. A further
improvement is achieved by using LN groups (of 3 in this model) instead of individual LNs. This work was
supported by the Biotechnology and Biological Sciences Research Council (grant number BB/F005113/1).
doi:10.3389/conf.neuro.06.2009.03.236

II-20. Nonlinear identification for modeling and analysis of adaptive
neuronal systems
Uwe Friederich1
Daniel Coca1
Stephen Billings1
Mikko Juusola1,2
1
2

U. FRIEDERICH @ SHEFFIELD. AC. UK
D. COCA @ SHEFFIELD. AC. UK
S . BILLINGS @ SHEFFIELD. AC. UK
M . JUUSOLA @ SHEFFIELD. AC. UK

University of Sheffield
Beijing Normal University

Adaptation enables efficient encoding of sensory information in neurons and neuron chains. The underlying
physiology of adaptating processes is often complex and difficult to integrate into a biophysical model. In
our study, we address the problem how to quantify and analyse adaptation accurately in a stepwise empirical modeling procedure. Here we present a method to quantify adaptation in nonlinear dynamical models
that are estimated from experimental input-output measurements from electrophysiological or imaging experiments. We employ a nonlinear system identification methodology to determine the structure and to
estimate the unknown parameters of discrete-time NARMAX models, based on relatively short input-output
measurements (200 300 samples). From these models, we compute analytically generated generalized
frequency response functions, which are used to investigate the systems’ nonlinear spectral characteristics
and to convert the discrete-time models into continuous-time nonlinear differential equations. Models identified from small data sets, measured from the system at different adaptive stages, are then used to track
and quantify adaptation in the frequency and time domain. Complex adaptive mechanisms in the real system may only cause changes in the model parameters. The knowledge of how these parameters change
can be used to identify and model the underlying adaptation mechanism and ultimately develop a complete
model. We used the proposed approach to study light adaptation in the fruit fly Drosophila photoreceptors.
The photoreceptors constitute the first visual processing layer and can be easily stimulated by a point light
source. Using conventional sharp microelectrodes, we measured intracellularly graded voltage responses
of photoreceptors (output) to controlled light inputs. Adaptation to light backgrounds (or luminance levels)
enables photoreceptors to discriminate small light intensity variations (light contrast) over a vast input range
(>10,000-fold in our experiments). We presented the same light contrast pattern to the photoreceptor at
different luminance levels and studied adaptation in its voltage responses over time. NARMAX models,
estimated from small data windows at all background light levels, allowed us to track and quantify adaptive changes in the voltage responses by analysing their frequency responses and the inferred differential
equation models. Parameter changes in models were used to fit adaptation functions/curves that relate the

148

COSYNE 09

II-21
light background to model parameters. The integration of these functions in the estimated model structure
formed an adaptive model that is able to predict photoreceptor responses accurately for any tested light
backgrounds. These models can be used, for example, to simulate collocated photoreceptors, thus generating inputs for modelling studies of higher-order neuronal processing. Photoreceptors were used as an
example to describe neural adaptation. The presented method however, is general and can be used to
analyze, visualize and model neural adaptation independent of the systems’ complexity; even when having
little or noisy data.
doi:10.3389/conf.neuro.06.2009.03.031

II-21. A Bayesian method to predict the optimal diffusion coefficient
in random fixational eye movements
David Pfau
Xaq Pitkow
Liam Paninski

DBP 2112@ COLUMBIA . EDU
XAQ @ NEUROTHEORY. COLUMBIA . EDU
LIAM @ STAT. COLUMBIA . EDU

Columbia University
Under natural viewing conditions, our eyes alternate between saccadic movement and fixation. However,
even during fixation there are constant small movements, which can be decomposed into miniature saccades and diffusion-like random eye movements. Some diffusion helps prevent adaptation to a particular
stimulus, but diffusion also blurs the image of the world across the retina. Despite this, humans can resolve
fine spatial detail very well, and this diffusion may even enhance the ability to distinguish high-frequency
components of an image [1]. This suggests that the brain compensates for fixational eye diffusion and may
even extract useful information from it. To investigate the effect of eye diffusion on image reconstruction,
we extended a generalized linear model (GLM) of retinal encoding/decoding to incorporate random-walk
drift of the image falling on the retina. GLMs have been successfully applied to modeling a range of neural
systems, including retinal ganglion cells [2]. Previously developed GLMs of the retina, directly estimated
from spiking data, generate simulated network spike trains with the correct spatiotemporal filtering and
correlation structure. Finally, given this network spiking encoding model and a statistical model of the spatiotemporal visual inputs, there is a natural Bayesian method for decoding the response [3]. For our model
incorporating fixational eye diffusion, the decoding model would assign a probability to all possible random
walks the image could take. However, the number of possible paths grows exponentially with time, making
this method computationally intractable. Instead, we approximate the posterior distribution of images given
the observed spikes as a mixture of Gaussians, and track the diffusive movements of the mixture components by a particle filtering approximation. This method is both computationally tractable and effective at
reconstructing the encoded image. Preliminary results show that the image reconstruction is poor at both
very low and very high diffusion rates, while reconstruction works reasonably well at intermediate diffusion
rates. Thus, a well-defined optimal diffusion rate exists, and in general depends on statistical properties
of both the stimulus and the retinal spatiotemporal receptive fields, such as the strength of the sustained
response component and whether the transient component lasts longer than the persistence time of the
eye movements. We are currently pursuing quantitative comparisons to the real diffusion coefficient during head-fixed viewing. References [1] Miniature eye movements enhance fine spatial detail. M. Rucci,
R. Iovin, M. Poletti, and F. Santini, Nature 447(7146):851-854, 2007. [2] Spatio-temporal correlations and
visual signalling in a complete neuronal population. J. W. Pillow, J. Shlens, L. Paninski, A. Sher, A. M.
Litke, E. J. Chichilnisky and E. P. Simoncelli, Nature 454(7202):995-999, 2008. [3] Model-based decoding,
information estimation, and change-point detection in multi-neuron spike trains. J. W. Pillow, L. Paninski.
http://www.stat.columbia.edu/ liam/research/pubs/decoding-nc.pdf, 2008.
doi:10.3389/conf.neuro.06.2009.03.050

COSYNE 09

149

II-22 – II-23

II-22. Temporal memory and network dynamics
Peter Latham1
Edward Wallace2
1
2

PEL @ GATSBY. UCL . AC. UK
EWALLACE @ UCHICAGO. EDU

Gatsby Computational Neuroscience Unit, UCL
Dept. of Mathematics, University of Chicago

The brain is easily able to process and categorize complex time-varying signals. For example, the two sentences ”it is cold in Utah this time of year” and ”it is hot in Utah this time of year” have different meanings,
even though the words ”hot” and ”cold” appear about 3000 ms before the ends of the two sentences. A
network that can perform this kind of processing must, therefore, have a long memory. In other words, the
current state of the network must depend on events that happened many seconds ago, as well as events
in the last few milliseconds. This is particularly difficult because neurons are dominated by relatively short
time constants – tens to hundreds of milliseconds. Recently Jaeger and Maass et al. (2002) proposed that
randomly connected networks could exhibit the long memories necessary for complex temporal processing.
This is an attractive idea, both for its simplicity and because little fine tuning is required. However, a necessary condition is that the underlying network dynamics must be neither chaotic nor must it be an attractor
network; that is, it must exhibit Lyapunov exponents very close to zero (White et al., 2004; Bertschinger
and Natschlager, 2004). Biologically plausible model networks, though, tend to be chaotic (van Vreeswijk
and Sompolinsky, 1998), an observation that we have corroborated based on an extension of the analysis
used by Bertschinger and Natschlager. Real networks also tend to be very noisy – synaptic failures occur about 50% of the time. The question we address here, then, is: given the chaotic dynamics and high
noise intrinsic to biologically realistic networks, can randomly connected networks exhibit memories that
are significantly longer than the time constants of their constituent neurons? The answer, not surprisingly,
is ”no”. This answer is consistent with recent work by Ganguli et al. (2008), in which they analyzed temporal
memory in a very general setting. Jaeger and Maass, Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication. Science 304:78-80 (2001). Maass, Natschlager and
Markram, Real-time computing without stable states: A new framework for neural computation based on
perturbations. Neural Comput. 14:2531-2560 (2002). White, Lee and Sompolinsky, Short-term memory in
orthogonal neural networks. Phys. Rev. Lett. 92:148102 (2004). Bertschinger and Natschlager, Real-Time
Computation at the Edge of Chaos in Recurrent Neural Networks. Neural Comput. 16:1413-1436 (2004).
van Vreeswijk and Sompolinsky, Chaotic balanced state in a model of cortical circuits. Neural Comput. 10:
1321-1371 (1998). Ganguli, Huh and Sompolinsky, Memory traces in dynamical systems. Memory traces
in dynamical systems. PNAS 105:18970–18975 (2008).
doi:10.3389/conf.neuro.06.2009.03.070

II-23. An analysis of functional connectivity across timescales
Ian Stevenson
Konrad Kording

I - STEVENSON @ NORTHWESTERN . EDU
KK @ NORTHWESTERN . EDU

Northwestern University
Methods for estimating functional or effective connectivity between neural signals are becoming increasingly popular. However, different techniques record neural signals at different rates. While spike data is
often recorded with a temporal resolution in the sub-millisecond range, other signals like fMRI are typically
recorded with much lower temporal resolution. Here examine under what conditions connectivity inferred
from slow timescales matches that inferred from fast timescales. Using multi-electrode spike recordings
from motor cortex we smooth, down-sample, and estimate the effective connectivity between the underly-

150

COSYNE 09

II-24
ing signals using pair-wise Granger causality. We find that fast time-scale connectivity is robust to low-pass
filtering down to 1Hz and down-sampling to 2Hz. Estimates are also fairly robust to fixed-SNR Gaussian
noise. These results suggest that there is a large space in which connectivity estimates are relatively independent of temporal filtering and sampling rates. Pooling results from connectivity studies using relatively
fast signals, such as intrinsic signal imaging and spike data into joint estimates, thus seems reasonable. It
also speaks to the question to how fast imaging data should be acquired to allow for inference of connectivity between individual neurons; such devices should have at least a temporal resolution of about 2Hz.
Averaging over populations of neurons may allow for the inference of functional connectivity from very slow
signals. This may explain the success of connectivity estimates from fMRI signals using very low temporal
resolution. To examine why connectivity should be robust to filtering we fit spike data to two models: an
Ising model (maximal entropy, Schneideman et al., 2006) and a generalized linear model (GLM, Pillow et
al., 2008). The Ising model assumes that spikes are generated through a Bernoulli process with pair-wise
interactions between neurons. The GLM, on the other-hand, assumes that the firing rate of each neuron
depends on a history of spikes from the observed neurons. Both models estimate connectivity between
neurons, but the GLM takes into account possible delays and variations in the strength of connectivity
over fast time-scales. Using these two models we simulate spike data and perform the smoothing, downsampling analysis above. Connectivity from the Ising model simulations was not robust to smoothing and
down-sampling. GLM simulations (200ms history) were slightly more robust but not to the extent of the
original data. These results suggest that slow time-scale connectivity, while correlated with fast time-scale
connectivity, is not caused by fast time-scale connectivity. One possibility may be that common-input affects
connectivity across both fast and slow timescales.
doi:10.3389/conf.neuro.06.2009.03.102

II-24. Efficient coding of binocular spontaneous activity for innate
learning in V1 development
Mark Albert
David Field

MVA 6@ CORNELL . EDU
DJF 3@ CORNELL . EDU

Cornell University
We present a model of spontaneous activity in the developing LGN which shows how simple patterns of activity can train the early visual system for natural binocular vision before the eyes open. This model uniquely
applies principles of efficient coding (sparse coding/ICA) to show that both spontaneous activity and natural
stimuli can lead to a code much like that found in primary visual cortex. Spontaneous, patterned neural
activity plays a necessary role in visual system development. For example, spontaneous activity has been
recorded in the developing retinae of a variety of animals including turtles, chicks, cats, mice, and monkeys;
these retinal waves are necessary for proper LGN layer segregation and refinement. However, there has
been converging evidence both experimentally and theoretically that spontaneous activity in the developing
visual system is not only permissive but also instructive. A previous model by Albert, Schnabel, and Field
[1] has shown that an efficient learning algorithm used for natural scenes can be applied to spontaneous
activity patterns to generate a visual code resembling V1 simple cells. However, models based on insights
from retinal waves have some important limitations, such as independent activity across the eyes. Here
we present a model which further demonstrates how spontaneous activity can play an instructive role in
visual development without these retinal limitations. We generalize our spontaneous activity model to fit
known prenatal activity statistics in the LGN. The model presented here includes the binocular correlations
demonstrated in LGN activity by Weliky and Katz [2]. This work demonstrates correlations in spontaneous
activity across eye-layers in the LGN, and the data indicates that such correlations may be from a traveling
wavefront of activity in the LGN. The inclusion of these partial correlations generates binocular filters with

COSYNE 09

151

II-25
a disparity distribution similar to that found in animals at eye-opening. This indicates that these patterns
may be important for establishing both monocular and binocular properties of visual neurons. Although
there is much less data on LGN/V1 spontaneous activity, we believe that models such as this one will help
in guiding experiments in this area, and further establishing the role of this activity as instructive for visual
development.
doi:10.3389/conf.neuro.06.2009.03.302

II-25. Correlation-based learning in resonate-and-fire neurons
David Bouchain
Florian Hauser
Günther Palm

DAVID. BOUCHAIN @ UNI - ULM . DE
FLORIAN . HAUSER @ UNI - ULM . DE
GUENTHER . PALM @ UNI - ULM . DE

Ulm University, Germany
We investigate the effect of correlation-based learning (CBL) applied to resonate-and-fire neurons (E.
Izhikevich, Neural Networks 14:883-894, 2001; P. Huerta and J. Lisman, Neuron 15:1053-1063, 1995). By
projecting excitatory synapses onto a resonator, we show that CBL can enhance the efficacies of synapses
which fire with the resonator frequency and phase. Using MATLAB, we simulate one resonate-and-fire neuron with excitatory afferent conductance-based synapses (S. Song et. al., Nature Neuroscience 3:919-926,
2000). We implement two input configuration cases. In the first case, all synapses are active with different
frequencies in the range from 5 to 15 Hz (theta range), where synapses with the same frequency also
have the same phase. For the second case, we use synapses having the same frequency, but different
phases. Both cases are tested with Hebbian correlation learning (D. Hebb, The Organization of Behavior, Wiley, 1949) and spike timing-dependent plasticity (G. Bi and M. Poo, The Journal of Neuroscience
18:10464-10472, 1998). The synapses are trained by artificially stimulating the neuron to cause it to fire in
its preferred frequency (eigenfrequency), enhancing the synapses whose activity correlates with the neuron’s spikes. Before the artificial input is applied, the synapses are unable to cause the neuron to fire. During
training, the artificial input to the neuron is strong enough not to be suppressed by the partially out-of-phase
activity of the synapses, and the neuron thus displays a constant firing rate. When the neuron has received
artificial input for a while and adapted the synapses accordingly, this input is switched off so that the neuron
is stimulated only by EPSPs. The neuron’s behavior then depends on the extent of the training period. If
the synapses have not been strengthened enough, no further spiking occurs and thus no further adaptation
takes place. If the synaptic efficacies are allowed to saturate, the neuron continues firing with its preferred
frequency. If, however, the synapses are just strong enough to provoke a postsynaptic spike, the neuron
first displays a low firing rate, emitting a spike when multiple presynaptic inputs overlap. Adaptation of the
synapses continues, and the neuron increases its firing rate until it reaches its preferred frequency. Suppressing the synaptic activity for a while and allowing the neuron to return to its resting potential does not
affect its excitability. Thus as soon as the synapses are activated again, the neuron continues to emit spikes.
Both input cases work with both synaptic learning mechanisms. With fixed phase, but different frequencies,
the postsynaptic neuron selects those inputs which have its eigenfrequency, whereas a fixed frequency
with different phases causes synapses with a phase similar to the neuron’s phase to be strengthened. This
shows that resonate-and-fire neurons are amenable to common synaptic plasticity mechanisms and can
therefore be used for future research on heterogeneous neural networks, incorporating both integrators and
resonators.
doi:10.3389/conf.neuro.06.2009.03.027

152

COSYNE 09

II-26 – II-27

II-26. Functional Connectivity between Neuronal Ensembles through
Nonlinear Modeling
Theodoros Zanos
Robert Hampson
Sam Deadwyler
Theodore Berger
Vasilis Marmarelis

ZANOS @ USC. EDU
HAMPSON @ WFUBMC. EDU
SDEADWYL @ WFUBMC. EDU
BERGER @ USC. EDU
VZM @ USC. EDU

University of Southern California
Increasing availability of multi-unit data gives new urgency to the need for effective tools to analyze the
recorded activity of neuronal ensembles. Moreover, the implementation of neuroprosthetic devices requires
a reliable and accurate quantitative representation of the input-output transformations present in the system under study. Nonparametric, data driven models with predictive capabilities are excellent candidates
for these purposes. When modeling input-output relations in multi-input neuronal systems, it is important
to select the subset of inputs that are functionally and causally related to the output. Inputs that do not
convey information about the actual transformation not only increase the computational burden but also
affect the generalization of the model. Moreover, a reliable functional connectivity measure can provide
patterns of information flow that can be linked to physiological and anatomical properties of the system.
We propose a method based on the Volterra modeling approach that selects distinct subsets of inputs
for each output based on the prediction of the respective models and its statistical evaluation using the
Mann-Whitney statistic. The algorithm builds successive models with increasing number of inputs and
examines whether the inclusion of additional inputs benefits the predictive accuracy of the overall model.
The method accounts for nonlinear causal relationships between the inputs and outputs. It also explores
possible second-order (inter-modulatory) cross-interactions among the inputs. The method’s robustness
to various cases of point-process noise (spurious spikes, spike jitter, deleted spikes, missasigned spikes)
was tested through simulated examples. Comparison of the proposed algorithm’s performance with widely
used methods such as Granger causality and cross-coherence based methods reveals greater robustness
to noise. The method was applied to multi-unit recordings from the CA3 (input) and CA1 (output) regions of
the hippocampus in behaving rats, in order to reveal spatiotemporal connectivity maps of the input-output
transformation taking place in the CA3-CA1 synapse. The contribution of the nonlinear components and
the inclusion of cross-interacting inputs accounts for 25% and 8% more connections respectively, when the
algorithm is applied to hippocampal data. The method provides a practical, data driven and computationally efficient way to explore causal connections among multiple neuronal ensembles, while accounting for
nonlinearities and cross-interactions among the inputs of the system.
doi:10.3389/conf.neuro.06.2009.03.116

II-27. A Biophysically Inspired Model for Contrast Adaptation
Yusuf Ozuysal
Stephen Baccus

OZUYSAL @ STANFORD. EDU
BACCUS @ STANFORD. EDU

Stanford University
Contrast adaptation is a process that changes the gain, kinetics and baseline membrane potential of retinal
ganglion cells over timescales that range from less than 100 ms to tens of seconds. Several models have
been proposed that can replicate some of these adaptive processes (Borst et al, 2005, Gaudry, Reinagel
2007, Mante et al, 2008), although it is not clear how these models translate into biophysical mecha-

COSYNE 09

153

II-28
nisms. Here we present a simple biophysically inspired model of an adaptive cell that tracks the gain with
sub-second precision and replicates fast kinetic and baseline membrane potential changes of salamander
retinal ganglion cells over a wide range of contrasts. The model is comprised of three sequential stages,
a linear filter, a static nonlinearity and a three state first order kinetic model. We recorded from ganglion
cells intracellularly while presenting a uniform field visual stimulus. The light intensity was drawn from a
Gaussian distribution every 30 ms, and every 20 s the standard deviation of the distribution changed randomly to between 10 % and 40 % of the mean intensity. The parameters of the model were fit using a
constrained optimization method to minimize the error between the model and the membrane potential response. The overall correlation between the actual and estimated responses was 72% for a single set of
model parameters whereas it was 65% when we used many separate Linear-Nonlinear (LN) models fit to
each contrast. In the adaptive stage of the model, the states can be thought of as representing “available”,
“active” and “unavailable” states of a biophysical mechanism. This flexible framework can be applied to ion
channels that inactivate, synapse vesicle pools that experience depression, or neurotransmitter receptors
that desensitize. With three states and one rate constant that is set by the output of the static nonlinearity,
increases in contrast produce a Weber-like change in gain, an acceleration in kinetics, an increase in baseline, and asymmetric dynamics in the response, as is seen experimentally. These changes are intrinsic
to the adaptive stage without the need for an additional pathway to control the response properties. The
incorporation of an additional state or input-dependent rate constant produces additional, slower timescales
of adaptation. Although this model is applied here to the process of contrast adaptation, the adaptive stage
changes the overall response properties by virtue of a change in mean produced by the output of the threshold nonlinearity at different contrasts. As such, this type of adaptive stage may be applicable to other types
of adaptation such as luminance adaptation with a different input stage. A simple biophysical model can
capture multiple phenomena of adaptation, indicating that seemingly distinct adaptive processes could be
generated even by a single molecule with just a few states. The parameters of this model can be to used
design and interpret experiments on the mechanisms that produce these adaptive properties.
doi:10.3389/conf.neuro.06.2009.03.346

II-28. Modeling the Temporal Bisection Task in humans and rats.
Charles Kopec
Carlos Brody

CKOPEC @ PRINCETON . EDU
BRODY @ PRINCETON . EDU

Princeton University
The perception and processing of temporal information is fundamental to all higher cognitive functions.
Two characteristics of temporal perception that have been extensively explored in both human and animal
subjects are: 1) Whether the neural representation of time is linear or logarithmic; 2) Whether the natural
bisection point between two reference durations is the geometric or arithmetic mean. A clear solution to
each of these points will serve as a foundation on which computational models can be constructed to better
explain how the brain processes time. These questions are often studied with the temporal bisection task
which is designed to measure a subject’s perceptual midpoint between two reference durations. Briefly,
subjects are first trained with reinforcement to discriminate two reference durations, DL “Long” and DS
“Short”. After training, subjects are then presented with intermediate durations and asked to categorize
each as “long” or “short” with no reinforcement. Human subjects tend to bisect arithmetically (indifference
pt (DL + DS)/2) when the spread between the reference durations is large (DL >4 DS ) but geometrically
(ind. pt.
sqrt(DL DS))when the spread is small (DL <2 DS). In contrast, animal subjects consistently
bisect geometrically even at large spreads (e.g., in our own experiments, where DL =10 DS). One possible
conclusion is that humans process temporal information differently from other animals. In particular, this
has led to the suggestion that humans represent time linearly (leading to arithmetic bisection) whereas
rodents represent time logarithmically (leading to geometric bisection). Here we take a model that uses a

154

COSYNE 09

II-29
linear representation of time and augment it by modeling a rat’s motivation to switch between “short” and
“long” response options. Since rewards are delivered only at the end of the stimulus presentation, we use
hyperbolic discounting curves as estimates of the rat’s motivation for reward at any point in time during each
trial. Counterintuitively, we find that once such motivation is taken into account, a linear representation of
time can nevertheless produce geometric bisection. We therefore propose that the difference between rat
and human behavior is not due to different internal representations of time. Instead, it is due to the fact that
humans are given specific verbal instructions on how to perfom the task, while rats base their performance
only on trial-and-error experience at the stimulus duration endpoints. A question that remains is why it is
that humans don’t always bisect arithmetically (but instead bisect geometrically when the reference duration
ration is small (DL <2 DS)). We performed a meta-analysis of the human temporal bisection literature and
were able to develop a two-step decision model that uses a linear representation of time and accurately
reproduces the main features in the data: geometric bisection at small spreads, arithmetic bisection at
large spreads, and increasing Weber ratio with increasing spread. In sum, linear representations of time
can be used to accurately explain both human and rat performance in bisection tasks, suggesting that the
neural mechanisms underlying time perception in these two highly disparate species may nevertheless be
very similar.
doi:10.3389/conf.neuro.06.2009.03.156

II-29. Nonlinearity, memory, and phase transitions in animal learning
Ilya Nemenman1,2
1
2

ILYA @ MENEM . COM

Los Alamos National Laboratory
CCS-3 and CNLS

Abstracting from physiological details, we present a theory that suggests an explanation behind critical periods and other phenomena in animal learning as a natural consequence of learning dynamics under a small
and realistic set of assumptions. Namely, we describe animal learning as a Bayesian inference problem,
where the subject tries to estimate a certain real-valued variable describing the state of the world using sensory data in the context of some a priori assumptions. Further, we assume that, in this process, the following
holds. (a) The animal has no access to the entire a posteriori distribution of the estimated quantity, and its
estimate is given by randomly sampling the distribution, rather than calculating the posterior expectation.
(b) The penalty for learning incorrectly (the loss function) is not Gaussian, but saturates at large mismatch
between the state of the world and the estimate, so that finding the mode of the posterior is not a liner process. Finally, (c) the subject’s a priori assumptions about the temporal dynamics of the estimated variable
are that is varies continuously, but the dynamics has multiple temporal scales, so that long-term, multiscale memory is essential to represent the temporal structure of the sensory data. We notice that typical
protocols in animal learning experiments often contradict the latter assumption, and, instead, the subject’s
sensory experience may involve long periods of stationarity punctuated by rapid changes designed to evaluate the response to them. We show that the three assumptions above and this mismatched structure of
experiments are sufficient to explain a variety of animal learning phenomena, capturing similarities and differences between different experiments, such as those on critical periods in auditory-visual maps alignment
in barn owls and matching experiments in rats. For example, dependence of the typical learning time on the
dynamics of changes in the prior experiences follows naturally from the theory. Similarly, critical periods will
emerge as phase transitions between local maxima of the posterior distribution, but this requires a saturating loss function, and, therefore, explains their absence in the rat matching experiments. Further, abrupt,
single trial learning can be predicted, and the tendency of subjects to reverse to the status quo following a
transient learning experience is also easy to explain. While currently available experiments are too crude
to verify or reject the theory, we suggest new, realistic experiments that can be used for this goal. To this
end, we propose experimental protocols that should introduce critical periods and reversals to status quo

COSYNE 09

155

II-30
into experimental systems where they have not been observed before, or remove these phenomena from
systems where they are known to exist. We emphasize that the theory developed here is not a physiological theory, able to explain mechanisms underlying the observed learning phenomena. Instead it should be
viewed as a learning-theoretic explanation for why such seemingly suboptimal learning mechanisms and
behaviors are expected to emerge given the properties of the surrounding world.
doi:10.3389/conf.neuro.06.2009.03.183

II-30. When can rates be reliably transmitted in feedforward networks?
Kendra Burbank1
Gabriel Kreiman2,3

BURBANK @ POST. HARVARD. EDU
GABRIEL . KREIMAN @ CHILDRENS . HARVARD. EDU

1

Children’s Hospital, Boston
Children’s Hospital Boston, Harvard Med.Sch.
3
Center for Brain Science, Harvard University
2

Under what conditions can firing rate information be quickly and reliably transmitted from layer to layer in
a feedforward network? Information transmission in the brain requires transforming inputs across many
layers of computation without pathological synchrony (where all/most neurons in a given layer fire in unison), firing rate explosion (a monotonic increase in firing rates from layer to layer), or exponential decays
in firing rates. Several authors have argued that such transmission is feasible whereby the neurons in a
given layer can be made to fire at the desired rate but at random, uncorrelated times [1,2]. The averaged
firing rate of many such unsynchronized neurons will accurately reflect the correct rate on timescales significantly shorter than the average inter-spike interval. Shadlen and Newsome claimed that a feedforward
network with balanced excitation and inhibition could transmit rate information without the buildup of synchrony [3]. However, Litvak et al disputed this claim, using computer simulations to show that, over as
few as 6 to 7 layers, this network would either tend towards synchronous firing or asynchronous firing at
a rate which was independent of the initial rate [4]. Van Rossum et al proposed combating the buildup of
synchrony by introducing noise into the system through the addition of a noisy background current [5]. We
propose an alternative solution, using a feedforward network where inhibition outweighs excitation. We use
analytical expressions for the mean and variance of a neuron’s firing given the statistics of its inputs [6,7].
We propose network parameters which produce a gain close to unity, that is, that allow reliable transmission
of rates over multiple layers. Furthermore, the network maintains a high variance in the distribution of rates
across neurons within a layer. We find such parameters both for models of current-based integrate-andfire neurons and for models of conductance-based neurons. Two additional constraints should be imposed
in the development of theoretical models for information transmission in feedforward networks: biological
plausibility and robustness of the simulations to parameter changes. We therefore examine the biological
plausibility of the proposed network, we ask whether weights need to be fine-tuned and evaluate the robustness to noise sources. Finally, we use simulations to confirm that networks with these parameters are
capable of robustly transmitting rate information over many layers without the buildup of synchrony. [1]
Softky and Koch, J. Neurosci, (1993) 13(l): 334-350 [2] Shadlen and Newsome, Curr. Op. Neurobiology
(1994) 4(4):569-579 [3] Shadlen and Newsome, J. Neurosci, (1998), 18(10):3870-3896 [4] Litvak et al. J.
Neurosci, (2003), 23(7):3006-3015 [5] van Rossum et al. J. Neurosci (2002) 22(5):1956-1966 [6] Brunel. J.
Physiol. (Paris) (2000) 94: 445 – 463 [7] Meffin et al. J. Comp. Neurosci (2004)
doi:10.3389/conf.neuro.06.2009.03.200

156

COSYNE 09

II-31 – II-32

II-31. A very general linear-nonlinear model for the spatio-temporal
characterization of visual cells from
Joaquin Rapela1
Gidon Felsen2
Jonathan Touryan3
Jerry M. Mendel1
Norberto M. Grzywacz

RAPELA @ USC. EDU
FELSEN @ CSHL . EDU
TOURYAN @ GMAIL . COM
MENDEL @ SIPI . USC. EDU
NMG @ BMSR . USC. EDU

1

University of Southern California
Cold Spring Harbor Laboratory
3
Science Applications International Corp.
2

The large dimensionality of inputs greatly complicates the characterization of visual cells from input-output
data. To make such characterizations possible, dimensionality reduction techniques are needed. A common
approach is to assume that the cell response follows a linear-nonlinear model. A generic version of this
model has too many parameters. So, previous methods have estimated simplified versions of this model.
Also, some of these methods can only be used with Gaussian noise inputs. Here, instead of simplifying
the model, we develop the extended Projection Pursuit Regression (ePPR) algorithm that, using an efficient
optimization strategy, allows the estimation of all the parameters of the generic linear-nonlinear model from
natural images. We first test the feasibility of ePPR with a model of a complex cell having suppressive
filters. We then use ePPR to characterize V1 cells in anesthetized cats. By showing that a first ePPR
model estimated from responses of a cortical complex cell to natural images is very similar to a second
ePPR model estimated from responses of the same cell but to random images, we verify that ePPR can
characterize visual cells using arbitrary stimuli. Despite their similarity, only the ePPR model estimated from
natural images showed late suppressive components, suggesting that the observable response properties
of visual cells depend on the statistics of the inputs used to probe them. Moreover, ePPR successfully
characterized a simple cell, verifying that it can be used to model a wide variety of visual cells. For both
type of stimuli and both types of cells, the predictions of the ePPR model are substantially better than those
of previous methods. To our knowledge ePPR is the first algorithm to estimate, from natural and random
images, a generic linear-nonlinear models that is spatio-temporal, uses two-dimensional images as inputs,
and whose linear component contains more than one filter.
doi:10.3389/conf.neuro.06.2009.03.286

II-32. Validating a Bayesian model of conflicting sensory inputs
Rama Natarajan1
Iain Murray1
Ladan Shams2,3
W. David Hairston4
Richard Zemel1

RAMA @ CS . TORONTO. EDU
MURRAY @ CS . TORONTO. EDU
LADAN @ PSYCH . UCLA . EDU
DAVE . HAIRSTON @ ARL . ARMY. MIL
ZEMEL @ CS . TORONTO. EDU

1

University of Toronto
UCLA
3
California Institute of Technology
4
US Army Research Laboratory
2

How do neural systems resolve the problem of determining which of a multitude of sensory inputs are
appropriate for integration? This issue has been widely studied by introducing sizeable spatial and/or tem-

COSYNE 09

157

II-33
poral discrepancies between multi-sensory stimuli, creating vivid illusions. Perceptual consequences of the
interactions between conflicting stimuli have been effectively explored in the ventriloquist illusion. Here,
localization of an auditory target is strongly biased towards a disparate visual distractor, often accompanied
by perceptual unification of the two stimuli. In one interesting study of this effect, Wallace et. al. (Exp.Brain
Res., 2004) reported that human response variability was much higher on those trials judged as non-unity.
Curiously, variance was highest for zero-disparity and decreased systematically with increasing stimulus
disparity. Localization judgments became increasingly biased against visual location. On unity trials, variance increased with increasing disparity and localization judgment was nearly completely biased towards
visual location regardless of disparity. These observations led us to wonder what computational principles
might underlie the non-linearities in response behavior. We build on the approach that visual and auditory
cues are evaluated under two discrete hypotheses regarding the causal structure underlying generation of
the stimuli: if there is a common cause, cues are combined, otherwise they are segregated. The standard
Bayesian approach to inference is to integrate evidence over both hypotheses weighted by the belief in
each of them. Earlier studies assumed that the sensory likelihoods and stimulus prior were well described
by Gaussian functions; models of the Wallace et. al. data focused on fitting just the summarized statistics
such as response variance and bias. While some of the predicted trends were similar to the behavioral
data, the models exhibited significant differences from experimental observations. In this study, we go one
step further to validate the Bayesian model more thoroughly than previously done, by analyzing its capacity
to explain salient details in the raw response histograms for different stimulus conditions. Particularly, our
analysis of the data revealed that the ratio of unity/non-unity responses for each disparity increased with
increasing distance of visual stimulus from straight-ahead. This was true even for zero-disparity, where in
fact the largest differences in ratio across stimulus conditions were observed; the magnitude of change in
ratio decreased with disparity. Detailed analysis of the model revealed that the sensory likelihoods and
stimulus prior should have substantially heavier tails than previously assumed. A heavy-tailed prior over
stimulus locations, narrowly centered in stimulus space, along with differences in precision of visual and auditory likelihoods, accounted for the non-linearity in unity/non-unity response ratio quite well. We show that
instead of Bayes-optimal inference, subjects employed a more approximate strategy by first selecting the
most probable hypothesis and basing subsequent inference on that hypothesis alone. Our approach made
testable predictions concerning (i) distribution of localization errors for incongruent stimulus conditions, (ii)
decrease in subjectsúnity perception during the course of an experiment and (iii) decrease in response
variability during the experiment. The data confirm all these predictions.
doi:10.3389/conf.neuro.06.2009.03.287

II-33. Sleep as a Monte-Carlo: offline training of grammar-like models
of semantic memory in the neocortex
Francesco Battaglia
Cyriel Pennartz

F. P. BATTAGLIA @ UVA . NL
C. M . A . PENNARTZ @ UVA . NL

Universiteit van Amsterdam
In one view of memory consolidation, information is exchanged between two interacting memory systems:
the first, centered around the hippocampus, stores episodes, defined as organized sets of memory items
together with their spatial and temporal context, and their interrelationships. A second, semantic memory
system, with a strong and distributed role for the neocortex, contains a rich and flexible depiction of the
regularities in the world, which could be conveniently described as a statistical generative model of the
world. Training such a generative model is a very hard task, as it targets extremely complex, and largely
unpredictable structures. Computational linguistics provides very powerful statistical tools to describe, in
a concise and computationally effective way, linguistic information, which is of comparable complexity.

158

COSYNE 09

II-34
Stochastic context-free grammars (SCFG) can be trained from a corpus of utterances through statistical
procedures, like the Inside-Outside (IOA) algorithm, which tunes the transition probabilities of a branching
process generating all possible parsing trees. Parsing trees, i.e. hierarchical groupings of items, are a
compelling strategy to represent non-linguistic semantic knowledge as well. However, the application such
algorithms is limited, in this case, by the lack of a sequential ordering of the items, which is characteristic of linguistic material. We adapted the IOA so that each episodic memory is expressed as a matrix of
associations between items, with this association matrix obviating the need for sequential ordering. Moreover, we show that the algorithm maps into an offline neural dynamics for the neocortex, orchestrated by
an input stage (roughly identifiable with the hippocampus) that reactivates the association matrix in terms
of the correlations between unit activities. The inside and outside probabilities in the IOA map on, respectively, bottom-up and top-down influences in the hierarchy of cortical areas, and the model can be trained
by means of a Hebbian learning rule local to each cortical micro-circuit. During sleep, the driving force for
learning are pairwise correlation patterns in the reactivated neural activity, which convey episode about the
content and the structure of each rehearsed episode. This process is modeled on some features of memory
replay as it has been experimentally demonstrated in the hippocampus and the neocortex. The model can
reproduce several memory consolidation-related effects, such as the decontextualization of memories and
the generation of ”insight”, i.e. the discovery of hidden, higher order structure.
doi:10.3389/conf.neuro.06.2009.03.009

II-34. What is stored in the hippocampus during tactile discrimination
behavior?
Pavel Itskov
Ekaterina Vinnik
Mathew Diamond

ITSKOVPA @ GMAIL . COM
LULSWINNIK @ GMAIL . COM
DIAMOND @ SISSA . IT

International School for Advanced Studies
Although there is a large body of anatomical evidence indicating that pathways from all sensory modalities
converge in the hippocampus, there is currently little understanding of the nature, quantity, and time course
of information present in hippocampal neurons during a controlled behavioral task. In the current work, we
recorded single neurons in the CA1 region of hippocampus while rats performed a whisker-guided tactile
texture categorization task. On each trial, the rat (i) perched at the edge of a platform and touched a texture
with its whiskers, (ii) turned to the reward spout (Left or Right), and (iii) collected the water reward. There
were 3 different textures: 2 of them were associated with one reward location (category “Left”) and the 3rd
was associated with the opposite location (category “Right”). Texture-Category rules were varied across
rats. Trials typically lasted 2s, with small variations. The main result is that most CA1 neurons (152 out of
217) carried a record of behavior through systematic modulation in firing rate. Specifically: 57% of neurons
carried significant information (quantified by Shannon’s mutual information) about the category of stimulus
(Left vs Right) encountered on a given trial (mean Info = 0.32 bits; entropy = 1 bit). 19% of neurons carried
significant stimulus-specific information, meaning that they fired at different rates for the two textures of
the same behavioral category (mean Info = 0.09 bits; entropy = 1 bit). 28% of neurons carried significant
information about the time course of the trial independently of the stimulus encountered (mean Info = 0.38
bits; entropy = 3.16 bits for 9 time bins). By seeing how whole-trial information magnitude varied according
to bin size, we could determine the time scale of encoding. Category information was optimized when
firing rate was measured in a sliding 400ms window, while Texture information was optimized in a sliding
25ms window; thus stimulus identity is represented by finer firing rate modulation than is stimulus category.
Several observations support the notion that hippocampus shifted between two discrete states during each
trial. The first observation was the temporal modulation in the quantity of information carried by neurons, as

COSYNE 09

159

II-35
revealed by K-means clustering of the temporal profiles. A large fraction of category-informative neurons
(35 of 125) exhibited 2 peaks of information – the first during texture sampling and the second during reward
collection. Moreover, during the approach to and contact with the texture, there was a notable increase in
LFP power in the theta range compared to pre-trial. Just prior to reward collection, there was a sharp
drop in the theta power and an increase in power in the beta and lower gamma range (25 – 45 Hz). We
hypothesize that during the first phase, CA1 was open to incoming sensory signals from neocortex. At the
transition, CA1 stopped collecting signals from neocortex and, instead, began to “recall” information about
the just-encountered stimulus and category. In this way, we speculate, hippocampus can support learning
by forming a bridge in time between the stimulus and the successful acquisition of reward.
doi:10.3389/conf.neuro.06.2009.03.290

II-35. The Secret Life of Kernels: Reconsolidation in Flexible memories
Dimitri Nowicki
Hava Siegelmann

NOWICKI @ CS . UMASS . EDU
HAVA @ CS . UMASS . EDU

University of Massachusetts Amherst
In this paper we are presenting memory reconsolidation in kernel associative memories. Reconsolidation
is an important process in memory dynamics that is observed both in neurophysiological and psychological studies, as well as modelled in various artificial neural systems. The memory tracks changes in the
environment or in associations among objects. Recent models predict that memory representations should
be sensitive to learning order, consistent with psychophysical studies of face recognition and electrophysiological experiments on hippocampal place cells. Our goal is to show that such reconsolidation effects are
possible in more flexible environment, dealing with large-scale data. As memory model we introduce a special neural system that while relying on some of the analysis developed by Hopfield has memory attractors
that do not lie in the input or neural space but rather in an abstract unbounded high-level kernel space.
For reconsolidation we choose the principle of global memory update. It is more stable than updating the
attractor closest to current input, and also this technique enables direct analogy to existing reconsolidation
methods in classical Hopfield networks. We establish a metric in between kernel associative memories. In
the feature space this metric is equivalent to the distance between the weight matrices of two networks,
but in input space it is a Riemannian metric. Subsequent application of this procedure implies dynamic
reconsolidation of memories that stay always consistent to the changing environment. Basing on this metric we construct an elementary update procedure – one step of reconsolidation. Memory is shifted in the
direction of new input along a geodesic curve in corresponding Riemannian space. Our first experiment
was made using the MNIST database of handwritten digits. We examined network’s ability to track images
gradually varying in time. For this purpose a learning set of rotating digits was created. The learning set
contained 9000 images. They were obtained from 100 original digits (10 per class) by rotating them counterclockwise on angle from 0 to 180o.Classification was tested on the set of 1000 images closest to the final
state. Obtained recognition quality was 96.4%. The second experiment is similar to one previously made
in humans. Attractor tracking was investigated using sequences of morphed faces from Productive Aging
Lab’s Face Database. When the learning order follows image order in the morphing sequence, attractors
changed gradually and consistently. The ability to recognize the initial set of images gradually decreased
when attractors tended to the final set. In case of random learning order attractors quickly got messy, with
no significant ability to distinguish faces. This experiment also demonstrates efficiency of reconsolidation
in kernel memories for high-dimensional data. Based on these results we conclude that reconsolidation in
kernel memories is both computationally efficient and biologically plausible, and it can model phenomena
previously observed in human memory.

160

COSYNE 09

II-36 – II-37
doi:10.3389/conf.neuro.06.2009.03.271

II-36. A study on medial temporal lobe online learning neuronal network model with active dendrites
Xundong Wu
Bartlett Mel

XUNDONGW @ USC. EDU
MEL @ USC. EDU

University of Southern California
Previously in a medial temporal lobe recognition memory model that incorporates evidence for local dendritic spikes, we found that applying certain biologically-plausible long-term potentiation (LTP) and homeostasis plasticity mechanisms can substantially boost one-shot learning memory storage capacity. It was
shown that capacity is dramatically boosted when (1) LTP is dendrite-specific and gated by two separate
learning thresholds that are connected with AMPA and NMDA currents respectively, and (2) homeostatic
synaptic depression inside dendrites preferentially targets the least-recently potentiated synapses. In this
work we have studied the scaling of storage capacity with network size. Theoretical predictions and empirical tests in simulated networks containing up to 41 million synapses indicate that capacity scales slightly
sublinearly with the number of synapses N in the network, following a power law of Nˆ0.86 using optimal
network parameters. We also show that capacity is highly sensitive to input spike noise, and discuss known
biological mechanisms that may exist to attenuate this particularly troublesome noise source. In contrast to
its sensitivity to input noise, we found that capacity decays only mildly in the presence of substantial distortion of the synaptic age tags used to record when synapses were potentiated. This releases the system
from the requirement to precisely track synaptic age. We are currently studying how dendritic morphology
interacts with the various noise sources to affect system storage capacity.
doi:10.3389/conf.neuro.06.2009.03.309

II-37. Tag-Trigger-Consolidation: A model of early and late long-term
potentation and depression
Claudia Clopath1
Lorric Ziegler1
Lars Buesing
Eleni Vasilaki
Wulfram Gerstner2,3

CLAUDIA . CLOPATH @ EPFL . CH
LORRIC. ZIEGLER @ EPFL . CH
LARS @ IGI . TU - GRAZ . AC. AT
ELENI . VASILAKI @ EPFL . CH
WULFRAM . GERSTNER @ EPFL . CH

1

LCN
Brain Mind Institute, EPFL
3
Computer and Communication Sciences, EPFL
2

Changes in synaptic efficacies need to be long-lasting in order to serve as a substrate for memory. Experimentally, synaptic plasticity exhibits phases covering the induction of long-term potentiation and depression
(LTP/LTD) during the early phase of synaptic plasticity, the setting of synaptic tags, a trigger process for
protein synthesis, and a slow transition leading to synaptic consolidation during the late phase of synaptic
plasticity. We present a mathematical model that describes these different phases of synaptic plasticity.
The model explains a large body of experimental data on synaptic tagging and capture, cross-tagging, and

COSYNE 09

161

II-38
the late phases of LTP and LTD. Moreover, the model accounts for the dependence of LTP and LTD induction on voltage and presynaptic stimulation frequency. The stabilization of potentiated synapses during the
transition from early to late LTP occurs by protein synthesis dynamics that is shared by groups of synapses.
The functional consequence of this shared process is that previously stabilized patterns of strong or weak
synapses onto the same postsynaptic neuron are well protected against later changes induced by LTP/LTD
protocols at individual synapses.
doi:10.3389/conf.neuro.06.2009.03.197

II-38. Structural plasticity and memory: Catastrophic forgetting, amnesia, and the spacing effect
Andreas Knoblauch
Marc-Oliver Gewaltig
Ursula Körner
Edgar Körner

ANDREAS . KNOBLAUCH @ HONDA - RI . DE
MARC - OLIVER . GEWALTIG @ HONDA - RI . DE
URSULA . KOERNER @ HONDA - RI . DE
EDGAR . KOERNER @ HONDA - RI . DE

Honda Research Institute Europe
The neurophysiological basis of learning and memory is commonly attributed to the modification of synaptic
strengths in neuronal networks. Recent experiments suggest also a major role of structural plasticity including elimination and regeneration of synapses, growth and retraction of dendritic spines, and remodeling of
axons and dendrites. Here we develop a simple model of structural plasticity and synaptic consolidation in
neural networks and apply it to Willshaw-type models of distributed associative memory [1]. Our model assumes synapses with discrete weights. Synapses with low weights have a high probability of being erased
and replaced by novel synapses at other locations. In contrast, synapses with large weights are consolidated and cannot be erased. Analysis and numerical simulations reveal that our model can explain various
cognitive phenomena much better than alternative network models employing synaptic plasticity only. First,
we show that networks with low anatomical connectivity employing structural plasticity in coordination with
stimulus repetition (e.g., by hippocampal memory replay) can store much more information per synapse by
“emulating” high effective memory connectivity close to potential network connectivity. Moreover, such networks suffer to a much lesser degree from catastrophic forgetting than models without structural plasticity
if the number of consolidated synapses remains sufficiently low. Second, we show that structural plasticity
and hippocampal replay lead to gradients in effective memory connectivity. This means, neuronal ensembles representing remote memories show a higher degree of interconnectivity than ensembles representing
recent memories. Correspondingly, our simulations show that recent memories become more vulnerable
to cortical lesions which is similar to Ribot gradients in retrograde amnesia. Previous models of amnesia
typically generated Ribot gradients by gradients in total replay time where the M-th memory obtains an 1/M
share of replay time, implicitely assuming infinite replay of all memories. In contrast, our model can generate Ribot gradients also with constant replay time per memory. This is more consistent with recent evidence
that novel memories are buffered and replayed by the hippocampus for a limited time only. Third, we show
that structural plasticity can easily explain the spacing effect of learning. This means the fact that learning
is much more efficient if rehearsal is spread over time compared to rehearsal in a single block. The spacing
effect has been reported to be very robust occurring in many explicit and implicit memory tasks in humans
and many animals being effective over many time scales from single days to months. For these reasons it
has long been speculated about a common underlying mechanism at the cellular level. We propose that
structural plasticity is this common mechanism. According to our model, ongoing structural plasticity can
reorganize the network during the long time intervals between two rehearsal periods by growing a lot of
new synapses at potentially useful locations. Therefore subsequent training can strongly increase effective
memory connectivity. In contrast, single block rehearsal can increase effective memory connectivity only

162

COSYNE 09

II-39
slightly above anatomical connectivity. [1] A.Knoblauch, Proceedings of the 11th Neural Computation and
Psychology Workshop, Oxford, 2008
doi:10.3389/conf.neuro.06.2009.03.059

II-39. Instructive cues for ON-OFF RGC segregation in the LGN of the
developing mouse visual system
Julijana Gjorgjieva
Stephen Eglen

JG 447@ CAM . AC. UK
SJE 30@ CAM . AC. UK

University of Cambridge
In the developing visual system retinal ganglion cells (RGCs) fire spontaneously in the form of waves, whose
spatiotemporal properties change during development [1]. Recent experimental evidence suggests that this
spontaneous retinal activity plays an instructive role in the remodelling of synaptic connectivity to the lateral
geniculate nucleus (LGN), where the correlated firing of neighbouring RGCs contains information driving
the refinement and segregation of functionally distinct RGCs [2, 3]. Such instruction is thought to manifest at synapses as a Hebbian mechanism, which strengthens synapses in the presence of synchronous
pre- and postsynaptic activity and weakens synapses exhibiting asynchronous activity. Using real multielectrode array recordings of spontaneous firing in two populations of RGCs, ON and OFF (responding to
light increments and decrements, respectively), in developing mouse retina (P12) [4], we investigated the
role of patterned spontaneous activity in driving ON-OFF segregation and synaptic refinement in the LGN.
We used spike time-dependent plasticity (STDP) to study the temporal evolution of synapses relaying information from ON and OFF RGCs onto a postsynaptic LGN neuron, and showed that STDP cannot explain
ON-OFF segregation because of the short timescale of pre- and postsynaptic spike integration. We also
examined the effect of increasing the timescale by an order of magnitude, and implemented postsynaptic
normalisation, but saw no consistent segregation across the recorded data sets. In contrast, a recently proposed burst time-dependent plasticity (BTDP) experimentally derived for the developing retinogeniculate
system of rat [5], which integrates pre- and postsynaptic bursts on a timescale of 1 second, can robustly
drive segregation and synaptic refinement in the LGN when used as a weight modification rule. We showed
that the experimentally identified temporal asynchrony between ON and the OFF RGCs, where OFF cells
fire 1 second after ON, is a necessary cue to drive their segregation. Additionally, the relative peak magnitudes of the cross-correlation function between neighbouring cells of the same kind (ON-ON and OFF-OFF)
and the number of ON and OFF convergent RGCs per LGN neuron also play an important role in predicting
the segregation outcome based on initial conditions. Segregation is the result of a combination between
the 1 second temporal firing asynchrony separating the two RGC types and giving an advantage to the ON
cells, and the postsynaptic drive to fire action potentials provided by RGCs converging to an LGN neuron.
Furthermore, we demonstrate with a linear analysis that only a temporally-symmetric rule like BTDP, which
modifies synapses based on the overlap between pre- and postsynaptic activity, can capture both possible segregation scenarios where either ON or OFF inputs dominate in adulthood. References [1] R.O.L.
Wong. Annu. Rev. Neurosci., 22:29–47, 1999. [2] B.M. Hooks and C. Chen. Neuron, 52:281–291, 2006.
[3] C.L. Torborg et al. Nat. Neurosci., 8:72–78, 2005. [4] D. Kerschensteiner and R.O.L. Wong. Neuron,
58:851–858, 2008. [5] D.A. Butts et al. PLoS Biol., 5:0651–0661, 2007.
doi:10.3389/conf.neuro.06.2009.03.095

COSYNE 09

163

II-40

II-40. Decision making and perceptual learning for speed discrimination
Stefan Ringbauer
Florian Raudies
Heiko Neumann

STEFAN . RINGBAUER @ UNI - ULM . DE
FLORIAN . RAUDIES @ UNI - ULM . DE
HEIKO. NEUMANN @ UNI - ULM . DE

University of Ulm
Problem. Perceptual learning in visual tasks can improve the performance of decision making (Dosher et
al., Psychological Review, 112, 2005). Instead of static inputs we investigate motion stimuli, particularly
perceptual learning for motion speed discrimination and how improved performance can be transferred between different motion patterns. We utilize configurations of moving random dot patterns simultaneously
displayed in four quadrants (central fixation) where in one quadrant a coherent motion pattern (target) was
displayed while the others contain random motion. In a 2AFC task of two subsequent displays a decision
needs to be made about which presentation contained the faster coherent stimulus speed. Through learning (using one target pattern) subjects can significantly improve their discrimination performance. The goal
is to evaluate whether performance is still improved when the coherent motion pattern is changed. Methods.
We propose a neural model of motion perception which consists of a hierarchy of areas to represent the
main cortical processing stages along the dorsal pathway, namely V1, MT, and MSTd (Bayerl & Neumann,
Neural Computation, 16, 2004; Ringbauer et al., LNCS 4669, 2007). Optical flow is detected in area V1
and integrated in area MT by speed and direction sensitive neurons. Global motion patterns are spatially
integrated subsequently in model area MSTd cells which are sensitive to rotational, radial, and laminar motion patterns (Graziano et al., J. of Neuroscience, 14, 1994). Model MSTd cells project to dorso-lateral area
LIP where cells temporally accumulate responses to judge and discriminate different motion configurations
(Hanks et al., Nature Neuroscience, 9, 2006). In our model speed activity is integrated over all directions
generating direction independent speed profiles for different quadrants. Such responses are spatially integrated over two presentation phases with different speed parameterizations. Activities of model MSTd
pattern cells allow to identify the target quadrant together with a confidence measure. Speed-selective responses are integrated separately with a bias on high speeds and are each fed forward to decision units
in model LIP neurons. Here, a recurrent competitive field of neuron pools exists tuned for different speeds
(Grossberg & Pilly, Vision Research, 48, 2008). The decision depends on both the input stimulus and the
strength of mutual inhibition between decision cells each controlled by a weighted difference of the two
speed profiles. These weights result from a trial by trial adaptation of the characteristics of the speed differences to support and enhance the decision making process. Results and Conclusion. After several trials
of training the model shows a decreased reaction time. Small speed differences can be discriminated more
accurately even if the target pattern has been changed against another that has not been probed during
training. Thus the model predicts that improved speed discrimination performance after perceptual learning
using a selected motion pattern can be transferred to other patterns. A psychophysical experiment with
human participants using the same visual stimuli is currently being developed to compare resulting data
with the model predictions and to adapt the model parameters in accordance to the experimental findings.
Funding: BMBF 01GW0763(BPPL); Grad.School Univ.Ulm
doi:10.3389/conf.neuro.06.2009.03.342

164

COSYNE 09

II-41 – II-42

II-41. Optimizing microcircuits through reward modulated STDP
Prashant Joshi1,2
Jochen Triesch1
1
2

JOSHI @ FIAS . UNI - FRANKFURT. DE
TRIESCH @ FIAS . UNI - FRANKFURT. DE

Frankfurt Institute for Advanced Studies
Johann Wolfgang Goethe University

Reservoir computing paradigms, such as Liquid State Machine and Echo State Network have been quite
successful on a range of computational tasks involving open-loop sensory processing [1] as well as tasks
requiring persistent or working memory [2]. A key feature of these paradigms is that learning is constrained
to only the synapses projecting from the generic neural microciruit to the linear readout leaving the recurrent circuitry intact. An obvious next step is to optimize the neural circuit for the class of computational
operations that the readouts have to perform on a certain input distribution [3,4]. It has been argued [3] that
the desirable features of such optimal circuit would be a low eigen-value spread of the weight matrix, high
entropy, highly decorrelated neural activity, and a large number of principle components needed to represent the circuit dynamics (liquid state). We show that modifying the recurrent synapses of a neural circuit
via STDP can optimize the circuit in an unsupervised fashion if the STDP is modulated by a global reward
signal. The network is simulated for periods of 200ms, which we refer to as one trial. The global reward
signal during one trial is constant and depends on the relative change in the 4 parameters during the previous trial, such that an improvement in the paramters leads to a positive reward. During a trial all synapses
are changed based on an STDP rule whose learning rate is multiplied with the positive or negative reward
for that trial. Results indicate that the approach presented here leads to the convergence of parameters
mentioned above to a steady-state value in as few as 30 trials. Moreover, the approach is quite robust in
nature and converges irrespective of the initial circuit dynamics being in a high or sparse firing regime. The
network manages to increase the recurrent inhibitory drive when the circuit is initialized with high excitatory drive and decrease the inhibitory drive when the circuit was drawn with a low excitatory drive with the
convergence point exhibiting slightly dominant inhibition. References: [1] W. Maass, T. Natschlaeger, and
H. Markram. Real-time computing without stable states: A new framework for neural computation based
on perturbations. Neural Computation, 14(11):2531–2560, 2002. [2] W. Maass, P. Joshi, and E. D. Sontag.
Computational aspects of feedback in neural circuits. PLOS Computational Biology, 3(1):e165, 1–20, 2007.
[3] H. Jaeger. Reservoir riddles: Suggestions for echo state network research. In IJCNN, pages 1460–
1462, 2005. [4] A. Lazar, G. Pipa, and J. Triesch. Fading memory and time series prediction in recurrent
networks with different forms of plasticity. Neural Networks, 20:312–322, 2007.
doi:10.3389/conf.neuro.06.2009.03.281

II-42. Brain’s strategy for perceptual estimates: model averaging, model
selection, or prob. matching?
Ladan Shams1,2
Ulrik Beierholm3

LADAN @ PSYCH . UCLA . EDU
BEIERH @ GATSBY. UCL . AC. UK

1

UCLA
California Institute of Technology
3
Gatsby Computational Neuroscience Unit, UCL
2

Background: We have previously shown that human observers’ estimates of spatial location of auditory
and visual stimuli are highly consistent with those of a Bayesian observer performing causal inference
(Koerding, Beierholm, Ma et al., 2007). In that study, we made the common assumption that observers

COSYNE 09

165

II-43
minimize the mean squared error of their responses, and thus use a decision rule based on the mean
of the posterior. This is equivalent to making an estimate which is the weighted average of estimates of
two models (model averaging): a model assuming common cause for the signals, and a model assuming
independent causes for the signals. Alternatively, one could argue that a more intuitive strategy would
be for the nervous system to select the more probable model and base the estimates of location solely
on the more probable model (model selec-tion). A distinct third possibility is that the posterior probability
of the two models is stochastically sampled, and thus, over the course of many trials, the probability of
responses match the probability of two models (probability matching), as has been observed in a number
of cognitive tasks (Vulkan 1998). Pupose: We investigated whether human perception is more consistent
with model selection, model averaging or probability matching by comparing human observer data from
four experiments ranging in task, modality, and stimulus type with the three strategies. Methods: The three
computational schemes were identical in their generative model (Koerding, Beierholm, Ma et al., 2007). In
model averaging scheme, the mean of the posterior (i.e., the weighted average of the common-cause and
independence hypotheses) was taken as the estimate. In the model selection scheme, the optimal estimate
corresponding to the more probable hypothesis (common-cause, or independent-causes) was taken as the
estimate of location. In the probability-matching scheme, the probability of common cause given the stimuli
was sampled, and a hypothesis (common-cause or independent-causes) was selected according to that
probability, and the estimates of location were then generated the same way as in the model selection
scheme. These three schemes were tested on four different datasets: a) auditory-visual spatial localization
task with discrete locations, b) auditory-visual spatial localization task with continuous locations, c) auditoryvisual temporal numerosity judgment task, d) visual-visual temporal numerosity judgment task. The models
were fitted to the data of individual subjects, and performance of the three different schemes were compared
for each of the four datasets, by applying paired ttests to log-likelihood of model given individual subject’s
data across subjects. Results: In all four datasets, model averaging accounted for the data very well
(R2>85%), and in tasks b, c, and d model averaging provided a better fit to the data than model selection or
matching (p<0.05). In task (a), no statistically significant difference was found among the three strategies.
Conclusions: These results suggest that the nervous system tries to minimize the mean squared error of
the sensory estimates when faced with multiple sensory signals, leading to a model averaging scheme of
processing. However, there are differences in strategy across individuals and tasks.
doi:10.3389/conf.neuro.06.2009.03.220

II-43. TD learning versus motivational salience accounts of dopamine
in animal models of OCD
Trent Toulouse
Henry Szechtman
Suzanna Becker

TOULOUTM @ MCMASTER . CA
SZECHTMA @ MCMASTER . CA
BECKER @ MCMASTER . CA

McMaster University
Disruption of the dopaminergic (DA) system is linked to a wide range of neurological disorders including attention deficit disorder, obsessive compulsive disorder (OCD), schizophrenia and Parkinson’s disease. An
influential contribution to understanding this system has been from reinforcement learning theory, using the
temporal-diference (TD) learning algorithm to model the correlation between dopamine cell firing rates in
the dorsal striatum and reward prediction errors in associative learning. The TD model, in its simplest form,
views the DA signal as conveying the difference between an expected reward and an actual reward. Another influential theory is that DA signals surprise and motivational salience and predicts an increase in firing
rates in response to improbable events [1]. To further differentiate between these two formulations of the
DA signal we looked at an animal model for OCD. Rats injected with Quinpirole, a dopamine D2/D3 recep-

166

COSYNE 09

II-44
tor agonist, exhibit compulsive checking behaviour [2]. The injection results in a hyperactivation of the DA
signal. We modeled this compulsive checking behaviour using both the TD and the surprise/salience model
of the DA signal. In the TD model the compulsive checking behaviour is reinforced because the reward prediction error signal is misreporting the behaviour as increasingly rewarding. In the salience/surprise model
the compulsive checking behaviour is induced when the animal perceives something improbable happening
during normal activity, and is continued until no unexpected consequence is experienced. Under Quinpirole, the DA signal is constantly reporting that improbable events are occurring so the checking behaviour
is never turned off. We found that both models showed similar compulsive checking with hyperactivation of
the DA signal, but differed significantly after the hyperactivation was turned off. When the DA signal was
not hyperactivated the TD model learned gradually over time that the checking behaviour was no longer rewarding. The checking behaviour was performed frequently at first but over time it significantly decreased.
In the salience/surprise model the checking behaviour ceased as soon as the hyperactivated DA signal was
turned off, since performance of the checking behaviour was no longer eliciting a surprise signal. We then
compared the results of the two models to a preliminary analysis of the behaviour of rats, that had received
previous injections of Quinpirole, during a trial where no drug was delivered. We found that the frequency
of checking behaviour appeared to drop off immediately, and did not show a significant decrease over time.
This suggests that the surprise/salience model of DA signal more accurately predicted animal behaviour
than the TD model. This evidence that the DA signal might be best modeled as conveying surprise and
motivational salience has important implications for our understanding of learning, action choice and how
disruptions in the dopaminergic system contribute to neurological disorders. [1]Toulouse, T. and Becker S.
(2008). COSYNE Abstract, Salt Lake City. [2]Szechtman, H., Sulis, W. & Eilam, D. (1998). Behav.Neurosci.
112 (6), 1475-1485.
doi:10.3389/conf.neuro.06.2009.03.032

II-44. A spiking network model for learning reward timing in cortex:
Harel Shouval1,2
Jeffrey Gavornik3
Marshall Shuler

HAREL . SHOUVAL @ UTH . TMC. EDU
GAVORNIK @ MAIL . UTEXAS . EDU
SHULER @ JHMI . EDU

1

Department of Neurobiolgy UT Houston
Department of Biomedical Engineering UT Austin
3
The University of Texas
2

The ability to represent time is an essential component of cognition but its neural basis is unknown. Although extensively studied both behaviorally and electrophysiologically, a general theoretical framework
describing the elementary neural mechanisms used by the brain to learn temporal representations is lacking. It is commonly believed that the underlying cellular mechanisms reside in high order cortical regions
but recent studies show sustained neural activity in primary sensory cortices that can represent the timing
of expected reward. In particular, a recent study by Shuler and Bear (2006) has shown learned, reward
timing dependent cortical activity in primary visual cortex. We postulate that a network of laterally connected cortical neurons is sufficient for generating the timing dependent neuronal dynamics, that the timing
information can be stored within synaptic efficacies, and that this cortical activity can be learned through a
novel variant of reinforcement learning. One of the advantages of the theory proposed here is that it does
not require specialized processes such as tagged delay lines of phase locked oscillators, as assumed explicitly or implicitly in previous theories of timing. We mathematically formulate the neuronal dynamics and
the learning rule, and implement our model computationally both in a rate based implementation and in a
model of conductance based spiking neurons. We show that this theory can account for various aspects of
the experimental data, but also explore the limitations of this model. We further analyze the spiking neural
model using a quasi-steady state mean field theory (e.g: Renart et al, 2003). This analysis reduces the

COSYNE 09

167

II-45
dynamics of the network to a single non-linear dynamical equation. We show that the results of the mean
field theory are in close agreement with the simulation results. Our theoretical model can account for existing experimental results, and can also produce novel predictions. We show several such predictions which
include change in pair wise correlations as a result of learning, and an increase in spontaneous and evoked
activity after learning. Using the original data from the Shuler and Bear (2006) paper we show that the experimental data is consistent with these predictions and that both evoked activity and spontaneous activity
are significantly increased by 74% and 40% respectively as a result of training. One prominent experimental
feature of temporal interval estimation is the scalar rule (Webber law), which states that the standard deviation of estimation errors increase linearly with the estimated interval. Using a simple population decoding
scheme based on the activity of the trained network of spiking neurons we show that this model produces
estimation errors that increase approximately linearly with the estimated interval. We explore the origin of
these results, and how they compare to experimental findings. Here we present a theory to account for the
specific results of Shuler and Bear (2006) which can also form the basis for a general theory of timing estimation in the cortex. We computationally implement and mathematically analyze the theory, and propose
experimental tests. We also show here experimental confirmation of some of our theoretical predictions.
doi:10.3389/conf.neuro.06.2009.03.313

II-45. The role of frontostriatal circuits in shifting between goal-directed
actions and habits
Christina Gremel1,2
Rui Costa
1
2

GREMELC @ MAIL . NIH . GOV
COSTARUI @ MAIL . NIH . GOV

Laboratory for Integrative Neuroscience
NIAAA/NIH

Understanding the neural mechanisms underlying the shift between goal-directed actions and habitual behavior may provide insight into dysfunctional decision making processes. Previous studies in rats and more
recently in mice have shown that ratio schedules of reinforcement predispose goal-directed actions which
are sensitive to changes in response outcome contingency and expected outcome value, while habit formation is more readily seen under interval reinforcement schedules and is less sensitive to alterations in
outcome value. Extended training and lesions of the dorsomedial striatum have previously been shown to
shift goal-directed responding to a more habitual behavior. On the other hand, inactivating the dorsolateral striatum can shift performance from habitual to goal-directed. These data suggest that animals are
able to simultaneously learn and readily switch between these two modes of responding. To examine this
possibility, we trained the same subject in both ratio and interval schedules of reinforcement in different
contexts, so that schedules were distinguished by different contextual cues. Mice were trained to press a
single lever for a reinforcer (pellets or sucrose), with the remaining reinforcer freely available in the home
cage. Every day mice were trained in both schedules: upon completion of one schedule, mice were immediately trained in the remaining schedule, with schedule order counterbalanced across subjects. For
each mouse, the lever position and the reinforcer obtained upon lever press were kept constant across
schedules/contexts. During acquisition, mice increased lever pressing under both random ratio and random interval schedules. Following acquisition, mice were given a devaluation test (2 days) where either the
reinforcer earned from lever pressing or the home-cage reinforcer was devalued via ad libitum exposure immediately before serial extinction tests in each context. Following devaluation treatment, when tested in the
ratio-trained context mice were sensitive to changes in the value of the earned reinforcer. However, when
tested in the interval-trained context, mice displayed more habitual responding and were less sensitive to
devaluation treatment. These results suggest that contextual information may be important in directing the
shift between goal-directed actions and habitual performance. We are currently investigating the role of the

168

COSYNE 09

II-46
anterior cingulate cortex (ACc) and the orbitofrontal cortex (OFC), which have previously been hypothesized
to modulate cued-behavioral control over motivated responding, in this contextual control over goal-directed
and habitual behaviors. Given that the ACc and OFC send heavy projections to the dorsal striatum, we are
also examining whether neuronal activity within or between the ACc/OFC and the dorsomedial/dorsolateral
striatum are related to the shift between goal-directed actions and habitual responding.
doi:10.3389/conf.neuro.06.2009.03.100

II-46. Recurrent neural network modeling of hierarchical motor control and analysis
Dongsung Huh
Emanuel Todorov

DHUH @ UCSD. EDU
TODOROV @ COGSCI . UCSD. EDU

UCSD
Current theories on biological motor control mostly concern with how the brain as a whole performs sensorymotor processing, with little reference to how such functions are distributed across the hierarchical structure.
Here, we study the fundamental design principle of biological motor control hierarchy, with a special focus
on the lowest-level-controllers (LLC), such as the primary motor cortex (M1) and spinal cord central pattern generators (CPG). As a component of a hierarchical structure, a LLC receives command signals from
higher-level-controllers and controls the body movement accordingly. Our study shows that an LLC should
exhibit stability and memoryless properties in order for the command signal to be properly reflected in the
body movement. These conditions imply that when command input is held fixed over time, the body state
should converge to a stable attractor that is uniquely determined by the input. It also means that the mapping
from command to body movement is unambiguous. Experiments confirm that biological motor systems indeed employ attractor dynamics. Graziano et all [1] showed that micro-stimulation of M1 drives limbs toward
a unique posture regardless of previous movement history. The result implies that M1 has point-attractor
dynamics. On the other hand, most CPGs in the spinal cord (e.g. locomotive CPG) have periodic movement patterns, which are limit-cycle attractors. By exploiting the attractor property, we calculate the true
complexity (dimensionality) of the LLC mapping problem (It is roughly twice the dimensionality of the attractor space). We also obtain an efficient training method for approximating the optimal LLC that is otherwise
a hopelessly high-dimensional learning problem. We train recurrent neural networks (RNN) to approximate
desired LLCs. These RNN models unify multiple theories of motor control (optimal control, equilibrium control, coordinate translation, synergy) and provide a bridge that connects the theories and neurophysiology
data. The RNN models reveal the following properties of LLC control: (1) Attractor dynamics of the LLC is
the basic building block of all movement generation. (2) The combined dynamics of LLC-body is simpler
than the pure body dynamics. (3) Due to the simplified dynamics, the higher-level-controllers can easily
manipulate body movements without having to dealing with complex physical properties of muscles and
joints. Our work questions the validity of the current view of motor control hierarchy, which assumes that
different levels of hierarchy have distinct functional roles (decision making -> movement trajectory planning -> execution of motor plan). Valuable progress has been made in domains like sensory process by
using feed-forward models with functionally distinct hierarchical processes. However, there is a growing
need to account for dynamical systems with feedback and recurrent processing, which resists the same
kind of functional decomposition. For example, controlling a movement trajectory does not only depend
on a high-level-command sequence, but also the dynamic properties of LLC-body. A useful alternative is
to consider the design principle of the whole hierarchy as successively simplifying the body dynamics, so
that higher-level-controllers can achieve abstract motor goals without dealing with the complexity of body
dynamics. [1] Graziano, et all, 2002. Neuron 34, pp. 841–851
doi:10.3389/conf.neuro.06.2009.03.262

COSYNE 09

169

II-47

II-47. The interaction of Purkinje cell and Inhibitory Interneuron plasticity during classical conditioning
Ivan Herreros-Alonso
Riccardo Zucca
Andrea Giovannucci
Paul F. M. J. Verschure

IVANHERREROS @ GMAIL . COM
RZUCCA @ IUA . UPF. EDU
ANDREA . GIOVANNUCCI @ GMAIL . COM
PAUL . VERSCHURE @ IUA . UPF. EDU

SPECS. Universitat Pompeu Fabra
Decades of experimental work using the the eye-blink conditioning paradigm points to the Cerebellar Cortex
(CC) as the locus of the motor conditioning engram. It is in the cerebellar cortex where a memory trace is
formed that allows both to associate Conditioning (CS) and Unconditioned Stimulus (US) and to elicit an
adaptively timed Conditioned Response (CR). Single cell recordings show that the evolution of the firing
rate of a Purkinje (PU) cell during the CS-US period provides a substrate for such learning (Jirenhed et al.,
2007). Classical computational theories of the Cerebellum (like the Marr-Albus-Ito model) predicted that
the mechanism responsible for motor learning was plasticity at the parallel fiber to PU (pf-PU) synapse.
However in vitro and in vivo studies have reported multiple loci of plasticity in the CC among them plasticity
at the parallel fibers to Inhibitory Interneuron (pf-II) synapse (Jörntell, & Ekerot, 2002). This site of plasticity is of particular interest because it is inversely related to the synaptic dynamics of the pf-PU synapse:
whenever the pf-PU synapse undergoes LTD the pf-II undergoes LTP, and vice versa. Hence, this suggests
that the engram is not restricted to the parallel fiber Purkinje cell synapse only. A particular challenge for
models that exclusive rely on the plasticity at pf-PU synapses is that they have difficulty to explain results
that have shown that increasing CS intensity shortens the latency of already acquired CRs (Svensson et
al, 1997). We hypothesize that the inclusion of the active role of the Inhibitory Interneurons (II) in learning
can account for this CS intensity effect. We have implemented a biologically constrained model aimed at
reproducing the learning process of a single Purkinje cell, in such a way that it will respond to presentations
of a CS with an adaptively timed pause. In previous work we demonstrated that having plasticity at the
pf-PU was sufficient to achieve this goal provided the cortico-nuclear-olivary negative feedback loop was
included (Verschure and Mintz, 2000; Hofstötter et al., 2002). Now, we have extended this model and also
included plasticity at the pf-II synapse in order to test its computational implications. Our results confirm
that potentiation of the II can account for the CS intensity effect. Our model also predicts that if these two
plasticity processes coexist it should be possible to identify different components in the dynamics of the
acquisition/extinction processes at the level of single Purkinje cells. Although two components have already
been founded in behavioral data (Ivarsson & Svensson, 2000) a confirmation of this prediction could only
be obtained by analyzing the responses of single Purkinje cells in such experiments. Hofstötter C., Mintz
M. and Verschure P. (2002). J Eur Neurosci, 16 (7) Ivarsson M and Svensson P (2000). J Neurophysioll,
83(2) Jörntell H and Ekerot CF (2002) Neuron, 34(5) Jirenhed D-A., Bengtsson F. and Hesslow G. (2007).
J Neurosc, 27(10) Svensson P, Ivarsson M and Hesslow G (1997). Learning & Memory, 4 Verschure P and
Mintz M (2000). Neurocomputing, 38
doi:10.3389/conf.neuro.06.2009.03.190

170

COSYNE 09

II-48 – II-49

II-48. The relationship between correlations and network states is explained by balanced network dynamics
Alexander Lerchner
Peter Latham

LERCHNER @ GATSBY. UCL . AC. UK
PEL @ GATSBY. UCL . AC. UK

Gatsby Computational Neuroscience Unit, UCL
Recent data from multiple-cell recordings of cortical neurons in-vivo (e.g., Poulet and Petersen, 2008, Nature, 454:881-887; Smith and Kohn, 2008, J Neurosci, 28:12591-12603) provide converging evidence that
the degree of correlations among cortical neurons depends on whether or not the brain is functionally
engaged in a task: both membrane potential and spiking activity exhibits stronger correlations during spontaneous activity than during task-dependent activation, and the increase in correlations is large (about
two-fold) and is particularly striking for membrane potentials, where correlation coefficients can approach
1. Such large correlations and their state-dependence are not part of standard theories for balanced networks, raising the question: what can these observations tell us about the underlying network dynamics
and computational properties? Here we show that the observed correlations are in fact entirely consistent with the dynamics of balanced networks, which has important implications for possible neural codes.
We study model networks comprised of a few thousand excitatory and inhibitory spiking neurons that are
connected via conductance-based synapses with realistically strong couplings. The connectivity is random
and sparse, with connection probabilities around 10% – in line with the statistics of cortical columns. The
network is driven by excitatory external input applied to a fraction (10-100%) of the neurons in the network.
We measure correlations of synaptic drives, membrane potentials and spike trains under different input
conditions. In agreement with intracellular and extracellular recordings in cortex, we find that the correlation
coefficients in the model are high when the network is driven weakly by slow periodic input or short bursts
of input (spontaneous activity), and that they drop substantially when the input is stronger and more sustained (task-dependent activation). Quantitatively, the correlation coefficients during spontaneous activity
and task-dependent activation agree well with experimental observations. These results show that strongly
coupled balanced networks exhibit the same state-dependence of correlations that are reported in intracellular and extracellular recordings in-vivo. Given the well-established theory for this class of models, we are
thus in a position to elucidate possible mechanisms behind this phenomenon. In short, during spontaneous
activity, the membrane potential dynamics and the spiking activity of any given neuron is dominated by
modulations of its mean-input, which is shared among neurons, leading to large correlations. Note that this
interpretation comes with a prediction: the correlations should be network-wide, meaning the activity of all
neurons should fluctuate together. During task-dependent activation, on the other hand, neuron-specific
fluctuations dominate, leading to de-correlation between pairs of neurons. This framework extends the
standard theory of cortical network dynamics to include a growing body of data that, viewed naively, might
seem to challenge its validity.
doi:10.3389/conf.neuro.06.2009.03.028

II-49. Analysis of biologically inspired artificial neural networks
Tarec Fares
Armen Stepanyants

FARES . T @ NEU. EDU
A . STEPANYANTS @ NEU. EDU

Northeastern University
Connectivity in most cortical networks is sparse and mainly excitatory, while the distribution of synaptic
strengths has been shown to exhibit a slow non-exponential decay. To explore the reasons behind these

COSYNE 09

171

II-50
three network features, we analyze robust artificial networks of excitatory and inhibitory McCulloch and Pitts
neurons. Over the years, there has been a great deal of interest in McCulloch and Pitts model networks
and their capacity. However, most of the theoretical and computational models that have been studied
thus far ignore biological constraints on the network architecture. Using the knowledge about synaptic
connectivity in the neocortex we investigated artificial neural networks with biologically plausible connectivity
properties. We analyzed networks of excitatory and inhibitory neurons ranging from 100 to 1000 cells in
size – numbers that correspond to different cortical units. We hypothesized that cortical networks must be
robust, metabolically inexpensive (low overall synaptic strength), and, at the same time, have large memory
storage capacity. Our computational results first showed that robust artificial neural networks that minimize
the overall strength of synaptic connectivity must be sparsely interconnected, which is in agreement with the
experimental data. Second, the capacity of such networks increases with the fraction of inhibitory neurons
(in the 0-50% range), accompanied with an increase in the overall synaptic strength. As a result of this
tradeoff between the network capacity and the overall synaptic strength, the optimal artificial networks must
contain only a small fraction of inhibitory neurons, consistent with real cortical networks. Finally, we showed
that the distribution of synaptic strengths in these optimal networks exhibits a slow non-exponential decay,
again in agreement with experimental observations.
doi:10.3389/conf.neuro.06.2009.03.062

II-50. Graph coloring predicts the dynamics of neuronal networks
Collins Assisi
Maxim Bazhenov

COLLINSA @ UCR . EDU
MAKSIM . BAZHENOV @ UCR . EDU

University of California, Riverside
Neuronal networks exhibit a rich dynamical repertoire, a consequence of both the intrinsic properties of
neurons and the geometry of the network. A number of recent studies have devised measures that characterize the global structure of networks. In contrast, a vast body of work is devoted to examining the
dynamics of individual neurons and synapses at varying levels of detail. The confluence of these two
lines of research, structure–dynamics relationships in networks, is yet restricted to two classes of networks
– those with simple structure that are capable of complex dynamics or those with elaborate structure but
completely synchronous dynamics. Neuronal networks, more often than not, fall outside these classes. The
dynamical repertoire of a neuronal network is enriched considerably by the action of inhibitory interneurons
that corral principal neurons into synchronously firing groups and impose precise temporal relationships
between these groups. The dynamics of an inhibitory network is necessarily constrained by its structure.
Using a realistic computational model of the insect olfactory system, we establish a relationship between a
structural property of the network, namely, it’s coloring [A coloring of a network is the assignment of colors
to the nodes of the network such that nodes that are directly connected to each other are assigned different colors], and the dynamics it constrains. We show that inhibitory neurons associated with the same
color tend to spike synchronously. Each synchronous group switches between an active and a quiescent
state after a time determined by intrinsic neuronal properties and specific features of the network geometry. The inclusion of excitatory principal neurons into the network does not compromise the coloring-based
dynamics of the inhibitory sub-network. In fact, excitation serves to increase the coherence within groups
of synchronously firing inhibitory interneurons. The principal neurons, in turn, exhibit patterns of synchrony
that are strongly dependent on pre-synaptic inhibition they receive. Principal neurons that receive similar
inhibitory input evolve along similar trajectories. This observation, in conjunction with our knowledge of the
coloring of the inhibitory sub–network allowed us to implement a novel construction, a space in which the
collective spiking activity of all principal neurons in a random network reliably formed a series of orthogonally propagating traveling waves. By reordering the neurons according to a prescription dictated by the
network structure, we were able to extract low dimensional dynamics (synchrony, clustering and traveling

172

COSYNE 09

II-51
waves) from a randomly connected network. While we use insect olfaction to elucidate the relationship
between the geometry of the network and the dynamics it constrains, our results are general enough to be
applicable to a wide variety of neuronal networks.
doi:10.3389/conf.neuro.06.2009.03.097

II-51. Neuronal circuit reconstruction using serial block-face scanning electron microscopy
Kevin Briggman
Moritz Helmstaedter
Thomas Euler
Winfried Denk

BRIGGMAN @ MPIMF - HEIDELBERG . MPG . DE
MORITZ . HELMSTAEDTER @ MPIMF - HEIDELBERG . MPG . DE
THOMAS . EULER @ MPIMF - HEIDELBERG . MPG . DE
WINFRIED. DENK @ MPIMF - HEIDELBERG . MPG . DE

Max Planck Institute for Medical Research
A fundamental limitation to understanding the function of neuronal circuits is the lack of complete wiring
(connection) diagrams. In many nervous systems, local circuits are comprised of neurons extending several
hundreds of microns. The tissue volumes that are required to reconstruct such circuits are on the order of
10ˆ8 um3 representing several terabytes of pixel information at the pixel size (20-30nm) needed to trace
all neuronal processes. It is therefore necessary to automate both the acquisition and analysis of the
data. Data acquisition of 3D data with the required resolution is possible in a fully automated way by using
serial block-face scanning electron microscopy (SBFSEM). Images are acquired from the face of a tissue
block with slices as thin as 25 nm removed by ultra-thin sectioning with an oscillating diamond knife. This
technique replaces the difficult and labor-intensive process of manually cutting, mounting, and imaging
sections in a transmission electron microscope. We acquire data with nearly isotropic resolution at lateral
resolutions of 20 nm and 25 nm section thickness. We envision the ability to automatically collect data
from volumes large enough to encompass complete circuits at a resolution capable of capturing all the fine,
convoluted neuronal processes. The segmentation of neurons from images is critically dependent on a
high signal contrast between structures of interest. We have developed a staining strategy that labels cell
surfaces with an electron dense product. This method leaves the cytosol and organelles largely unstained
and thus results in a high contrast between intracellular space and cell surfaces, allowing the profiles of
neuronal processes to be easily identified. The manual tracing of each and every neuron through a large
volume is prohibitively slow, hence the tracing of neurons must be automated. We first manually segment a
representative sub-volume into intracellular and cell surface regions. This segmented sub-volume becomes
a training dataset for machine learning algorithms. Local cubes of data (5-7 voxelsˆ3) are used to generalize
voxel connectivity from the training to test data sets. A crucial step, however, is the validation of the obtained
segmentations so as to obtain an estimate of the reconstruction reliability. We have developed tools that
allow browsing through large 3D data sets and the efficient creation of 3-dimensional skeletons by human
tracers. The comparison between skeletons and automated segmentations allows the estimation of break
and merger probabilities, which are needed for the optimization of the classification and segmentation
algorithms. Also, this data provides a measure for the reliability of the reconstructed circuitry. Finally, we
are attempting to correlate functional recordings with circuit structure. Proof of principle experiments have
shown that bulk loading of calcium indicators in the mammalian retina allow functional signals to be imaged
without significantly damaging tissue ultrastructure.
doi:10.3389/conf.neuro.06.2009.03.103

COSYNE 09

173

II-52

II-52. Biologically plausible saliency networks for object recognition
Sunhyoung Han1
Dashan Gao2
Nuno Vasconcelos
1
2

S 1 HAN @ UCSD. EDU
GAODA @ GE . COM
NUNO @ ECE . UCSD. EDU

University of California, San Diego
General Electric Global Research

Various biologically inspired neural network architectures have been proposed for object recognition. These
networks are usually hierarchical, solving the recognition problem through a sequence of processing stages
that implement different trade-offs between selectivity and invariance. Recently, HMAX networks (which
differ from earlier models mostly through the choice of pooling function) have been shown to achieve stateof-the-art performance across a number of recognition tasks [4, 3]. In this work, we introduce an alternative
family of biologically plausible networks for visual recognition, based on the principle of discriminant saliency
(DiscSal). This principle has been shown to provide a unified formulation for bottom-up and top-down
processing in visual cortex. In the bottom-up realm, extensive evidence is now available on the ability of
optimally discriminant center-surround detectors to predict the psychophysics of human saliency. These
predictions are quantitative, with precision far superior to those of other popular saliency models [2]. With
respect to top-down processing, discriminant saliency has been shown to detect regions of the visual field
which are more informative for object recognition than those provided by bottom-up interest point detectors.
This work extends the DiscSal principle to the design of object recognition networks with a 1-to-1 mapping to
the standard model of cortical networks [1]. A comparison to their HMAX counterparts reveals that DiscSal
networks have identical structure but several advantages. First, it is shown that all their computations are
widely accepted as biologically plausible. Second, it is shown that, under DiscSal, all network units have a
precise computational interpretation: simple cells implement an optimal decision rule (classification), while
complex cells compute the discriminant power of visual features (more precisely the mutual information
between the responses of each feature and its class label). Third, it is shown that DiscSal networks can
be tuned to the solution of multiple vision problems. We consider two examples, by deriving the optimal S
(simple) and C (complex) units for bottom-up and top-down saliency. This is particularly useful for object
recognition, where it is shown that bundles of S and C units can be tuned to detect locations of the visual
field that are informative for the presence of the objects of interest. Finally, an extensive experimental
comparison of recognition performance shows that a single-layer DiscSal network matches the previous
best results obtained with a highly optimized two-layer HMAX network, on standard image classification
benchmarks. [1] M. Carandini, J. Demb, V. Mante, D. Tolhurst, Y. Dan, B. Olshausen, J. Gallant, and N
Rust. Do we know what the early visual system does? J. Neurosci., 25, 2005. [2] D. Gao, V. Mahadevan,
and N. Vasconcelos. On the plausibility of the discriminant center-surround hypothesis for visual saliency.
Journal of Vision, 8(7):1-18, 6 2008. [3] J. Mutch and D. Lowe. Object class recognition and localization
using sparse features with limited receptive fields, IJCV, 80:45-57, 2008. [4] T. Serre, L. Wolf, S. Bileschi,
M. Riesenhuber, and T. Poggio. Robust object recognition with cortex-like mechanisms. IEEE Trans. on
PAMI, 2007
doi:10.3389/conf.neuro.06.2009.03.099

174

COSYNE 09

II-53 – II-54

II-53. Summation properties of frequency-dependent disynaptic inhibition between pyramidal cells
Thomas Berger1
Gilad Silberberg2
Rodrigo Perin de Campos
Henry Markram
1
2

THOMAS . BERGER @ EPFL . CH
GILAD. SILBERBERG @ KI . SE
RODRIGO. PERIN @ EPFL . CH
HENRY. MARKRAM @ EPFL . CH

Ecole Polytechnique Federale, Lausanne
Dept. of Neuroscience, Karolinska Institute

The mammalian neocortex consists of neurons that form an intricate network of recurrent circuits. A disynaptic pathway and dynamic circuit mechanism was recently reported which allows an activity dependent
recruitment of inhibition: Frequency Dependent Disynaptic Inhibition (“FDSI”) is a common pathway in the
somatosensory cortex that is dynamically regulated by the firing rate and the number of presynaptic PCs.
In response to high frequency stimulation of a PC, Martinotti Cells (MCs) are recruited via a facilitating
synapse, thus providing a level of inhibition that depends on the previous excitation level in the network.
Here, we used multi-neuron whole cell recordings to characterize summation properties of FDSI between
layer 5 thick tufted PCs within the dimensions of a neocortical column. FDSI is strongly shaped by I(h) in
distal PC dendrites, which determines the effective time window for integration of inhibitory and excitatory
inputs. 3-4 PCs firing simultaneously are sufficient to generate FDSI in all PCs within the dimensions of a
cortical column and 8-9 PCs can saturate the amount of hyperpolarization recorded at their somata. A brief,
high frequency burst in only a few PCs can therefore constitute a gating mechanism for further excitatory
input to the apical dendrites of the entire column. Functional implications for dynamically balanced E-I, its
saturation levels, and the importance of compartmentalization are discussed.
doi:10.3389/conf.neuro.06.2009.03.141

II-54. Constructing dopaminergic signals in response to transient inputs in the ventral tegmental area
Boris Gutkin
Michael Graupner

BORIS . GUTKIN @ ENS . FR
MICHAEL . GRAUPNER @ ENS . FR

Ecole Normale Supérieure, Paris
Midbrain dopaminergic (DA) neurons signal motivational properties of natural reinforcers and addictive
drugs important for the acquisition of conditioned behavior. The phasic DA response has been suggested
to signal the discrepancy between the predicted and received reward, signaling a crucial learning term for
reinforcement learning. The relevant dopaminergic neurons are found in two nuclei, the substantia nigra
pars compacta and the ventral tegmental area (VTA). Electrophysiological recordings have demonstrated
that transient inputs to the VTA, e.g. glutamatergic and cholinergic, convey salient information about the
environment such as novel stimuli, received rewards, and reward predictive sensory cues. However, how
transient inputs to the VTA are converted into DA output is poorly understood. In particular, the mechanism
and location of the difference computation between expected and received rewards remain elusive. Furthermore, how addictive drugs such as nicotine modulate the VTA response to afferent inputs is not know. We
address those questions using a biologically realistic model of the local VTA circuitry that includes both the
neuronal dynamics and the kinetics of the relevant cholinergic receptors. We implement the neuronal microcircuit of the VTA in a network model accounting for the local VTA connectivity, the afferent projections to
the VTA, the location of the nicotinic acetylcholine receptors (nAChRs) and their subtype-specific activation

COSYNE 09

175

II-55
and desensitization properties. The VTA contains DAergic and GABAergic neurons receiving cholinergic
(ACh) and glutamatergic (Glu) afferent input from subcortical and cortical structures. The DA response
to endogenous acetylcholine is mediated by various nAChR subtypes expressed on: (i) DA neurons, (ii)
GABAergic neurons, and (iii) presynaptic Glu terminals. The very same nAChRs are responsible for the
impact of exogenous nicotine on DA signaling. The glutamatergic input activates synapses located on DA
and GABAergic neurons. We first constrain our model by requiring it to account for both in vivo and in vitro
experimental data obtained from recordings during nicotine exposures. In particular, we use the data to pin
down the specific distribution of nAChRs. We then investigate how the VTA DA neurons respond to transient
afferent inputs. We show that glutamatergic inputs phasically increase DA output. The impact of transient
ACh inputs depends crucially on the the distribution of alpha4 beta2 subunit containing nAChRs in the VTA.
Transient cholinergic inputs decrease DA output if alpha4 beta2 nAChRs are predominantly expressed by
GABAergic cells. In this case, the VTA DA signal reflects the difference between the Glu and ACh inputs.
This leads us to speculate that the ”no reward” prediction signal from the habenula may be mediated via
the cholinergic projections to the VTA. Finally, we show how nicotine subverts those signaling pathways
by removing the impact of ACh signals on DA activity. Our investigations suggest biological mechanisms
explaining how salient stimuli encoded in transient inputs to the VTA are translated into dopamine output.
doi:10.3389/conf.neuro.06.2009.03.138

II-55. Phase locking of pulse-coupled oscillators with delays is determined by the phase response curve
Marmaduke Woodman1,2
Carmen Canavier1
1
2

WOODMAN @ CCS . FAU. EDU
CCANAV @ LSUHSC. EDU

LSU Health Sciences Center
Florida Atlantic University

Gamma oscillations (30-70 Hz) can synchronize with near zero phase lag over multiple cortical regions and
between hemispheres, and between two distal sites in hippocampal slices in which gamma was induced
by tetanus. Additionally, oscillations the gamma band can synchronize with nonzero phase lag between
pairs of EEG electrodes. There are two mechanisms by which long range synchronization could occur:
by locking to a common input or via reciprocal coupling. Here we address phase-locking that arises via
bidirectional coupling of two limit cycle oscillators with a conduction delay. It is often implied that for two
distal oscillators, phase lags equal to the conduction delay are expected, and how synchronization can take
place over long distances in a stable manner is considered an open question. Here we assume that under
conditions in which two local circuit generators of gamma oscillations arise separated by some distance,
these circuits function as intrinsic nonlinear oscillators that are bidirectionally coupled with a conduction
delay. Nothing is assumed regarding the oscillators other than that the phase response curve (PRC) can
be measured using an input that approximates the one received from the other oscillator, and that the effect
of the coupling is dissipated within one network period such that the coupling is effectively pulsatile. Here
we derive existence and stability criteria based on the PRC for phase locking with delays of any length,
and test them on oscillators with a wide variety of PRCs and varying levels of heterogeneity. The method
is accurate for any shape PRC that we examined and therefore very general. For identical frequencies,
identical coupling strength, and identical delays, synchrony emerges as a consequence of symmetry. We
show that for both homogenous and heterogenous networks, in phase synchronization alternates with out
of phase synchronization as the delay is increased, with regions of bistability in the intermediate values. In
practice, the frequencies of the local circuits are unlikely to be identical; therefore, the cycle periods need
to be altered via phase resetting in order to reach a common frequency. If the coupling is strong, as it must
be in order for synchronization to occur rapidly, and the frequencies, conduction delays and PRCs of the

176

COSYNE 09

II-56
oscillators are similar, then the input to each neuron will arrive at a similar phase, resulting in a small phase
lag in the firing times. The stability of the phase locking depends upon the slope of the phase resetting
curve at the phase at which the input is received in the phase locked mode. In other circuits, the conduction
delays are asymmetric, in which case additional mechanisms may be required to achieve near zero phase
lag. Having an explicit and general solution for the existence and stability of in phase locking with delay can
provide insight into how such locking is achieved in the brain.
doi:10.3389/conf.neuro.06.2009.03.139

II-56. Phase Resetting Curves Predict Network Activity in Networks of
Neural Oscillators
Srisairam Achuthan
Carmen Canavier

SACHUT @ LSUHSC. EDU
CCANAV @ LSUHSC. EDU

LSU Health Sciences Center
A phase response curve (PRC) characterizes the change in cycle period of an otherwise stable oscillator
to perturbations applied by precisely timed stimuli. Networks of model neurons were constructed and their
activity was predicted using an iterated map based solely on the phase resetting curves (PRCs). The
predictions were quite accurate provided that the resetting to simultaneous inputs was calculated using the
sum of the simultaneously active conductances, obviating the need for weak coupling assumptions. Fully
synchronous activity was observed only when the slope of the PRC at a phase of zero, corresponding to
spike initiation, was positive. A novel stability criterion was developed and tested for all to all networks of
identical, identically connected neurons. When the PRC generated using N-1 simultaneously active inputs
becomes too steep, the fully synchronous mode loses stability in a network of N model neurons. Therefore,
the stability of synchrony can be lost by increasing the slope of this PRC either by increasing the network
size or the strength of the individual synapses. Existence and stability criteria were also developed and
tested for the splay mode in which neurons fire sequentially. Finally, N/M synchronous sub-clusters of M
neurons were predicted using the intersection of parameters that supported both between cluster splay
and within cluster synchrony. Surprisingly, the splay mode between clusters could enforce synchrony on
sub-clusters that were incapable of synchronizing themselves. Although all to all networks were used as
examples for simplicity, these methods can be extended to heterogenous networks in two ways. First, the
results for the homogeneous network apply approximately to slightly heterogenous networks, and second,
the procedure of using the PRCs to predict which firing patterns will be exhibited can be applied to arbitrary
circuits that contain both inhibition and excitation with network topologies that are not all to all. When
heterogeneity is introduced by allowing variability in the intrinsic frequency or coupling conductances, the
solution structure of the homogeneous network is perturbed, such that exact synchrony within a cluster
becomes near synchrony, and a symmetric splay mode between clusters, such as the antiphase mode,
becomes a near antiphase mode. For the parameter ranges examined in this study, the PRCs of component
neural oscillators were demonstrated to contain all the information necessary to predict network activity in
heterogeneous networks of oscillatory neurons as well. These results can be used to gain insights into the
activity of networks of biological neurons whose PRCs can be measured.
doi:10.3389/conf.neuro.06.2009.03.113

COSYNE 09

177

II-57 – II-58

II-57. On the origin of low frequency fluctuations in the brain resting
state
Etienne Hugues
Gustavo Deco

ETIENNE . HUGUES @ UPF. EDU
GUSTAVO. DECO @ UPF. EDU

Universitat Pompeu Fabra
Brain imaging studies (fMRI, MEG and EEG) have shown that the brain exhibits large scale very slow
fluctuations (< 0.1 Hz). These fluctuations even appear when the subject is not engaged in a particular
task, in the so-called “resting state”. In certain cases, these fluctuations have been shown to correlate with
behavioral performance. Spatially, the brain exhibits large-scale coactivated networks which are believed
to support a fundamental functional organization of the brain. As the brain constantly receives external
inputs, this spatiotemporal pattern of activity in the resting state is a priori surprising. What is the origin
of these fluctuations? Are they induced by the brain alone? If yes, do they result from the interaction
between brain areas or are they locally created in the neural tissue? In this study, we consider a single local
neural network and investigate the possibility of the emergence of spontaneous fluctuations, even when the
network receives no external fluctuations. The network is composed of excitatory and inhibitory integrateand-fire type of neurons, sparsely connected as in the cortex, and with instantaneous synapses. Each
neuron receives external spike train inputs. We formulate the mean-field description of this model and write
the corresponding Fokker-Planck equation (FPE) which describes the evolution of the neuronal membrane
potential probability distribution. We investigate analytically the stability of the stationary solution of the
FPE for constant input. For low inputs and a class of neuron models, we show that this solution becomes
unstable in the presence of synaptic delays and in the inhibition dominated regime. Numerical simulations
confirm that the neuronal firing rate oscillates in the predicted parameter region, at a frequency which
decreases with the input. Our results suggest that the low frequency fluctuations could be intrinsic and
originate locally. The next step is to explore the type of dynamics generated when, as in he brain, such
local networks are the nodes of a larger scale network, and to see if the observed spatiotemporal patterns
of activity can emerge when a realistic interareal connectivity and delay structure is taken into account.
doi:10.3389/conf.neuro.06.2009.03.198

II-58. Stimulus space topology vs. network topography in the ring
model
Vladimir Itskov1
Carina Curto2,3

VLADIMIR @ NEUROTHEORY. COLUMBIA . EDU
CURTO @ COURANT. NYU. EDU

1

Columbia University
Courant Institute, NYU
3
CMBN, Rutgers
2

The allowed patterns of activity among neurons in a recurrent network are constrained by both the structure
of inputs and the structure of recurrent connections. Nevertheless, there is a growing body of evidence
suggesting that, in the mature brain, patterns of spontaneous activity are similar to patterns of evoked
activity, even in the absence of structured inputs. It is therefore possible that one outcome of learning
is that recurrent networks serve to constrain activity patterns to be ”sensible” – i.e., to reflect the same
structure that is normally present during evoked activity even when the inputs are unstructured. If so,
what can be inferred about connectivity in recurrent networks whose constraints reflect relations between
single-cell receptive fields? We address this question in a simple class of models, in which the function of

178

COSYNE 09

II-59
a recurrent network is to gate inputs so that only a selected set of persistent activity patterns is allowed.
By ”persistent activity pattern” we mean a subset of stably co-active neurons. An elegant feature of these
models is that one can analytically determine the set of all stable steady states (”permitted sets”) from
the synaptic matrix alone, and these activity patterns are highly constrained even when the allowed inputs
are not [1]. If the allowed activity patterns are consistent with overlapping receptive fields, one can infer
topological features of the underlying stimulus space [2]. This allows us to directly relate recurrent network
connectivity to the topology of the represented stimulus space. We use this paradigm in an analysis of the
ring model [3] for the case of unconstrained inputs. In this model, neurons are labeled by angles varying
along a circle, and the strengths of recurrent connections depend only on the differences in angles between
pairs of cells. If the stimulus space topology obtained from neural activity patterns simply reflected the
topographic organization of the recurrent network, one would expect a circle topology for the ring model.
We find instead that, depending on parameters, there are three possibilities: the topology may be either
that of a point, a circle, or too complex to reflect a low-dimensional stimulus space. Only in the case of circle
topology are the activity patterns consistent with the usual interpretation that individual cells have convex
and overlapping receptive fields, each spanning a limited and continuous interval of angles on a circular
stimulus space; this parameter regime determines preferred patterns of connectivity required for the ring
model to represent a circular variable in the case of unconstrained inputs. We suggest that knowledge of
the stimulus space represented by a recurrent network can provide new insights into network connectivity,
even in cases where the topographic organization of the network is known. This work was supported by the
Swartz Foundation and NSF DMS-0818227. [1] Hahnloser, Seung, and Slotine. Permitted and forbidden
sets in symmetric threshold-linear networks. Neural Comput. 2003. [2] Curto & Itskov. Cell groups reveal
structure of stimulus space. PLoS Comp. Bio. 2008. [3] Ben-Yishai, Bar-Or, Sompolinsky. Theory of
orientation tuning in visual cortex. PNAS 1995.
doi:10.3389/conf.neuro.06.2009.03.191

II-59. The spatial profile of inhibitory circuitry modulates oscillatory
activity in auditory cortex
Anne-Marie Oswald1,2
Brent Doiron3
John Rinzel
Alex D Reyes

AMMOSWALD @ GMAIL . COM
BDOIRON @ PITT. EDU
RINZEL @ CNS . NYU. EDU
REYES @ NYU. EDU

1

New York University
Carnegie Mellon University
3
University of Pittsburgh
2

The manner in which information is processed and transmitted in the cortex depends on the connection
architecture and synaptic dynamics between neurons. We investigated the spatial distribution and synaptic
properties of inhibitory connections between pyramidal cell (PC) and fast-spiking (FS) interneuron pairs in
L2/3 of primary auditory cortex (AI). Simultaneous whole cell current-clamp recordings were made from
up to four morphologically identified neurons in a mouse thalamocortical slice preparation. We found high
probabilities of both excitatory (PE) and inhibitory (PI) connections between PC-FS pairs. Excitatory connections declined significantly with distance between 20 and 100 µm, while inhibitory connections were
uniformly distributed. At distances < 30 &#956;m reciprocally connected pairs (RC) were 5 times more
likely (PRC: 0.43) than non-RC pairs (PnRC: 0.08). The spatial profile of PRC paralleled that of PE such that
PRC decreased with distance to an average of 0.27 between 50 and 100 &#956;m. Our results suggest
that the PRC at any given distance can be predicted based on independent connectivity provided the spatial
profile of PE is taken into account. We then assessed the synaptic responses of RC versus non-RC pairs

COSYNE 09

179

II-60
in triple neuron recordings consisting of a single presynaptic FS cell that was reciprocally connected to one
PC and unidirectionally innervated another. The excitatory connections as well as the GABAA mediated
portion of the inhibitory connections did not differ between RC and non-RC pairs. However, in response to
high frequency stimulation (40-80 Hz) of the FS cell, we found that the sustained GABAB mediated inhibition is significantly weaker in RC versus non-RC pairs. Taken together the data suggests that PCs within
50 µm of an FS cell are likely to be reciprocally connected and thus receive less GABAB inhibition than PCs
further away. To assess the functional implication of these findings, we created a spiking network model that
incorporated the experimentally measured spatial connectivity profile and inhibitory dynamics. The input to
the network was mimicked by Poisson distributed excitatory inputs delivered to a local subset of pyramidal
and FS cells producing a center of activation. When we varied the size of the center between 40 and 120
µm, while keeping the total stimulus energy fixed, we found that firing rates of the neurons were unaffected.
However, synchronous network gamma oscillations were significantly larger when that the spatial extent of
stimulation coincides with the 50 &#956;m region dominated by RC pairs compared to those produced by
broader stimulation (70-120 µm). Gamma oscillations are a prominent feature of A1 activity during acoustic
stimulation. Our results suggest that the connectivity profile and inhibitory dynamics of PC-FS cell interactions in L2/3 impart spatial constraints on the genesis of gamma band activity and subsequent processing
of auditory stimuli.
doi:10.3389/conf.neuro.06.2009.03.210

II-60. Generation of short-term memory for items and order by unsupervised learning
Mark Bourjaily1,2
Miguel Tierz3
Paul Miller3

MARKBOUR @ BRANDEIS . EDU
TIERZ @ BERKELEY. EDU
PMILLER @ BRANDEIS . EDU

1

Volen Center for Complex Systems
Department Neuroscience, Brandeis University
3
Brandeis University
2

In a multiple-item sequential memory task, animals can remember the identity of successive items and their
order of appearance [1]. Such ability puts demands on standard models of short-term memory arising from
persistent activity of neurons, which typically encode either the first stimulus or the most recent stimulus. In
this presentation we show how repeated presentations of stimulus sequences to a network, with an initially
limited, transient response to stimuli, can generate the necessary connectivity pattern to solve a two-item
sequential memory task. We find that essential plasticity mechanisms within the network are: 1) Hebbian
plasticity, which we implement through a well-characterized triplet form of spike-timing dependent plasticity
(STDP) [2]. STDP in a sparse network generates an increase in recurrent excitation that is essential to
produce persistent activity and memory. 2) Long-term potentiation of inhibition [3] (LTPi), which increases
inhibition from active interneurons to inactive pyramidal cells. LTPi produces the cross-inhibition necessary
to prevent memory activity from spreading, thus maintaining the specificity of memory. 3) Homeostasis by
multiplicative scaling of synaptic strengths [4] prevents a single attractor state from dominating the network,
allowing for multiple, approximately equally visited states to be maintained in the network. We produce a
simplified, intuitive example of sequential memory as a “bump” of activity, which requires local excitation
to persist following a transient stimulus and which moves toward neurons tuned to subsequent stimuli,
provided some longer-range connections exist. Thus we suggest that such a small-world architecture in the
pattern of connections between neurons is essential to generate multiple-item sequential memory. [1] M.R.
Warden and E.K. Miller, Cerebral Cortex 2007. [2] J.P. Pfister and W. Gerstner, J. Neuroscience 2006. [3]
A. Maffei, K. Nataraj, S.B. Nelson and G.G. Turrigiano, Nature 2005. [4] G.G.Turrigiano, Cell 2008.

180

COSYNE 09

II-61
doi:10.3389/conf.neuro.06.2009.03.217

II-61. Probing mechanisms of gamma rhythmogenesis with cell typespecific optical neural control
Giovanni Talei Franzesi1,2
Xiaofeng Qian3
Mingjie Li1
Xue Han
Christoph Borgers4
Nancy Kopell
Fiona LeBeau5,6
Miles Whittington5,6
Edward Boyden1,3

GEGGIO @ MIT. EDU
QXIAOFENG 2000@ YAHOO. COM
MINGJIE @ MIT. EDU
XUEHAN 1@ GMAIL . COM
CBORGERS @ TUFTS . EDU
NK @ MATH . BU. EDU
FIONA . LEBEAU @ NCL . AC. UK
M . A . WHITTINGTON @ NEWCASTLE . AC. UK
ESB @ MEDIA . MIT. EDU

1

Massachusetts Institute of Technology
Synthetic Neurobiology Group
3
MIT Media lab
4
Tufts University
5
Institute of Neuroscience
6
Newcastle University
2

Rhythms within the gamma frequency range, 30-80Hz, are involved in the brain’s strategy for neural coding
and information processing. They are generated by networks of neurons in mammalian cortex but the precise nature of the neurons that constitute these networks is far from certain. Many electrophysiological and
computational studies have implicated the interplay between inhibitory interneurons and excitatory principal cells as critical for gamma rhythmogenesis. However, lack of neuron subtype-specific tools to probe
network function has hampered further investigation. For example, it is still unclear precisely which of the
many subtypes of cortical interneuron are critical for generating and modulating gamma rhythms; nor is it
clear just how such network activity interacts with principal cells to temporally modulate network outputs.
This information is vital if we are to understand the processes underlying disruption of gamma rhythms associated with neurological illnesses such as schizophrenia and many neurodegenerative conditions. Here
we demonstrate neuron subtype-specific targeting of the light-activated cation channel channelrhodopsin2 (ChR2) and the light-activated chloride pump halorhodopsin (Halo/NpHR) to either excitatory pyramidal
neurons or parvalbumin (PV)-positive interneurons in the mouse hippocampus and neocortex. We use
these techniques in the context of in vitro pharmacological models of persistent gamma rhythms, showing
that bi-directional modulation of the activity levels of these different cell types results in predictable and
repeatable changes in amplitude of the gamma oscillation. These preliminary data demonstrate that each
neuronal subtype makes a critical contribution to the emergent network dynamics, and suggest that, far
from being a simple ‘clock-like’ rhythm, the gamma oscillation is an exquisitely labile network phenomenon
whose manifestation is controlled by the pattern of excitation of both principal neurons and local circuit
interneurons. The specificity of these manipulations allows the acquisition of data ideally suited to computational modeling of network dynamics. These methods can be used, not only for the gamma rhythm,
but for the mechanistic exploration of other EEG rhythms that can be produced in vitro and in vivo. Thus,
they provide new ways to probe the pathophysiology of neurological illnesses in which disruption of brain
dynamics are a core feature.
doi:10.3389/conf.neuro.06.2009.03.299

COSYNE 09

181

II-62 – II-63

II-62. Detection of non-stationary higher-order spike correlation
Hideaki Shimazaki1
Shun-ichi Amari1
Emery Brown2
Sonja Gruen1
1
2

SHIMAZAKI @ BRAIN . RIKEN . JP
AMARI @ BRAIN . RIKEN . JP
ENB @ NEUROSTAT. MIT. EDU
GRUEN @ BRAIN . RIKEN . JP

RIKEN Brain Science Institute
Massachusetts General Hospital

Precise spike coordination in the spiking activities of a neuronal population is discussed as an indication
of coordinated network activity in form of a cell assembly relevant for information processing. Supportive
evidence for its relevance in behavior was provided by the existence of excess spike synchrony occurring
dynamically in relation to behavioral context [e.g. Riehle et. al., Science (278) 1950-1953, 1997]. This
finding was based on the null-hypothesis of full independence. However, one can assume that neurons
jointly involved in assemblies express higher-order correlation (HOC) between their activities. Previous
work on HOC assumed stationary condition. Here we aim at analyzing simultaneous spike trains for timedependent HOCs to trace active assemblies. We suggest to estimate the dynamics of HOCs by means
of a state-space analysis with a log-linear observation model. A log-linear representation of the parallel
spikes provides a well-defined measure of HOC based on information geometry (Amari, IEEE Trans. Inf.
Theory (47) 1701-1711, 2001). We developed a nonlinear recursive filtering/smoothing algorithm for the
time-varying log-linear model by applying a log-quadratic approximation to its posterior distribution. The
time-scales of each parameter and their covariation are automatically optimized via the EM-algorithm under
the maximum likelihood principle. To obtain the most predictive model, we compare the goodness-of-fit
of hierarchical log-linear models with different order of interactions using the Akaike information criterion
(AIC; Akaike, IEEE Trans. Autom. Control (19) 716-723, 1974). While inclusion of increasingly higherorder interaction terms improves model accuracy, estimation of higher-order parameters suffers from large
variances due to the paucity of synchronous spikes in the data. This bias-variance trade-off is optimally
resolved with the model that minimizes the AIC. Complexity of the model is thus selected based on the
sample size of the data and the prominence of the higher-order structure. Application of the proposed
method to simultaneous recordings of neuronal activity is expected to provide us with new insights into the
dynamics of assembly activities, their composition, and behavioral relevance.
doi:10.3389/conf.neuro.06.2009.03.019

II-63. On the cumulants of the spike count distribution of stationary
stochastic point processes.
Carl van Vreeswijk1,2
1
2

CORNELIS . VAN - VREESWIJK @ UNIV- PARIS 5. FR

CNRS UMR 8119
Univeristy Paris Descartes

In a celebrated paper written 50 years ago, W.L. Smith showed that for a large observation time window
T, the cumulants, k n, of the spike count distribution approach k n = a n T + b n, for renewal processes.
He also determined how the constants a n and b n depend on the moments of the inter-spike interval (ISI)
distribution. However, the methods he used to obtain these results were extremely involved. Furthermore,
they can only be used in the case where beginning of the observation is conditioned on a spike. Here
we consider the following questions: With reasonable simplifying assumptions, is it possible to develop a
simpler method to obtain these results? Can this method also be used for equilibrium renewal processes?

182

COSYNE 09

II-64
Can anything similar be done for stochastic point-processes that do not have the renewal property? Assuming that all moments of the ISI distribution are finite, we determine the Laplace transform, G L(x,s), of
the moment generating function, G(x,T), of the spike count distribution. It is straightforward to show that G L
approached infinity as s approaches s 0, given by p L(s 0)=exp(-x), where p L is the Laplace transform of
the ISI distribution. Analysis near s 0 show that for large T, G(x,T) can be approximated by G(x,T)=exp[ a(x)
T + b(x)]. Here a(x) and b(x), and hence a n and b n, can be related to the moments of the ISI distribution.
Since it is simple to find an analytical expression for G L both for a process where the start of the observation window is conditioned on a spike and for an equilibrium process, a n and b n can be determined in
both cases. We find that a n is the same, while b n is different in these two cases. We extend the method
to stochastic processes without the renewal property. Such processes can be described by units with state
variables A, where, if after spike n-1 the state is given by A n-1, the next interval, D n, and state, A n, have a
probability density Pr(D n,A n—A n-1)=Q(A n—A n-1)p(D n—A n,A n-1), where Q describes the transition
of the state variable and p the ISI distribution given A n and A n-1. Assuming that all moments of the ISI
distribution are finite for and Q has one eigenvalue equal to 1 while all others have an absolute value less
than 1. In this case we can again determine G L(x,s) analytically, and as above show that the cumulants of
the spike count distribution approach k n = a n T + b n. However here a n and b n do not only depend on
the moments of the ISI distribution, but also on correlations between ISIs. In particular, negative (positive)
correlations in the ISIs narrow (broaden) the spike count distribution.
doi:10.3389/conf.neuro.06.2009.03.350

II-64. Neurons as Monte Carlo Samplers: Sequential Bayesian Inference in Spiking Neural Populations
Huang Yanping
Rajesh Rao

HUANGYP @ U. WASHINGTON . EDU
RAO @ CS . WASHINGTON . EDU

University of Washington
Animals constantly face the problem of estimating unknown world states from noisy and ambiguous sensory
input in a dynamical environment. Neuro-psychophysical experiments have suggested brains handle these
sensory uncertainties using approximate Bayesian inference. This indicates that neurons both maintain the
probability distribution over world states and combine prior knowledge of the environment with likelihood of
sensory observations in a probabilistic manner. Here we explore how spiking neurons can implement a form
of approximate Bayesian inference known as particle filtering. The time-varying posterior probability distribution of hidden world states is represented directly by spikes in a population of neurons, without involving
complicated decoding methods. Each spike represents a sample of a particular world state. Each possible
world state is represented by an ensemble of nearby identically tuned neurons.Spikes across the entire
population approximate the complete posterior probability distribution. Neural variability in spiking arises
naturally as a consequence of sampling during inference. The posterior spike distribution is recursively
updated by an array of neurons, each of which integrates feed-forward Poisson spike trains whose intensities represent the likelihood of sensory measurements, with recurrent inputs representing propagation of
previous posterior probability distribution, according to Bayes’ rule. Since Bayesian calculation requires
neurons to multiply probabilities of previous posterior and new sensory measurements, we show how such
multiplication can be carried out approximately by a variety of neurons, ranging from simple LIF neurons
to detailed four-compartment pyramidal neurons. Model parameters were selected to be consistent with
those reported for typical CNS neurons. We present simulations that demonstrate how spikes in a neural
population might encode the dynamic environmental state whose values follow a continuous trajectory over
time. The framework we introduce here can also be extended to Bayesian inference over other types of
probabilistic graphical models.

COSYNE 09

183

II-65
doi:10.3389/conf.neuro.06.2009.03.048

II-65. Sparse and Invariant Representations of Multi-component Odors
in the Mushroom Body
Kai Shen
Sina Tootoonian
Anusha Narayan
Gilles Laurent

KAI @ CALTECH . EDU
STOOTOON @ CALTECH . EDU
ANUSHA @ CALTECH . EDU
LAURENTG @ CALTECH . EDU

California Institute of Technology
How does the brain form singular and invariant percepts from complex stimuli? We address this question in
the insect brain by examining processing in the antennal lobe (AL) and the mushroom body (MB), the first
and second neuropils of the olfactory pathway. The MB is a structure analogous to the vertebrate olfactory
cortex and plays a key role in the formation and recall of olfactory memories. Its intrinsic neurons are
50,000 Kenyon cells (KCs), which receive direct input from 830 projection neurons (PNs) of the AL. Odor
representations differ dramatically in the AL and MB. Representations are distributed and dynamic in the
AL, but are sparse, brief and synthetic in the MB. Ensemble PN responses can be described geometrically
by stimulus-specific trajectories reflecting the state of the AL network. How do these trajectories change
with changes in the stimulus? To answer this question, we first probed the PNs with stimuli changing
progressively from one to the other. This produced a progressive rather than abrupt transformation from
one odor-specific trajectory to another, suggesting that the PN network may be able to optimize the use
of its encoding space. It has been shown that families of concentration-specific trajectories form odorspecific manifolds, (Stopfer et al., 2003). We find that the PN trajectories can form manifolds along various
stimulus parameters. For example, by keeping the ratio of the binary mixture the same, we can form
ratio-invariant manifolds. By combining odors progressively, we can form manifolds across odor mixtures.
Next, we systematically increased the complexity of the mixture by combining odors in various ways to
make mixtures with increasing number of components (1-8). We estimated the PN trajectories to mixtures
from those evoked by their components using a linear model, and examined the deviation between these
estimated trajectories and the experimentally observed trajectories. We found that linear predictions are
reasonably good for binary mixtures, but that they worsen rapidly as more components were added. PN
trajectories for single odors (e.g., A) were very different from those produced when 3-7 other odors were
added (e.g., ABCDW). Surprisingly, we found that many KCs could detect single components (e.g., A)
contained in a mixture (e.g., AB, ABC, - ABCDWXYZ). How is this possible, when PN population output
seems so different across mixture conditions? The intuition is that individual KCs sample only subspaces
of the entire PN space, and within these subspaces, representations of components and mixtures overlap.
To test this intuition, we developed a simple KC model, to be tested with experimental PN data as input.
The model consists of 4 features derived from experimental findings: (i) random 50% PN-KC connectivity,
(ii) delayed feed-forward inhibition to mimic Lateral Horn interneurons, (iii) non-linear EPSP amplification
and sharpening, and (iv) adaptive KC spike-threshold for gain control. We find that model KCs match the
probability of KC responses and ensemble PSTHs, and in some instances, individual model KCs perform
odor segmentation just as real KCs do. This model thus seem sufficient to explain high-level computations
that arise in the MB.
doi:10.3389/conf.neuro.06.2009.03.108

184

COSYNE 09

II-66 – II-67

II-66. Distinct adaptive modes for weak and strong signals in a retinal
population
David Kastner
Stephen Baccus

DKASTNER @ STANFORD. EDU
BACCUS @ STANFORD. EDU

Stanford University
Contrast adaptation enables the retina to avoid saturation. When stimulated with large fluctuations in intensity, retinal ganglion cells typically lower their gain and raise their threshold to encode the signal over a
large range. However, upon a subsequent challenge by an environment of low contrast, previous adaptation
causes a cell to fail to signal until it repositions its gain and threshold—a process that can take seconds. We
report on a class of salamander ganglion cell that compensates for this loss of information in a low contrast
environment by adapting in reverse. Following a switch from a high to a low contrast, these cells respond
robustly, and then slowly decrease their sensitivity as other cell types lower their threshold to regain the
ability to signal. This novel mode of adaptation persists across a range of stimulus conditions, including
different spatial frequencies, stimulus sizes, and contrast distributions. This indicates that reverse-adapting
cells assume the prime responsibility for encoding the stimulus after a sudden transition to a low contrast
environment. We further examined the linear, nonlinear, and adaptive properties of these two classes of
Off-type ganglion cells, which have very similar temporal properties, but differ in their modes of adaptation.
We presented randomly flickering stimuli having a Gaussian distribution with a constant mean and a standard deviation that varied from 3 – 36% of the mean. To describe the kinetics, gain and threshold of each
cell, we recorded spiking responses using a multielectrode array, and fit them with a linear-nonlinear (LN)
model, consisting of a linear temporal filter followed by a static nonlinearity. Reverse-adapting cells had a
much lower threshold and a larger spatial receptive field, in accordance with the function of encoding weak
signals. Across this tenfold range of contrasts, the kinetics of the two cell classes changed, yet remained
similar to each other. The linear range of the two cell classes shifted together across contrasts, being adjacent but having little overlap. In response to a change in contrast, reverse-adapting cells exhibited near
complete adaptation to contrast, meaning that they quickly changed their gain in inverse proportion to the
standard deviation. As a result, these cells rarely saturated when encoding small intensity fluctuations, even
if the recent stimulus history contained large fluctuations. In distinction, the high threshold cells showed incomplete adaptation and evaded saturation for large intensity fluctuations. These two populations appear
to share the responsibility for encoding a changing distribution. Each population specializes in encoding a
particular type of signal and avoiding a particular type of saturation, one for weak signals, and the other for
strong.
doi:10.3389/conf.neuro.06.2009.03.121

II-67. Feedback inhibition in the mushroom body and gain control
Maria Papadopoulou1
Glenn Turner2
Gilles Laurent1
1
2

PAPADOPO @ CALTECH . EDU
TURNER @ CSHL . EDU
LAURENTG @ CALTECH . EDU

California Institute of Technology
Cold Spring Harbor Lab

The giant GABAergic neuron (GGN) is a single, paired, non-spiking neuron that arborizes extensively in
the mushroom body (MB)(Leitch and Laurent, 1996), where it overlaps with the dendrites and the axons of
Kenyon cells (KCs). KCs are the intrinsic neurons of the MB and are thought to be required for learning and

COSYNE 09

185

II-68
memory (Heisenberg, 2000). We are interested in understanding the function of GGN in olfactory processing: in particular, its pattern of arborization makes it an attractive candidate for controlling or modulating
KC responses to odors, with potential implications for learning and recall. Physiological recordings of KCs
in locust show that these neurons respond sparsely to odors, by contrast with their excitatory input from
the antennal lobe (projection neurons or PNs) (Perez-Orive et al., 2002). Inhibition appears to be critical to
control KC response threshold, probability and duration during odor stimulation (Perez-Orive et al, 2002).
We show that there exists a feedback loop whereby KCs provide excitatory input to GGN, which, via its
GABAergic output contributes to the control of KC excitability. Using electrophysiological techniques, we
are studying the properties and modes of action of GGN in locust. Our data suggest that this neuron acts
to control the gain of PN-to-KC information transfer and normalize KC-population output, making it independent of input strength. Preliminary results suggest a global action of GGN as measured both at the KC
input and output level. To probe the mechanisms by which GGN achieves gain control we are assessing
the effects of manipulating GGN activity during odor stimulation. Lastly, we investigate the consequences
of eliminating this neuron from the circuit. 1. Leitch and Laurent (1996) J Comp Neurol. 4 :487-514 2.
Heisenberg M. (2000) Nat Rev Neurosci. 4: 266-275 3. Perez-Orive J., Mazor O., Turner G.C., Cassenaer
S., Wilson R & Laurent G. (2002) Science 297: 359-365 This work is supported by NIDCD.
doi:10.3389/conf.neuro.06.2009.03.106

II-68. A likelihood framework to decode the timing of information in
spiking and LFP activity
Arpan Banerjee
Heather Dean
Bijan Pesaran

ARPAN @ CNS . NYU. EDU
DEAN @ CNS . NYU. EDU
BIJAN @ NYU. EDU

New York University
Information about ongoing behavior is encoded in neural activity distributed across multiple brain regions.
How neuronal activity encodes behavioral information, how the timing of this information co-varies across
neural signals, and how the timing of this information co-varies with behavioral response times are open
questions. To address these questions in a computational framework, we are using eye-hand coordination
as a model for a distributed neural process. Before a movement is made to one of two simultaneously
presented visual targets, we can define a target selection time, which is the time at which neural activity
reliably discriminates which target the animal will eventually select. Estimating target selection times from
spiking and local field potential (LFP) activity and comparing them on a trial-by-trial basis is a challenging
problem. This is, in part, due to the differing statistical properties of these measures of neuronal activity
which can introduce biases in the distribution of estimated target selection times. These biases result in
potentially misleading comparisons between measurements. Here, we present a unifying framework for
the analysis of spiking and LFP activity and apply it to the problem of characterizing stages of processing
when subjects perform coordinated eye-hand movements. The key idea is to develop statistical models of
spiking and LFP activity and use them to obtain time varying likelihood ratios for selection of each movement
direction. Thresholds imposed on the accumulation of the log-likelihood ratios using sequential probability
ratio test allow us to determine the target selection times. Likelihood ratios unify the measurement of
target selection times in spiking and LFP activity because accumulated log-likelihood ratios follow a driftdiffusion process irrespective of the individual statistical properties of the underlying activity. We first validate
our procedure using simulated data. We then apply this framework to detect target selection times from
neural recordings in the posterior parietal cortex of awake, behaving monkeys performing a look-reach
movement. In this task, the onset of two visual cues, one in the response field of neuronal activity and
another in the opposite hemifield, cues a look-reach movement. We modeled the spiking activity using two

186

COSYNE 09

II-69
point process models. One model ignores the dependence of present spiking activity on past spike times
and assumes they follow a Poisson process with time-varying rate. The other model uses a conditional
intensity function to account for the past spiking history. Use of a novel conditional intensity function allowed
us to distinguish background firing from instantaneous input driven firing. Analogous to the modeling of
spiking activity, we modeled LFP activity using two Gaussian process models that did and did not include
history dependence respectively. The autoregressive Gaussian process captured the background ongoing
activity and the instantaneous input to the recording area which varies in amplitude and latency trial-by-trial.
Thereafter, we computed the target selection times trial-by-trial from spiking and LFP activity recorded in
lateral intraparietal area (LIP). Our approach allows us to decode the onset of a movement planning stage
and, more generally, information represented in the neural signal trial-by-trial in a unified framework.
doi:10.3389/conf.neuro.06.2009.03.122

II-69. Towards a biophysical basis of spike based inference
Timm Lochmann
Sophie Deneve
Boris Gutkin

TIMM . LOCHMANN @ ENS . FR
SOPHIE . DENEVE @ ENS . FR
BORIS . GUTKIN @ ENS . FR

Ecole Normale Supérieure, Paris
Cells in the retina, thalamus, and sensory cortices integrate inputs from thousands of synaptic afferents.
Sensory stimulation varies over time and in response to specific events in this stream, these cells emit
spikes. What are the generic dynamical properties of sensory neurons allowing them to extract information
from their input and signal it efficiently? How can these properties be realized by biophysical mechanisms?
We previously derived dynamical equations for an optimal “Bayesian” spiking neuron. We used a generative
model (GM) to describe how synaptic input is caused by the dynamic stimulus. This formulation allowed
us to derive in a principled way an equation for optimalsynaptic integration. In addition, we supposed that
the model is self consistent, i.e. that the same GM can be used to estimate the stimulus from the output
spike train. This implied a spike generation mechanism based on an adapting firing threshold. Since the
model is designed to optimally transmit information, it can be used as a tool to understand neural processing. It provides a normative yardstick against which to compare empirical spike data. Furthermore, it
helps to understand how different components in the complex neural dynamics affect sensory processing.
The Bayesian model, however, is motivated by optimality principles rather than biological realism. In order
to bridge the gap between normative and biophysical models, we analyzed its dynamics for different input
regimes where the dynamics can be described by mechanisms known from biophysical models. Our results
suggest that elements necessary to instantiate optimal information transfer are (a) a leak current adjusting
the time constant to the temporal stimulus statistics, (b) a voltage-dependent depolarizing term preventing
the membrane potential to become too hyperpolarized, and (c) spike based adaptation increasing the membrane conductance after each spike. Signatures of these elements have been found in thalamic and cortical
neurons: besides a variety of leak currents, many cells show hyperpolarisation activated depolarizing I h
currents and Ca2+ controlled K-currents represent a significant source of spike based adaptation. Such
detailed descriptions contrast with stochastic models of sensory neurons like the Linear Nonlinear Poisson
model. We argue that Bayesian spiking neurons are both more efficient at transferring information and
provide additional insight into biophysical detail. Nevertheless, they are efficient to simulate and account
for the apparent Poisson-like response variability of cortical neurons. Our analysis provides important links
between biophysical mechanisms and GMs describing the stimulus statistics e.g. how often stimuli appear
and how long they last. The probabilistic interpretation clarifies the functional significance of different currents and how they enable cells to adapt to different input regimes. Showing how information transmission
breaks down if specific components are blocked forms a basis to understand how natural neuromodulators or pharmacological substances acting on the corresponding channels affect sensory processing. By

COSYNE 09

187

II-70
putting emphasis on the inference task as the “problem to be solved” and linking it to more phenomenological descriptions of neural dynamics we can advance our understanding of the link between the biophysics
of sensory neurons and perception.
doi:10.3389/conf.neuro.06.2009.03.268

II-70. Optimal correlation codes in populations of noisy spiking neurons
Gasper Tkacik1
Jason Prentice1
Elad Schneidman2
Vijay Balasubramanian1
1
2

GTKACIK @ SAS . UPENN . EDU
JPRENTIC @ SAS . UPENN . EDU
ELAD. SCHNEIDMAN @ WEIZMANN . AC. IL
VIJAY @ PHYSICS . UPENN . EDU

University of Pennsylvania
Weizmann Institute of Science

In most areas of the brain, information is encoded in correlated activity of large populations of neurons. We
ask how neural responses should be coupled to best represent information about different ensembles of
correlated stimuli. Three classical population coding strategies are independence, decorrelation, and error
correction. Here we demonstrate that balance between the intrinsic noise level and the statistics of the input ensemble induces smooth transitions between these three coding strategies in a network composed of
pairwise-coupled neurons and tuned to maximize its information capacity. We extend recent work (Schneidman et al., 2006; Shlens et al., 2006) and theoretically explore small networks of neurons of the “Ising”
form whose joint probability of firing is determined both by external inputs and by couplings between pairs of
neurons. The neurons are taken to be binary, to represent spiking or silence. We then find the pairwise couplings to maximize information conveyed by neural states about different input ensembles in the presence
of intrinsic noise. We consider two kinds of ensembles – binary patterns, and correlated Gaussian inputs –
and vary the noise levels parametrically to scan the range of network behaviors. For binary input ensembles
at a high noise level, the optimal neural coupling reinforces the input correlations: a simple form of autoassociative error correction. As the noise level decreases, the coupling goes to zero - the neurons become
independent. The Gaussian input ensemble leads to the same optimal network behavior at high noise as
for the binary ensemble. This regime is characterized by the emergence of metastable states which serve
as ”memories” of the input patterns in the sense of a Hopfield network. The weights in a Hopfield net are
often chosen by hand to store the desired patterns; here autoassociative memory emerges as an automatic
consequence of maximizing information capacity. At an intermediate noise level for Gaussian input ensembles, independence of the network neurons is favored. But at low noise there is a new optimal network
strategy: decorrelation of the stimulus. The absence of a decorrelating regime in the binary ensemble can
be understood intuitively by the fact that a correlated binary stimulus ensemble has fewer input states than
the number of network output states. Since the network is capable of representing all possible binary inputs
in a one-to-one fashion, decorrelation confers no extra benefit. The Gaussian ensemble, by contrast, has
an infinite number of possible input states, and decorrelation allows the network to fill its limited bandwidth
efficiently. In the autoassociative regime the “neural code” has many patterns encoding the same stimulus.
In this situation, single neuron variability overestimates the variability of the code – i.e., the sum of single
neuron entropies exceeds the joint network entropy. This suggests that “noise” in single neuron recordings
is partly a misinterpretation of redundant population codes. Our analysis predicts specific changes in experimentally measured network couplings in responses to stimuli with different statistics. Schneidman E et
al.(2006). Nature 440(7087):1007-12; Shlens J et al.(2006). J Neurosci. 26(32):8254-66.
doi:10.3389/conf.neuro.06.2009.03.169

188

COSYNE 09

II-71

II-71. Phase coding of faces and objects in the superior temporal sulcus
Kari Hoffman1
Hjalmar Turesson2
Asif Ghazanfar2
Nikos Logothetis
1
2

KHOFFMAN @ YORKU. CA
TURESSON @ PRINCETON . EDU
ASIFG @ PRINCETON . EDU
NIKOS . LOGOTHETIS @ TUEBINGEN . MPG . DE

York University
Princeton University

Phase coding - stimulus coding by the timing of spikes with respect to the phase of local oscillations - is an
alternative, complementary coding strategy to that of rate coding. One neocortical mechanism for phase
coding posits that rhythmic inhibition in the gamma frequency range may interact with stimulus-evoked excitation, producing spikes earlier in an oscillatory cycle for preferred than non-preferred stimuli (Fries et
al. 2007). Thus, the enhanced response for preferred stimuli commonly seen in the slowly-evolving rate
code may also be coded through differences in spike timing within a single gamma cycle. Theoretically, this
would provide a downstream target with a faster readout than would be possible with rate coding. Evidence
for phase coding of visual stimuli was demonstrated recently in V1 of the anesthetized macaque (Montemurro et al. 2008), but only for lower frequencies (<12 Hz). Another study of spike-field phase coding in
the secondary somatosensory cortex of the awake monkey also failed to find phase coding in the gamma
frequency range (Ray et al. 2008). To address the generality and frequency-dependence of phase coding, we tested whether phase coding would be observed in an object-selective brain region in the awake
macaque. Two monkeys passively viewed images of faces, clip-art objects, and computer-generated ‘greebles’ during broadband recordings from the upper bank superior temporal sulcus (STS; N=13 sessions).
For all stimulus-responsive single units, trials were grouped according to the stimulus category presented
and spiking was compared to the phase of the frequency components of the local field potential on that
trial. For the majority of these cells (N=15), the phase at which firing occurred differed across stimulus
categories. The category-selective phase differences were most common in two frequency bands: below
20Hz and in the gamma range (60-80Hz). The phase differences were not sustained throughout the image
presentation, but rather were limited to roughly the first 200ms following stimulus onset, with no difference
in the time course across frequencies. These results suggest that the visual category displayed can be extracted from the oscillatory phase when firing occurs. This holds not only for primary cortical areas known
for their precise spike timing, but also for cells in association cortex, such as the upper-bank of the STS.
Unlike previous studies, we found evidence of phase coding in the gamma frequency range, suggesting that
there may be a regional specificity to the coding strategies used. The superior temporal sulcus receives
highly-processed signals from multiple modalities, via projections from widespread cortical areas. As such,
cells in STS may be less strictly driven by any given sensory input than are cells in early sensory cortical
areas. Timing with respect to an internal gamma ‘clock’ may be one means by which such association areas
maintain precise codes, as has been demonstrated previously for other cortical association areas such as
the hippocampus (e.g., Buzsaki & Chrobak 1995). The phase coding observed in STS may indicate one
role for intrinsic rhythms in the coding of extrinsic - or stimulus-driven - inputs.
doi:10.3389/conf.neuro.06.2009.03.170

COSYNE 09

189

II-72

II-72. Transformation of Neural Representations in Probabilistic Population Codes
Lei Shi1,2
Thomas Griffiths
1
2

LSHI @ BERKELEY. EDU
TOM GRIFFITHS @ BERKELEY. EDU

Helen Wills Neuroscience Institute
University of California, Berkeley

Probabilistic population coding has been suggested as a general encoding-decoding framework for interpreting population activities in the brain. It carries information about uncertainty [1], and can combine
information from two populations and integrate information over time in a statistically optimal way [2, 3]. A
central operation of neural systems is transforming representations derived from perceptual raw data into
useful forms. For example, a visuomotor map is a coordinate transformation system that transforms visual
coordinates x into motor coordinates y (y = T(x)). In a more complicated example, visual information, first
in the form of retinal images, is transformed into frequency domain by Gabor filters in the primary visual
cortex. This requires neurons in the cortex to integrate multiple inputs from the retina. Moreover, stimuli
often do not contain a single value in the perceived domain – a sinusoidal grating affects many photoreceptors in the retina. Although the stimulus can be characterized by two parameters (frequency, phase), it is
initially represented as many dark and bright dots on the retina. A critical function of perception is to extract
the abstract representation by transforming information from one domain, such as space, to another, such
as frequency. We consider how a neural system can operate optimally in a Bayesian sense throughout
these transformations. To take a specific example, assuming the input signals are some sinusoidal waves
sin(f*x), where f follows a prior distribution p(f), given a (noisy) sinusoidal stimulus in the spatial domain,
we would like to build a neural circuit to detect the true frequency f*. Solving this problem requires inferring the true values behind the stimulus and transforming these values into the frequency domain, while
appropriately maintaining uncertainty. In previous research we have shown that Generalized Radial Basis
Function (GRBF) units (either with continuous or spiking activations) can be used as a form of population
coding that supports Bayesian inference in cue combination [4]. These results are based on the relationship between this approach and the Monte Carlo method of importance sampling: each unit represents an
“exemplar” drawn from the prior, and activation of those exemplars can be done in a way that incorporates
information about the likelihood [5]. In this work, we study how exemplar models, implemented in multiple
layers, can be used to transform neural representations while maintaining Bayesian optimality. References
[1] R. S. Zemel, P. Dayan, and A. Pouget. Probabilistic interpretation of population codes. Neural Comput,
10(2):403–430, 1998. [2] W. J. Ma, J. M. Beck, P. E. Latham, and A. Pouget. Bayesian inference with
probabilistic population codes. Nat.Neurosci., 9(11):1432–1438, 2006. [3] J. M. Beck, W J Ma, R. Kiani, T.
Hanks, and M.N. Latham P.E. Pouget A. Churchland, A.K. Roitman J.D. Shadlen. Probabilistic population
codes for bayesian decision making. Neuron, 2008. [4] L. Shi and T. L. Griffiths. Probabilistic inference using stored examples. In COSYNE, 2008. [5] L. Shi, N. H. Feldman, and T. L. Griffiths. Performing bayesian
inference with exemplar models. In Proceedings of the 30th Annual Conference of the Cognitive Science
Society, 2008.
doi:10.3389/conf.neuro.06.2009.03.215

190

COSYNE 09

II-73

II-73. The significance of a nonlinear transformation and a role of local
neurons in an olfactory circuit
Ryota Satoh1
Masafumi Oizumi1
Hokto Kazama2
Masato Okada1
1
2

SATOH @ MNS . K . U - TOKYO. AC. JP
OIZUMI @ MNS . K . U - TOKYO. AC. JP
HOKTO KAZAMA @ HMS . HARVARD. EDU
OKADA @ K . U - TOKYO. AC. JP

The University of Tokyo
Harvard Medical School

Recent investigations have shown that, in the Drosophila olfactory system,&#12288;olfactory receptor neurons (ORNs) are comparatively narrowly tuned to odors&#12288;while second-order projection neurons
(PNs) are broadly tuned (Bhandawat et al. 2007). This results from a nonlinear transformation between
ORN and PN responses. Because the histogram of PN response magnitudes becomes flatter than that of
ORN response magnitudes, this type of sensory transformation has been termed ’histogram equalization’.
Since PNs will be able to use their dynamic range more efficiently than ORNs, this transformation is considered to be helpful for the decoder to discriminate between odors. However, there are no quantitative
arguments for this role. In this study, we modeled a neural circuit of the Drosophila primary olfactory center,
and examined the shape of the transformation that maximizes the mutual information between stimuli and
PN responses. We constructed a McCulloch-Pitts model where every ORN is connected to every PN in the
same glomerulus with equal strength, and calculated the transformation function that maximizes the mutual
information. The shape of a nonlinear transformation was parameterized simply by one variable. We found
that the shape of the optimal transformation was similar to that observed in the experiment. This transformation produced a flat PN response histogram from a skewed ORN response histogram. However, there is
a difference between the PN histogram obtained above and that obtained experimentally. Although actual
PN odor responses are slightly clustered toward the weak end of PN’s dynamic range, our model does not
produce this feature. We hypothesized that this feature is conferred by inhibitory local neurons (LNs) whose
function was characterized in recent experiments (Olsen and Wilson 2008). We thus introduced LNs in our
model and described the mutual information with two parameters, the shape of nonlinear transformation
and the strength of LN output. We examined whether the overall strength of LN output is excitatory or
inhibitory when mutual information is maximized. We also asked how the shape of PN response histogram
changes after LNs are incorporated to the model. When we explored the two-dimentional parameter space
and calculated the mutual information, we found two peaks. In the first peak, nonlinear transformation was
convex upward and the LNs were inhibitory. This is an experimentally observed combination. In the second
peak, the combination of parameters was opposite; nonlinear transformation was convex downward and the
LNs were excitatory. The reason why the second combination is not employed in the Drosophila antennal
lobe is probably that the overall output of LNs is excitatory. Inhibitory LNs can perform important roles such
as adaptive gain control, but excitatory LNs cannot. When we used the first combination of parameters,
the revised model produced higher mutual information than the model without LNs. It also produced a PN
response histogram having the aforementioned feature. These results suggest that both the nonlinear transformation and the inhibitory local input contribute to increase the mutual information between stimuli and
PN responses, which potentially helps the downstream neurons to more accurately discriminate between
different odors.
doi:10.3389/conf.neuro.06.2009.03.088

COSYNE 09

191

II-74 – II-75

II-74. Laminar-, event-, and state-dependent population coding in the
auditory cortex
Shuzo Sakata
Kenneth Harris

SHUZO. SAKATA @ GMAIL . COM
KDHARRIS @ RUTGERS . EDU

Cntr for Molecul and Behav Neurosci, Rutgers
The laminar architecture of neocortex is likely an important key to its function. However, it is still unclear
1) how population activity across cortical layers orchestrates to process signals, 2) how spatiotemporal
patterns evoked by sensory signals differ from internally generated ones, and 3) how global brain states
affect neuronal activity in cortical microcircuits. To address these issues, we combined juxtacellular method
for morphological identification of single neurons with large-scale extracellular recordings in both urethaneanesthetized and head-restrained unanesthetized rats. Based on >1500 neurons including morphologically
identified pyramidal cells (PCs) in the auditory cortex, we investigated: 1) laminar organization of auditory information processing; 2) similarities and differences between auditory evoked responses and spontaneous
activity; and 3) state-dependence of spontaneous activity across cortical layers. We found that; 1) strategies
of neuronal population coding are different across layers. For example, both spectral and temporal stimuli
were sparsely encoded in population of layer (L) 2/3 PCs, but densely in larger L5 PCs. Neuronal Correlations in L2/3 are local but stronger, compared with L5; 2) spatiotemporal patterns differ between auditory
evoked responses and internally generated “up-states” in their laminar profile. Although evoked activity
propagates from thalamic recipient layers to others, up-state spreads from deeper toward superficial layers; and 3) spontaneous activity of superficial pyramidal cells is suppressed during desynchronized states.
In conclusion, these results suggest that distinct spatiotemporal patterns of activity in the sensory cortex
might allow downstream structures to distinguish sensory responses from internally generated activity. This
work is supported by NIH Grants MH073245, the Alfred P Solon Foundation (KDH), Japan Society for the
Promotion of Science Postdoctral Fellowships for Research Abroad and the Sound Technology Promotion
Foundation (SS).
doi:10.3389/conf.neuro.06.2009.03.037

II-75. Contextual Effects in Neuronal Responses to Complex Sounds
Differ between Areas AI and AAF
Misha Ahrens1
Maneesh Sahani1
Jennifer Linden2
1
2

AHRENS @ GATSBY. UCL . AC. UK
MANEESH @ GATSBY. UCL . AC. UK
J. LINDEN @ UCL . AC. UK

Gatsby Computational Neuroscience Unit, UCL
UCL Ear Institute

The primary auditory cortex (AI) and anterior auditory field (AAF) are both ”core” auditory cortical areas that
receive direct thalamic input. When stimulated with complex dynamic sounds, neurons in mouse AI and AAF
exhibit different linear filtering properties as assessed through analysis of spectrotemporal receptive fields
(STRFs) (Linden et al., J Neurophysiol 90:2660, 2003). Generally, AI filters are slower, and more broadly
tuned, than those in AAF. We have recently proposed a new way to characterise auditory cortical response
properties (Ahrens et al., J Neurosci 28:1929, 2008) with models that incorporate the nonlinear effects of
short-term acoustic context. We used a variant of this ”context model” to study responses in AI and AAF.
This new model identified an inseparable ”contextual reweighting field” (CRF) for each cell, which described
how the efficacy of spectrotemporal elements within the stimulus was modulated by elements nearby in time

192

COSYNE 09

II-76
and frequency, before integration by the STRF. Many contextual effects, including phenomena previously
probed only with simple stimuli (such as forward suppression and combination sensitivity) can be captured
within the response to a complex acoustic stimulus by this inseparable context model. We found that CRFs
in both AI and AAF were typically inseparable; the predictive power of the inseparable context model was
higher than that previously reported for separable context models by Ahrens et al., 2008, and substantially
higher than that of linear STRF models. Moreover, in both AI and AAF, the effects of the CRF could be
decomposed into a generally facilitatory, and often asymmetric, interaction between different frequencies
present simultaneously, and a generally suppressive interaction between similar frequencies at different
times. However, CRFs in AI and AAF differed in other respects: (1) CRF modulation was greater in AI
than AAF; and (2) contextual effects in AI were slower and longer-lasting than those in AAF. These results
indicate that neurons in both AI and AAF are sensitive to combinations of simultaneous tones, and that
contextual interactions between sequential tones differ between AI and AAF. In combination with previous
results, our new study suggests that AAF may be specialised for rapid, temporally precise processing,
while AI neurons integrate more broadly along the temporal dimension. More generally, the CRF analysis
demonstrates that nonlinear effects of acoustic context play a substantial role in shaping the responses of
neurons in core auditory cortical areas.
doi:10.3389/conf.neuro.06.2009.03.135

II-76. Auditory Learning Involving Complex Sounds Affects Nonlinear
Integration within Cortical Responses
Jennifer Linden1
Itzel Orduna4
Ross Williamson2,3
Misha Ahrens2
Eduardo Mercado III4
Michael Merzenich5
Maneesh Sahani2

J. LINDEN @ UCL . AC. UK
IORDUNA @ BUFFALO. EDU
R . WILLIAMSON @ UCL . AC. UK
AHRENS @ GATSBY. UCL . AC. UK
EMIII @ BUFFALO. EDU
MERZ @ PHY. UCSF. EDU
MANEESH @ GATSBY. UCL . AC. UK

1

UCL Ear Institute
Gatsby Computational Neuroscience Unit, UCL
3
CoMPLEX, UCL
4
State University of New York at Buffalo
5
University of California, San Francisco
2

To examine how primary auditory cortex is affected by learning involving complex sounds, we trained rats
to discriminate sequences of fast and slow upward moving frequency sweeps and then analysed neuronal
responses in both trained and naive animals. Extracellular recordings were obtained in anaesthetized
animals during presentations of spectrotemporally rich dynamic random chord (DRC) stimuli. We fit linear
spectrotemporal receptive field (STRF) models to the DRC-driven responses, as well as multilinear context
models (Ahrens, Linden and Sahani, J Neurosci 28:1929, 2008) that capture nonlinear local interactions
related to forward suppression and combination sensitivity. STRFs appeared to be modified in a subset
of animals with very extensive exposure to upward moving frequency sweep sequences, showing both
greater spectrotemporal inseparability and greater selectivity for upward moving sweeps (as assessed by
power asymmetry in the modulation transfer function), relative to STRFs in naive controls. In a second
group of rats, trained to the same level of behavioural performance but with considerably less exposure
to the stimuli, these STRF effects were not seen. However, in both groups of animals, the contextual
reweighting fields in the multilinear models were altered, showing changes in sensitivity to tones within the
DRC that were preceded by tones at slightly lower frequencies. These contextual effects are consistent

COSYNE 09

193

II-77 – II-78
with enhanced discrimination of fast and slow upward moving sweeps. Our results suggest that persistent
effects of plasticity may first be manifest in nonlinear response properties of cortical neurons, and only later
become evident in linear estimates of the response function such as the STRF.
doi:10.3389/conf.neuro.06.2009.03.277

II-77. Spectrotemporal Modulations Underlying Speech and Timbre
Perception
Taffeta Elliott1
Liberty Hamilton1
Frédéric Theunissen2
1
2

TAFFETA @ BERKELEY. EDU
LIBERTY. HAMILTON @ BERKELEY. EDU
THEUNISSEN @ BERKELEY. EDU

Helen Wills Neuroscience Inst., UC Berkeley
University of California, Berkeley

Human speech and music are rich in spectral and temporal modulations, which are fluctuations in either
amplitude or frequency. Speech intelligibility, melody perception, and identification of source characteristics (e.g., vocal gender or instrumental timbre) depend on spectrotemporal modulations but can withstand
drastic spectral and temporal degradations. We systematically explored which restricted spectrotemporal
modulations are essential to the perception of complex sounds. Degraded sentences were obtained by a
novel modulation filtering procedure performed on the sound spectrogram. Temporal modulation filtering
smeared the amplitude envelope by removing changes above a cutoff in Hz. Spectral modulation filtering smeared the spectral energy across frequency bands by removing changes above a cutoff in cyc/kHz.
We further complemented this low-pass filtering with more specific notch-filtering. Speech intelligibility and
gender recognition were assessed. We determined that spectral modulations below 3.75 cyc/kHz, and
temporal modulations between 1 and 7 Hz are essential for speech comprehension, whereas gender identification depends on the presence of higher modulations associated with the glottal pulse. We are expanding
these psychoacoustic experiments to dissimilarity judgments of orchestral instrument tones that have been
normalized in level, tuning and duration. Our goal is to relate the multiple perceptual dimensions of timbre,
such as brightness, sharpness of attack, and spectral flux, to underlying acoustic differences in the spectrotemporal modulation spectrum. We will represent the timbral distance in spectrotemporal modulations
using multidimensional scaling (MDS) [1]. Then subjects will rate tones along 15 subjective rating scales.
We will perform a principal components analysis (PCA) on the timbre ratings to reduce their dimensionality,
helping us to assign qualitative labels to the timbre space obtained by the MDS. Our research could be
used in the design of optimal signal processing in hearing aids and cochlear implants, as well as in music
synthesis and transcription. Acknowledgments We thank D. Wessel and J.-M. Mongeau for helpful discussion. References [1] Timbre space as a musical control structure. D. L. Wessel, Computer Music Journal
3:45-52, 1979.
doi:10.3389/conf.neuro.06.2009.03.234

II-78. An ideal observer for passive tactile spatial perception
Daniel Goldreich

GOLDRD @ MCMASTER . CA

McMaster University
How does the sense of touch resolve the spatial details of stimuli pressed against the skin? Two popu-

194

COSYNE 09

II-79
lar tests of passive (finger stationary) tactile spatial acuity are the classic two-point discrimination task, in
which the subject reports the number of caliper points (one or two) in contact with the skin, and the newer
grating orientation task, in which the subject reports the orientation (horizontal or vertical) of an impressed
square wave grating. Experiments suggest that the firing rates of slowly adapting type-1 (SA1) mechanoreceptive afferents encode the structure of these and other static tactile stimuli, but how the brain decodes
these inputs is unknown. We developed an ideal Bayesian observer that decodes simulated somatosensory neuronal firing patterns evoked by spatially structured stimuli. The generative model incorporates the
known receptive field structures and density of SA1 afferents, and firing rate noise that is either low (Fano
factor < 1), characteristic of SA1 afferents, or high (Fano factor = 1, Poisson), characteristic of primary
somatosensory cortical neurons. The Bayesian observer uses model selection to perform two-alternative
forced-choice tasks (two-point discrimination, grating orientation), and parameter estimation for graded perceptual judgments (point stimulus localization, skin indentation estimation). Qualitatively, the Bayesian
observer performs similarly to human subjects. For instance, the Bayesian observer yields psychometric functions that increase monotonically with point separation (two-point discrimination task) and spatial
period (grating orientation task). Quantitatively, the Bayesian observer outperforms humans. When the
Bayesian observer operates on high-fidelity (low-noise) inputs, its psychometric functions lie well to the left
of humans’. The introduction of Poisson input noise degrades performance, but even in this condition the
tactile acuity of the Bayesian observer exceeds that of humans. The results suggest that the brain lacks
access to the low-noise signal conveyed by individual peripheral afferents, and instead relies upon noisier, presumably somatosensory cortical activity to infer stimulus structure. Since human performance falls
short of the Bayesian observer’s, even when the observer operates on noisy inputs, human performance
is presumably compromised by more than just cortical noise. Several peripheral inputs may converge to
generate a somatosensory cortical neuronal receptive field, with consequent loss of spatial resolution.
doi:10.3389/conf.neuro.06.2009.03.058

II-79. What you show is what you get: sampling biases in determining
biological sensory function
Alexander Dimitrov1,2
1
2

ALEX @ CNS . MONTANA . EDU

Montana State University
Center for Computational Biology

Classical studies of biological sensory systems use the following main technique: sensory stimuli are drawn
from a pre-determined distribution P(stim) and presented to the animal; the ensemble associated with sensory response is collected and used to characterize the conditional distribution P(stim—resp) (or parameters thereof) as a model of sensory system function. However, most of the standard statistical tool used in
neuroscience to estimate P(stim—resp) are valid under a very fundamental condition – that the samples
used to estimate P(stim—resp) are drawn from the same distribution. This is obviously not the case in most
studies of sensory system, where the samples are drawn explicitly from a different distribution, P(stim) (the
sampling distribution), selected by the scientist. We demonstrate here that in this case the observed conditional distribution is P*(stim—resp) = P(stim—resp)*P(stim) and expectations estimated with this dataset
are parameters of P*, not P. To characterize the actual functional properties of the system, one needs to use
estimators developed within unequal probability sampling theory[1]. We apply one of these estimators, the
Horvitz-Thompson estimator of the mean m HT = sum i x i/P(x i), to observations x i from the cricket cercal
sensory system and illustrate the ensuing changes in apparent functionality. References 1. Thomson SK:
Sampling, 2nd Edition. New York: Wiley Interscience; 2002
doi:10.3389/conf.neuro.06.2009.03.182

COSYNE 09

195

II-80 – II-81

II-80. Adaptive precision pooling of model neuron activities predicts
efficiency of human visual learning
Robert Jacobs

ROBBIE @ BCS . ROCHESTER . EDU

University of Rochester
Are human visual judgments based on the activities of all neurons sensitive to a stimulus or just the activities
of a specialized subgroup? If the latter, can human visual learning be characterized as an improvement
in the selection of neurons to include in this subgroup? When performing a perceptual task, precision
pooling occurs when an organism’s decisions are based on the activities of a small set of highly informative
neurons. The Adaptive Precision Pooling (APP) Hypothesis links perceptual learning and decision making
by stating that improvements in performance occur when an organism starts to base its decisions on the
responses of neurons which are more informative for a task than the responses which the organism had
previously used. The APP Hypothesis has its roots in theories stating that visual learning effects are due to
reweightings of the “read-out” connections from early visual representations. A contribution of the current
work is its examination of an extreme form of this idea in which the weightings are biased to be sparse,
meaning that only a relatively small number of weights are non-zero and, thus, only a small subset of
neurons contribute to decision making. We trained human subjects on a visual slant discrimination task,
and measured their learning performances. We also computed the performances of a statistical model,
referred to as an “ideal observer”, on the same task. The results show that subjects did not learn as well
as the ideal observer, meaning that they did not learn as much during training as they theoretically could
have. Why were subjects sub-optimal learners? Our simulation results suggest that the APP Hypothesis
provides a possible explanation, namely that there are few neurons providing highly reliable information
for the perceptual task, and learning involves searching for these rare, informative neurons during the
course of training. Although the APP Hypothesis potentially accounts for many characteristics of human
visual learning, a key motivation for the work presented here is the need to account for differences in
individuals’ learning performances. A common observation among scientists studying visual learning is
that participants in experiments often show different dynamics in their performances with some individuals
showing no performance improvements, other individuals showing gradual improvements during training,
and still others showing abrupt improvements. The APP Hypothesis accounts for all three types of learning
dynamics. Acknowledgements: This work was supported by AFOSR research grant FA9550-06-1-0492
and NSF research grant DRL-0817250.
doi:10.3389/conf.neuro.06.2009.03.242

II-81. Stability and persistence in visual cortex.
Philip Ulinski

PULINSKI @ UCHICAGO. EDU

University of Chicago
Neurons in the cerebral cortex form microcircuits that receive excitatory inputs from the thalamus and contain feedforward and feedback circuits formed by intracortical connections [1]. General features of these
circuits are that thalamic inputs constitute a small fraction of the synapses on cortical neurons while recurrent excitatory connections constitute a large fraction. The circuits must, thus, be sensitive to on-going
synaptic activity in a minority of the synapses but remain stable in the presence of strong excitatory feedback. A number of studies (e.g. [2]) have modeled cortical circuits using non-linear differential equations
and suggested the cortical circuitry shows multiple, stable fixed points or attractor states. A challenge to
understanding the dynamics of cortical circuits is that analysis requires information on the activity of each

196

COSYNE 09

II-82
population of cortical neurons as a function of time, but this kind of information is not available with current
technologies. We have approached this issue by using a large-scale, biophysically detailed model of turtle
visual cortex [3]. Earlier work suggests this cortex is intrinsically stable and can show persistent states in
which all of the populations of neurons remain active following simulated visual stimuli. However, basic features such as stability and persistence cannot be studied rigorously using large-scale models. This study
developed a dynamical systems model that was fit to the activities of each of four populations of neurons in
the large-scale model. A surprising result was that the output of the large-scale model could not be fit with
non-linear differential equations, but could be described by a family of linear, non-autonomous ordinary differential equations that model the interactions between populations of cortical and geniculate neurons. The
system was then studied using analytic methods from control and dynamical systems theory. Responses
of the system to simulated light flashes can be visualized as trajectories in a phase space spanned by the
fraction of each population of neurons that are active (or spike) as a function of time. The system has a
single fixed point at the origin. The stability of the fixed point can be demonstrated by constructing a timedependent Lyapunov function. Changing the balance of excitation and inhibition in the system generates
unstable persistent states in which all of the populations of neurons are active. These states can be studied
using average Lyapunov functions. References [1] R. J. Douglas et al. 1991 J. Physiol. 440: 659. [2] H.
R. Wilson and J. D. Cowan 1972 Excitatory and inhibitory interactions in localized populations of model
neuron. Biophys. J. 12: 1. [3] W. Wang et al. 2005 J. Comput. Neurosci. 19: 263. Support Collaborative
Research in Computational Neuroscience Program at the NSF.
doi:10.3389/conf.neuro.06.2009.03.345

II-82. Orientation processing without orientation maps in the pigeon
analogue to the primary visual cortex.
Shien Wei Benedict Ng
Agnieszka Grabska-Barwinska
Onur Guentuerkuen
Dirk Jancke

BENEDICT. NGSHIENWEI @ RUB . DE
AGNIESZKA . GRABSKA - BARWINSKA @ RUB . DE
ONUR . GUENTUERKUEN @ RUB . DE
JANCKE @ NEUROBIOLOGIE . RUB . DE

Ruhr-University Bochum
In most mammalian brains, orientation selective neurons of the primary visual cortex are clustered into
processing arrays that align vertically to the surface, such that orientation preference is varied systematically in a pinwheel-like arrangement. However in rodents, particularly the highly visual squirrel, no such
clustering has been found despite clear orientation selectivity at the single cell level, thereby suggesting
that extensive spatial clustering of similarly selective units is not necessary for basic visual capabilities.
In this study, we evaluated the functional organization in the visual wulst of the pigeon, an avian model
known to depend heavily on its visual abilities. The pigeon visual wulst is widely assumed to be functionally
analogous to the mammalian primary visual cortex, mainly due to its input connectivity to the geniculate
nucleus in the thalamofugal pathway. However, further details as to its specific functional role, including its
functional architecture for orientation processing is virtually unknown. Using voltage sensitive dye imaging
and electrophysiology, we examined neuronal population activity evoked by drifting gratings of various spatiotemporal frequencies and orientations covering a large portion of the contralateral visual field ( 120◦ x80◦ ).
Here we report that these large stimuli evoked an activation that first emerged from a local region 25ms
after stimulus onset. Local emergence was followed by a rapid spread of activity that covered the whole visual wulst, peaking at 200ms before rapidly adapting to near-baseline levels (after 600ms) despite ongoing
visual input. We were able to distinguish two different regions; a local, central input region that was sensitive to stimulus duration, and a surround region that showed delayed suppression independent of stimulus
duration. We did not detect any clustering of orientation selective units into regular and distinct orientation

COSYNE 09

197

II-83
domains with particular orientation preferences, but instead found a dominant representation of the vertical
orientation. In conclusion, our results show that wulst processing of oriented stimuli differs from that in the
striate visual cortex. In response to a large stimulus, we observed the local emergence, followed by brisk
spread and rapid adaptation of activity. This argues for a local input area via which incoming information is
distributed to the remainder of the visual wulst, while co-activating strong suppressive mechanisms. Further, the lack of orientation maps in the pigeon reinforces the idea that similar-preference clustering may
not be crucial to orientation processing. Finally, we suggest that the dominance of the vertical orientation
can be a functional adaptation to the specific visual demands of flight.
doi:10.3389/conf.neuro.06.2009.03.051

II-83. Timing precision in population coding of natural scenes in the
early visual system
Gaelle Desbordes1
Jianzhong Jin2
Chong Weng2
Nicholas Lesica3
Garrett Stanley1
Jose-Manuel Alonso2

GAELLE . DESBORDES @ GATECH . EDU
JJIN @ SUNYOPT. EDU
CHONGWENG @ GMAIL . COM
LESICA @ ZI . BIOLOGIE . UNI - MUENCHEN . DE
GARRETT. STANLEY @ BME . GATECH . EDU
JALONSO @ SUNYOPT. EDU

1

Georgia Institute of Technology
State University of New York
3
LMU Munich
2

The precision of neuronal spike trains is at the center of a fundamental debate in neuroscience as to what
aspects of neuronal signaling are important in representing information in the brain. Individual neurons can
have extremely precise and repeatable responses to the visual stimuli that strongly drive them (down to
1-ms variability), but they exhibit seemingly degraded temporal precision in response to suboptimal stimuli.
In the presence of natural scenes, the activity of individual neurons is precisely timed across repeated
presentations of the visual stimulus, even though natural stimuli tend to vary on a time scale that is several
times slower. However, in most natural circumstances, the brain does not have access to multiple repetitions
of the same identical stimulus, and, therefore, it is the precision of spiking across neuronal sub-populations
on single trials that is ethologically relevant. While synchrony across neurons in the retina and visual
cortex has been reported at various time scales, which can depend on the visual stimulus, the temporal
precision of the neural code directly entering primary visual cortex, and its dependence on the stimulus,
are still unknown. In this study (Desbordes et al., PLoS Biology, in press), we used natural visual stimuli
to investigate spike timing precision in populations of geniculate neurons that serve as the direct input
to visual cortex. A short movie of a natural scene recorded from a “cat-cam” (Kayser et al., 2003) was
presented repeatedly to anesthetized cats while recording extracellular activity of multiple single units in the
lateral geniculate nucleus (LGN) in vivo. To test how spike timing precision was affected by the properties
of the visual stimulus, the same movie was presented both at high contrast and at low contrast. The
absolute timing of firing events (or groups of closely spaced spikes) changed from trial to trial, and more
so at low contrast than at high contrast, indicating that the response of individual neurons is less precise
across stimulus repetitions when contrast is reduced. However, the relative timing of spikes occurring in the
same trial was insensitive to changes in stimulus contrast—not only within cells but also across neighboring
cells. At the population level, spike timing precision across LGN cells remained on the order of 10 ms,
irrespective of contrast. While it is well known that the response properties of single cells are strongly
modulated by contrast adaptation, which has effects including slower temporal dynamics and increased gain
and selectivity at lower contrast, our results indicate that the temporal precision of the LGN population code

198

COSYNE 09

II-84 – II-85
is globally maintained in the face of a reduction in contrast. Since closely timed spikes are more likely to
induce a spike in the downstream cortical neuron to which they are projecting (Alonso et al., Nature, 1996),
and since fine temporal precision is necessary in representing the more slowly varying natural environment
(Butts et al., Nature, 2007), preserving the relative timing of spikes at a resolution of 10 ms may be a crucial
aspect of the neural code entering primary visual cortex.
doi:10.3389/conf.neuro.06.2009.03.325

II-84. Temporal dynamics of surround suppression in the corticogeniculate feedback pathway
Farran Briggs
W Martin Usrey

FBRIGGS @ UCDAVIS . EDU
WMUSREY @ UCDAVIS . EDU

University of California, Davis
Neurons in the early visual system respond in a selective manner to stimuli that vary in size. Within the
lateral geniculate nucleus of the thalamus (LGN), neurons display a similar time course of excitation to
stimuli restricted to the classical receptive field and suppression to stimuli that extend into the extra-classical
surround. Given the implication that surround suppression arises from feedforward retinal mechanisms, we
explored the temporal dynamics of suppression across distinct populations of neurons in primary visual
cortex (V1), including those that provide feedback projections to the LGN. To do so, we measured neuronal
responses to drifting sinusoidal gratings optimized for preferred orientation, direction, spatial and temporal
frequency in alert monkeys performing a fixation task for juice rewards. Gratings varied in size (diameter)
between 0.2 and 10 degrees. We then compared the temporal dynamics of visual responses to stimuli
that varied in size and estimated the timing and magnitude of suppression. Our results show that different
classes of corticogeniculate neurons display different temporal profiles for surround suppression, indicating
circuit-specific mechanisms for the generation of suppression in the corticogeniculate feedback pathway.
This work was supported by NIH grants EY013588 and EY12576; and the McKnight Foundation.
doi:10.3389/conf.neuro.06.2009.03.077

II-85. Contributions of single neurons in visual area MT to variability
in smooth pursuit eye movements
Sonja Hohl
Stephen Lisberger

HOHLS @ PHY. UCSF. EDU
SGL @ PHY. UCSF. EDU

University of California San Francisco
Movements are inherently variable. In principle, a number of neural sources closer or farther from the
motor output could contribute to such variability. Behavioral measurements have suggested that variability
in smooth pursuit eye movements arises from imperfect estimation of the sensory parameters necessary for
guiding eye movements. Visual area MT provides sensory input to the pursuit system and neurons in MT
are tuned for both direction and speed of motion. Therefore MT is an ideal candidate area to be a source
of sensory variability for pursuit eye movements. If pursuit variability arises from MT, then there could be
significant trial-by-trial correlations between the variability in individual MT responses and the associated
pursuit eye velocity vector. We recorded from single neurons in macaque area MT in two monkeys during

COSYNE 09

199

II-86
a step-ramp pursuit task. The pursuit target, a patch of coherently moving random dots, appeared in the
receptive field of a neuron and moved in a given direction at a given speed. For each neuron, we tested
different directions and/or speeds and each condition was balanced with equal numbers of target motions
in the opposite direction to minimize anticipatory responses. We found significant trial-by-trial correlations
between fluctuations in firing rate of MT neurons and eye velocity. The correlations appeared consistently
and were uniformly positive in many trial conditions across many neurons at the onset of the transient neural
response. Whether firing rate and velocity were correlated depended on the stimulus rather than the neuron
under study. The correlation values were greater for trial conditions that evoked higher firing rates. At times
later in the neural response but still in the first 100 ms of pursuit, we found trial-by-trial correlations with
statistical significance in excess of chance, but with equal numbers of positive and negative correlations
across the population. Our results show that neurons as early in the pursuit pathway as visual area MT
contribute to variability in the initiation of smooth pursuit eye movements. Finally, we use the patterns of our
measured neuron-behavior and known neuron-neuron correlations to help us constrain how downstream
areas decode the population response in MT to guide pursuit eye movements.
doi:10.3389/conf.neuro.06.2009.03.087

II-86. Extracting MAX-pooling receptive fields with natural image fragments
Michel Vidal-Naquet1
Shimon Ullman2
Manabu Tanifuji1
1
2

MICHEL @ BRAIN . RIKEN . JP
SHIMON . ULLMAN @ WEIZMANN . AC. IL
TANIFUJI @ RIKEN . JP

Riken - Brain Science Institute
The Weizmann Institute of Science

A major undertaking in system neuroscience is to uncover the relation between visual stimuli and neuronal
response. A common model used to represent the Input/Output relationship in V1 and other visual areas
consists of computing the dot product (correlation) of the visual input with one or several filters, and then
applying a non-linear stage to obtain a single value expressing, for example, the firing rate. The system
identification problem is then reduced to recovering the visual filter(s). In this context, spike triggered and
iterative optimization techniques are effective approaches to recover the filter components. However, recent
computational models, such as the C1 cells in Serre et al [PNAS, 104(15), 2007], propose that the response
of complex visual neurons represents the maximum value of a local convolution map (MAX-pooling computation), in contrast to computing only the dot product between a filter and the visual input. Here, we
address the problem of how to determine visual filters that produce a set of observed MAX-pooled values
in response to a set of natural stimuli. The main obstacle to discovering the filters stems from the strong
non-linearity induced by the max operation. In contrast to the dot product operation assumed in common
approaches, the MAX-pooling model implies that the exact spatial location in the stimuli that produced the
response is unknown. Consequently, spike triggered methods are not adapted and iterative optimization
techniques are expected to provide poor local optima. We demonstrate a novel approach capable of extracting MAX-pooling receptive fields that overcomes these limitations. Using simulations, we show that
small and unknown MAX-pooling filters can be effectively recovered by appropriately searching through a
large set of image fragments ( 100,000). The method takes advantage of image statistics by restricting
the set of candidate filters to fragments of natural images, thereby reducing the otherwise daunting size of
the search space. The selected filters can be further improved with iterative optimization techniques. We
test the performance of our method with stimuli of different sizes and show that even for larger inputs, that
induce larger convolution maps, the unknown filters are effectively recovered. Furthermore, we show that
in the MAX-pooling context, using solely iterative optimization fails to recover the filters. We also show that

200

COSYNE 09

II-87
Gabor filters, even optimally selected, cannot explain the responses of the hidden filters as well as the best
filters extracted from natural images. Finally, we suggest how we can extend the method to higher level
areas. Several conclusions arise from these results. First, in contrast to common receptive field extraction
techniques, our approach can effectively recover unknown visual filters in the context of MAX-pooling neurons. Second, we show that filters representing natural image fragments have richer response properties
than standard Gabor filters, suggesting that more complex filters can be usefully applied in the early layers
of visual processing to represent information about the visual world. It will be critical to test our method
with physiological data, from V1 and higher level areas, such as IT, and examine the nature of the extracted
models.
doi:10.3389/conf.neuro.06.2009.03.117

II-87. The effect of global context on the encoding of natural scenes.
Robert Haslinger1,2
Bruss Lima3
Gordon Pipa3,2
Emery Brown1
Sergio Neuenschwander3

ROBHH @ NMR . MGH . HARVARD. EDU
BRUSS @ MPIH - FRANKFURT. MPG . DE
PIPA @ MPIH - FRANKFURT. MPG . DE
ENB @ NEUROSTAT. MIT. EDU
NEUENSCHWAND @ MPIH - FRANKFURT. MPG . DE

1

Massachusetts General Hospital
Massachusetts Institute of Technology
3
Max-Planck Institute for Brain Research
2

The complexity of natural stimuli has made it difficult to understand how cortical neurons encode and process information about them. Even in V1, where neurons have well characterized receptive field properties,
efforts to deduce which features of a natural scene stimulus a neuron responds to have generally been
unable to fully account for most neurons’ spiking statistics. It has been proposed that this is at least partly
due to activity of the network in which the neuron is embedded. Such activity depends on the stimulus in
the whole visual field, not only that in the neuron’s receptive field. We set out to quantify the extent to which
neuronal activity in V1 of the behaving macaque monkey is modulated by visual stimuli outside a neuron’s
classical receptive field, and by extension by network activity. We simultaneously recorded neuronal spiking activity and local field potential (LFP) from the regions of central and peripheral representation of the
visual field (4 and 10 degrees of eccentricity). After mapping the receptive fields (using moving bars), we
presented natural scene movies (trees, flowing water, a busy street, etc.) under two conditions. In the
un-masked condition we displayed the unobscured movie. In the masked condition we displayed the same
movie but with only the portion corresponding to the neuron’s receptive field visible, the remainder of the
frame was obscured by an opaque Gaussian mask. To rigorously quantify differences in spike statistics
between these two conditions we fitted Generalized Linear Models (GLMs) of the history dependent spike
probability to the spiking responses. The GLM included a PSTH-like term accounting for the stimulus and
a second renewal process-type term accounting for the neuron’s previous spiking history. For almost all
neurons, we obtained excellent goodness of fit as quantified by Kolmogovov Smirnov tests. Notably, we
found that for many neurons (9 of 15 to date) the spiking responses were substantially different between
the unmasked and masked conditions, indicating the strong influence of stimuli outside the neuron’s receptive field. We then attempted to determine what proportion of the difference in spiking response could be
accounted for by the LFP, a population-averaged network activity measure. We decomposed the LFP into
different time scales using a Daubechies wavelet multi-resolution analysis, and included a non-linear autoregressive model of each LFP scale in the GLM. For many neurons (6 of 9 to date) we found that much of
the difference between the un-masked and masked conditions could be explained through LFP variations.
Our goal is to determine which LFP time scales and features most inform the differences between masked
and unmasked conditions by recasting the AR coefficients as non-linear filters and studying their frequency

COSYNE 09

201

II-88
and phase response properties. Our study suggests that analysis of receptive field properties is not sufficient to account for spiking responses to complex stimuli such as natural scenes. The global context of the
stimulus, possibly mediated by network activity, remains an essential component. Supported by NIH grants
K25 NS052422-02, DP1 OD003646-01, MH59733-07 and the Max Planck Society.
doi:10.3389/conf.neuro.06.2009.03.115

II-88. Visual response properties of V1 neurons projecting to V2 in
macaque
Yasmine El-Shamayleh
Romesh Kumbhani
Neel Dhruv
J Anthony Movshon

YASMINE @ CNS . NYU. EDU
ROMESH . KUMBHANI @ NYU. EDU
N . DHRUV @ UCL . AC. UK
MOVSHON @ NYU. EDU

New York University
The second visual area in the macaque (V2) is the major recipient of feed-forward ‘driver’ projections from
striate cortex (V1). As a gateway to the ventral stream, V2 may underlie important aspects of visual form
processing. The convergence of multiple V1 inputs onto larger V2 receptive fields suggests that V2 neurons
may combine information about simple image elements across space, giving rise to selectivity for complex
image features. V2 neurons have been reported to be selective for angles, complex shapes, illusory contours, object boundaries, and relative binocular disparity. It is unclear how these selective responses are
generated from V1 inputs. We therefore identified V1 neurons projecting to V2 by antidromic electrical stimulation, and measured their visual response properties. We placed bipolar stimulating electrodes in middle
V2 layers, and recording electrodes in the retinotopically-matched V1 location. To determine connectivity,
we delivered single monophasic electrical pulses in V2 (200 µsec; 50-100 µA) and recorded evoked V1
spikes. V1 cells that passed the collision test (Bishop, Burke and Davis, J. Physiol., 1962) were taken to
be V2-projecting, showing collision of spontaneous orthodromic spikes with electrically elicited antidromic
spikes. Antidromically-activated spikes had minimal latency jitter and short conduction velocities (1-5 ms).
We measured the responses of V2-projecting neurons with sinusoidal gratings that varied in contrast, direction, spatial frequency, drift rate, size, chromatic modulation, and relative interocular spatial phase. To
quantify neuronal selectivity, we fitted appropriate model functions and computed tuning indices. We compared our results with those from larger V1 data sets recorded without knowledge of neuronal connectivity
to V2. Most projection neurons in our sample were complex (F1/DC < 1), suppressed by stimuli that engaged the receptive field surround, moderately selective for grating orientation, and unselective for direction.
Interestingly, most showed significant binocular phase interactions, and were better driven by luminancemodulated than chromatically-modulated stimuli. The response properties of the V2-projecting neurons
suggest that they mostly carry information about visual form. As such, they may provide information that V2
neurons use to generate selectivity for complex features. These results motivate future studies of how V1
signals shape the spatial structure of V2 receptive fields, and place informative constraints on computational
models of cortical processing in V2.
doi:10.3389/conf.neuro.06.2009.03.114

202

COSYNE 09

II-89 – II-90

II-89. Homeostatic and gain control mechanisms in a developmental
model of orientation map formation in V1
Judith Law
Jan Antolik
James Bednar

J. S . LAW-1@ SMS . ED. AC. UK
J. ANTOLIK @ SMS . ED. AC. UK
JBEDNAR @ INF. ED. AC. UK

University of Edinburgh
Numerous studies have shown that cortical neurons can self-regulate their response gain (i.e., their output
in response to an input). Theoretical studies of such gain control have primarily considered single cells
or small networks of neurons in the adult brain. However, gain control is likely to be particularly important
during development, because the amount and distribution of input activity can change dramatically between
neurogenesis and adulthood. For instance, the developing visual system at first receives intrinsically generated input, such as retinal waves or spontaneous cortical activity, and in later stages (after eye opening)
receives direct visual stimulation from the environment. In this study we examine how gain control can
interact with basic homeostatic mechanisms to reproduce the experimentally observed patterns of development in a large scale model of an orientation map in the primary visual cortex (V1). Using this model, we
have identified a small set of mathematical rules that can reproduce the following experimentally observed
phenomena: stable orientation map development (Chapman et al. J. Neurosci., 1996, 16:6443–6453),
contrast independent orientation tuning (Alitto et al. J Neurophysiol., 2004 91:2797–2808), and orientation
map development that is robust against changes in the levels or distributions of input activity over time
(Crair et al. Science, 1998, 279:566–570). We show that the above constraints can be met by using a
simple but plausible gain control mechanism at the level of the Lateral Geniculate Nucleus (LGN) or retina,
plus a mechanism that maintains a constant ratio between the strength of different input types (afferent vs.
feedback, and excitatory vs. inhibitory) to each individual neuron. By directly maintaining these specific
interaction ratios, it is sufficient to use a simple threshold adjustment rule for each neuron, rather than the
more complex intrinsic excitability adjustment rules previously designed for more abstract networks (Triesch
ICANN, 2005, 65–70). This model thus highlights the benefit of studying these phenomena in neural models
whose architecture is constrained by the known connectivity of neural structures (such as V1).
doi:10.3389/conf.neuro.06.2009.03.341

II-90. Simultaneous multielectrode recording and multi-photon imaging of retinal visual responses
Michael Menz
Stephen Baccus

MDMENZ @ STANFORD. EDU
BACCUS @ STANFORD. EDU

Stanford University
In the second synaptic layer of the retina, visual information undergoes a number of important transformations, including direction selectivity, sensitivity to differential motion, and adaptation to spatio-temporal
patterns. At this level, a diverse population of inhibitory interneurons, amacrine cells, shapes visual responses, but most of their specific roles are unknown. To measure how the interneuron population in the
inner retina translates the visual scene into retinal output, we have constructed a custom two-photon laser
scanning microscope to image the visual responses of retinal interneurons while simultaneously recording
the spiking output of ganglion cells with a transparent multielectrode array. We have used this system to
record subcellular calcium responses and voltage responses from a population of amacrine cells in the
salamander retina. We used second harmonic generation (SHG) imaging to record the membrane po-

COSYNE 09

203

II-91
tential responses of approximately 100 amacrine cells in a field. The styryl dye FM 4-64 was applied to
the bathing medium to produce the voltage-sensitive SHG signal. We presented a flashing 1 Hz stimulus,
and computed the average response at each spatial location in an image. Responses were then clustered
based on their temporal profile. At least seven types of responses were seen, consistent with the known
diversity of amacrine response types including On, Off, On-Off, sustained and transient cells of different
timescales. These measurements allowed us to assess overall properties of the inhibitory amacrine population. Responses to light onset were of longer latency (150-275 ms) and were more sustained, whereas
responses to light offset were of shorter latency (50-150 ms), and were more transient. Latencies were as
fast as those seen for ganglion cell spiking and local field potentials measured simultaneously. An additional asymmetry in amacrine responses was that although the magnitude of the response was similar for
light increments and decrements across the population, the transient responses to light offset were more
diverse. This indicates that decrements in intensity are processed by a greater number of distinct inhibitory
temporal channels. Differences in kinetics between On and Off responses first arise at the photoreceptor to
bipolar cell synapse. The resulting diversity of Off responses of the amacrine population may arise due to
the inherent greater temporal bandwidth of the Off pathway.
doi:10.3389/conf.neuro.06.2009.03.157

II-91. Inhibition facilitates fast, robust motion detection in the visual
cortex
Audrey Sederberg1
Julia Liu1
Matthias Kaschube1,2
1
2

ASEDERBE @ PRINCETON . EDU
JULIALIU @ FAS . HARVARD. EDU
KASCHUBE @ PRINCETON . EDU

Princeton University
Lewis-Sigler Institute

A classical question of computational and systems neuroscience addresses the mechanism underlying
direction selectivity of simple cells of primary visual cortex. In a prominent class of models, a cortical
cell integrates excitatory inputs of different response latencies and offset receptive field positions arranged
such that inputs arrive synchronously for stimuli moving in the preferred direction, but dispersed in time
for stimuli in the null direction. Selectivity is assumed to be sharpened by a spike threshold. Here we
probe this mechanism quantitatively: in particular, in the presence of an additional noisy input (due to, for
example, random background activity), which is known to affect the threshold nonlinearity of the input-output
relationship. Furthermore, we explore a possible role for inhibition, which has been found to be tuned for
the same direction as excitation, but with different relative timing (Priebe and Ferster, 2005). We implement
this model based on an exponential integrate-and-fire neuron (Fourcaud-Trocme et al., 2003). We model
the noisy input by i) an additional noise current and/or ii) different degrees of spontaneous activity of input
neurons. Our basic results are captured also by a simple rate model. We find that if only excitatory inputs
are present, small levels of noisy input reduce direction selectivity strongly. For large noise, the mean firing
rate for both stimulus directions is approximately equal, resulting in a very weakly selective neuron. In this
regime, the firing rate is approximately proportional to input strength. Since the total number of inputs is
the same for both directions (only their temporal order differs) the mean firing rate is approximately equal
for both directions. In contrast, when inhibitory inputs are included and arranged in a push-pull fashion,
as observed by Priebe and Ferster (2005), selectivity is fully recovered even for large levels of noise. For
the null direction, both excitatory and inhibitory inputs arrive dispersed in time and therefore largely cancel
each other resulting in a firing rate close to baseline. For the preferred direction, excitatory and inhibitory
inputs each arrive synchronously, but with a temporal lag relative to each other. Since firing rate is bounded
by zero, the increase in firing rate due to the excitatory inputs can greatly exceed the decrease caused by

204

COSYNE 09

II-92
the inhibitory inputs, resulting in a strongly direction selective neuron. A similar result holds when probing
the neuron with a drifting grating and defining selectivity based on the first Fourier component of firing rates
during a stimulus period. Interestingly, in the presence of inhibition, the neuron responds selectively even
after one stimulus cycle ( 100ms) for various levels of noise. We conclude that inhibition in a push-pull
organization is an essential part of the mechanism underlying cortical direction selectivity, providing robust
selective responses in the presence of noisy or unrelated background activity, even after brief stimulus
presentations.
doi:10.3389/conf.neuro.06.2009.03.149

II-92. OFF direction-selective cells in the mouse retina
Yifeng Zhang1
In-Jung Kim2,1
Joshua Sanes
Markus Meister1
1
2

ZHANG 2@ FAS . HARVARD. EDU
IKIM @ MCB . HARVARD. EDU
SANESJ @ MCB . HARVARD. EDU
MEISTER @ FAS . HARVARD. EDU

Harvard University
Dept. of MCB, Ctr for Brain Science

OFF direction-selective cells in the mouse retina Yifeng Zhang, In-Jung Kim, Joshua Sanes, and Markus
Meister Harvard University, Molecular and Cellular Biology, Center for Brain Science Our vision is extremely
sensitive to object motion. Detection of motion starts in the retina, and the motion information is further
extracted, processed, and refined along the visual pathway. The mouse retina has three types of retinal
ganglion cell that report object motion in a specific direction. Based on morphology and response properties, one distinguishes: the ON-OFF direction selective-ganglion cells (DSGCs), the ON DSGCs, and
the recently discovered OFF DSGCs, also known as J-RGCs [1]. For both the ON-OFF and the ON DSGCs, direction selectivity depends on synaptic input from starburst amacrine cells, which is itself already
directionally selective. J-RGCs, on the other hand, receive little or no input from starburst cells, and therefore must generate direction selectivity by different means. To explore these mechanisms, we exploited a
transgenic mouse line in which J-RGCs are fluorescently marked, and targeted these neurons for electrical
recording. J-RGCs have a highly asymmetric organization: Their dendritic arbors are fan-shaped and point
from dorsal to ventral across the retina. This same axis corresponds to the preferred direction for spot
movement. We recorded the synaptic input currents of J-RGCs and studied their roles in the emergence
of direction selectivity. Using reverse correlation to flicker stimuli, combined with whole-cell voltage clamp
at a range of holding potentials, we measured the receptive fields for both the excitatory and inhibitory
inputs. The receptive field for inhibition has an ON center and is limited to a region close to the soma.
By contrast the receptive field for excitation has an OFF center that extends over the entire dendritic tree.
Moreover, the excitatory input to the J-RGCs is “directionally asymmetric”: a stimulus applied at the distal
dendrites paradoxically excites the cell with shorter delay than one at the proximal dendrites. We propose that this timing difference between spatially offset excitatory inputs combined with a spiking threshold
could produce the observed direction selectivity in J-RGCs. Acknowledgments This work was supported by
NIH grant EY10020-11 and Damon Runyon Fellowship. References [1] Molecular identification of a retinal
cell type that responds to upward motion. Kim IJ, Zhang Y, Yamagata M, Meister M, Sanes JR. Nature
452(7186):478-82.
doi:10.3389/conf.neuro.06.2009.03.316

COSYNE 09

205

II-93 – II-94

II-93. Controlling response gain and input gain with noisy synaptic
input
Asli Ayaz
Frances Chance

AAYAZ @ UCI . EDU
FCHANCE @ UCI . EDU

UC Irvine
Many factors, including attention, head/body position and stimulus contrast, can affect the gain of visual
neuron responses. Within the visual system, multiple forms of gain control have been observed. For
example, different studies in visual cortex describe the effect of attention as a modification of either response
gain (where the output of a neuron is multiplicatively scaled, see [1-3]) or contrast gain (better described
as a multiplicative scaling of the input contrast, see [4-6]). How do these phenomena arise in vivo? The
mechanism that underlies these effects, or even if they arise from the same basic mechanism, is unknown.
Several biophysically plausible mechanisms for multiplicative gain modulation have been proposed in recent
years. In this study we examine whether one of them, varying levels of noisy synaptic input, is a viable
mechanism for modulating both input gain and response gain in cortex. We study the responses of a
model neuron incorporated into a cortical network very similar in spirit to that of the “normalization model”
proposed by Heeger and colleagues [7-9] in that local activity is “pooled” and transmitted through recurrent
connections. The model neuron thus receives a high background level of noisy synaptic input, as is typical
of in vivo cortex. External input, driven by either a sensory stimulus or another modulatory signal, such
as attention, changes both the level of noisy synaptic input and also the balance between excitation and
inhibition. We examine how modulatory signals, directed towards specific populations of cortical neurons
in the model, control changes in either response gain or input gain by modifying the level of noisy synaptic
input. It has been previously established that increasing excitatory and inhibitory synaptic inputs divisively
scales neuronal responses when excitation is balanced against inhibition in a particular configuration [1013]. Such balanced synaptic input leads to a change in response gain in our model. If excitatory and
inhibitory inputs are recruited in a different configuration, we find that noisy synaptic input can also drive
changes in input gain. 1. McAdams CJ, Maunsell JH (1999) J Neurosci 19:431-441. 2. Treue S, Martı́nezTrujillo JC (1999) Nature 399:575-579. 3. Williford T, Maunsell JH (2006) J Neurophysiol 96:40-54. 4.
Reynolds JH, Desimone R (1999) Neuron 24:19-29. 5. Reynolds JH, Pasternak T, Desimone R (2000)
Neuron 26:703-714. 6. Martı́nez-Trujillo JC, Treue S (2002) Neuron 35:365-370. 7. Heeger DJ (1992)
Vis Neurosci 9:181-197. 8. Heeger DJ (1993) J Neurophysiol 70:1885-1898. 9. Carandini M, Heeger DJ,
Movshon JA (1997) J Neurosci 17:8621-8644. 10. Chance FS, Abbott LF, Reyes AD (2002) Neuron 35:773782. 11. Fellous JM, Rudolph M, Destexhe A, Sejnowski TJ (2003) Neurosci 122:811-829. 12. Prescott
SA, De Konick Y (2003) Proc Natl Acad Sci USA 100:2076-2081. 13. Shu Y, Hasenstaub A, Badoual M,
Bal T, McCormick DA (2003) J Neurosci 23:10388-10401.
doi:10.3389/conf.neuro.06.2009.03.195

II-94. Context effects on visual search and rapid animal detection
Jan Drewes
Julia Trommershäuser
Karl R. Gegenfurtner

JAN . DREWES @ PSYCHOL . UNI - GIESSEN . DE
JULIA . TROMMERSHAEUSER @ PSYCHOL . UNI - GIESSEN . DE
KARL . R . GEGENFURTNER @ PSYCHOL . UNI - GIESSEN . DE

Department of Psychology, Giessen University
Human observers are capable of detecting animals within novel natural scenes with remarkable speed and
accuracy. Recent studies found human response times to be as fast as 120ms in a dual-presentation (2-

206

COSYNE 09

II-95
AFC) setup (Kirchner, Thorpe 2006). In most previous experiments, pairs of randomly chosen images were
presented, frequently from very different contexts (e.g. a zebra in Africa vs. the New York Skyline). Here, we
tested the effect of context on performance by using a new, contiguous-context image set. We also examined the effect an increased uncertainty in target location. From the Tuebingen Natural Image Database we
chose images containing a single animal surrounded by a large, animal-free image area. The image could
be positioned and cropped in such a manner that the animal could occur in one of eight evenly spaced
positions on an imaginary circle (radius 10 deg visual angle). As a position anchor the center of gravity
of the visible parts of the animal was chosen. In the first (8-location) experiment, all eight positions were
used, increasing the uncertainty of the target location, whereas in the second (2-location) and third (2-afc)
experiment the animals were only presented on the two positions to the left and right of the screen center.
In the third experiment, additional rectangular frames were used to mimic the conditions of earlier studies.
Absolute hit ratios were on average slightly lower for the display in which the target was presented at 8
possible locations than in both other conditions (8-location:81%; 2-location:88%; 2-afc:87%), yet the rangenormalized results show a slight advantage in performance for the 8-location condition (8-location:78%,
2-location:75%, 2-afc:73%). Average latencies on successful trials were similar in all three conditions (8location:207ms, 2-location:198ms, 2-afc:203ms), indicating that the number of possible animal locations
within the display does not affect decision latency. The average number of saccades required to reach
the target was slightly higher for the 8-location experiment than for both other conditions (8-location:1.30,
2-location:1.21, 2-afc:1.18). These results illustrate that animal detection is fast and efficient even when the
animals are embedded in their natural backgrounds and can occur in arbitrary locations in an image.
doi:10.3389/conf.neuro.06.2009.03.186

II-95. A motion model based on a recurrent network
Jeroen Joukes1,2
Bart Krekelberg1
1
2

JEROEN @ VISION . RUTGERS . EDU
BART @ RUTGERS . EDU

Rutgers University
University of Amsterdam

Successful interaction with a dynamical environment requires a neural mechanism for the detection of motion. Current motion models such as the Reichardt detector, the motion energy model, and the gradient
model, rely heavily on the neural delays of single cells. It is not clear, however, whether neurons in the motion processing pathway of primates have the temporal organization to support this assumption. Moreover,
it is well-known that cortical areas have numerous recurrent connections – a feature that is conspicuously
absent from almost all motion models. We investigated whether a motion model without explicit delay lines,
but with recurrent connectivity is a viable candidate for the neural processing of motion. We trained an
Elman recurrent neural network (Elman, 1990) to reproduce the recorded response of neurons in area MT
of the macaque to a moving stimulus (Krekelberg et al., 2006). After successful training, we probed the
hidden and output units of the model network with physiological methods, which allowed us to compare
their features with properties of real neurons in the motion processing pathway, and gave insight into how
the network solved the complex task of motion processing. Our first unexpected result was that the output
layer of the network not only captured the response dynamics of the motion sensitive cells but also correctly
predicted the interaction between stimulus speed and contrast observed in MT (Krekelberg et al., 2006).
Second, reverse correlation analysis of the model MT neurons revealed slanted space time response maps
that matched the original speed tuning curves of the MT cells. This shows that a non-linear system like
our recurrent neural network can behave like a simple feed forward model when probed with single flashes.
Third, the hidden units of the neural network had speed tuning, direction selectivity, and temporal response
properties comparable to neurons in the primary visual cortex. Finally, we found that the network analyzes
the speed of motion by combining transient and sustained units in the hidden layer much like the weighted

COSYNE 09

207

II-96
intersection model proposed by Perrone and Thiele (2002). We conclude that a recurrent network may
indeed be a good candidate for the neural mechanism of motion detection; it incorporates the response
properties of feed-forward models, matches known anatomical and functional connectivity, and can account
for known imperfections in the percept of motion. References Elman, J. L., (1990). Cognitive Science 14,
179-211. Krekelberg, B., et al. (2006). J. Neurosci. 35, 8988-8998. Peronne, J.A., Thiele, A., (2002). Vision
Research 42, 1035–1051
doi:10.3389/conf.neuro.06.2009.03.321

II-96. Effects of GABAa somatic inhibition on orientation tuning and
contrast sensitivity in visual cortex
Steffen Katzner
Laura Busse
Andrea Benucci
Matteo Carandini

S . KATZNER @ UCL . AC. UK
L . BUSSE @ UCL . AC. UK
ANDREA @ CARANDINILAB . NET
MATTEO @ CARANDINILAB . NET

University College London
The selectivity of neurons in sensory cortex is thought to be at least partially shaped by inhibitory connections. In primary visual cortex (V1) many studies have examined how orientation tuning is affected by
application of the GABAa antagonist bicuculline. These studies, however, have reported inconsistent results: some have seen a loss of selectivity and others only minor effects. A possible reason lies in the
broad spectrum of effects of bicuculline; much more selective blockage of GABAa receptors is achieved
with the potent antagonist gabazine (SR95531). We examined the effect of local gabazine iontophoresis
on orientation tuning in V1 of anesthetized cats. Using extracellular recordings, we measured orientation
tuning with drifting gratings. Gabazine was iontophoresed immediately above the recording electrode. An
array of 96 electrodes was placed 0.5 mm away to examine the spatial spread of gabazine. If gabazine
did not affect responses measured with the array, we assumed that its effects were mostly restricted to the
soma and proximal dendrites of neurons at the site of iontophoresis. Blocking GABAa-receptor mediated,
somatic inhibition increased responses to all orientations by 150%, broadened tuning width by 22%, and
reduced direction selectivity by 32% (medians, n=23). The resulting tuning curves resembled responses
typically measured in subthreshold membrane potential, suggesting that GABAa provides untuned inhibition, keeping responses to most orientations below threshold. We tested this hypothesis using a simple
rectification model. The model relies on the finding that the shape of orientation tuning curves for firing rates
can be predicted by applying an appropriate threshold and gain to noisy membrane potential responses.
Given these parameters, we sought to determine two hypothetical membrane potential tuning curves, which
were allowed to differ only in their mean responses across orientations, to predict the tuning curve for firing
rates in both the control and gabazine condition. This rectification model captured the observed effects of
blocking GABAa-receptor mediated, somatic inhibition, i.e. increase in responses, broadening of orientation
tuning, and decrease of direction selectivity. The success of the model suggests that inhibition hyperpolarizes the membrane potential by a constant amount, resulting in fewer threshold crossings, and therefore
decreased firing rates and increased selectivity. Next, we asked whether inhibition depended on overall contrast. We measured contrast response functions with drifting gratings of optimal orientation before, during,
and after gabazine administration. Blocking inhibition led to increases in firing rates that were stronger for
high-contrast than for low-contrast stimuli. These effects are consistent with the notion that inhibition grows
with contrast. We conclude that GABAa-receptor mediated, somatic inhibition plays an important role in
orientation and direction selectivity by keeping membrane potential responses to most stimulus orientations
below threshold, but not necessarily by shaping the tuning curve itself. Our results are consistent with the
view that somatic inhibition provides a constant hyperpolarization, whose magnitude depends on stimulus

208

COSYNE 09

II-97
contrast, but not on orientation. Support: NIH EY017396.
doi:10.3389/conf.neuro.06.2009.03.203

II-97. The negative BOLD response and its behavioral correlates
Alex Wade1,2
Jess Rowland1
1
2

WADE @ WADELAB . NET
ROWLAND @ SKI . ORG

Smith-Kettlewell Eye Research Institute
UCSF

Introduction: The negative BOLD response (NBR) describes a phenomenon seen in functional magnetic
resonance imaging in which a stimulated cortical region that responds with a positive blood oxygenationlevel dependent (BOLD) signal is flanked by an unstimulated region of cortex exhibiting a negative response
(Shmuel, Yacoub et al. 2002). It is most easily seen in visual cortex where well-ordered retinotopic maps
make it easy to identify stimulated and unstimulated locations. The NBR is primarily a neural effect: reductions in the BOLD signal correlate with reductions in both LFP activity and firing rates (Shmuel, Augath et
al. 2006). Using event-related fMRI, we measured the amplitude of the NBR as a function of ongoing background activity in order to test three potential models of response control: response gain control, contrast
gain control and a purely subtractive mechanism. We also measured human psychophysical performance
using a stimulus very similar to that in our fMRI experiments in order to identify behavioral correlates of the
signal changes that we observe. Methods: General: Our stimulus consisted of a central disk D (diameter 2
degrees) containing a grating of contrast Cd and a surrounding annulus A (diameter 5 degrees) containing
a grating of contrast Ca. These components were presented on a uniform mean gray field with a small gap
between them. Cd and Ca could be set independently. In our event-related fMRI experiments, the difference
between conditions Cd=0% (no center) and Cd=90% (high-contrast center) gave the magnitude of the NBR.
We measured the NBR as a function of Ca (0%, 5%, 20% and 45% contrast). In our psychophysical experiments, we used a staircase procedure to measure threshold-versus-contrast curves for both D and A in the
presence and absence of high-contrast, spatially-remote maskers (A and D respectively). fMRI methods:
N subs=5, B0=3T, EPI TR=2s, resolution 2x2x2mm. All data from independently-localized regions in V1
defined on flattened, retinotopically-mapped cortex. Behavioral methods: N subs=5, thresholds estimated
using QUEST 2 interval forced choice procedure. Results: Our fMRI results show that the NBR is best
modeled as a multiplicative gain control mechanism with the strongest effects occurring at low background
contrast. In addition, the NBR exhibits a strong spatial asymmetry: We measure a strong NBR in the periphery but not in the fovea. Our psychophysical data are in agreement with these results: We measure only
weak suppressive effects of a high-contrast annulus on the foveal target but a significant amount of suppression of the surround annulus due to the high-contrast center. Conclusion: The NBR is a manifestation
of a multiplicative gain control mechanism that acts to suppress neural activity in the periphery when a highcontrast stimulus is present in the fovea. Shmuel, A., M. Augath, et al. (2006). ”Negative functional MRI
response correlates with decreases in neuronal activity in monkey visual area V1.” Nat Neurosci 9(4): 56977. Shmuel, A., E. Yacoub, et al. (2002). ”Sustained negative BOLD, blood flow and oxygen consumption
response and its coupling to the positive response in the human brain.” Neuron 36(6): 1195-210.
doi:10.3389/conf.neuro.06.2009.03.282

COSYNE 09

209

II-98

II-98. Functional analysis of identified interneurons in the mouse visual cortex
Hatim Zariwala
Julia Berzhanskaya
Theresa Zwingman
Allan Jones
Ed Lein
Hongkui Zeng
Kurt Ahrens

HATIMZ @ ALLENINSTITUTE . ORG
JULIAB @ ALLENINSTITUTE . ORG
THERESAZ @ ALLENINSTITUTE . ORG
ALLANJ @ ALLENINSTITUTE . ORG
EDL @ ALLENINSTITUTE . ORG
HONGKUIZ @ ALLENINSTITUTE . ORG
KFAHRENS @ GMAIL . COM

Allen Institute for Brain Science
GABAergic interneurons are integral to cortical circuit structure and function but the breadth of functions
subserved by each of subtypes of this highly diverse cortical neuron class is less understood in an in vivo
preparation. Conventional extracellular electrophysiology falls short in distinguishing interneuron subtypes.
With a custom built two photon scanning microscope and in vivo calcium indicator dye imaging we are
studying tuning properties of genetically identified subtypes of cortical interneuron in anesthetized mouse
V1 to moving grating stimuli. Genetically identified neuronal types in cortex can be reliably labeled with
fluorescent reporters which persists throughout the development of the organism. Therefore, our goal is to
study a few selected interneuron subtypes at time points around developmentally important critical period
for ocular dominance plasticity in V1. In our preparation, mice are anesthetized and ventilated with 0.7%
isoflurane in a 20:80, O2:N2 mixture. We bulk-load calcium indicator dye, Oregon Green BAPTA-1 (OGB),
using patch pipettes into layer 2/3 of the V1. Anesthetized mice were presented with moving gratings visual
stimuli at 2Hz temporal frequency, 0.02 cycles per degree spatial frequency, six orientations and twelve
directions. We were able to distinguish GFP (or red tdTomato) labeled cell from the OGB signal by using two
different 2-photon excitation wavelengths i.e., 800nm and 950nm respectively (Sohya 2007). Preliminary
analysis of somatostatin positive interneurons in 28-days (P28) old GIN (GFP labeled interneuron) mice
suggest that these neurons (n = 6 cells) are sparse in layer 2/3 and show only very modest calcium dye
response amplitudes compared to the other nearby cortical neurons. The weakly evoked responses were
non-selective to orientation and directions of gratings compared to 10% of nearby neurons (n = 54 cells)
showing responses selectively tuned to an orientation and/or direction. The lack of response in these
cells could be attributed to non-optimal visual stimulation. To overcome this we used a contrast invariant
Gaussian noise movie (Niell 2008) which in principle drives most visually tuned V1 neurons (though not
necessarily optimally). The noise stimuli was successful in driving a large number of neurons but the
calcium responses from GIN neurons were at the best weak and sparse. These results are in contrast
with recent reports of synaptically driven spiking and calcium transients of Martinotti cells (GIN cells) in
response to activity in neighboring pyramidal neurons in cortical slices (Kaiser 2004, Fanselow 2008) .
We are duplicating our study in P21 (+2) and P56 GIN mice in order to identify developmentally relevant
changes. We are also performing a comparative study with parvalbumin and GAD-positive interneurons.
Sohya K, Kameyama K, Yanagawa Y, Obata K, Tsumoto T., J Neurosci. 2007 Feb 21; 27 (8): 2145-9 Niell
CM, Stryker MP., J Neurosci. 2008 Jul 23;28(30):7520-36 Fanselow EE, Richardson KA, Connors BW., J
Neurophysiol. 2008 Nov;100(5):2640-52 Kaiser KM, Lübke J, Zilberter Y, Sakmann B., J Neurosci. 2004
Feb 11;24(6):1319-29. Support: Allen Institute founders, Paul G. Allen and Jody Patton
doi:10.3389/conf.neuro.06.2009.03.312

210

COSYNE 09

II-99

II-99. The increasing importance of secondary stimulus dimensions
to V1 firing with kurtosis
Ryan Rowekamp1,2
Tatyana Sharpee2
1
2

RROWEKAMP @ SALK . EDU
SHARPEE @ SALK . EDU

University of California - San Diego
The Salk Institute for Biological Studies

Neural adaptation to stimulus mean and variance is observed almost universally across species and sensory modalities. There is much less consensus with respect to adaptive changes triggered by higher-order
statistical parameters, such as kurtosis. For example, measurements indicate that subcortical visual neurons do not adapt to statistical parameters beyond the mean and the variance [1], whereas auditory cortical
neurons adapt [2]. These differences could represent either differences between sensory modalities or
levels of neural circuitry. To study this question, we examined neural coding in the primary visual cortex
(V1) as a function of the kurtosis of the stimulus distribution. We analyzed responses of 40 simple and 20
complex V1 neurons to natural stimuli, which are known to have strong higher-than-second order correlations that cannot be described by a Gaussian distribution. For each neuron, we have determined the two
stimulus dimensions that together accounted for the largest amount of mutual information in its responses,
and analyzed how the firing rate changed as a function of stimulus components along these two dimensions. In a natural stimulus ensemble, different stimulus dimensions are described by substantially different
kurtosis values (ranging from 1 - 6). Because relevant stimulus dimensions are different for each neuron,
some neurons are driven by stimuli with larger kurtosis values than other neurons. We found that there
was a significant correlation (p<0.0001, t-test) between the contribution of the second relevant stimulus dimension to neural firing and how non-Gaussian the stimulus distribution was along those dimensions. The
effect was observed both for simple and complex cells. Finally, we also found that the contribution of the
second dimension on the spike probability was stronger when the same neurons were probed with natural
stimuli compared to Gaussian noise stimuli. Both of these results agree with previous work indicating that
optimal encoding of Gaussian and non-Gaussian stimuli should be different. In the framework of neural
decision boundaries [3], which separate stimuli that elicit spikes in a given neuron from stimuli that do not
elicit spikes, the optimal boundaries are planar in the case of Gaussian distributions. This means that neural responses should depend only on the value along one stimulus dimension. In contrast, in the case of
exponentially distributed stimuli, which have statistics closer to natural stimuli, neural decision boundaries
were found to be curved. Therefore, our findings about the increasing importance of secondary stimulus
dimensions for firing in V1 neurons with increasing magnitude of kurtosis in the relevant subspace suggest
that higher-order statistics do affect response properties of visual cortical neurons. Taken together with
previous studies demonstrating that cortical auditory neurons adapt to kurtosis [2] whereas visual thalamic
neurons do not [1], our results suggest that higher-order statistics have a greater effect on the responses
of cortical neurons than subcortical neurons across sensory modalities. [1] V. Bonin, V. Mante, and M.
Carandini, J Neurosci. 2006 26, 6346 (2006). [2] M. N. Kvale and C. E. Schreiner, J Neurophysiol 91, 604
(2004). [3] T. O. Sharpee and W. Bialek, PLOS One (2007).
doi:10.3389/conf.neuro.06.2009.03.288

COSYNE 09

211

II-100 – III-1

II-100. Contextual interactions in natural image contours and their
possible neural implementation
Chaithanya Ramachandra
Bardia Behabadi
Rishabh Jain
Bartlett Mel

CRAMACHA @ USC. EDU
BEHABADI @ USC. EDU
RISHABH @ USC. EDU
MEL @ USC. EDU

University of Southern California
Contour integration is a critical function of the primary visual cortex. With the goal to understand how the
neurons, dendrites and synapses of primary visual cortex work together to solve the difficult problem of
contour extraction in natural images, in this work we look for neural mechanisms for classical and extraclassical interactions using human labeled natural images supplemented with a detailed biophysical model.
We defined a ”classical” Gabor-like oriented edge filter CRF(x), and two ”contextual” filters sensitive to
different aspects of long-range contour structure: M1(x) responded to aligned edge ”flankers” just outside
the classical receptive field (CRF), and M2(x) consisted of an oriented filter superimposed with the CRF
but at a coarser scale. Using human-labeled van Hateren images, we computed the contour probability
CP = Prob(contour — CRF(x), Mi(x)) separately for both contextual modulators. We found that both M1
and M2 did in fact boost the gain of the CP function in response to increasing CRF input, providing direct
support from natural contour statistics for a multiplicative CRF-extraclassical interaction. We compared the
measured psychophysical functions to the nonlinear interactions we observed between synaptic inputs delivered to proximal vs. distal basal dendrites in a detailed compartmental model of a neocortical pyramidal
neuron using the NEURON simulation environment. We found good matches between the two sets of functions, suggesting that nonlinear synaptic integration within basal dendrites of pyramidal neurons could help
mediate contextual interactions in neocortex.
doi:10.3389/conf.neuro.06.2009.03.301

III-1. Fly VS neurons compared to optimized motion detectors
Benjamin Torben-Nielsen
Robert Sinclair
Klaus M. Stiefel

BTORBENNIELSEN @ GMAIL . COM
SINCLAIR @ OIST. JP
STIEFEL @ OIST. JP

Okinawa Institute of Science and Technology
A function of dendrites is synaptic-integration. The specific electrophysiological dynamics involved in this
process have been a subject of neurobiological research for many decades. However, some questions
remain unresolved, such as what determines the morphology of dendrites: the need to perform signal
transformations or to connect to incoming axons in an efficient manner, or both? We employ a recently
developed inverse approach to study the dendritic morphology-function relationship [2]. Briefly, this approach consist of (1) defining a computational function to be performed by a model neuron, (2) optimizing
a model neuron endowed with a dendritic morphology to perform the targeted function, and (3) analyze the
emerged contingencies to quantify the morphology-function relationship. By specifying a desired function
we evidently focus on the functional characteristics of dendrites. We investigated the hypothesized function
of the VS cell from the fly lobular plate which is spatio-temporal integration of small-field direction sensitive
inputs. This integration step is also referred to as wide-field motion detection, i.e., detection of motion of
over the complete visual field of the fly. We constructed a model in which the small-field motion detection is
performed by Reichardt detectors [1] and their output was projected in a realistic way onto the model neu-

212

COSYNE 09

III-2
ron that has to perform the spatio-temporal integration. The output signal of the integration cell is a smooth
signal indicating the direction of the movement. The morphology and the distribution of potassium and
sodium conductances is optimized by using the inverse approach. We ran three optimization runs for three
different velocities of movement. All neurons could perform the spatio-temporal integration well. Moreover,
we found that some (5 out of 9) from a qualitative point of view (visually) resembled the VS cells. Moreover, we found similar distribution of ion-channels in all model neurons. Thus, the dendritic morphology is
optimized for performing a computational-electrophysiological function instead of only collecting inputs. It
can be argued that due to the biologically realistic constraints imposed by the projection of the small-field
sensitive inputs onto the model neuron, the neuron is actually forced to take a shape that corresponds
to the shape of the true cells. However, since some model neurons had an entirely different morphology
than observed in nature, but still performed the function well, we can rule this option out. Hence, we can
conclude that dendritic morphology is not only optimized for wiring, but also to perform specific function
in some neuronal substrate. References [1] Juergen Haag, Arthur Vermeulen, and Alexander Borst. The
intrinsic elec- trophysiological characteristics of &#64258;y lobula plate tangential cells: III Visual response
properties. Journal of Computational Neuroscience, 7:213–234, 1999. [2] Klaus M. Stiefel and Terrence J.
Sejnowski. Mapping dendritic function onto neuronal morphology. Journal of Neurphysiology, 98:513–526,
2007.
doi:10.3389/conf.neuro.06.2009.03.007

III-2. Control of output gain in CA1 pyramidal cells using somatic
shunting inhibition
Fernando Fernandez
John White

FERNRF @ GMAIL . COM
JOHN . WHITE @ UTAH . EDU

University of Utah
Introduction Gain modulation refers to a change in the scaling between the input and output of a system.
In neurobiology, it has been shown that gain control is a fundamental computation performed by numerous
brain regions (1, 2). At the cellular level, gain control can manifests itself as a change in the slope of the
spike frequency-current (F-I) relationship. Results from cortical pyramidal and cerebellar granule cells have
shown that, unlike the subthreshold current-voltage (I-V) relationship, an increase in membrane conductance does not decrease the slope of the F-I relationship (3-7). Several studies, however, have shown that
gain control can be implemented using membrane voltage fluctuations (3, 6). An assumption in investigating the relationship between gain and conductance has been that changes in membrane leak conductance
do not alter the intrinsic firing dynamics of the neuron outside of the expected changes in membrane time
constant and input resistance. Recent data, however, has shown that increased somatic conductance can
profoundly alter intrinsic properties (8, 9). In the current study, we investigated the ability for different levels
of constant leak conductance applied at the soma of CA1 pyramidal cells to modulate the gain of the F-I
relationship without noise. Methods Patch-clamp recordings from hippocampal CA1 pyramidal cells were
implemented using standard in vitro slice electrophysiology. A leak conductance with a reversal of -65 mV
was introduced at the soma using dynamic clamp. The effects of leak conductance on the spike output gain
of CA1 pyramidal cells were quantified by measuring the steady-state F-I relationship in the presence of different levels of added leak conductance (5, 10 and 15 nS). Results and Conclusions We find that increased
conductance has a strong divisive effect on the steady-state F-I relationship of CA1 pyramidal cells. The
reduction in gain with increased leak conductance is associated with a more depolarized operating voltage
range and increase in spike threshold. The more depolarized operating voltage results in a progressive
increase in the magnitude of spike frequency adaptation through increased Na+ conductance inactivation.
It has been established that spike frequency adaptation has a divisive effect on the F-I relationship (10, 11).

COSYNE 09

213

III-3
Hence, the increase in spike frequency adaptation with leak conductance provides a mechanism for gain
control. 1. Salinas, E. & Sejnowski, T. J. (2001) Neuroscientist 7, 430-40. 2. Salinas, E. & Thier, P. (2000)
Neuron 27, 15-21. 3. Chance, F. S., Abbott, L. F. & Reyes, A. D. (2002) Neuron 35, 773-82. 4. Gabbiani,
F., Midtgaard, J. & Knopfel, T. (1994) J Neurophysiol 72, 999-1009. 5. Holt, G. R. & Koch, C. (1997) Neural
Computation 9, 1001-1013. 6. Mitchell, S. J. & Silver, R. A. (2003) Neuron 38, 433-45. 7. Ulrich, D. (2003)
Eur J Neurosci 18, 2159-65. 8. Fernandez, F. R. & White, J. A. (2008) J Neurosci 28, 3790-803. 9. Prescott,
S. A., Ratte, S., De Koninck, Y. & Sejnowski, T. J. (2006) J Neurosci 26, 9084-97. 10. Benda, J. & Herz, A.
V. (2003) Neural Comput 15, 2523-64. 11. Ermentrout, B. (1998) Neural Comput 10, 1721-9.
doi:10.3389/conf.neuro.06.2009.03.329

III-3. Gene expression analysis and metabolic optimization in cortical
fast-spiking interneurons
Andrea Hasenstaub
Terrence Sejnowski
Ed Callaway

ANDREA @ SALK . EDU
TERRY @ SALK . EDU
CALLAWAY @ SALK . EDU

Crick-Jacobs Center, Salk Institute
The neocortex contains a wealth of neuronal subtypes, distinctive in their locations, anatomy, connectivity,
and physiology. Among these, the parvalbumin-positive fast-spiking interneurons are of particular interest
for their role in regulating or timing neuronal activity in the healthy neocortex, and for the clear association
between their structural and functional abnormalities and neuropsychiatric disorders such as schizophrenia. These neurons are uniquely identifiable by their high activity levels, narrow action potentials, lack of
spike-frequency adaptation, and high expression of the calcium binding protein parvalbumin. As well, a
variety of theoretical, anatomical, and functional studies have suggested that the activity of these neurons
is peculiarly metabolically expensive. Is gene expression in fast-spiking interneurons altered or optimized
to accommodate high metabolic demand? We investigated this question by applying information theory to
the analysis of previously collected microarray data (Sugino and Hempel et al). This technique identifies
the genes that are most informative regarding whether or not a sample was collected from neurons of a
given type. This analysis complements that provided by more common statistical techniques, which either
identify genes that are dramatically over- or under-expressed compared to the entire sample (i.e. t-test)
or identify genes that are differentially expressed across the entire dataset, irrespective of in which cell
type the gene is distinctively expressed (i.e. simple ANOVA). To identify the functions of these maximally
informative genes, we queried gene ontology databases to determine which annotations were overrepresented among our gene set. We then used NIH-developed software (DAVID) to identify functional clusters
among these annotations, and finally classified these clusters according to their roles in neuronal function
(signal transduction, synaptic transmission, and so on). Each step of this analysis (from the identification of informative genes to the classification of overrepresented functions) was checked for significance
using Monte Carlo and/or bootstrap techniques, and our cluster classification algorithm was validated by
an unbiased reviewer. We found that each studied subtype of cortical neuron was characterized by the
significant differential expression of hundreds or thousands of genes. In four of the five types of neuron
studied, the most informative genes were predominantly related to cell structure, synaptic transmission,
and signal transduction, and to a lesser extent to ion channel expression and electrophysiology. However,
among the genes whose expression levels best distinguished fast-spiking cells from their neighbors, genes
related to energy generation, including genes related to lactate-pyruvate conversion, the electron transport
chain, mitochondrial structure, and other aspects of cellular respiration, were dramatically and significantly
overrepresented, as were ion channel genes. Thus, we conclude that neocortical fast-spiking interneurons,
in addition to being electrophysiologically unique, are also distinctive in their expression of genes related to

214

COSYNE 09

III-4
energy generation and metabolism. We propose that these alterations in metabolic gene expression are an
adaptation associated with the unusually high energy consumption required to sustain activity in networks
of fast-spiking interneurons. In addition, we suggest that information theoretic techniques provide a valuable
alternative to standard statistical techniques for the identification of genes of interest in microarray data.
doi:10.3389/conf.neuro.06.2009.03.175

III-4. Detection of extracellular potentials using a mechanical-based
nanosensor
Akram Sadek
Rassul Karabalin
Jiangang Du
Michael Roukes
Christof Koch
Gilles Laurent
Sotiris Masmanidis

SADEK @ CALTECH . EDU
RASSUL @ CALTECH . EDU
JIANGANG . JOHN . DU @ GMAIL . COM
ROUKES @ CALTECH . EDU
KOCH @ KLAB . CALTECH . EDU
LAURENTG @ CALTECH . EDU
SOTIRIS @ CALTECH . EDU

California Institute of Technology
Implantable high density microelectrode arrays hold enormous promise as tools for understanding neuronal
circuits in vivo. At present, a limitation of this technology arises in signal extraction from the recording site to
the macro-scale. This is typically carried out via lithographically defined wiring. In order to minimise tissue
damage, higher density electrode arrays will require narrower wiring with submicron dimensions. However,
the effects of impedance and capacitive coupling prohibit an arbitrary reduction in size. Realising ultra-high
resolution extracellular recordings over large volumes will thus require novel methods of signal extraction.
Here we present a new paradigm that allows parallel signal extraction from potentially hundreds to thousands of electrodes through a single output, by transducing signals in the time-domain. Our approach
relies on the use of nanoscale, piezoelectrically coupled transducers known as D-NEMS (depletion-mode
nanoelectromechanical systems). A key feature of these devices is that they exhibit mechanical resonance
effects; their precise structural dimensions determine their characteristic vibrational frequencies. The piezoelectric effect enables conversion of an electric field into mechanical strain. The devices we constructed
were beam structures of different lengths, analogous to the strings on a guitar. Each beam was coupled to a
different microelectrode, and as they were of varying lengths, they exhibited dissimilar resonant frequencies
when driven in the RF range. Much like the tones of guitar strings can be modulated by stress, a voltage
applied through the coupled microelectrode can be used to shift the vibrational frequency of the piezoelectric beam resonator by altering the tension within it. This presentation discusses our early attempts
to harness this effect to develop a nanoscale, mechanical-based detection scheme for extracellular potentials. In a proof-of-concept experiment, the mechanical displacement of the resonators were measured in
parallel using laser interferometry. All mechanical transduction thus occurred via a single, optical signal
transmission path. As the fundamental frequency of each microelectrode-coupled resonator is set much
further apart than the variation that occurs during signal detection, each of the microelectrodes is addressable. The mechanical response is linear, and thus microelectrode signals can be transduced back into an
amplitude post-extraction. Extracellular action potentials in the thoracic ganglia of the locust were successfully recorded using the frequency modulation scheme, with our devices coupled to multi-channel neural
probes. In addition, multiplexed voltage detection was demonstrated with two recording electrodes coupled
to separate piezoelectric resonators. Finally, the response of the system was systematically characterised,
and its limitations and prospects for improvements are presented.
doi:10.3389/conf.neuro.06.2009.03.304

COSYNE 09

215

III-5 – III-6

III-5. Location of modulatory inputs influences the ”scaling competence” of the NMDA channels.
Monika Jadi
Bartlett Mel

JADI @ USC. EDU
MEL @ USC. EDU

University of Southern California
Recordings from cortical neurons indicate that their responses to ”classical” sensory inputs can be multiplicatively and divisively scaled by various types of modulatory inputs. The neural mechanisms that underlie
response scaling remain poorly understood, however. Using a compartmental model of a neocortical pyramidal cell, we previously showed that subthreshold response scaling based on voltage-dependent NMDA
currents could be both accurate and precise over a substantial range of scaling factors (Jadi & Mel, Society
for Neuroscience Abstracts, 2007, 2008). Here we show that a neuron’s ”scaling competence” depends
heavily on the NMDA/AMPA ratio of its synapses, and that near optimal single trial scaling results can be
attained by time-averaging the responses of a small population of (10) neurons in as little as 100 ms. We
then considered the scaling competence of a pyramidal cell model operating in the more realistic spike rate
regime (rather than subthreshold voltage). We found that response scaling improved dramatically when
the classical inputs were delivered on distal basal dendrites while the modulatory inputs were delivered
on proximal basal dendrites, in contrast to cases with co-localized inputs, or cases where the modulatory
inputs terminated distally. Our findings supports the proposal of Behabadi et al. (Society for Neuroscience
Abstracts, 2007) that multiplicative attentional, contextual and other modulatory inputs may target more
proximal regions of neocortical basal dendrites.
doi:10.3389/conf.neuro.06.2009.03.306

III-6. Localizing the origin of executive control over distributed processing to prefrontal cortex
Matthew Chafee1,2
Shikha Jain1
Rachael Blackman1
1
2

CHAFE 001@ UMN . EDU
JAINX 057@ UMN . EDU
BLACK 343@ UMN . EDU

University of Minnesota
Brain Sciences Center

Human cognition and behavior are characterized by flexibility. A fixed pattern of sensory input can evoke
innumerable actions, depending on which goals or strategies the brain has engaged. The consensus is
that prefrontal cortex is essential to this computational flexibility, but the neural mechanisms responsible
are not fully understood. To address this question, we trained a monkey to flexibly assign visual stimuli
to different spatial categories according to a variable grouping criterion that altered how categories were
defined across trials. We then used dual, depth adjustable 16 microelectrode arrays to simultaneously
record neural activity in parietal (309 neurons) and prefrontal (427 neurons) cortex during task performance.
In this task, we presented a line that divided the display area into two regions. Each region comprised a
spatial category containing a set of positions having the same spatial relationship to the category boundary.
For example, when the category boundary was vertical, it divided the display area into the categories ‘left’
and ‘right’. When the boundary was horizontal, it re-parsed the same set of spatial positions into the
spatial categories ‘above’ and ‘below’. By changing the orientation of the category boundary we required
the brain to remap one set of spatial positions to alternative spatial categories according to a variable
rule, placing spatial cognition under executive control. This enabled us to isolate neural signals coding

216

COSYNE 09

III-7
spatial position and spatial category, as well as examine how these signals were modulated as a function
of the rule mapping positions to categories. The first question we wished to answer was whether some
neurons coded spatial category independently of spatial position. We found that the activity of significantly
more neurons in prefrontal cortex (27% of task-related neurons) than parietal cortex (11% of task-related
neurons) coded the spatial category of the stimulus independently of its spatial position. This suggested
that the neural representation of category was more fully abstracted from stimulus feature information in
prefrontal than in parietal cortex. The second question we wished to answer was where neural activity
coding category was first or most strongly modulated by the rule in effect, in order to localize the origin
of executive control in the network. For that purpose we applied linear discriminant analysis to decode
spatial position, spatial rule, and spatial category (defined by the interaction between rule and position)
from successive 50 ms bins of neural population activity in prefrontal and parietal cortex. We found that
both parietal and prefrontal neurons sustained representations of category that exhibited rule-dependence
(and therefore reflected executive control), but that signals coding category in prefrontal cortex were more
powerfully modulated by the rule at earlier times in the trial. This is consistent with prefrontal cortex being the
origin of executive control over distributed spatial cognitive processing mediated by the parietal-prefrontal
network.
doi:10.3389/conf.neuro.06.2009.03.226

III-7. Dynamic allocation of limited resources in human visual working
memory
Paul Bays
Masud Husain

P. BAYS @ ION . UCL . AC. UK
MHUSAIN @ ION . UCL . AC. UK

University College London
Our ability to remember what we have seen is remarkably limited, even across a brief interruption such as
a blink or an eye movement. A longstanding and influential model of visual working memory proposes a
fixed number of discrete memory slots, each storing one visual item [1, 2]. This model was conceived to
account for an apparent discontinuity in performance on memory tasks: for small numbers of items, change
detection is close to perfect, but accuracy rapidly declines once the number of items to be remembered
exceeds about four. However there has been little investigation of the resolution with which this visual
information is maintained. Using a discrimination task, we probed the precision of subjects’ memory for
the location and orientation of multiple visual items.We find that the resolution with which items are held
in memory is highly sensitive to the number of items stored, even when this number is well below the
proposed limit. Rather than being stored in separate slots, these results suggest that discrete visual items
share a common memory resource which must be distributed between them. The proportion of resources
allocated to each item determines the precision with which it is remembered, a relationship that we show
is governed by a power law. We demonstrate that this simple model of working memory also predicts
the apparent discontinuity in change detection performance that provided the original incentive for dividing
memory into separate slots. Crucially, this limited resource can be distributed flexibly between objects:
when we instructed subjects to make an eye movement to an item, or covertly drew their attention to it,
overall performance was unaffected. However, the target item was retained with far greater precision than
other objects in the scene, consistent with a dynamic redistribution of memory resources towards the salient
item. When subjects made a sequence of eye movements, memory resources were reallocated with each
saccade, with the result that, at the time of an eye movement, the majority of resources were allocated to the
target of the next fixation. References [1] Luck, S. J., & Vogel, E. K. (1997). The capacity of visual working
memory for features and conjunctions. Nature, 390(6657), 279. [2] Cowan, N. (2001). The magical number
4 in short-term memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences,

COSYNE 09

217

III-8
24(01), 87-114.
doi:10.3389/conf.neuro.06.2009.03.054

III-8. Qualitative differences between decision-making for strongly and
weakly attended stimuli
Dobromir Rahnev
Brian Maniscalco
Elliott Huang
Hakwan Lau

DAR 2131@ COLUMBIA . EDU
BRIAN @ PSYCH . COLUMBIA . EDU
HUANG . ELLIOTT @ GMAIL . COM
HAKWAN @ GMAIL . COM

Columbia University
Introduction Inattentional and change blindness are characterized by an inability to detect unattended
events. Interestingly, subjects consistently overestimate their ability to detect such events and are surprised when informed that an undetected event has occurred. We hypothesized that this effect is due to the
subjective visibility of unattended stimuli being disproportionately higher than what would be warranted by
their objective information processing. We tested this hypothesis using the formal tools of Signal Detection
Theory. Behavioral Methods and Results Subjects detected strongly and weakly attended gratings whose
contrasts were adjusted online to produce the same discriminability (d’). We found that subjects were conservative in detecting the strongly attended gratings and liberal in detecting the weakly attended gratings.
In order to ensure that this effect was not due to the simple fact that stimulus contrast was higher for the
weakly attended gratings, in a follow-up study we used a number of fixed contrasts for both the strongly
and weakly attended stimuli. We found that the influences of contrast and discriminability on subjects’ bias
were significantly different for strongly and weakly attended stimuli. Further analyses showed that in the
weakly attended condition, subjects were close to optimal (i.e. unbiased) for all contrasts used, while in the
strongly attended condition, subjects became very conservative for lower contrasts. Thus, surprisingly, subjects were less optimal in detecting the more strongly attended stimuli. We confirmed that this was still the
case even when subjects were encouraged to be unbiased by explicitly specifying pay-offs, were informed
about the probabilities of occurrence of the gratings, and were given feedback after each trial. Finally, when
d’ was matched in a discrimination experiment, participants gave higher subjective ratings of visibility for
the weakly attended gratings, confirming that our previous results were due to differences in the subjective
visibility of the stimuli rather than simple detection biases. Modeling We modeled the data using Signal
Detection Theory to test whether the strongly and weakly attended stimuli were handled by the same decision making system. We analytically demonstrated that such a possibility would require that the variance of
the noise signal should be greater for the weakly attended stimuli, while the variance of the stimulus signal
should be greater for the strongly attended stimulus. Such a combination, however, is highly unlikely and
is at odds with previous single-neuron recordings. Therefore, it appears that strongly and weakly attended
stimuli are explicitly differentiated in the brain and decisions made about them are executed by different systems. Discussion We demonstrated that the subjective visibility associated with weakly attended signals is
much higher than what would be warranted by the quality of these signals. This effect can explain subjects’
surprise at their bad performance in inattentional and change blindness experiments. It can also be useful
in everyday life. Indeed, it is advantageous to detect non-attended stimuli liberally since one can always
switch attention to the stimulus but liberal detection of attended stimuli would lead to frequent hallucinations
which could be costly.
doi:10.3389/conf.neuro.06.2009.03.083

218

COSYNE 09

III-9 – III-10

III-9. Anterior cingulate cortex encodes action-outcome associations
in working memory
Chung-Hay Luk1,2
Jonathan Wallis
1
2

CH LUK @ BERKELEY. EDU
WALLIS @ BERKELEY. EDU

University of California, Berkeley
Helen Wills Neuroscience Institute

In a dynamic environment an action that satisfies a particular goal can often change. Hence, to select
the most appropriate action, it becomes necessary to actively update remembered contingencies between
actions and outcomes (AO associations). Two regions implicated in action selection are lateral prefrontal
cortex (LPFC) and anterior cingulate cortex (ACC). While both regions connect to motor areas, only ACC
receives strong inputs from areas processing reward, placing it in a better anatomical position to control
outcome-guided action. To examine this, we trained a monkey to perform a task that required him to monitor
AO contingencies on a trial-by-trial basis. The monkey performed two sequential movements, separated
by a delay, each of which was rewarded with a specific juice (apple, orange or quinine) and then had to
repeat the movement that was previously paired with his preferred juice. Thus, during the first delay, the
monkey had to remember both the movement he made as well as the juice he received. We recorded the
activity of 77 LPFC neurons and 84 ACC neurons during the performance of the task. In ACC, 24% of the
neurons encoded which movement the monkey had made and 33% of the neurons encoded what juice the
monkey had received. In contrast, in LPFC 40% of the neurons encoded the movement and 13% encoded
the juice. These findings support the hypothesis that ACC rather than LPFC is important for the control of
outcome-guided action.
doi:10.3389/conf.neuro.06.2009.03.013

III-10. Role of Medial Prefrontal Cortex and Secondary Motor Cortex
in Withholding Impulsive Action
Masayoshi Murakami1,2
Zachary Mainen1,2
1
2

MASAYOSHI . CSHL @ GMAIL . COM
ZACH . IGC @ GMAIL . COM

Champalimaud Neuroscience Programme
Instituto Gulbenkian de Ciencia

Withholding impulsive responding for short term gains to achieve long-term goals is an important aspect of
goal-directed behavior. Although it has been extensively studied how the brain evaluates rewards available
at different time points, few studies have investigated how brain withholds response for short term gains to
achieve more valuable goals. In this study, we investigate the role of medial prefrontal cortex (mPFC) and
secondary motor cortex (M2) in withholding impulsive responding in an inter-temporal choice task. In our
task, there are two nose poke ports: one is for waiting, and the other is for delivering water reward. If a rat
succeeds in keeping its nose inside the waiting port for 400 ms, a tone is played. If the rat goes to the reward
port after this tone, it receives a small amount of water reward. If the rat waits longer in the waiting port,
a second tone is played after a random delay period (exponential distribution with 700 ms at the earliest
and around 2000 ms mean). If the rat goes to the reward port after the second tone, it receives a large
reward (3 to 4 times the small reward). Rats learn to withhold responding for a small reward to obtain a
large reward. We found that the time a subject is willing to wait for a given set of reward amounts and delays
varies randomly from trial to trial. Thus, in combination with electrophysiological recording, we can compare
the activity of neurons in trials with different waiting times. We hypothesized that neuronal activity in those

COSYNE 09

219

III-11
brain areas responsible for suppressing impulsive behavior may correlate with how long a rat will withhold
responding. We made recordings from mPFC and M2 neurons using implanted movable tetrodes during this
task. We found that activity of subset of neurons in mPFC and M2 can indeed predict the length of time the
subject will withhold a response; that is, the firing rate of these neurons during a time window before the end
of waiting was significantly correlated with waiting time of the rat. Individual neurons showed both positive
and negative correlations with waiting time. Predictive activity occurred at different time points during the
waiting period and even before waiting period. Among 121 neurons recorded from M2, 21 neurons (17%)
are predictive of waiting time. In mPFC, only 6% (7 out of 117) of neurons showed predictive activity, a
significantly smaller fraction (P < 0.01, &#967;2 test). These results suggest the involvement of M2 in
withholding impulsive choice in an inter-temporal choice task and may help to constrain circuit models of
action withholding. In the future, we plan to examine whether the same subset of neurons are recruited for
different types of behavior involving response withholding in order to define whether neurons in these areas
subserve a more abstract function of response withholding. Acknowledgments We thank members of the
Mainen laboratory for helpful discussions. This work was supported by funding from the Uehara Memorial
Foundation (M.M.) and the Champalimaud Foundation (Z.F.M.).
doi:10.3389/conf.neuro.06.2009.03.046

III-11. A spiking neural circuit model for conflict resolution between
reflexive and voluntary responses
Chung-Chuan Lo1,2
Xiao-Jing Wang
1
2

CCLO @ MX . NTHU. EDU. TW
XJWANG @ YALE . EDU

National Tsing Hua University
Yale University

The ability to suppress reflexive responses in order to execute voluntary actions is crucial for controlled
behavior in everyday life. This ability has been studied using an anti-saccade task, in which a subject is
required to make a saccadic eye movement away from a peripheral target instead of looking toward it as in
a pro-saccade task. Recent electrophysiological studies in non-human primates have started to associate
neural processes in several brain areas with the generation of anti-saccades at the level of single neurons.
To elucidate the underlying circuit mechanisms, we propose a large-scale spiking neural network model
based on neurophysiological observations from frontal eye field, supplementary eye field, dorsal lateral prefrontal cortex and superior colliculus. The model proposes that an anti-saccadic eye movement involves
two pathways: a fast pathway that carries out the reflexive response (driven by the visual stimulus) and a
slower pathway that performs the voluntary action (generated in a decision making circuit that inverts the
sensory-response map and is endowed with attractor dynamics). The fast pathway has to be suppressed
by a transient top-down control in order for the voluntary response from the slow pathway to drive an antisaccade. A fast error occurs if the transient top-down control fails to suppress the fast pathway while a slow
error is produced if the decision-making circuit in the slow pathway fails to invert the sensory-response map.
This model accounts for neural activity observed in several brain areas, and provides a neural explanation
for the broad reaction time distribution of erroneous saccades observed in various studies of anti-saccade
task. The proposed mechanism may have implications for understanding impaired executive control associated with psychiatric disorders.
doi:10.3389/conf.neuro.06.2009.03.063

220

COSYNE 09

III-12 – III-13

III-12. Chronic stress affects decision-making strategies: structural
and physiological correlates
Eduardo Dias-Ferreira1,2,3
Irene Melo2
Xin Jin1
João C. Sousa2
João J. Cerqueira2
Nuno Sousa2
Rui Costa1

FERREIRAED @ MAIL . NIH . GOV
IRENE . MELO @ GMAIL . COM
JINX @ MAIL . NIH . GOV
JCSOUSA @ ECSAUDE . UMINHO. PT
JCERQUEIRA @ ECSAUDE . UMINHO. PT
NJCSOUSA @ ECSAUDE . UMINHO. PT
COSTARUI @ MAIL . NIH . GOV

1

NIAAA, National Institutes of Health
University of Minho
3
University of Coimbra
2

The stress response is vital to maintain homeostasis. However, chronic exposure to stress can trigger maladaptive response and predispose to conditions ranging from neuropsychiatric disorders to everyday lapses
of attention. Even though previous reports have implicated chronic stress in executive function impairment,
a conceivable role in decision-making processes remains to be clarified. Competing corticostriatal circuits
are thought to control heterogeneous decision strategies: while the prelimbic (PL) cortex and the dorsomedial striatum (DMS, or associative striatum) have been implicated in goal-directed actions, the dorsolateral
striatum (DLS, or sensorimotor striatum) has been implicated in automatic or habitual choices. Here we
show that chronic stress impairs the decision-making process, predisposing to habitual behavior in detriment of goal-directed strategies. Using two different criteria to test for action-outcome behavior in a lever
pressing task, we found that responses from rats and mice submitted to chronic stress became insensitive to
both outcome devaluation and contingency degradation. Furthermore, we found that chronic stress causes
opposing structural changes in associative and sensorimotor corticostriatal circuits. Whereas chronic exposure to stress resulted in selective atrophy of pyramidal neurons in layer II/III of the PL and infralimbic
(IL) sub-regions of medial prefrontal cortex (mPFC) and in medium spiny neurons (MSNs) of the DMS, it
triggered an opposite effect in MSNs of the DLS. To determine if this structural reorganization of frontostriatal circuits has functional consequences, we recorded the simultaneous activity of neuronal ensembles in
mPFC, DMS and DLS of control and stressed mice during behavioral training and testing. This approach
will allow us to investigate if the changes in wiring observed in the associative and sensorimotor circuits
underlie changes in neural activity in these circuits that could explain the bias from goal-directed towards
habitual behavior observed in stressed subjects.
doi:10.3389/conf.neuro.06.2009.03.348

III-13. Dissociations in ensemble dynamics between rat dorsal striatum, ventral striatum, and hippocampus
Matthijs van der Meer1
Adam Johnson2
Neil Schmitzer-Torbert1,3
A. David Redish1

MVDM @ UMN . EDU
ADAM - JOHNSON @ BETHEL . EDU
TORBERTN @ WABASH . EDU
REDISH @ UMN . EDU

1

University of Minnesota
Bethel University
3
Wabash College
2

COSYNE 09

221

III-14
Across a variety of domains, such as learning and memory, spatial navigation, and decision-making, it has
been suggested that multiple brain systems, with different properties, can support adaptive behavior. There
is much evidence for a distinction between (1) a flexible “planning” system that takes the outcomes of actions into account and is engaged during novel or changing situations, and (2) an incrementally learning,
rigid “habit” system that efficiently supports behavior in familiar circumstances. A popular hypothesis is that
the habit system implements algorithms similar to those used in reinforcement learning and includes the
lateral (“actor”) and ventral striatum (“critic”), while a number of other structures, including both hippocampus and ventral striatum, are thought to contribute to planning. Understanding how neural representations
within these areas support such functions is critical for theories that will ultimately lead to targeted treatment of dysfunctional decision-making, such as occurs in addiction. However, comparisons of neural firing
correlates between these structures have been few, partial, and indirect, and have only involved single cell
analyses that neglect dynamics accessible through ensemble decoding. To address this, we recorded neural ensembles from hippocampus, dorsal striatum, and ventral striatum as rats ran laps on the Multiple-T
maze, where choosing the correct side at the last of four T’s lead to food reward (Figure S1). The spatial distribution of firing rates on the maze was different between structures: for projection neurons, hippocampal
firing was relatively uniform; dorsal striatum was particularly active on the T-sequence, and ventral striatum
showed a ramping up reward sites. Spatial ensemble decoding analyses revealed that hippocampal decoding accuracy was similarly uniform, dorsal striatum was best on the T sequence, and ventral striatum was
best just before the reward locations, although overall it was much poorer than both other structures. Dorsal
striatal decoding showed a gradual increase in decoding accuracy within each session, while the accuracy
of other two structures remained constant. This pattern of results was consistent with spatial ensemble
correlation analyses, which showed a development in dorsal striatum but not in the other structures. In contrast, examining representations as rats paused at the final choice point, dorsal striatum was found to be
mostly local, while hippocampal representations swept ahead of the animal, and ventral striatum showed increased representation of reward. These data support the idea of a division of labor between hippocampus,
dorsal striatum and ventral striatum that changes with experience. Hippocampus rapidly established a spatial map of the environment, which can represent distant (non-local) possible paths at decision points early
in the session. Covert ventral striatal representations of reward occurred with a similar spatial and temporal profile, suggesting that such signals contribute to an integrated outcome representation-evaluation
system. In contrast, dorsal striatum showed no evidence of non-local representation. Instead, its spatial
distribution of firing rates and decoding accuracy indicates specialization for action selection, the contents of
which are gradually refined over a session. Ventral striatum might also contribute to such gradually learned
performance by a value-like ramping effect.
doi:10.3389/conf.neuro.06.2009.03.353

III-14. Motor planning in the rat superior colliculus
Gidon Felsen1
Zachary Mainen2,3

FELSEN @ CSHL . EDU
ZACH . IGC @ GMAIL . COM

1

Cold Spring Harbor Laboratory
Champalimaud Neuroscience Programme
3
Instituto Gulbenkian de Ciencia
2

The superior colliculus (SC) plays an important role in spatial orienting, across many species. We recently
recorded SC activity in freely-moving rats performing a reaction-time version of a spatial choice task. In
this task, an odor cue delivered at a central port determines whether a water reward will be delivered upon
entry into the left or right reward port. As soon as the rat decides on the identity of the odor, it exits the odor
port, orients left or right, and enters the selected reward port. We found that that the activity of single SC
neurons immediately preceding the exit from the odor port is predictive of the rat’s choice (left vs. right), and

222

COSYNE 09

III-15
that this activity is necessary for normal movement production. We now ask whether this activity is related
to the initiation of the movement or to some aspect of planning the movement (e.g., selecting its direction).
Conceivably, the movement could be planned in a separate region (or regions) upstream, and the SC would
simply be responsible for initiating the planned movement. Alternatively, the SC could be involved in the
planning itself. We addressed this issue in two ways. First, we reasoned that if the SC were involved only in
movement initiation, its activity should be independent of the stimulus. We therefore asked whether some
feature of the stimulus was encoded in the SC activity, in addition to the movement direction. We found that,
preceding movement initiation, a subset of cells encoded the identity of the odor in addition to the upcoming
movement, potentially linking the stimulus and the action. This suggests that the SC is not only responsible
for initiating movement. Next, we recorded SC activity during performance of a delayed-response version
of the spatial choice task. On each trial of this task, the rat was required to remain in the odor port for a
period of 500 or 1000 ms following odor presentation before initiating its movement toward the reward port.
Presumably, the rat still selects the direction of movement within a few hundred ms, as in the reaction-time
task. Thus, this task dissociates the time corresponding to the decision from the time of movement initiation.
We found that the activity of many choice-predictive cells peaked soon after odor presentation began and
remained elevated until the rat exited the odor port. The timing of this activity is more consistent with a role
for the SC in selecting or planning movements, rather than simply initiating them. Thus, the SC may be a
critical component of the circuit for motor planning.
doi:10.3389/conf.neuro.06.2009.03.176

III-15. Decision making without bounds? Evidence from humans and
monkeys
Juan Gao
James McClelland
Alan Rorie
Rebecca Tortell
William Newsome

JUANGAO @ STANFORD. EDU
MCCLELLAND @ STANFORD. EDU
RORIE @ STANFORD. EDU
TORTELL @ STANFORD. EDU
BNEWSOME @ STANFORD. EDU

Stanford University
Decision making in perceptual classification tasks is generally assumed to depend on accumulation of noisy
information. In the reaction time paradigm, behavioral and neurophysiological data support the notion that
responses are initiated when a threshold or boundary is reached. However, the termination rule is less
conclusive in time-controlled paradigms, where the experimenter controls the timing of the response signal.
Two approaches have been used in such cases: 1) At the response cue, a decision is made by comparing
the levels of evidence favoring the two choices (Bogacz et. al. 2006, Ratcliff 1978); 2) Alternatively, evidence
accumulation may continue only until a bound is reached, and this may occur before response cue onset. If
the bound is reached, neural activity and behavioral responses reflect the state of a binary decision variable
(Kiani et. al. 2008). We looked at two relevant experiments, one with human participants and the other with
monkeys. The human participants saw a rectangle offset to the left or right of fixation and were required
to indicate the direction of shift immediately after a response signal that could occur at a delay of 0 to
2000 msec relative to stimulus onset. In the monkey experiment, the stimulus in each trial was coherently
moving dots lasting 500 msec. Monkeys were trained to discriminate the motion direction and indicate their
decisions by an eye movement after a signal occurring 300-550 msec after the motion termination. In both
experiments, a reward cue was presented prior to stimulus onset indicating which response would, if correct,
receive a larger reward. For each stimulus condition, we sorted trials into two groups according to the
behavioral response latency following the response cue (shorter or longer than the median), and analyzed
choices separately for each group. Surprisingly, we found that choices differed systematically between these

COSYNE 09

223

III-16
groups, even for long trial durations in which any hypothesized bound should already have been reached.
Human choices were simply less accurate for trials with longer response latencies (5 subjects), whereas
monkeys were more biased toward the larger reward option at longer latencies (2 subjects). These effects
were observed for trial durations of up to 2.7 seconds in humans, and up to 1.3 seconds in monkeys. These
data indicate that decisions in these tasks are not determined solely by a decision variable that crosses
a hard bound before receipt of the response cue. Our observations can, however, be accommodated
within the framework of the leaky competing accumulator model (Usher & McClelland, 2001) in which the
representation of the evidence for each of the alternatives remains unbounded and continuous even after
the response signal, with the continuous values influencing both the outcome and the time taken to respond.
doi:10.3389/conf.neuro.06.2009.03.189

III-16. Abstract rule representations in a bilinear model.
Kai Krueger
Peter Dayan

KRUEGER @ GATSBY. UCL . AC. UK
DAYAN @ GATSBY. UCL . AC. UK

Gatsby Computational Neuroscience Unit, UCL
A key aspect of cognitive flexibility is abstraction, i.e., the ability to represent rules governing appropriate
behavior separately from the identities of the particular objects on which those rules presently act. This
permits one set of rules to be generalized to different circumstances and inputs. A simple example is the
delayed matching task of Miller et al. (1991). In this case, the rule requires responding to the second
appearance of the object that is presented at the outset of a trial, no matter what that object is. There is
evidence that rules are represented in the activity of prefrontal cortical neurons - for instance Shima et al.
(2007) reported that many such cells are selective for abstract sequence categories (such as ABAB, AAAA),
generalizing over particular actions A and B such as ‘push’ or ‘pull’. From an algorithmic perspective,
the objects are variables in the rules, and generalization is a form of variable substitution. Despite the
critical importance of this form of flexibility, and indeed some connectionist approaches (Touretzky 1990;
Shastri 1999), the more neurally-inspired models of prefrontal cortex and associated areas lack any way
of representing this abstraction. Instead, they combine rule structure and stimulus identity inseparably in
the weights of their networks. Here, we extend one such model, the bilinear framework of Dayan (2007),
to support variables. We demonstrate our new model on an abstracted version of the 12-AX task (Frank
et al., 1991), which has become a prime test-case for rule-based models. The original version of this
task requires a subject to respond to one of a set of target sequences (letters AX and BY in the original)
depending on the context, which itself is indicated by elements of the sequence (digits 1 and 2). We
consider an extended version in which the assignment of input stimuli to components of the task is flexible,
thus permitting variations such as A2-X1 or DC-21. The new model specifies a set of gated working memory
and action modules controlled by learned, bilinear, forms. The bilinearity offers an ascetic form of hidden
unit (Rigotti et al., 2007) or basis function (Poggio, 1990) representation of nonlinear contingencies. It
permits the key operation required for variable substitution, namely assessing the match between one
quantity stored in a memory module and another quantity presented in the inputs. Given this, switching
the stimulus identities becomes as easy as updating the appropriate memory cells. Using matching as a
primitive, we provide an algorithm for solving the full task that is couched as a set of rules, only one of which
is active at any time. Each such rule is represented as a set of bilinear weights, one per action, acting on
the joint vector of inputs and the current state of all memory modules. Rules are triggered by changes in
working memory or inputs. We show how to capture these rules in the overall bilinear structure.
doi:10.3389/conf.neuro.06.2009.03.180

224

COSYNE 09

III-17

III-17. Causal model attribution in sequential decision making
Paul Schrater
C. Shawn Green
Charlie Benson
Daniel Kersten

SCHRATER @ UMN . EDU
CSGREEN @ UMN . EDU
BENSO 595@ UMN . EDU
KERSTEN @ UMN . EDU

University of Minnesota
Human behavior in binary choice tasks is notoriously suboptimal. Given repeated choices between two
options, one with a higher probability of being the correct option than the other, the obvious optimal solution
is to choose only the higher probability option. Interestingly, this optimal strategy is rarely observed. The
more typical finding is that subjects sample the options in proportion to their respective probabilities of being
correct - a tendency known as probability matching. In contrast to model-free reinforcement-learned based
models of decision making that learn to choose better options by collecting outcome statistics, we propose
that humans engage in causal model inference in order to reduce unexplained variability in outcome. Only
when participants can infer a causal model that can account for unexplained outcome variability will choice
behavior approach optimal, otherwise humans will continue to explore, reducing the probability of selecting
the better option. To test this hypothesis, choice behavior was compared in several conditions in which the
outcome statistics were held fixed, but different cues were provided that changed causal model attribution.
This was accomplished by manipulating certain environmental characteristics that subjectively appeared to
more or less account for the outcome variability. Because we ensured that the mapping between choices
and observed outcomes was statistically equivalent, any differences in behavior cannot be attributed to
actual differences in outcome statistics. The basic task was a “jet-fighter” game, in which subjects flew a
craft down a tunnel past pairs of targets. For each pair, the subject was instructed to chose one, align their
ship with their chosen target, and fire a bullet at the target once in range. Successes were indicated by the
target exploding when impacted by a bullet. Causal attribution was manipulated by altering characteristics
of these targets. In Condition 1, cues were provided to attribute target explosion to target characteristics
rather than motor error ( subjects believed the target stochastically exploded after being hit). In accord with
equivalent choice task results, probability matching behavior was observed. In Condition 2, subjects were
led to attribute target explosion to their own motor skills (pilot/shot error). In this case subject behavior
was a mixture of random choices (not surprisingly, as motor skill should not vary as a function of choice)
and probability matching (in those subjects who realized that motor skill could not fully account for the observed statistics). Finally, in Condition 3, cues were provided to lead participants to attribute explosions to
observable characteristics inherent to the targets (speed and size). In this condition, choice behavior was
near-optimal when the larger, slower moving target had a higher probability of exploding and choice behavior was random when these contingencies were reversed. We interpret these findings as supporting the
idea that human sequential choice behavior may require a model-based learning approach with structure
inference as a key component.
doi:10.3389/conf.neuro.06.2009.03.208

COSYNE 09

225

III-18

III-18. Neurobiological foundations for ”dual system” theory in decision making under uncertainty
Ulrik Beierholm1
Cedric Anen2
Steven Quartz2
Peter Bossaerts2,3

BEIERH @ GATSBY. UCL . AC. UK
CEDRIC @ CALTECH . EDU
STEVE @ HSS . CALTECH . EDU
PBS @ HSS . CALTECH . EDU

1

Gatsby Computational Neuroscience Unit, UCL
California Institute of Technology
3
EPFL
2

It has been suggested in many circumstances that human decision making involves (at least) two systems:
one reflexive and fast, and a second one reflective but slow (for a survey, see Evans). This ”dual system”
conjecture is consistent with many aspects of human behavior, such as the tendency to apply heuristics
when fast answers are needed (Kahneman and Frederic). As far as decision making under uncertainty is
concerned, however, there has so far only been suggestive evidence of a neurobiological foundation to this
dual system theory. In past experiments (e.g., De Martino et al.), the evidence was based on a comparison
of differential brain activations across subjects, leaving open the possibility that a single but subject-specific
neural system generated observed behavior. Here we discuss an experiment that generated dual neural
signals in an intra-subject analysis, consistent with the dual system theory. The experiment was a learning
task in a three-option ranking game; subjects were presented with three doors and informed that money
was hidden behind one of the doors. Subjects’ task was to rank the probability of the money being behind
each door based on previous experience. The setup was designed to generate minimal correlation (0.43)
between valuations from the reflective system (interpreted here, as in Daw et al., as the Bayesian, modelbased learning system) and the reflexive system (simple reinforcement learning based on past actions).
Subject behavior showed a tendency to switch between following the recommendations of the reflective
Bayesian and reflexive RL models. fMRI analysis of brain activation revealed, among others, distinct activations in medial prefrontal cortex correlating with the two values; the usual prediction error activations
in striatal areas correlated only with the prediction error from the reflexive learning system. Adjudication
appeared to happen when uncertainty was highest, consistent with recent theoretical modeling (Daw et al.
2005). Further, we found activity in medial prefrontal cortex that correlated with subjects’ adherence to each
of the two models, i.e. decreases in BOLD shortly before subjects were about to defect from the respective
strategies. The presence of two concurrent signals in PFC, one correlating with the Bayesian value of the
game representing a reflective model, and another correlating with the RL value corresponding to a reflexive model, provides the first unequivocal neurobiological evidence in support of the dual systems approach.
References: Evans. Dual-Processing Accounts of Reasoning, Judgment, and Social Cognition, Ann. Rev.
Psychol, 2008. Kahneman and Frederick, Representativeness Revisited: Attribute Substitution in Intuitive
Judgment, Heuristics and Biases, Cambridge University Press 2002. De Martino, et al, Frames, Biases,
and Rational Decision-Making in the Human Brain, Science, 2007. Daw, Niv and Dayan, UncertaintyBased Competition between Prefrontal and Dorsolateral Striatal Systems for Behavioral Control, Nature
Neuroscience, 2005.
doi:10.3389/conf.neuro.06.2009.03.213

226

COSYNE 09

III-19

III-19. Active learning using uncertainty: behavioral evidence and neural correlates in rats
Adam Kepecs1
Naoshige Uchida2
Zachary Mainen3,4

KEPECS @ CSHL . EDU
UCHIDA @ MCB . HARVARD. EDU
ZACH . IGC @ GMAIL . COM

1

Cold Spring Harbor Laboratory
Harvard University
3
Champalimaud Neuroscience Programme
4
Instituto Gulbenkian de Ciencia
2

Statistical learning theory proposes that “active learners” use their uncertainty estimates to optimally set
their learning rate so as to learn more when uncertain and less when certain. In the machine learning
literature it is well established that active learning based on informative data points speeds up learning.
However, little is known about whether these principles apply to animal learning. Here we test the hypothesis
that rats are active learners and use uncertainty estimates to learn optimally. To study this issue we used
an odor-mixture categorization task performed by rats and examined the trial-by-trial updating of behavioral
strategy. As animals learn to perform a categorization task they use reinforcement feedback to establish the
decision-boundary, yet this boundary may be continually updated during on-going performance after overt
learning asymptoted. Indeed, we found that animals were dynamically adjusting their decision strategy
even after extensive training. For difficult decisions (those near the category boundary) the outcome is
very informative about location of the decision boundary, while the outcome of easy decisions (those far
from the decision boundary) reveals little about the boundary. Accordingly, the decision boundary should
be adjusted more following difficult trials with high uncertainty than for trials with no uncertainty. Indeed,
we found that rats biased their decisions toward the more recently rewarded direction as if their decision
boundary was shifted. This bias, however, was only observed for difficult decisions suggesting that the
category boundary and not the choice-bias was being updated. Moreover, the magnitude of this bias was
proportional to the uncertainty of the previous decision, as predicted. Therefore on-going learning appears
to depend on a graded prediction error signal combining reward feedback and uncertainty estimates. These
behavioral data could be quantitatively explained by an “active” delta learning rule, where reward predictions
are computed based on decision uncertainty. We also show how a trial-by-trial uncertainty estimate can
be naturally computed in this class of models (see also Shenoy et al, COSYNE 2008). To understand the
neural basis of this process we recorded neurons in the orbitofrontal cortex of rats performing the olfactory
categorization task. We found a population of neurons whose firing rate during the reward anticipation
period closely resembled the expected signal of decision uncertainty. About half of these neurons also
carried information about the reward outcome of the previous trial based on a regression analysis. This
type of activity closely matches the learning signal predicted by active learning models. Taken together
the behavioral and computational results show that rats are “active learners”, combining reward feedback
and decision uncertainty estimates to update their decision strategy. Moreover orbitofrontal cortex neurons
carry information that is relevant for such active learning process.
doi:10.3389/conf.neuro.06.2009.03.284

COSYNE 09

227

III-20

III-20. Dynamics of coupled thalamocortical modules
Jonathan Drover
Nicholas Schiff
Jonathan Victor

JOD 2017@ MED. CORNELL . EDU
NDS 2001@ MED. CORNELL . EDU
JDVICTO @ MED. CORNELL . EDU

Weill Medical College of Cornell
The electroencephalogram (EEG) is a readily-obtained, noninvasive, and objective measure of brain activity.
In contrast to functional brain imaging, the EEG can be recorded in almost anyone, including subjects who
are moving, and patients whose medical condition requires close observation and/or support equipment,
and it is possible to obtain prolonged recordings spanning many behavioral states. At present, the utility
of the EEG as a scientific tool has been limited by the indirect nature of the relationship between surface
recordings and the underlying brain activity. To bridge this gap, Robinson and colleagues proposed a model
of EEG dynamics at the level of neuronal populations. The Robinson model consists of a single thalamocortical module with four homogeneous populations (cortical excitatory, cortical inhibitory, thalamic relay
nucleus, and reticular nucleus). Each population has two dynamical variables: an average firing rate and
an average potential. Their evolution is governed by a system of delayed-differential equations (DDE’s).
The Robinson model accounts for the spectral features of the EEG across a range of normal behavioral
states (eyes open, eyes closed, and the four stages of slow-wave sleep). These spectral features can be
recovered from the linearized behavior of the model near its fixed points. However, because of its very
simple spatial structure, the Robinson model cannot account for patterns of spatial interactions across cortical regions or changes in the EEG due to focal brain abnormalities. It also does not account for state
transitions; different states are obtained by changing the effective connectivity strengths “by hand.” To begin
to develop a population model that could account for these phenomena, we investigated the dynamics of a
system consisting of two “Robinson modules”, each with its own cortical and thalamic components. Corresponding to known anatomy, we coupled these modules by adding a third population of reticular neurons
that are shared between the modules. The shared population is reciprocally connected with the relay nuclei
of both modules, and receives input from the cortical components of both modules. We studied the global
dynamics of this system across a two-parameter family of coupling scenarios: one parameter indicates the
strength of the connections with the shared population, and one indicates the strength of the connections
with the specific populations. A complete bifurcation diagram was constructed for the coupling of “Robinson
modules” in the eyes-open state. The bifurcation diagram delineates three regions of qualitatively different
behavior. In the first region, a symmetric fixed point is asymptotically stable. This corresponds to synchronization of the modules. In the second region, a symmetric periodic solution, arising at a Hopf bifurcation, is
asymptotically stable. This also corresponds to synchronization, but the periodic nature of the solution leads
to a sharpened spectral peak. The third region arises via a pitchfork bifurcation, spawning two additional
fixed points while destabilizing the symmetric solution. Qualitatively, this corresponds to winner-take-all
behavior because one of the modules maintains an elevated activity level and the other is suppressed. In
sum, coupling two simple thalamocortical modules together can yield several different qualitative behavioral
regimes.
doi:10.3389/conf.neuro.06.2009.03.001

228

COSYNE 09

III-21 – III-22

III-21. Point Process Model for Precisely Timed Spike Trains
Il Park
Murali Rao
Thomas DeMarse
Jose Principe

MEMMING @ CNEL . UFL . EDU
MRAO @ MATH . UFL . EDU
TDEMARSE @ BME . UFL . EDU
PRINCIPE @ CNEL . UFL . EDU

University of Florida
When the same stimulation is presented to a neuronal system, the train of action potentials that are observed as a result sometimes show a highly repeatable spatio-temporal pattern at the millisecond time
scale. Recently these precisely timed spike trains (PTST) are reported both in in vivo and in vitro preparations. It has been speculated that this spatio-temporal pattern can support temporal coding. Despite being
highly reproducible, different forms of trial-to-trial variability have also been observed. It is crucial to understand this variability since to utilize a precisely timed spike pattern as a temporal code, the system should
presumably be robust to its variability structure, and possibly learn to reduce it. The variability also restricts
the amount of information that can be encoded in such a manner by setting the resolution for distinguishable
spike patterns in the spike train space. These variabilities of PTSTs are not appropriately described by the
widely used point process models such as Poisson process or renewal process. Although the PTSTs have
been previously quantified in practice, there has not been a formal point process model to describe them.
We provide a mathematically rigorous point process model, and describe its properties and implication. We
define the counting process of a single precisely timed action potential from intuitive notions of precision
and reliability, then we build a collection of indistinguishable precisely timed action potentials. In addition, to
represent additive action potentials and rate modulated responses, the model can be super-positioned with
a Poisson process. We will illustrate the usefulness of our proposed model via spike trains collected in vitro
from cultured cortical neurons on micro electrode arrays. A simple heuristic method to estimate the model
parameters is also presented.
doi:10.3389/conf.neuro.06.2009.03.227

III-22. Using Brainbow and GRASP for detailed reconstruction of complete circuits with light microscopy
Yuriy Mishchenko

GMYURIY @ HOTMAIL . COM

Columbia University
Acquiring detailed and complete structure of neural circuits is one of the central problems of neuroscience.
Until present, the only method for generation of such data was thought to be the circuit reconstruction from
serial Electron Microscopy, which could require decades to complete a small circuit. Here, we suggest
that already existing light microscopy and genetic tools may be used to obtain much faster and cheaper
reconstructions of larger complete neural circuits. We describe how a collection of specifically prepared
genetically targeted light probes of connectivity may be produced and used to systematically deduce feasible circuit configurations up to the point of pinpointing circuit’s structure exactly. We suggest to represent
different such probes mathematically as constraints on the circuit’s connectivity matrix. This then leads to a
prescription for how to combine results of such probes, performed in the same or different animals, computationally to obtain detailed structural information about the neural circuit. This paradigm may be employed
with light probes of connectivity, such as recently described ChR2-assisted circuit mapping, GRASP or modified transsynaptic viruses, and gene expression techniques such as Brainbow, MARCM, or UAS/Gal4. It
may be used in model organisms such as C. Elegans, Drosophila, zerbafish, mouse, etc. We exemplify this

COSYNE 09

229

III-23
paradigm by showing how GRASP and Brainbow may be used for detailed reconstruction of complete neural circuit in C. Elegans within one week or less with essentially off-the-shelf genetic and light microscopy
tools. Even though wiring diagram in C. Elegans is thought to be known from previous sEM reconstruction, unprecedented ability to extract such circuits routinely and rapidly in different settings will offer unique
opportunities for systems neuroscience.
doi:10.3389/conf.neuro.06.2009.03.011

III-23. Temporal precision of spikes in pulse-coupled networks of oscillating neurons
Jun-nosuke Teramae1,2
Tomoki Fukai
1
2

TERAMAE @ RIKEN . JP
TFUKAI @ BRAIN . RIKEN . JP

Brain Science Institute
RIKEN

Reliable information processing requires a code that can be represented and transmitted reliably within
the precision of the devices. In the brain, neurons use spikes to convey information to other neurons, and
temporal code may hypothesize that information is encoded into fine spatio-temporal structure of preciselytimed spike trains. Some degree of temporal precision in spike trains seems required in sensory information
processing, where stimuli have to be decoded from spikes. Single neurons are known to generate highly
precise spike trains when they are repeatedly activated by the same fluctuating input. Neurons, however,
work collectively rather than individually in their network. It remains unclear whether precise elements put
in a network can still respond precisely since mutual couplings may affect the response of the individual
neurons. To answer this question, we investigate the temporal precision of responses of a pulse-coupled
network of neurons when a set of independent fluctuating inputs, i.e., frozen noise, is repeatedly applied
to the network. To study this problem analytically, we introduce uncoupled copies of the network that commonly receive the set of fluctuating inputs. Suppose that the original network repeatedly generates identical
precisely-timed spike responses. Then, we can interpret the collection of these responses across trials as
responses of the individual copies in a trial. Thus, in-phase synchronization between identical networks implies precisely-timed responses of a single network across trials. Such noise-induced synchronization was
previously studied between single oscillators. Here, we study the noise-induced synchronization between
networks of oscillating neurons to clarify whether each network is able to encode information about fluctuating inputs into precisely-timed spikes. We develop a mean-field theory of noise-induced synchronization
between the copies of the pulse-coupled network of neurons. We can analytically derive a self-consistent
equation for the distribution of spike-time differences between the corresponding neurons and obtain the
distribution as a function of the coupling strength of the network. The distribution allows us to reveal the
nontrivial effect of mutual couplings on the temporal precision of response spikes in the recurrent network
of neurons. The width of the distribution increases monotonically as we increase the coupling strength, yet
an obvious peak remains in the distribution. The positive width implies that even if all the constituents, i.e.
single neurons here, are faithful to the input, it is not the case for the system as a whole. Spike sequence
generated by the network is thus not perfectly the same across trials. Instead, the result implies enough
coherence of spike trains between different trials, as indicated by the obvious peak of the distribution. The
width of the peak measures a degree of the coherence, up to which spike trains can be used reliably. The
result tells us how the coherence changes qualitatively as a function of the coupling strength in the network.
doi:10.3389/conf.neuro.06.2009.03.018

230

COSYNE 09

III-24 – III-25

III-24. Dynamics and cortical distribution of figure-ground activity in
human visual cortex
Anthony Norcia
Vladimir Vildavski
Richard Miller

AMN @ SKI . ORG
VLADVIL @ SKI . ORG
RICH @ SKI . ORG

Smith-Kettlewell Eye Research Institute
Figure/ground segmentation is a necessary precursor of object-level processing that involves the detection
of image discontinuities as well as the integration of border information and surface cues for shape. Here we
used EEG source imaging and a novel multi-input non-linear analysis method to study the temporal evolution
of figure ground segmentation in early and mid-level areas of human visual cortex. Figure and background
regions of simple texture defined forms were modulated with separate pseudo-random binary sequences.
A Volterra kernel analysis was used to derive groups of response kernels associated with the figure and
background regions and their joint interaction. Activity for each response group was measured in a series
of visual areas defined either on the basis of retinotopy or functional specificity in individual observers using
fMRI. We find that the lateral occipital complex is the first region to differentially process the figure region,
but that lower-level cortical areas, such as V2/V3/V3A distinguish these regions at longer latencies. The
later activity in first-tier visual areas is consistent with feedback activity arriving from higher-order areas such
as the lateral occipital complex. The validity of the kernel groups was verified by using specific summations
of the three groups to reconstruct responses to different periodic versions of the two-input kernel-mapping
stimuli. The periodic responses were obtained in separate recordings. Reconstructions of the measured
periodic responses from the kernels were very accurate for responses occurring before about 250 msec.
After that time-point, systematic deviations were apparent for some of the tests, suggesting that periodic
stimuli may evoke qualitatively different responses than random ones do. These deviations may be the
result of sequence learning/temporal cuing effects.
doi:10.3389/conf.neuro.06.2009.03.035

III-25. Extremely high-speed simulation of sparsely connected networks
Terence Sanger

TERRY @ SANGERLAB . NET

Stanford University
A computational model of neurological development is only able to make predictions about future behavior if
the simulation can operate much faster than real-time. The goal of this project is to predict the effect of early
brain injury on development by using large-scale simulations of spiking neurons that operate at more than
365 times real-time, so that one year of development can be simulated in a single day of computation. In
order to accomplish this, I use field-programmable gate array (FPGA) hardware and the new mathematical
theory of Likelihood Calculus. The FPGA implementation allows efficient parallel or pipelined simulation
of sparsely connected networks. Likelihood Calculus provides the mathematical basis for approximation of
a fully-connected network by a sparsely connected network. I demonstrate the use of this technology to
simulate plasticity in a simple model of spinal cord processing. The model includes two-neuron circuits for
the short-latency and long-latency stretch reflexes, and spike-timing dependent plasticity is implemented at
the cortico-motoneuronal and afferent-motoneuronal synapses. Therefore four cell populations control the
agonist muscle, and an additional four cell populations control the antagonist. Dynamics are simulated for
a human knee joint controlled by Hill-type muscle fibers, and afferent information is provided by a simple

COSYNE 09

231

III-26
model of proprioceptive afferent responses. Neurons are implemented using the Izhikevich approximation
to the Hodgkin-Huxley equations. Since there are 1000 neurons in each population with a total of 8000
neurons in the complete network, the number of possible connections between neurons is on the order
of 10e7. Therefore the fully-connected network is approximated by a sparsely connected network that
consists of 1000 parallel networks of 8 neurons each, where within each network a single neuron assumes
all the functions of each of the distinct cell populations. Using Likelihood Calculus, it can be shown that the
expected behavior of the sparsely-connected parallel dynamic networks is equal to the expected behavior
of the fully-connected network if certain resampling conditions are satisfied by the proprioceptive afferents.
The full network of 8000 neurons can be run at 50 times real-time on a single Xilinx XC3S1500 FPGA
chip. Alternatively, 800 neurons can be run at 500 times real-time on a single chip. Simulations show
the development of spasticity in the context of reduced threshold in the short-latency stretch reflex, and
instability reminiscent of dystonia can be produced by changes in the cortical activation pattern or the
threshold of the long-latency stretch reflex. These experiments demonstrate the feasibility of extremely
high-speed faster-than-real-time simulation of sparsely connected networks of spiking neurons.
doi:10.3389/conf.neuro.06.2009.03.332

III-26. Implementation of a probabilistic inference in a photoreceptor
cell
Audrey Houillon
Pierre Bessière
Jacques Droulez

AUDREY. HOUILLON @ COLLEGE - DE - FRANCE . FR
BESSIERE @ IMAG . FR
JACQUES . DROULEZ @ COLLEGE - DE - FRANCE . FR

CNRS/Collège de France
In recent years, several probabilistic models have been shown to efficiently describe perceptive and behavioural tasks. The particularity of these models is that they account for the ability to reason with incomplete knowledge about the external world. In this study we consider how the probability distributions that
need to be estimated in these models could be represented and processed at single cell level. We have
considered the possibility that molecular interactions within a single cell can implement some elementary
probabilistic reasoning. The Hidden Markov Model (HMM) is one of the simplest form of time-dependent
probabilistic models, and is widely used in cognitive probabilistic modeling. We show that a photoreceptor cell can compute a Bayesian inference in a HMM. More precisely, the cell estimates the current state
probability distribution by means of its underlying biochemical interactions. The interactions between the
different molecular messengers, ions, enzymes and channel proteins within the cell are described by a set
of nonlinear coupled differential equations, whereas the HMM is described by a discrete recurrence equation. We derived, under steady-state conditions (ie. for a constant input), a formal equivalence between
the probabilistic inference and some of the biochemical mechanisms involved in phototransduction. We can
then point out the similarity of the solutions and find the relations between the parameters of both systems.
Furthermore, we performed numerical simulations in order to extend the results to the dynamic case.
doi:10.3389/conf.neuro.06.2009.03.072

232

COSYNE 09

III-27 – III-28

III-27. Transfer of correlations in neural oscillators
Andrea Barreiro
Eric Shea-Brown
Evan Thilo

BARREIRO @ AMATH . WASHINGTON . EDU
ETSB @ WASHINGTON . EDU
THILOE @ U. WASHINGTON . EDU

University of Washington
Populations of neurons in a variety of brain regions show temporal correlations between their spike trains.
A potential source of such correlations is common external input, which is transformed onto correlated
output through the neural dynamics. We are interested in how particular neural dynamics will affect how
these inputs are mapped into outputs; in this study we examine correlation transfer in pairs of uncoupled
oscillators receiving partially correlated input. METHODS We consider two uncoupled cells, driven by both
independent and common noise. As a measure of input correlation we take the fraction of noise variance
that is in common, c. As a measure of output correlations we look at the normalized covariance of spike
counts in a finite time window T, Rho T. We start with a neuron model driven by sufficient background current
to be intrinsically oscillating, and consider its reduction to a scalar equation for the phase characterized by
a phase-resetting curve (PRC). Specifically, we consider a one-parameter set of PRCs given by the linear
combination of two prototypical examples: the theta model PRC, typical of Class I excitable neurons, and
the PRC near a Hopf bifurcation for a Class II excitable neuron, as demonstrated by Ermentrout (Neural
Computation 1996) and Ermentrout and Kopell (SIAM J. Math Analysis 1984), among others: Z(theta) = a *
(1-cos(theta)) + (1-a)*(-sin(theta)) We examine the correlation coefficient between spike counts over a time
window T. For very long time (T -> infinity) we use linear response theory for renewal processes to write
this quantity as the ratio of integrals related to exit time moments. We show that for a large class of models,
including those under consideration, these can be computed by simple quadrature of smooth functions. We
check our computations by comparison with the quadratic integrate-and-fire neuron, which is known to be
equivalent to the theta model. A related procedure, in progress, will allow us to compute the same quantities
for finite T. RESULTS We find that correlation transfer over long time scales exhibits striking differences from
the short-time (synchrony) correlation levels computed by Marella and Ermentrout (PRE 2008); they also
differ qualitatively from the results on the linear integrate-and-fire neuron presented in de la Rocha et al.
(Nature 2007, PRL 2008). We find that correlation transfer for neural oscillators is nearly independent of
both input statistics (mean variance of afferent currents) and output statistics (firing rate and CV). Moreover,
Class I neurons maintain a positive limiting correlation coefficient; correlation in the Class II case decays to
near zero. These findings may have consequences for coding by neural oscillators over long time scales,
in that stimuli that affect firing rates would nonetheless produce stimulus-independent correlations. This
contrasts with the strong stimulus-dependence that holds in the excitable regime, or for neural oscillators
over short time scales. REFERENCES Full text of references is provided in the supplementary material.
doi:10.3389/conf.neuro.06.2009.03.327

III-28. Neural mechanisms of visual motion integration and ego-motion
estimation – a modelling investigation
Florian Raudies
Jan Bouecke
Heiko Neumann

FLORIAN . RAUDIES @ UNI - ULM . DE
JAN . BOUECKE @ UNI - ULM . DE
HEIKO. NEUMANN @ UNI - ULM . DE

University of Ulm
Problem. During locomotion, temporally varying light patterns of the ambient optic array impinge the retina

COSYNE 09

233

III-29
to generate the so called optical flow. Characteristic global patterns of optical flow are produced by different
forms of self-motion (Gibson, The perception of the visual world, 1950). How is the steering and spatial
navigation through a complex environment controlled by the visual processing of optical flow? Neural analysis of flow is mainly realized in areas V1, MT, MSTd, and MST along the dorsal pathway in visual cortex.
Yet, the interaction of cells in the different areas and the interaction with form representations in the ventral
pathway remains an ongoing research topic. In particular, it is unclear how the visual processes adapt
and how they generate incrementally more complex representations as a function of navigational steering behaviours. Methods. We present a dynamic model of primate motion analysis to solve navigational
tasks and discuss how more complex and distributed representations are recruited when more detailed
information is required, e.g., to avoid moving obstacles in cluttered scenes. Unlike previous approaches
our model combines mechanisms of optical flow detection and integration with mechanisms of ego-motion
estimation. The proposed model consists of model areas V1, MT and MST, to simultaneously integrate and
segregate motion into different components, and is augmented by a heading map which is fed by MSTd
responses. Initial processing detects raw motion from spatio-temporal correlations utilizing a bank of Gabor
filters. These responses are integrated in area MT to build a distributed velocity representation of speed
and direction (Rodman & Albright, Vision Research, 27, 1987). Local center-surround interactions allow
the representation of multiple motions at one spatial location in MT. Activity is integrated by cells over large
spatial regions of the visual field (Duffy & Wurtz, J. of Neurophysiology, 65, 1991; Graziano et al., J. of Neuroscience, 14, 1994). Unlike previous approaches, the model selectively integrates different speed channels
with similar direction selectivity to robustly estimate heading direction independent of scene depth (Perrone
& Stone, Vision Research, 34, 1994). On the other hand, different speeds are integrated to segregate
different object motions, unlike (Mingolla et al., J. of Vision [Abstracts], 8, 2008). Results and Conclusion.
Model simulations demonstrate how the model performs in navigational tasks in which a simulated agent
approaches a target. It is demonstrated how mechanisms of motion detection in model V1 and MT integrate and segregate distinct object motions and their direction. Systematic errors in heading estimation
occur when translational observer motion is overlaid by rotations or when independently moving objects
disturb the motion pattern (Royden et al., Vision Research, 34, 1994). Motion of independent objects can
be indicated by the response of motion contrast cells. The model has been probed with synthetic as well as
real sequences such as those acquired by a camera mounted in a moving car in different traffic scenarios.
Future work will focus on the question how the distributed representations influence the decision-making to
control the behavioural strategies during locomotion. Supported by EU-Project 027198(DiM) and Graduate
School Univ.Ulm.
doi:10.3389/conf.neuro.06.2009.03.090

III-29. A Volterra kernel approach to non-linear functional connectivity
Richard Miller
Vladimir Vildavski
Anthony Norcia

RICH @ SKI . ORG
VLADVIL @ SKI . ORG
AMN @ SKI . ORG

Smith-Kettlewell Eye Research Institute
The availability of multi-electrode arrays makes it possible to record (or infer with EEG/MEG) evoked activity
simultaneously at multiple, possibly interconnected brain sites. We are developing a method of assessing
linear and non-linear functional connectivity between sites that combines graphical models and a Volterra
kernel analysis of responses driven by external stimuli and endogenous signals of local origin. In this approach, the output of a node in the graph is a function of its inputs from one or more external stimuli or
from other nodes in the graph. In addition, each node can have its own endogenous activity (local noise)
that can contribute to the measured nodal output. The goal of the method is, for a given graph, to identify
each node’s functional operator in terms of a multi-input Volterra series representation. Externally evoked

234

COSYNE 09

III-30
activity alone is sometimes all that is necessary to completely characterize a graph and its nodal operators.
However, in other cases, externally evoked activity may not be able to fully tease apart (even theoretically)
the internal functional operators. Here, local noise, which differs between nodes and acts as a “stimulus”
to downstream activity, can further (and sometimes completely) pin down the operators. To the extent that
external inputs force the system into a particular operational configuration, the computed operators reflect
that configuration. The effective system results from non-linear interaction between external inputs and endogenous signals that are generated locally at each node. The method fully models both evoked as well as
endogenous activity, yielding a more accurate estimation of the nodal operators. In the context of functional
connectivity, this non-linear method subsumes causality measures that are based on linear regression. It
improves on the more general all-but-one (conditional) Granger optimization procedure (multivariate autoregression or other measures), giving a better result without having to do separate calculations that remove
nodal connections one at a time. Finally, the method characterizes connectivity in terms of operator descriptions as well as probability measures. We have also used a bivariate version of the method to detect
non-linear functional connectivity between pairs of EEG electrodes. Importantly, the method cleanly isolates
true neural interactions from volume conduction effects. Volume conduction appears at zero latency in the
first-order kernel.
doi:10.3389/conf.neuro.06.2009.03.085

III-30. A model shows how closed loop motor control can spontaneously develop in the rat whisker system
Leor Gruendlinger
Misha Tsodyks

LGRUENDL @ WEIZMANN . AC. IL
MISHA @ WEIZMANN . AC. IL

Weizmann Institute of Science
The rat is a nocturnal animal that actively moves its facial whiskers back and forth when expecting or exploring objects in the immediate environment. This rhythmic behavior, called active whisking, appears during
the second postnatal week, and it is still unknown whether or how this behavior is learned. It has been
observed, however, that several patterns of seemingly random and nonfunctional activity start to occur
already in utero in whisker-related motoneurons of the brainstem, and similar patterns persist and continuously change after birth. Following the works of Schouenborg et al (reviewed in Schouenborg, 2008) on the
way muscle twitches shape the development of nociceptive withdrawal reflexes in rat pups, we developed
a simple model that suggests how a pup can spontaneously learn a more complex behavior, i.e., rhythmic
whisking, from seemingly random movements. The main aim of the model was to show how correct connections could gradually form between the whisker-related sensors and whisker-related motoneurons in the
brainstem, resulting in a functional circuitry. The change is suggested to occur already at the level of the
brainstem, where the movements create temporal patterns of correlated activity in motor and sensory neurons, inducing intrinsic and synaptic plasticity. Three tasks were implemented: a) Stabilizing the whiskers
at a desired position. b) Developing the whisker withdrawal reflex, and c) Rhythmically whisking in free
air. These behaviors can easily be switched by changing two parameters in the model that are putatively
related to known biological actions of neuromodulators. The movement produced by the model is shown
to be efficient according to several kinematic and dynamic criteria. Thus, this model suggests that a rat
indeed can and does learn how to efficiently whisk. In the context of the ongoing debate between genetic
preprogramming of behavior versus learning from experience, the genetic infrastructure is hypothesized
to include a repertoire of preprogrammed learning rules. Using this as a basis, a rat pup can spontaneously develop a functional motor-sensory feedback loop without any external supervision or reward. This
work was supported by the Israel Science Foundation References: Schouenborg J. 2008. Brain Res Rev
57(1):111-117.

COSYNE 09

235

III-31
doi:10.3389/conf.neuro.06.2009.03.352

III-31. Complementary roles of the Left Posterior Parietal Lobe and
Basal Ganglia in reference frame usage
Elizabeth Torres
Kenneth Heilman
Howard Poizner

EBTORRES @ RCI . RUTGERS . EDU
HEILMAN @ NEUROLOGY. UFL . EDU
HPOIZNER @ UCSD. EDU

Rutgers University
The generation of voluntary reaches requires the integration of sensory input from different modalities.
Sensory cues arising from the extra-personal world must be combined with internally sensed cues from
our own body motions while some system(s) continuously monitor(s) this interplay. This process gives rise
to a general transformation problem involving different space-time frames of references. The relevance of
a given frame of reference may shift according to the task demands and to the context in which the task
must be resolved. An important and new question in this regard is whether or not different brain systems
play complementary roles in weighting a particular frame of reference. We addressed this question in the
context of a continuous corrective process in which goals in different domains switched priorities as the
motion evolved. Nine Parkinson’s patients, nine normal controls and a patient with a lesion, confined to
the left Posterior Parietal Lobe, performed continuous forward and back three-dimensional pointing motions
in the dark while the reference frame for sensory guidance was systematically changed. Forward motions
were to a visual target whereas backwards motions were to a proprioceptive target defined by the starting
arm posture. The experiment had a block design. In block 1, the target was flashed and the subject had
to point to the memorized location and back to the original posture. In block 2, the target remained ON
throughout the motion (extra-personal reference). In block 3 the target was extinguished so the subject had
to memorize the location as in block 1 but received guidance from a light located on the pointing finger tip
(egocentric reference). We used a theoretic geometric framework to (1) perform the general transformations
taking place back and forth between the internal and the extra-personal representations, and (2) to generate
motion paths and initial times in both internal and extra-personal spaces. Simulations of these postural and
hand motion paths gave rise to a speed-independent trajectory symmetry which reflected a transformation
invariance. This symmetry, which holds in general in complex primate arm motions, independently of the
shape of the hand’s speed profile, was also evident in the normal controls. In the compromised system,
however, the symmetry broke down and it was repaired in a complementary fashion in patients with parietal
and basal ganglia lesions. Our work shows that for sensory guidance, the parietal patient relied more
on the extra-personal reference frame whereas the Parkinson’s patients relied more on the egocentric
reference frame. Both patients re-derived the initial timing of their reaches from spatial distance in allocentric
(parietal) and egocentric (Parkinson’s) modes. They were able to repair the transformation symmetry, but
only when the appropriate reference frame was used. These results have new implications for the design
of rehabilitation therapies and explain how the nervous system treats a new sensory-motor transformation
invariant of the primate arm system.
doi:10.3389/conf.neuro.06.2009.03.118

236

COSYNE 09

III-32 – III-33

III-32. Gaussian-process factor analysis for low-d single-trial analysis
of neural population activity
Byron Yu1,2
John Cunningham1
Gopal Santhanam1
Stephen Ryu1
Krishna Shenoy
Maneesh Sahani2
1
2

BYRONYU @ STANFORD. EDU
JCUNNIN @ STANFORD. EDU
GOPALS @ STANFORD. EDU
SEOULMAN @ STANFORD. EDU
SHENOY @ STANFORD. EDU
MANEESH @ GATSBY. UCL . AC. UK

Stanford University
Gatsby Computational Neuroscience Unit, UCL

Neural responses are typically studied by averaging noisy spiking activity across multiple trials to obtain
firing rates that vary smoothly over time. However, particularly in cognitive tasks (such as decision making or motor planning), the timecourse of the neural responses may differ on nominally identical trials. In
such settings, it is critical that the neural data not be averaged across trials, but instead be analyzed on a
trial-by-trial basis (Churchland et al., Curr Opin Neurobiol, 2007). With the ability to record simultaneously
from a neural population (currently tens to hundreds of neurons in awake behaving primates), we consider
techniques for extracting a smooth low-dimensional ”neural trajectory” summarizing the recorded activity
on a single trial. Beyond the benefit of visualizing the high-dimensional noisy spiking activity in a compact
denoised form, such trajectories can offer insight into the dynamics of the underlying neural circuitry. One
way to extract a neural trajectory is to first estimate a smooth firing rate profile for each neuron on a single
trial (e.g., by convolving each spike train with a Gaussian kernel), then apply a static dimensionality reduction technique (e.g., principal components analysis, PCA, or factor analysis, FA). We developed a novel
alternative approach, Gaussian process factor analysis (GPFA), which performs the smoothing and dimensionality reduction operations simultaneously rather than serially. This allows the degree of smoothness
and the relationship between the low-dimensional ”neural trajectory” and high-dimensional recorded activity
to be jointly optimized. We applied these techniques to the activity of 61 neurons recorded simultaneously
in macaque premotor cortex during a center-out delayed reaching task. To evaluate model goodness-of-fit,
we left out one unit at a time and asked how well each technique could predict the activity of that unit, given
the activity of all other recorded units. Our findings are as follows: (1) GPFA yielded the lowest prediction
error, followed by FA and PCA. (2) The dimensionality of the linear subspace within which the recorded
neural activity evolved during reach planning and execution for a single target ranged from 8 to 12. (3) For
each of the 14 reach targets, the extracted low-dimensional neural trajectories converged during the delay
period, an effect which previously could only be inferred indirectly (Churchland et al., J Neurosci, 2006).
By studying trials with outlying reaction times, we also find that such methods can be a powerful tool for
relating the spiking activity across a neural population to the subject’s behavior on a single-trial basis.
doi:10.3389/conf.neuro.06.2009.03.153

III-33. Rapid, scalable neuronal network simulations using MNet
Corey Acker1
John White2
1
2

ACKER @ UCHC. EDU
JOHN . WHITE @ UTAH . EDU

University of Connecticut Health Center
University of Utah

As more experimental data is gathered detailing the biophysical properties of individual neurons along

COSYNE 09

237

III-34
with details of connectivity patterns among neurons, it becomes more and more difficult to construct and
simulate computational models of these microcircuits. Significant flexibility is required to incorporate high
level structure such as patterns of connectivity that statistically mimic those observed experimentally, while
computational efficiency is required for rapid simulations of large networks of biophysically realistic models.
We have developed a simulation tool, MNet, to construct and simulate neuronal networks with the goals
of flexibility and speed in mind. MNet was originally called “PINetworks” and developed to study rhythms
in networks of hippocampal neurons involving two cell types, pyramidal neurons (P) and interneurons (I)
(Rotstein et al. 2005), but has been extended to multiple cell types. MNet is a MATLAB based program,
with the numerical methods coded in C using MATLAB’s application program interface (API, “mex”). The
high-level issues such as network construction, connectivity patterns, post-simulation data processing, and
so forth are all taken care of in the high-level language MATLAB. An important example of the flexibility
that this affords is that network size is flexible, or “scalable”, meaning the actual numbers of neurons in the
networks can be changed and simulations can immediately be repeated keeping all other properties of the
network constant. Meanwhile, the differential equations are solved in the low-level language C for speed.
Event-based synapses (Lytton, 1996) are utilized, which tremendously improves the scaling behavior of
network simulation time as a function of increasing numbers of neurons in the network. Rotstein HG, Gillies
MJ, Pervouchine D, Acker CD, White JA, Buhl EH, Whittington MA, Kopell N (2005). Slow and fast inhibition
and an h-current interact to create a theta rhythm in a model of CA1 interneuron network. J Neurophysiol.
94(2):1509-18 Lytton WW (1996). Optimizing synaptic conductance calculation for network simulations.
Neural Comput.. 8 (3):501-9
doi:10.3389/conf.neuro.06.2009.03.194

III-34. Prediction of pairwise maximum entropy model parameters by
maximizing information
Jeffrey Fitzgerald1,2
Tatyana Sharpee2
1
2

JFITZGERALD @ PHYSICS . UCSD. EDU
SHARPEE @ SALK . EDU

University of California, San Diego, CTBP
The Salk Institute for Biological Studies

Recent studies have shown that pairwise maximum entropy models describe the correlation structures in
networks of neurons in the retina with a high degree of accuracy both when neurons are probed with natural
[1] or white noise stimuli [2]. In each case, the parameters of the models were obtained from experimental data, and the question remains as to how the parameters of the maximum entropy model will change
when the statistics of the stimuli are varied. Here we propose a method of predicting the parameters of
the pairwise maximum entropy models that maximize information between stimuli and the neural responses
based on the concept of neuronal decision boundaries [3]. Specifically, responses of individual neurons
in the population are assumed to be binary “spike”/“no spike”, as is the case for the pairwise maximum
entropy model. The neural decision boundary of a given neuron separates stimuli that elicit a spike from
stimuli that do not. When the theory of decision boundaries is extended to networks of neurons, we found
that the network’s boundaries are made up of single neuron boundaries with additional constraints arising
at intersections between decision boundaries of different neurons. The configuration of individual decision
boundaries determines the response probabilities of different firing patterns in the population, from which
parameters of the pairwise maximum entropy model can be derived. Thus, the decision boundary model of
the neural population provides a link between the average properties of a neural population, as described
by the pairwise maximum entropy models, and specific neural responses to different stimuli. By selecting
such configurations of decision boundaries that maximize the mutual information between neural responses
and stimuli, one can determine the parameters of the optimal maximum entropy model. We applied this ap-

238

COSYNE 09

III-35
proach to exponentially distributed stimuli, because their statistics capture the statistics of large deviations
in the naturally occurring stimuli [4,5]. We were able to find an optimal configuration of decision boundaries for a network of two neurons that maximizes mutual information between the neural responses and
exponentially distributed stimuli. The corresponding optimal parameters of the pairwise maximum entropy
model were in agreement previous findings [1], supporting the hypothesis of that early sensory processing
is optimized for maximum information transmission. This work was supported by grants from NIMH, the
Alfred P. Sloan and Searle Foundations, and the NSF-sponsored Center for Theoretical Biological Physics.
1 Schneidman E, Berry MJ, Segev R, Bialek W (2006) Weak pairwise correlations imply strongly correlated
network states in a neural population. Nature 440:1007-1012. 2 Shlens J, Field GD, Gauthier JL, Grivich MI,
Petrusca D, Sher A, Litke A, Chichilnisky EJ (2006) The structure of multi-neuron firing patterns in primate
retina. J Neurosci 26:8254-8266. 3 Sharpee T, Bialek W (2007) Neural decision boundaries for maximal
information transmission. PLoS ONE 2(7) e646. 4 Ruderman DL, Bialek W (1994) Statistics of natural
images: scaling in the woods. Phys Rev Lett Vol. 76, Num 6:814-817. 5 Simoncelli EP, Olshausen BA
(2001) Natural image statistics and neural representation. Annu Rev Neurosci 24:1193-1216.
doi:10.3389/conf.neuro.06.2009.03.269

III-35. Unsupervised learning of Lie group operators from image sequences
Jimmy Wang
Jascha Sohl-Dickstein
Bruno Olshausen

CMWANG @ BERKELEY. EDU
JASCHA @ BERKELEY. EDU
BAOLSHAUSEN @ BERKELEY. EDU

Redwood Center for Theoretical Neuroscience
University of California, Berkeley
A fundamental problem in vision is understanding how the visual world evolves over time. If this evolution
can be better described, it will prove useful in neuroscience, machine vision and video compression. In
addition, the ability to describe the natural evolution of the visual world will likely aid in the identification of
transformation-dependent, or dynamic, attributes - such as location, scale, orientation and lighting, as well
as the construction of representations which are invariant to those attributes. One of the most appealing
descriptions of transformations (for reasons of algebraic simplicity and computational richness) involves the
use of a Lie, or continuous transformation, group. The Lie group is built by first describing all infinitesimal
transformations which an image may undergo. The full group is then generated out of all possible compositions of those infinitesimal transformations. This representation allows for transformations - such as
translation or rotation - to be applied smoothly and continuously by adjusting a transformation coefficient.
Earlier approaches to learning Lie group operators have been implemented via first-order approximations
for computational tractability (Rao and Ruderman, 1999, Miao and Rao 2007). This restricted the learned
models to a superposition of discrete transformations, rather than a composition of continuous transformations. The contribution of this work is to provide a tractable learning method for the Lie group operators.
By parameterizing each Lie group operator in terms of its eigenvectors and eigenvalues we are able to
apply and perform learning in our framework with a computational cost of the same order as multiplication
of the operator matrix. This allows fast learning of transformations without approximation, and fast application of the learned transformations to images. We demonstrate learning of Lie group transformations in
an unsupervised fashion from time varying image sequences. When trained in a controlled setting where
the transformation is known (e.g. translation), our results demonstrate the recovery of the corresponding
transformation. We also present a full set of transformations learned on natural movies.
doi:10.3389/conf.neuro.06.2009.03.344

COSYNE 09

239

III-36 – III-37

III-36. Virtual brain reading: A neural network approach to understanding fMRI patterns.
Rosemary Cowell1
Garrison Cottrell2
1
2

R . A . COWELL @ KENT. AC. UK
GARY @ CS . UCSD. EDU

University of Kent
University of California, San Diego

Is the fusiform face area (FFA) a module specialized for processing faces, or does it simply support generic
visual expertise? Researchers have investigated this question using Multi-Voxel Pattern Analysis (MVPA)
applied to fMRI results. Haxby et al. (2001) showed that patterns of neural activation in object-selective
visual cortex can be used to discriminate object categories, even when voxels selective for those categories
are removed. This provided evidence for a distributed neural code, in which information about faces exists
outside the FFA. In contrast, Spiridon and Kanwisher (2002) showed that activation patterns seen in faceselective cortex were more effective for making face vs. non-face discriminations than for non-face vs.
non-face discriminations, whereas this was not true for other object categories, such as man-made objects.
This implied FFA neurons contain special information about faces, but that there is no specialized module
for other object categories. We present a neurocomputational model of visual processing, in which object
representations are organized topographically. Photographic images are subject to Gabor filtering, then
PCA, then input to a Kohonen network – a self-organizing neural network that groups similar inputs together,
forming a two-dimensional “semantic map” of stimulus space. We present a new method for “virtual MVPA”,
in which we assume that activations of units in the Kohonen network layer correspond to neural activity
in ventral visual cortex, and may be mapped onto voxel activations measured by fMRI. We trained the
model on images of cups, cans, books and faces. The Kohonen network developed a region dedicated
to each category. In line with Haxby et al. (2001), each of these dedicated regions still had different
response patterns to stimuli from the other categories, such that activation patterns in areas of the semantic
map dedicated to one category (e.g., faces) can be used to distinguish between other categories (e.g.,
cups versus cans). However, in line with Spiridon and Kanwisher, the face area is better at distinguishing
faces from non-faces than at distinguishing non-face categories from each other, while non-face areas
of the semantic map are, on average, equipotent at both tasks. In the model, this can be explained by
lower within-category variability of the representations of faces compared to, say, cups. This is due to
higher within-category visual similarity for faces than for other categories. Hence, with a model of visual
cortex possessing no special mechanism for face processing, we simulate Spiridon and Kanwisher’s results,
casting doubt on their interpretation in favor of a specialized face module.
doi:10.3389/conf.neuro.06.2009.03.338

III-37. Attention and location effects on spatial memory: Testing the
predictions of a computational model
Xue Han1
Suzanna Becker1
Patrick Byrne2
Michael Kahana
1
2

HANX 3@ MCMASTER . CA
BECKER @ MCMASTER . CA
PBYRNE @ YORKU. CA
KAHANA @ PSYCH . UPENN . EDU

McMaster University
York University Centre for Vision Research

Byrne, Becker, and Burgess (2007) proposed a computational model of the neural mechanisms underlying

240

COSYNE 09

III-38
spatial memory and mental imagery, which we shall refer to as the BBB model. They proposed that egocentric information about space from the dorsal visual pathway is combined with object information from
the ventral visual pathway to form allocentric representations of spatial scenes in long-term memory at
the level of the hippocampus. Similarly, memories about spatial locations can be retrieved from allocentric
(view-invariant) long-term memory in the hippocampus and mapped through reciprocal neuronal pathways
to generate egocentric mental images. The BBB model postulates some of the neural mechanisms that
may underlie allocentric spatial memory, but it does not tell us what sort of features might contribute to the
creation of these memories. We conducted five experiments using a virtual driving task to further elucidate
the conditions under which objects would be encoded as part of an allocentric spatial map as per the BBB
model, versus an egocentric visual snapshot memory. We found that the spatial locations of objects at
navigationally relevant points (i.e. decision points) were generally better remembered than were the locations of other objects, despite equivalent recognition memory for both types of objects. Additionally, the
variance in spatial memory errors across viewpoints was significantly smaller for the decision-point objects
than for the non-decision-point objects, indicating that the participants were more likely to have employed
egocentric representations to encode the non-decision-point objects and allocentric representations to encode the decision-point objects, as predicted. However, some participants appeared to use predominantly
an allocentric strategy (as identified by performance on a subsequent mapping test – ”good mappers”) while
others used an egocentric strategy (”poor mappers”). The good mappers showed decreasing variability in
spatial memory errors between the tested viewpoints across experimental blocks, while the poor mappers
showed increasing variability across blocks. Interestingly, manipulating attention to the non-decision-point
objects eliminated the navigational relevance effect in good mappers, but enhanced it in poor mappers.
Overall, these results are consistent with the predictions of the BBB model. Information about identity
and location are stored separately and integrated at the level of the hippocampus and peri-hippocampal
regions (e.g. perirhinal and parahippocampal cortices). Objects may either be processed within the ventral visual stream when treated as independent objects, or within the dorsal visual stream when treated
as landmarks integrated into a large-scale allocentric spatial representation, or both. However, the results
reported here suggest a further refinement of the BBB model, that is, objects in the environment may or
may not be treated as landmarks, depending on where they are located (i.e. relevant to navigation or not)
and how they are attended to. It would be of interest to know which neural structures are more active in our
task. Whereas Janzen and van Turrenout (2004) found more hippocampal and parahippocampal activation
for decision-point objects than non-decision-point objects, we predict that non-decision-point objects could
evoke equally strong hippocampal activation when attention to those objects is appropriately manipulated.
doi:10.3389/conf.neuro.06.2009.03.023

III-38. Speed versus accuracy in spiking attractor networks
Jean-Pascal Pfister
Mate Lengyel

JEAN - PASCAL . PFISTER @ ENG . CAM . AC. UK
M . LENGYEL @ ENG . CAM . AC. UK

University of Cambridge
Networks of recurrently coupled neurons have been suggested to be the biological substrates for many of
the sophisticated computations the nervous system can perform. In particular, the computations carried
out by such networks are often considered to take the form of dynamically evolving patterns of activity that
tend towards attractor states. The rate of convergence to these attractors is of crucial importance because
it implies a fundamental trade-off between the time within which the state of a network comes close enough
to the desired attractor (speed), and how close it comes to that attractor (accuracy). Here, we present an
analysis of this trade-off and show how it depends on the time constants governing the dynamics of attractor
networks. Importantly, the time-accuracy trade-off cannot be analysed meaningfully in the most common
formulation of attractor networks that characterises neural activities solely by continuous firing rates. Under

COSYNE 09

241

III-39
that formulation, the time constants determining the speed of convergence can be set to arbitrarily small
values in theory, and biophysical limits on time constants can only be treated as constraints. Thus, we
consider a more realistic scenario in which neurons interact through spikes that are stochastically generated
based on their underlying ‘firing rates’, or membrane potentials. We find that under such conditions the
speed-accuracy trade-off is optimally balanced by finite, rather than infinitely small, time constants. This
is because each neuron effectively needs time to estimate the firing rate of its presynaptic partners, and
longer time constants will lead to more accurate estimates but slower convergence. We derive analytical
relationships between the time constants governing the dynamics of spiking networks and the bias and
variance of network states relative to the true attractor state. We also show how these relations change
with other parameters of interest, such as the size of the network, and the average firing rates of neurons.
In particular, we show that the bigger the network is, the faster it will come close enough to the attractor.
We also report the results of numerical simulations that show good agreement with the theory. Computing
the bias and variance also allows us to compute the error the network is making in reaching its attractor
state, which in turn is used to determine the best time constants to minimize the error. Our results on
the dependence of optimal time constants on average firing rates also suggest interesting normative roles
for adaptation of time constants by slow components of membrane dynamics and by short-term synaptic
plasticity. This work was supported by the Wellcome trust.
doi:10.3389/conf.neuro.06.2009.03.131

III-39. Entorhinal cortex — dentate gyrus system implements an optimal device for distance rep
Zoltan Somogyvari1
Mihály Bányai2,3
Zsofia Huhn
Tamas Kiss
Peter Erdi

SOMA @ RMKI . KFKI . HU
BANMI @ DIGITUS . ITK . PPKE . HU
ZSOFI @ RMKI . KFKI . HU
BOGNOR @ RMKI . KFKI . HU
PETER . ERDI @ KZOO. EDU

1

KFKI Res. Inst. Particle and Nuclear Physics
Dept. of Biophysics, MTA KFKI RMKI, Budapest
3
Laboratory of Robotics, PPKE ITK, Budapest
2

Most vertebrates are able to make detours and find shortcuts to achieve economical navigation. This ability
requires the animal to keep track its direction and distance from specific locations. In rodents, direction
of the animal is coded by the activity of head direction cells present in several regions of the brain, but
distance information is only indirectly available, through the entorhinal cortical grid cell system. A neural
system downstream from the entorhinal cortex seems to be necessary to extract the distance information
from the periodic activity of grid cells. We propose that a system of such cells store the distance of the
animal from important locations in the dentate gyrus region of the hippocampus and these ”distance cells”
might be identified with the dentate granule cells. First, we show analytically that the grid cell system
forms an optimal representation of places for the purpose of distance estimation in terms of the number
of necessary neurons. The necessary cell number increases at least with the second power of the linear
size of the environment in case of simple solutions. In contrast to this, grid cell system make use of the
inherent symmetries of the two dimensional eucledian distance and provides a solution where the number
of necessary cells increases only with the logarithm of the size of the environment. Grid cells provide the
possibility to build up a really scalable system, suitable to real world applications. Second, a computational
model is set up to study the neural mechanism of distance information decoding from the ensemble of grids
cells. In the computational model, the proposed distance cells receive innervation from entorhinal grid cells,
the connection strength between grid cells and distance cells is set by a one-shot-learning rule and the

242

COSYNE 09

III-40
distance cell activity is affected by a winner-take-all mechanism. Simulation results of this model verifies
that the activity of the distance cell population is able to unambiguously code the distance of the animal from
important places. The proposed distance cells have a multi-peaked, patchy spatial activity pattern similar
to the firing pattern of granule cells in dentate gyrus.
doi:10.3389/conf.neuro.06.2009.03.158

III-40. Embedding multiple trajectories in recurrent neural networks in
a self-organizing manner
Jian Liu
Dean Buonomano

LIUJK @ UCLA . EDU
DBUONO @ UCLA . EDU

University of California, Los Angeles
Complex neural dynamics produced by the recurrent architecture of neocortical circuits is critical to the
cortex’s computational properties. However, the synaptic learning rules underlying the emergence of stable activity in these recurrent networks remain poorly understood. Theoretical studies have shown that, in
response to continuous Poisson inputs, randomly connected networks can exhibit several regimes (Brunel,
2000; Vogels et al., 2005), including chaotic dynamics. However, these networks generally exhibit catastrophic behavior in response to physiologically relevant stimuli such as brief input pulses (Mehring et al.,
2003). Here, we examined synaptic learning rules in recurrent networks with the goal of achieving patterns
of activity that: (1) propagate throughout the entire network in response to a brief stimulus (while avoiding runaway excitation); (2) exhibit spatially and temporally sparse dynamics; and (3) incorporate multiple
neural trajectories, i.e, different input patterns should elicit distinct trajectories. We trained a network of
excitatory and inhibitory IAF neurons with weak synaptic weights by presenting a brief input over hundreds
of trials. Presynaptic-dependent scaling (PSD; Buonomano, 2005) generated a state in which activity propagated through the network in a controlled fashion in response to the trained input; a behavior in which
different neurons fired at different points in time (‘synfire’-like activity) – similar to experimental findings in
the songbird system (Hahnloser et al, 2002). By connecting the recurrent neurons to a set of output units, it
was possible to generate arbitrary spatio-temporal output patterns that mimic motor behaviors. To quantify
the degree of network recurrence, we developed a recurrency index (RI) that revealed that the learning rule
had essentially converted an initially random connection into a structured feed-forward network. By training
the network with multiple input patterns, we showed that: (1) multiple nonoverlapping stable trajectories can
be embedded in the network; and (2) the RI progressively increased as a function of the number of training
patterns. In addition to PSD, we examined synaptic scaling (SS) + STDP, and PSD + STDP. The addition of
STDP improved the ability of the network to embed multiple trajectories. SS with or without STDP did not
produce stable trajectories.These results establish that it is possible to embed multiple trajectories within
a network in a self-organizing manner. These trajectories can in turn be used to control the spatial and
temporal dynamics of motor neurons.
doi:10.3389/conf.neuro.06.2009.03.315

COSYNE 09

243

III-41 – III-42

III-41. An ecological basis for the oblique effect
Michael Falconbridge
Donald MacLeod

MFALCONBRIDGE @ UCSD. EDU
DMACLEOD @ UCSD. EDU

Univeristy of California, San Diego
Psychophysical observers are better at a range of discrimination and detection tasks when stimuli are
oriented horizontally and vertically rather than at oblique angles. This is known as the Oblique Effect. As
there is a higher prevalence of horizontal and vertical energy in natural scenes, it seems natural to ask
whether the effect flows from the visual system’s attempt to efficiently represent its input. One approach
is to find out if models of vision that efficiently incorporate the statistics in natural images also exhibit the
oblique effect. Here I show that the Olshausen and Field model of simple cells, which attempts to extract
the sparse, independent, linear components of natural images, produces (1) more units tuned to cardinal
than oblique angles, (2) narrower orientation tuning for units tuned to cardinal axes, and (3) greater overall
activity in response to cardinal inputs than oblique. The narrower orientation tuning is specifically due to
non-linear interactions between competing output units. All of these attributes have either been observed
in primate brains or have been postulated as possible causes for the psychophysically observed oblique
effect. In the first two cases listed above, the oblique versus cardinal differences for the model are greater
(in the range of 20% to 60%) than those observed in neural studies. When images of a modern, carpentered
environment are used in place of natural images, the differences become larger still. In other words, efficient
representation of naturally encountered images under the Olshausen framework predicts an exaggerated
oblique effect. This indicates that efficiency is likely to be an important contributor to the oblique effect but
that other factors also play a role, effectively moderating the exaggerated effect produced by efficient coding
considerations on their own.
doi:10.3389/conf.neuro.06.2009.03.066

III-42. Low frequency oscillations facilitate STDP-based pattern learning and decoding
Timothée Masquelier
Etienne Hugues
Gustavo Deco
Simon Thorpe

TIMOTHEE . MASQUELIER @ ALUM . MIT. EDU
ETIENNE . HUGUES @ UPF. EDU
GUSTAVO. DECO @ UPF. EDU
SIMON . THORPE @ CERCO. UPS - TLSE . FR

Universitat Pompeu Fabra
Recent experiments have established that information can be encoded in the spike times of some neurons
relative to the phase of a background low frequency (1-10 Hz) LFP oscillation [1, 2, 3] – a phenomenon
referred to as ‘phase-of-firing coding’ (PoFC) [3]. These firing phase preferences could result from combining an oscillation in the input current with a stimulus-dependent static component that would produce the
variations in preferred phase [4, 5, 6, 7]. However, it remains unclear whether this phase preference is only
an epiphenomenon or if it really affects neuronal interactions – only then could it have a functional role.
Here we demonstrate that PoFC at low frequencies has remarkable effects on downstream learning and
decoding when coupled with Spike Timing Dependent Plasticity (STDP). Consider the abstract problem of a
single neuron equipped with STDP receiving activation patterns via thousands of afferent excitatory fibers.
How might the neuron detect a repeating activation pattern that affects an unknown subset of its afferents,
under conditions where the duration of the patterns is unpredictable and where the patterns recur at unpredictable intervals? Two conventional coding options would involve converting the activation patterns into

244

COSYNE 09

III-43
spikes using either a Poisson rate-coding scheme or Leaky-Ingrate-and-Fire neurons that generate variable
firing rates. However, learning to detect the repeating patterns with STDP using this sort of coding is difficult
or even impossible. In contrast, when the afferent population receives an oscillatory modulation current, and
remarkably even when a small fraction of its afferents ( 10%) exhibits PoFC, we show that a neuron can
rapidly learn to respond selectively every time the repeating activation pattern occurs. The ability of STDP
to detect repeating patterns had been noted before [8], but not in an oscillatory mode. Here we show how
the partial formatting of the spike times by the oscillation, so that they mainly depend on the current input
current, can be efficiently decoded by STDP and recognized after learning in just one oscillation cycle. This
suggests a main functional role for the low frequency oscillations that are found almost everywhere in the
brain. More generally, our study demonstrates that an oscillation and the STDP mechanism, by the existence of their intrinsic timescales, can together have a functional role. References [1] Lee, H., Simpson, G.
V., Logothetis, N. K., and Rainer, G. Neuron 45(1), 147–156 Jan (2005). [2] Hafting, T., Fyhn, M., Bonnevie,
T., Moser, M.-B., and Moser, E. I. Nature 453(7199), 1248–1252 Jun (2008). [3] Montemurro, M. A., Rasch,
M. J., Murayama, Y., Logothetis, N. K., and Panzeri, S. Curr Biol 18(5), 375–380 Mar (2008). [4] Hopfield,
J. Nature 376(6535), 33–36 (1995). [5] Mehta, M. R., Lee, A. K., and Wilson, M. A. Nature 417(6890),
741–746 Jun (2002). [6] Buzsáki, G. and Draguhn, A. Science 304(5679), 1926–1929 Jun (2004). [7] Fries,
P., Nikolic, D., and Singer, W. Trends Neurosci 30(7), 309–316 Jul (2007). [8] Masquelier, T., Guyonneau,
R., and Thorpe, S. J. PLoS ONE 3(1), e1377 (2008).
doi:10.3389/conf.neuro.06.2009.03.024

III-43. Modeling the interaction of biological elements involved in Hebbian and homeostatic plasticity
Taro Toyoizumi
Kenneth Miller

TARO @ NEUROTHEORY. COLUMBIA . EDU
KEN @ NEUROTHEORY. COLUMBIA . EDU

Columbia University
Multiple biological elements with different time constants are involved in an activity dependent modification
of synaptic strength. However, it is not known how those elements, such as AMPA receptor (de)phosphorylation
and endo(exo)cytosis, changes in receptor scaffolding proteins, and changes in spine morphology or density regulate the ultimate synaptic strength and how they interact during experience-dependent plasticity.
Genetic and molecular techniques have began to elucidate the detailed dynamics of synaptic strength and
to identify biological mechanisms involved at each plasticity stage. Ocular dominance plasticity has been a
standard system to study experience-dependent plasticity. Under monocular deprivation (MD) – the closure
of one eye – during a critical period in early life, responses to the deprived eye weaken and those to the open
eye strengthen. Subsequent binocular vision – reopening the previously closed eye – causes both eyes’
response levels to return to normal. Recently three separable processes have been identified underlying
this plasticity (Kaneko et al. 2008). Weakening of the deprived eye is rapid, and is not affected by blockade
of the molecules TNFalpha or BDNF. Strengthening of the open eye is slower, commencing only after about
3 days, and is specifically prevented by blockade of TNFalpha (but not of BDNF), which blocks a global form
of homeostatic synaptic scaling. Recovery from MD under binocular vision, which is thought to be mediated
by anatomical changes, is specifically blocked by blockade of BDNF. We have constructed and simulated
a simple model that includes these three separable plasticity processes. We modeled the total synaptic
strength as the product of three factors: a synapse-specific anatomical factor, representing something like
the number of spines; a postsynaptic-cell-specific scaffolding factor, representing something like the number of potential AMPA receptor sites per spine; and a synapse-specific strength, representing the percent of
potential AMPA receptor sites that are actually occupied. A rapid, LTP/LTD-like correlation-based process
controls the strength; a slower homeostatic scaling controls the scaffolding; and a still slower BDNF- and

COSYNE 09

245

III-44
strength-dependent process controls the anatomical factor. The model captures the transient behaviors of
ocular dominance plasticity during the critical period, which many traditional models do not. In particular,
to achieve a slow homeostatic response, either the homeostatic process can include delays, which leads to
oscillations, or its magnitude per unit time must be small. The latter would render homeostasis negligible
relative to the stronger LTP/LTD process in most previous models, but does not do so in the present model
because homeostasis and LTP/LTD control independent factors that multiply to determine synaptic strength,
allowing the homeostatic response to build up slowly without being overwritten by LTP/LTD.
doi:10.3389/conf.neuro.06.2009.03.140

III-44. Temporal difference learning does not always lead to STDP
Razvan Florian1
Catalin Rusu1,2
1
2

FLORIAN @ CONEURAL . ORG
RUSU @ CONEURAL . ORG

Center for Cognitive and Neural Studies
Babes-Bolyai University

Temporal difference learning does not always lead to STDP Razvan V. Florian, Catalin V. Rusu Center for
Cognitive and Neural Studies (Coneural), Str. Ciresilor nr. 29, 400487 Cluj-Napoca, Romania, and BabesBolyai University, Department of Computer Science, Str. Al. Kogalniceanu nr. 1, 400084 Cluj-Napoca,
Romania florian@coneural.org It has been previously shown that a form of temporal difference (TD) learning
for predicting the value of the membrane potential of a neuron, at a fixed delay after the neuron received a
presynaptic spike, results in a learning rule that is very similar to hebbian STDP (Rao and Sejnowski, 2001).
Since this result was obtained using a relatively complex neural model (two-compartmental, with HodgkinHuxley-like currents) and a simple setup (a single presynaptic spike followed by a single current pulse), we
investigated whether it holds for simpler neural models and more general situations. This is relevant both
for the theoretical understanding of this phenomenon and for verifying that it holds in common simulations
of spiking neural networks. We studied the same phenomenon using both IAF and Izhikevich neurons,
through simulations and, for the IAF neuron, also analytically. Postsynaptic spikes were generated by single
current pulses (as in the original study), by the irregular firing of synaptic afferents, or by a constant input
current. For the IAF neuron, we found a hebbian, STDP-like plasticity rule only when postsynaptic spikes
were generated by a single current pulse and the reset potential of the neuron was positive. For the same
input and negative reset potential, as well as for constant input current (regardless of the reset potential),
the resulting plasticity rule was anti-hebbian. For the Izhikevich neuron we obtained hebbian STDP-like
plasticity for both constant and pulsed input current. There is a qualitative difference with respect to the IAF
case because the Izhikevich neuron incorporates the dynamics of the membrane potential during the onset
of the action potential. By adding an action potential of non-zero duration to the IAF model, the shape of the
plasticity function changes significantly and becomes similar to hebbian STDP. This shows that the plasticity
function resulted from TD learning depends critically on whether the neuron adapts its synapses to learn
the shape of its action potential or not. However, the shape of the action potential is commonly considered
not to carry information. When we consider just the TD learning of the sub-threshold dynamics of the
membrane potential, the shape of the resulted learning function can loose its similarity with hebbian STDP.
For both neural models, in the case of irregular synaptic input there was no clear relationship between the
plastic changes predicted by TD learning and the temporal delay between the pre- and postsynaptic spikes.
Moreover, the sign of these plastic changes did not depend uniquely on the sign of the temporal delay.
In conclusion, TD learning in spiking neurons does not always lead to hebbian STDP. Reference: R.P.N.
Rao and T.J. Sejnowski (2001), Spike-timing-dependent hebbian plasticity as temporal difference learning,
Neural Computation 13, pp. 2221-2237.
doi:10.3389/conf.neuro.06.2009.03.205

246

COSYNE 09

III-45

III-45. Neural noise shapes perceptual landscapes for different forms
of plasticity in perceptual learning
Ching-Ling Teng1
Joshua Gold2
1
2

TENG @ VIRGINIA . EDU
JIGOLD @ MAIL . MED. UPENN . EDU

University of Virginia
University of Pennsylvania

Some forms of perceptual learning are thought to involve changes in how sensory information is represented
in the brain. Others involve changes in how the sensory representation is read out to form decisions. Little is known about the conditions that give rise to these different forms of plasticity or their implications
for perceptual abilities. We implemented a simple model that separates the sensory representation from
the population readout to gain insights into how perceptual decisions can be influenced by changes in
each process alone or interactions between the two. Specifically, we examined how changes in the tuning
curves of individual neurons and selective pooling of task-specific sensory information affect perceptual
performance in detection, coarse discrimination, fine discrimination and estimation tasks. We first derived
analytical solutions for the optimal readout for each task of a population of neurons with uniform Gaussian
tuning curves, Poisson variability and interneuronal correlations. For the detection task, the optimal readout selectively pools activity from neurons whose preferred directions are closest to the detection signal.
For coarse discrimination, the optimal readout generates ‘neuron’ and ‘anti-neuron’ pools with positive and
negative peaks centered at the discriminated stimuli. For fine discrimination, the optimal readout operates
as local differentiator whose peaks are away from the discriminated stimuli. For stimulus estimation, the
optimal readout is scaled linearly as a function of the parameter to estimate. Further, we solved analytically the optimal sizes of neural pools for detection, coarse and fine discriminations for different sensory
representations. The optimal pool size increases linearly with increased tuning widths, decreases as a
power-law function with increased neural noise and can depend substantially on the level of interneuronal
correlations. Once the readout is optimized for a particular sensory representation, our model suggests
that further changes to the representation can enhance or degrade performance in a task-specific manner.
Estimation is impaired by any change in the representation. Detection and coarse discrimination improve
with broadening of tuning curves, whereas fine discrimination improves with narrowing of tuning curves. We
created “perceptual-learning landscapes” for each task, summarizing the effects of changes in representation and readout on performance. These landscapes indicate which kinds of change are most effective for
improving performance under different conditions. For each task, the landscape depended substantially on
the level of neural background activity, which therefore helped to determine preferences for particular forms
of plasticity for perceptual learning. These insights are discussed in the context of experimental observations of changes in sensory representations for auditory and somatosensory tasks and changes in readout
for visual tasks during perceptual learning.
doi:10.3389/conf.neuro.06.2009.03.237

COSYNE 09

247

III-46

III-46. Multiple timescales of reward memory in lateral habenula and
midbrain dopamine neurons
Ethan Bromberg-Martin1,2
Masayuki Matsumoto2
Kae Nakamura3,4
Hiroyuki Nakahara5,6
Okihide Hikosaka2

BROMBERGE @ MAIL . NIH . GOV
MATSUMOTOM @ NEI . NIH . GOV
NAKAMKAE @ TAKII . KMU. AC. JP
HIRO @ BRAIN . RIKEN . JP
OH @ LSR . NEI . NIH . GOV

1

Brown University, Providence RI, USA
National Eye Institute, Bethesda MD, USA
3
Dept. Physiology, Kansai Medical Univ
4
PRESTO, Japan Science and Technology Agency
5
RIKEN Brain Science Institute
6
Tokyo Institute of Technology
2

How do we learn to predict rewards? One approach, embodied by current computational models of reinforcement learning, is to learn a ”value function” which maps each state of the world to its predicted future
yield of rewards. However, there is evidence [1,2] that the brain contains a multitude of neural elements
that learn about the world at different rates, some learning quickly and others learning slowly. If this was
true of the reward system, then reward-predicting neurons would not be bound to a single value function,
but instead could choose between multiple estimates of reward value, each computed by integrating past
evidence over a different timescale. Here we present direct evidence for this proposal. We trained monkeys
to perform a reward-biased saccade task in which the reward value of each trial was predictable based on
the past reward history [3,4]. We then analyzed single-neuron data [4] recorded from a major source of
reward predictive signals, midbrain dopamine neurons, and one of their main input structures, the lateral
habenula. Dopamine neurons behaved consistently with their role in signaling “reward prediction errors”,
carrying signals related to the trial’s predicted value in response to two task events: (1) a cue indicating the
start of the trial, and (2) a cue indicating the trial’s reward outcome. However, these two neural judgments
of the trial’s value were based on very different memories for the past. When the trial began, dopamine
neuron activity was influenced by only a single previous reward outcome. But when the trial’s outcome
was revealed, dopamine neurons suddenly improved their memory to reflect at least three previous reward
outcomes, close to the timescale of memory seen in animal behavior. Lateral habenula neurons showed
the same pattern, a short-timescale memory when the trial began and a long-timescale memory when the
outcome was revealed. In addition, many habenula neurons encoded the reward history in their level of
tonic activity. This allowed us to see that the timescale of memory developed gradually throughout the trial,
lengthening in anticipation of the reward-predictive cue, and then fading back to a one-trial memory during
the inter-trial interval. These findings suggest that reward-predicting neurons can switch between multiple
timescales of reward memory to suit the computational demands of the task at hand, making predictions
based on a short-timescale memory during lulls in the task, but switching to a more accurate long-timescale
memory when new reward information is imminent and the need for prediction is greatest. [1] Gläscher J,
Büchel C. Formal learning theory dissociates brain regions with different temporal integration. Neuron 47,
295-306 (2005) [2] Smith MA, Ghazizadeh A, Shadmehr R. Interacting adaptive processes with different
timescales underlie short-term motor learning. PLoS Biol. 4, e179 (2006) [3] Nakahara H, Itoh H, Kawagoe
R, Takikawa Y, Hikosaka O. Dopamine neurons can represent context-dependent prediction error. Neuron
41, 269-280 (2004) [4] Matsumoto M, Hikosaka O. Lateral habenula as a source of negative reward signals
in dopamine neurons. Nature 447, 1111-5 (2007)
doi:10.3389/conf.neuro.06.2009.03.047

248

COSYNE 09

III-47

III-47. A single cell with active conductance’s can learn timing and
multi-stability
Jeffrey Gavornik1
Harel Shouval2,3

GAVORNIK @ MAIL . UTEXAS . EDU
HAREL . SHOUVAL @ UTH . TMC. EDU

1

The University of Texas
Department of Neurobiolgy UT Houston
3
Department of Biomedical Engineering UT Austin
2

Recent experimental studies have demonstrated that neural responses in primary sensory cortex can significantly outlast the duration of the sensory input. In particular, a recent study by Shuler and Bear (2006)
has shown learned, reward timing dependent cortical activity in primary visual cortex, activity that represents the expected duration of a reward. We have shown how such activity can be represented and
learned in a network of interacting neurons (paper submitted). However, recent experimental results and
computational models (Egorov et al. 2002,Fransen et al. 2002,2006) have shown that single neurons with
active conductance’s can account for persistent activity, suggesting that they might also contribute to the
prolonged decaying activity observed in primary sensory cortex. Here, we first develop a simple model of
a neuron with active conductances which can in principle account for bi-stability, multi-stability and slowly
decaying activity. This model is based on voltage dependent calcium channels, and on calcium dependent
cationic channels, a model inspired by the previous experiments and computational studies. In contrast to
previous computational model this is a reduced single compartment model, which makes it more amenable
to analysis, while still maintaining a biophysical basis. Using simulations we show that this model, with
parameters chosen correctly, can exhibit bi-stability critically slowed neuronal dynamics as well as multistability. Using an analytical pseudo steady-state approach we reduce this model to a single non-linear
dynamical equation, and we show that the reduced model is in very close agreement with the full simulated
model. Using the reduced model we develop single parameter bifurcation diagrams of the model, and we
show that it is robustly bi-stable. The reduced model can account both for the phase transitions and for
the detailed dynamics in the falling phase of the sub-threshold regime. The reduced model also makes
it possible to intuitively understand the origin of bi-stability, multi-stability as well as critically slow dynamics. Next we introduce a reinforcement based plasticity rule for the internal conductance’s which critically
control the neuronal dynamics. We demonstrate that this learning rule can learn appropriate reward times,
and when these reward times are learned it produces neuronal dynamics that are similar to those observed
experimentally. We also show that the same learning rule, when applied at a different parameter regime
can learn a target firing rate for the ‘up’ state in a bi-stable system. Unlike our previous stochastic spiking
model, this single cell model is deterministic and produces periodic firing in the up state. This stand in
contrast to in-vivo experimental results in which the CV is typically &#8776;1 (Barbieri and Brunel, 2008).
However, these active conductance’s, when combined with the cortical stochastic background activity might
contribute to coding and learning of representations of time and of persistent activity levels.
doi:10.3389/conf.neuro.06.2009.03.254

COSYNE 09

249

III-48 – III-49

III-48. MI self- vs. instructed initiation of reaches during force-field
learning: A multi-electrode study
Jorge José1
Elizabeth Torres2
Kanuresh Ganguli
Jose Carmena
1
2

JJOSEV @ RESEARCH . BUFFALO. EDU
EBTORRES @ RCI . RUTGERS . EDU
KANURESH . GANGULY @ UCSF. EDU
CARMENA @ EECS . BERKELEY. EDU

SUNY at Buffalo
Rutgers University

Releasing “the brake” on a permanent motor memory (as in a ballistic reach) or suppressing a transient
motor memory (such as a recently acquired reaching force-strategy) may require in both cases inhibitory
patterns of neural activity. In awake-behaving primates it is not known what inhibitory role cells in the arm
region of the primary motor cortex (MI) may play in these situations. We have investigated, by using chronic
electrode implants in both brain hemispheres, the suppression question in the macaque MI region using a
delayed center-out reach task within the context of force-field learning in both hemispheres. The delayed
reach paradigm started with a center light to bring the hand to the initial position, a target cued the reach
location and a delay of 1000ms was followed by a GO cue to instruct the initiation of the reach. The actual
reach followed the GO signal with a holding period at the target of 500ms and then a juice reward. The
study had a block design A1-B-A2 of 25 trials (per target) in each block. Normal reaches A1 (null force-field)
were from the center to each of 8 targets cued at random and equally spaced on a circle. The B reaches
that followed were similar to A1 but performed under a force-field perturbation simulating a linear spring.
The earlier trials of the last block of normal reaches A2 required de-adaptation from the learning of the
previous force field block. To study the suppression/release of a ballistic reach (SUPP1) we gathered the
trials in which the subject initiated the reach before the GO signal (SELF-GO) and contrasted them with
the same temporal length of trials in which the subject waited for the GO signal (INSTRUCTED-GO). To
study the suppression of a recently acquired strategy in force-learning (SUPP2) we gathered in A2 earlier
trials where the subjects had to override the persistent transient memory of the force field resulting in overshutting after-effects. In both cases we tracked the same cells in suppression vs. force-learning. The
force-field learning evoked significant excitation across all cells of ipsi- and contra-lateral MI. The spikewidth distribution of these cells was not bimodal as reported in visual (V1, V4) and parietal (PRR) areas yet
we identified a sub-population of narrow-spiking cells (spike width <250microseconds) that in both ipsi- and
contra-lateral MI was significantly inhibited during SUPP1 and SUPP2 even though during B their excitation
was significant relative to A1. Our data suggest that across hemispheres MI is involved in the temporary
inhibition of a permanent or a transient motor memory according to the task demands.
doi:10.3389/conf.neuro.06.2009.03.126

III-49. Central sources for acoustic variation in birdsong
Samuel Sober
Michael Brainard

SAM @ PHY. UCSF. EDU
MSB @ PHY. UCSF. EDU

University of California, San Francisco
Birdsong is a learned behavior remarkable for its high degree of stereotypy. Nevertheless, adult birds display substantial rendition-by-rendition variation in the structure of individual song elements or ”syllables.”
Previous work suggests that some of this variation is actively generated by the avian basal ganglia circuitry
for purposes of motor exploration. However, it is unknown whether and how natural variations in premotor

250

COSYNE 09

III-50
activity drive variations in syllable structure. Here, we recorded from the song premotor nucleus RA (robust
nucleus of the arcopallium) in Bengalese finches and measured whether neural activity covaried with syllable structure across multiple renditions of individual syllables. We found that variations in premotor activity
were significantly correlated with variations in the acoustic features (pitch, amplitude, and spectral entropy)
of syllables in approximately a quarter of all cases. In these cases, variations in neural activity predicted
8.5 +/- 0.3% (mean +/- SE) of the behavioral variation, and in some cases accounted for 25% or more of
trial-by-trial variations in acoustic output. Our data allow us to distinguish between several competing models of how behavioral variability is driven by the population of RA neurons. The prevalence and strength
of neuron-behavior correlations indicate that variations in the measured acoustic features are driven by ensembles of a few thousand neurons that modulate their activity in a coordinated (correlated) manner, rather
than being driven by a population of independently varying neurons. Additionally, we found that correlations
with pitch (but not other features) were predominantly positive in sign, supporting a model of pitch production based on the anatomy and physiology of the vocal motor apparatus. Collectively, our results indicate
that trial-by-trial variations in spectral structure are indeed under central neural control at the level of RA,
consistent with the idea that such variation reflects motor exploration.
doi:10.3389/conf.neuro.06.2009.03.160

III-50. Irregular vs. Synchronized activity in Basal Ganglia Circuits
Choongseok Park
Robert Worth
Leonid Rubchinsky1,2
1
2

PARK @ MATH . IUPUI . EDU
RWORTH @ IUPUI . EDU
LEO @ MATH . IUPUI . EDU

Indiana Univ Purdue Univ Indianapolis (IUPUI)
Indiana University School of Medicine

The basal ganglia (BG) are a group of interconnected subcortical nuclei which are, among other things,
involved in neural control of movement. They become impaired in Parkinson’s disease (PD), characterized by akinetic behavior. Recent studies indicate that patterns of oscillatory synchronous activity in BG
are strongly relevant to BG physiology and BG disorders. In particular, neuronal activity in the beta-band
significantly contributes to akinetic symptoms. The dynamics of these oscillations, their mechanisms and
potential functional significance are the subjects of this study. Experimental data: we record neuronal activity during microelectrode-guided target localization in the PD patients undergoing surgery for implantation
of DBS electrodes. Extracellular spiking activity and LFPs are recorded simultaneously. Modeling: we also
use conductance-based models of subthalamic and pallidal cells to model rhythmic activity in subthalamopallidal circuits of BG. Model LFPs are constructed from model neuron’s activity. Data analysis: after
appropriate filtering, Hilbert phase is constructed for both signals. Short running window analysis (with
surrogates to determine statistical significance) is used to characterize temporal patterns of synchrony in
experiment and model. First return maps for the phases are also used to characterize synchronization and
compare model with experiment. The results of the analysis indicate that the dynamics of beta-band oscillations in BG is marked by intermittency of synchronized episodes. Oscillations tend to be desynchronized
for relatively short time, although the desynchronizing events are quite frequent. The model has large areas
in the parameter space, where it generates either irregular and uncorrelated firing patterns or rhythmic correlated activities. There is also an area of intermittent synchrony and, importantly, the first return maps for
the phases from model and experiment are very similar in that area. Thus the model’s phase space is organized similarly to those of experimental system during rest in PD. The domain of the existence of intermittent
dynamics is in between incoherent regime and synchronized regime, in the area which is characterized by
the presence of different types of dynamics. Simulation of noisy version of our model confirmed the robustness of the observed dynamics (and thus its relevance to experiment). These observations suggest that
in a pathological state, BG networks operate in a regime, which is quite close (in a parameter space) to

COSYNE 09

251

III-51
an irregular activity. Although we do not have microelectrode recordings form normal humans, vast body
of other experiments with animal models and humans suggest that healthy state is characterized by the
absence of beta-band oscillations at rest. BG still needs them for movement preparation. The organization
of the parameter space of the model and similarity of the model and experimental phase space indicate that
in a disease (Parkinsonism), BG circuits are relatively close to the presumably healthy uncorrelated state
and suggest that this healthy state is very close to the birth of oscillations. This closeness of the irregular
healthy state to the pathological regular synchronized state may be justified by the efficiency of producing
synchronized oscillations for movement generation. Dopaminergic degeneration in PD shifts the system
into a pathological synchronized activity.
doi:10.3389/conf.neuro.06.2009.03.228

III-51. Ramping activity is an inefficient estimator of time intervals
Juha Kesseli
Christian Machens

JUHA . KESSELI @ ENS . FR
CHRISTIAN . MACHENS @ ENS . FR

Ecole Normale Supérieure, Paris
In many areas of the brain, ramping activity or climbing activity, i.e., persistent neural activity that steadily
increases as a function of time, has been linked to the anticipation of forthcoming events. A particular
example is given by working memory tasks, in which a monkey memorizes a stimulus and then anticipates
another stimulus after a fixed time interval. During the delay period of such a task, the firing rates of many
neurons in the prefrontal cortex and elsewhere climb linearly with time. When the time interval between the
two events is doubled, the slope of the ramping activity adjusts to half the original slope [e.g. Brody et al,
2003, Cereb Cortex 13:1196-1207] so that the system reaches exactly the same state at the end of the delay
period as before. Based on these observations, ramping activity has been considered an internal estimate
of elapsed time. Experiments with rescaled delay periods do indeed suggest that the respective neural
systems strive to be in a particular, time-independent state at the end of the delay period. We therefore
investigated the hypothesis that ramping activity is a solution to the problem of ”being in the right state at the
right time”. In particular, we asked how the observed solution - ramping activity - compares with the optimal
solution to this problem in arbitrary neural networks. To study the problem with sufficient generality, we used
firing rate models whose dynamics are prescribed by abstract energy landscapes. We considered systems
with various types of noise (additive, multiplicative, correlated, etc.) All systems were constrained to start
at one point and reach a predefined target area after a certain time interval. Once the system reaches the
separatrix delimiting the target area, the delay period ends, and a learned response suitable to the task
follows. The optimal system was defined to be the one that would reach the target area with the highest
temporal precision in repeated runs. In one-dimensional systems with additive noise, we find that the
optimal solution is indeed ramping activity. Among all possible models, the ones that yield linearly growing
firing rates lead to the smallest timing error. However, when considering higher-dimensional systems, much
better solutions exist. In particular, constructing a suitable long funnel in an energy landscape with high
constant-velocity dynamics gives more and more accurate timing as the length of the path is increased. In
fact, the standard deviation of first passage times is inversely proportional to the velocity in this case. These
results hold for various types of noise and several other constraints such as energy consumption. For all
of these systems, ramping activity is among the worst of all possible solutions. Optimal representation of a
fixed time interval is thus not, in itself, a sufficient explanation for the existence of ramping activity. Other
considerations and constraints may be more important. For instance, the ability to quickly relearn time
intervals may be more influential in shaping the ramping activity solution than the accuracy of timing per se.
doi:10.3389/conf.neuro.06.2009.03.053

252

COSYNE 09

III-52 – III-53

III-52. Computing correlations analytically
David Barrett
Peter Latham

BARRETT @ GATSBY. UCL . AC. UK
PEL @ GATSBY. UCL . AC. UK

Gatsby Computational Neuroscience Unit, UCL
Neurons in the brain, especially nearby neurons, are often correlated, in the sense that activity in one set
of neurons partially predicts the activity in another. These correlations are of interest because they play
a major role in both representation and computations. Importantly, the precise role they play depends in
detail on the correlational structure (for a review, see Averbeck et al., 2006). It would, therefore, be highly
desirable to know what that structure is. Directly measuring it would seem desirable, but this is hard because the amount of data one needs is exponential in the size of the population (and large populations is
the regime of interest). The alternative is to compute the correlational structure as a function of connectivity
and single neuron properties. While this has the disadvantage that connectivity in the brain is not known, it
has the advantage that it can provide us with general principles about the relationship between connectivity
and correlations, and between correlations and computations. As a first pass at this problem, we consider a
very simple model consisting of McCullough-Pitts neurons receiving correlated input. Following Roudi and
Latham (2007), the connectivity consists of two components: strong, random background connectivity, and
weak structured connectivity. (Here ”strong” and ”weak” refer to scaling with the number of connections per
neuron: strong connectivity scales as 1/sqrt(K) and weak as 1/K, where K is the number of connections
per neuron). For this model, we are able to compute the full correlational structure analytically. Our main
finding is that the cross-correlation matrix between pairs of neurons is related, in a nontrivial way, to the
eigenvalue spectrum of the full connectivity matrix. This is reminiscent of the results found by Ginzburg and
Sompolinsky (1994) for weakly coupled stochastic networks, although at a quantitative level the relationship
is, of course, very different. An important corollary to this finding is that the random and structured components of the connectivity matrix make approximately equal contributions to the correlational structure, even
though the random background connectivity is sqrt(K) times stronger (30-100 times stronger for cortical
networks). This is because the largest eigenvalues of the two connectivity matrices are on the same order
(both are order(1)). The fact that both contribute rules out a simple relationship between connectivity and
correlational structure. This analytical calculation paves the way for several more interesting questions:
First, given an input that contains information about some quantity in the outside world (e.g., the orientation
of a bar), how does the information in the network about that quantity depend on network connectivity?
Second, and much harder, what are the tradeoffs in terms of computations? More precisely: are there
situations in which networks in the brain would reduce the ability of a network to retain information in order
to enhance its computational abilities? References: Averbeck, Latham, Pouget. Nature Reviews Neuroscience, 7:358-366, 2006. Roudi and Latham. PLoS Computational Biology, 3:1679-1700, 2007. Ginzburg
and Sompolinsky. Physical Review E, 50:3171-3191, 1994.
doi:10.3389/conf.neuro.06.2009.03.123

III-53. A biophysical model of cortical tuning and invariance operations using a transient population code
Jim Mutch1
Ulf Knoblich2
Tomaso Poggio1
1
2

JMUTCH @ MIT. EDU
KNOBLICH @ CSAIL . MIT. EDU
TP @ AI . MIT. EDU

Massachusetts Institute of Technology
McGovern Institute for Brain Research, MIT

COSYNE 09

253

III-54
Models of the computations underlying object recognition in the ventral visual pathway must satisfy many
anatomical and physiological constraints. It is well-known that both feature complexity and the receptive
field size of neurons increase at successively higher stages of processing. Category information appears in
inferotemporal cortex at around 110 ms after stimulus onset, and a recent study showed that most of the this
information is available in a brief window of spikes as narrow as 12.5 ms. We present a feedforward spiking
model that illustrates how tuning and invariance operations could be performed under these constraints
and that assigns a specific computational role to three common classes of neurons: pyramidal excitatory
neurons and regular and fast-spiking inhibitory interneurons. Requirements of speed and accuracy are met
by assuming that information is transmitted by the transient responses of synchronously firing populations
of cells – the populations being 2d planes of neurons that smoothly and redundantly encode their feature
spaces, and which are analogous to the cortical laminae. Large-scale simulations involving thousands
of neurons are made possible by utilizing a GPGPU framework. We show that the proposed tuning and
invariance operations are plausible under the biophysical constraints, and our results are consistent with a
normalization role for regular-spiking interneurons and a gain-control role for fast-spiking interneurons.
doi:10.3389/conf.neuro.06.2009.03.120

III-54. Towards fast in vivo neuronal imaging using objective coupled
planar illumination microscopy
Diwakar Turaga
Timothy Holy

TURAGAD @ MSNOTES . WUSTL . EDU
HOLY @ WUSTL . EDU

Washington University School of Medicine
Towards fast in vivo neuronal imaging using objective coupled planar illumination microscopy Diwakar
Turaga and Timothy E. Holy Dept. of Anatomy and Neurobiology, Washington University in St. Louis.
Abstract: Understanding neural circuits requires the ability to quickly and accurately measure activities
of large populations of neurons. Fluorescence microscopy, using calcium and voltage sensitive probes,
has allowed for measuring activities of neurons at such high spatial and temporal resolutions. However,
conventional fluorescence microscopy has its limitations: 1) Epifluorescence microscopy lacks the optical sectioning needed to image deep into tissue, and 2) Laser scanning microscopes (LSM, confocal and
multi-photon) are constrained by their fundamental trade-off between speed of acquisition, number of pixels
acquired and the signal-to-noise ratio. These limitations of conventional fluorescence microscopy can be
addressed using light sheet microscopy. In our implementation of light sheet microscopy – called Objective Coupled Planar Illumination (OCPI) microscopy – we rigidly couple the optics needed to form a light
sheet to the objective, illuminating just the focal place of the objective. We then place the objective on a
piezo manipulator, such that both light sheet and the objective can be rapidly translated to achieve three
dimensional scanning. To permit imaging of extended neural tissues, we tilt the objective (with the coupled
laser sheet) from the traditional face-on imaging to an angle of 45 degrees. This minimizes the extent to
which both the excitation light and the emitted light travels through the tissue. Using OCPI microscopy on
an ex vivo neural tissue preparation, we imaged neuronal activity at pixel rates 100 times faster than LSMs.
To allow for in vivo murine OCPI imaging, we made two improvements in the microscope design. First, we
miniaturized the objective coupler by using a uniaxial gradient-index lens to produce the light sheet. This
miniaturization allowed for the tilting of the objective from the previous 45 degrees angle to a more face-on
60 degree angle, permitting greater access to the neuronal tissue. Second, we showed that refractive index
mismatch between immersion fluid and tissue leads to significant defocus aberration. By introducing the
ability to tune the angle of the light sheet to correct this aberration, we obtained cellular resolution at depths
of 75 – 125 µm (depending on tissue) in a neuronal slice. As a proof of principle for OCPI microscope’s
ability for in vivo imaging, we imaged GAD65-GFP labeled olfactory bulb in an anesthetized mouse. We

254

COSYNE 09

III-55
opened a cranial window of size 2 x 4 mm over the olfactory bulb. A custom mount was placed over the
cranial window to permit the immersion of the GRIN lens and the objective in immersion fluid. We obtained
the first in vivo images using an OCPI microscope, clearly showing multiple cell bodies at various depths.
Thus we demonstrate that the enhanced OCPI microscope can be used to image neuronal populations in
an in vivo anesthetized mice preparations at pixel rates at least 100 times faster than LSMs.
doi:10.3389/conf.neuro.06.2009.03.091

III-55. Central roles of bipolar cells in retinal neural circuits.
Hiroki Asari1,2
Markus Meister1
1
2

ASARI @ FAS . HARVARD. EDU
MEISTER @ FAS . HARVARD. EDU

Harvard University
Center for Brain Science

One key step in visual processing is the transmission of signals from photoreceptors to retinal ganglion
cells by bipolar cells. There are about 10 types of bipolar cells in a vertebrate retina, and they form parallel
channels where each bipolar cell type carries a distinct type of visual information from the outer to inner
retina. The signals from these bipolar channels are then integrated by ganglion cells through intricate
interactions with about 30 types of amacrine cells. Specific combinations of bipolar and amacrine inputs
generate about 10 functional types of ganglion cells, and thus such interactions in the inner plexiform
layer are the most interesting but least understood processings in the retinal circuitry. By simultaneously
recording from multiple ganglion cells while manipulating the bipolar cell activity intracellularly, here we
explored how the signal from an individual bipolar cell is distributed to the various types of ganglion cells. We
found that injecting current into an individual bipolar cell elicited significant effects on the visual responses of
many ganglion cells. (1) The contribution of a single bipolar cell was generally excitatory at short distances
(<0.3 mm) and inhibitory at longer distances ( 0.5 mm). This is consistent with the presumed role of
amacrine cells as inhibitory interneurons. (2) Within the excitatory region, different ganglion cells showed
distinct response patterns. A sustained depolarization of the bipolar cell produced a transient burst of spikes
in some ganglion cells, but a sustained firing in others. Furthermore, some ganglion cells responded to the
bipolar cell stimulation in a linear fashion, whereas others showed a highly rectifying nonlinearity. These
results emphasize the diversity of neural circuits that distribute signals from the same bipolar cell to various
ganglion cells. Specifically, the distinction between transient and sustained response dynamics in ganglion
cells is not simply determined by what bipolar channels they receive, but in large part by differential circuitry
in the inner plexiform layer.
doi:10.3389/conf.neuro.06.2009.03.104

COSYNE 09

255

III-56

III-56. Detecting functional connectivity in networks of phase-coupled
neural oscillators
Alexander Huth1
Charles Cadieu1
Corby Dale2,3
Darren Weber2
Dimitrios Pantazis4
Felix Darvas5
Richard Leahy4
Gregory Simpson2
Kilian Koepsell1

ALEX . HUTH @ BERKELEY. EDU
CADIEU @ BERKELEY. EDU
CORBY. DALE @ RADIOLOGY. UCSF. EDU
DARREN . WEBER @ RADIOLOGY. UCSF. EDU
PANTAZIS @ SIPI . USC. EDU
FDARVAS @ U. WASHINGTON . EDU
LEAHY @ SIPI . USC. EDU
GREG . SIMPSON @ RADIOLOGY. UCSF. EDU
KILIAN @ BERKELEY. EDU

1

University of California, Berkeley
University of California, San Francisco
3
NCIRE/SF Veterans’ Affairs Medical Center
4
University of Southern California
5
University of Washington
2

It is well accepted that the brain processes information using networks of coupled but distributed regions.
When such networks form and dissolve contingent on behavior or task they are called functional networks.
Communication and processing within these functional networks are thought to be mediated by neural oscillations. Thus to understand functional networks neuroscientists must be able to detect the presence of
and changes in coupled neural oscillations during behaviorally relevant tasks. Previous efforts to model
neural oscillations and infer network coupling from measurements have been hindered by a lack of statistical models of multivariate phase variables. Many efforts have focused on reproducing the phenomenology
of oscillating neural networks by qualitatively imitating the phase dependencies observed in neurophysiological data. It has been difficult, however, to fit these models to experimental data. In many situations
researchers have attempted to infer network coupling from measurements, but they have been forced to
use bivariate measures such as phase correlations or spectral coherence. These methods do not capture
the full dependency structure between multiple phase variables and can produce spurious couplings. In
this work, we leverage a recent model of multivariate phase distributions and algorithm for the estimation
of its parameters [1]. The model has two primary advantages over previous work: 1) it infers the couplings
between oscillators from measurements and is not susceptible to the same spurious couplings as bivariate
analysis techniques, and 2) it effectively estimates the distribution for a large number of dimensions. Here
we extend this statistical model and estimation technique to the case of time varying coupling parameters.
Our previous work considered only a static model of phase coupling. However, functional networks are likely
to be dynamic and to depend on behavioral conditions. Therefore we introduce an estimation procedure
for inferring network coupling in a conditional manner. We also introduce measures of significance based
on the bootstrap method. We apply this technique to simulated data of phase-coupled oscillators where
the ground truth of the underlying coupling is known. We show that our technique is able to recover both
the strength and the phase offset of all pairwise couplings and to track their changes over time more accurately than simple phase correlation. In particular, we are able to detect transient changes in the coupling
parameters reflecting changes in functional connectivity. Finally, we apply our analysis technique to data
recorded using Magnetoencephalography during a cued covert spatial attention task. We are able to track
dynamic functional networks among brain regions involved in multiple sub-second cognitive processes. Using our measure of statistical significance we are able to recover sparsely connected functional networks
that are time and cue dependent. Furthermore, we can use our technique to distinguish different mental
states corresponding to different experimental conditions in a behaviorally relevant task. The techniques
introduced here can be used in any context in which multiple phase variables are measured and thus it
will be a valuable tool in the analysis of neurophysiological data. [1] C.F. Cadieu and K. Koepsell (2008) A

256

COSYNE 09

III-57
multivariate phase distribution and its estimation. arXiv:0809.4291v1
doi:10.3389/conf.neuro.06.2009.03.258

III-57. Decoding center-surround interactions in population of neurons for the ocular following response
Laurent Perrinet1
Nicole Voges2
Jens Kremkow1
Masson Guillaume
1
2

LAURENT. PERRINET @ INCM . CNRS - MRS . FR
NICOLE . VOGES @ INCM . CNRS - MRS . FR
JENS . KREMKOW @ INCM . CNRS - MRS . FR
MASSON @ INCM . CNRS - MRS . FR

INCM / CNRS – Univ. de la Méditerranée
INCM / CNRS – Univ. Aix-Marseille

Short presentation of a large moving pattern elicits an Ocular Following Response (OFR) that exhibits
many of the properties attributed to low-level motion processing such as spatial and temporal integration,
contrast gain control and divisive interaction between competing motions. In fact, the machinery behind the
visual perception of motion and the subsequent sensorimotor transformation is confronted to uncertainties
which are efficiently resolved in the primate’s visual system. We may understand this response as an ideal
observer in a probabilistic framework by using Bayesian theory (Weiss et al., 2002) and we extended in the
dynamical domain the ideal observer model to simulate the spatial integration of the different local motion
cues within a probabilistic representation. We proved that this model is successfully adapted to model the
OFR for the different experiments reported (Perrinet and Masson, 2007), that is for different levels of noise
with full field gratings, with disks of various sizes and also for the effect of a flickering surround. However,
another ad hoc inhibitory mechanism dependent on the frequency of the grating has to be added in this
model to account for suppressive effects of the surround. We explore here an hypothesis where this could
be understood as the effect of a recurrent prediction of information in the velocity map. In fact, in previous
models, the integration step assumes independence of the local information while it is obvious that local
information in natural scenes is, due to the rigidity and inertia of physical objects in visual space, highly
predictable. We implement this in a realistic model of a layer representing velocities in a map of cortical
columns, where predictions are implemented by lateral interactions within the cortical area. We observe at
convergence that, as may be predicted, the connection patterns form patchy connections which connect in
preference columns with congruous velocities. We apply this model tuned over a set of natural scenes and
measure the response when presenting gratings of increasing sizes. We observe a lower global read-out
of the ocular response dependent on the frequency of the grating which may be interpreted as a divisive
inhibition. This result confirms the previous model and the Ratio-Of-Gaussian model (Perrinet and Masson,
2007) but relates it to a neural implementation with a functional role of predicting local information. This
model may be applied to algorithms for the efficient representation of video sequences since it gives a
Partial Derivate Equation (PDE) for an optimal dynamical evolution of the representation for natural scenes.
doi:10.3389/conf.neuro.06.2009.03.266

COSYNE 09

257

III-58

III-58. Better luck next time: How prior behavioral outcomes influence
network state in the frontal cortex
Nathaniel Smith1,2
Nandakumar Narayanan3,2
Mark Laubach4

NJSMITH 34@ GMAIL . COM
KUMAR . NARAYANAN @ YALE . EDU
MARK . LAUBACH @ GMAIL . COM

1

Yale University School of Medicine
Interdepartmental Neuroscience Program
3
The John B Pierce Laboratory
4
Yale University
2

Medial areas of the frontal cortex are crucial for the proactive control of action (Narayanan and Laubach,
2006; Boulinguez et al., 2008). Neurons in these cortical areas fire persistently during delay periods
in primates (Niki and Watanabe, 1979) and rodents (Narayanan and Laubach, 2006; Cowen and McNaughton, 2007). These neurons fire differently during correct and incorrect responding (Laubach et al.,
2000; Narayanan et al., 2005; Narayanan and Laubach, 2006). Most recently, we have observed that neurons in the dorsomedial prefrontal cortex (dmPFC), but not the motor cortex (MC), fire persistently through
the inter-trial interval following incorrect responding (Narayanan and Laubach, 2008). Such neurons may be
involved in error monitoring, performance adjustment, and/or evaluation of trial outcome. As we have previously shown, disruptions of activity in dmPFC lead to alterations in delay period activity in MC (Narayanan
and Laubach, 2006). Therefore, it is possible that error-sensitive neurons in dmPFC alter the level of network activity in MC in a dynamic manner based on the animal’s prior success in the task. To examine this
issue, we measured “noise correlations” (correlated variability) for pairs of neurons recorded in dmPFC and
MC during a simple reaction time task. In the task, rats pressed on a lever over a delay period of 1 sec,
until an auditory stimulus was presented; then, rats released the lever and earned a liquid reward if the
reaction time was less than 0.6 sec. Noise correlation was measured for pairs of neurons by summing
spikes in a time-window of 500 ms that was stepped in increments of 50 ms for the period from1 sec before
the start of the trial to 1 sec after the presentation of the stimulus. We found that noise correlation occurred
at different times during the trials. Correlations between neurons in dmPFC were highest at the start of the
trial. Correlations between neurons in MC were highest during the delay period. More importantly, noise
correlations were greater for trials that were preceded by errors compared to trials that were preceded
by correct responses. To investigate whether noise correlations reflected changes in network activity, we
analyzed local field potentials (LFPs) recorded at the same time as the spike data. We observed strong
oscillations between 4 and 10 Hz in dmPFC at the start of trials, especially on trials that were preceded by
errors. These oscillations occurred at the same time as when neurons in dmPFC showed increased noise
correlations following errors. To investigate how neurons fired in relation to the 4-10 Hz oscillations of the
LFPs, we measured spike-field coherence and observed coupling between spikes and fields in the 4-10 Hz
range. Together, these results suggest that there is a change of network state following an error and this
may enable improved task performance on the subsequent trial.
doi:10.3389/conf.neuro.06.2009.03.143

258

COSYNE 09

III-59 – III-60

III-59. Avalanches in simple stochastic spiking network models.
Edward Wallace
Marc Benayoun
Wim van Drongelen
Jack Cowan

EWALLACE @ UCHICAGO. EDU
MARCB @ UCHICAGO. EDU
WVANDRON @ PEDS . BSD. UCHICAGO. EDU
COWAN @ MATH . UCHICAGO. EDU

University of Chicago
Neuronal avalanches are synchronous aperiodic bursts of spiking activity that generally follow a power law
distribution in size. Such avalanches have been observed in a variety of neural networks in vitro and in vivo.
It is important to investigate what gives rise to them and why they are such a widespread feature of spontaneously active nervous tissue. We address these questions using Cowan’s continuous-time model of spiking
networks, the simplest version comprises coupled 2-state Markov processes, each representing a neuron.
The networks studied have populations of thousands of neurons, 80% excitatory and 20% inhibitory, with
random homogenous connectivity at different levels of sparseness. A comparatively novel feature of our
simulation is the use of the Gillespie algorithm at the network level, to provide an exact stochastic simulation of coupled neurons in continuous time. We find that when the overall ratio of inhibition to excitation (I:E)
is too low, the neurons fire as independent Poisson processes. At a threshold in the I:E ratio, a transition
to synchronous activity occurs suggestive of a bifurcation. Above this threshold, spikes are organized into
avalanches which follow a power law distribution in both size and duration. The degree of synchrony in the
network, measured by the coefficient of variation, grows smoothly with the I:E ratio above the threshold, as
the mean firing rate falls smoothly. In other words, as the network fires more sparsely, spikes tend more to
be grouped temporally into avalanches, as inhibition is increased relative to excitation. The study indicates
that avalanches are a network property, which may arise from very simple elements connected in unstructured ways. Our model produces synchronous dynamics and neuronal avalanches despite lacking many
of the features proposed to account for these properties. There is no synaptic depression, nor synaptic
plasticity or learning of any kind generating synchrony here. Neither is there any small-world or power law
structure in the network connectivity to account for the power law distributions observed in network firing
patterns. It appears that a whole region in the network parameter space, sensitive only to the bulk statistics of connectivity and excitability, supports synchronous firing grouped into avalanches. In particular, the
avalanches appear above a threshold, rather than only appearing at some critical parameter values – there
is no self-organized criticality here either. While the observed power law behaviour is qualitatively robust,
the value of the power law exponents varies with the I:E ratio as well as with time bin size and other choices
made in analyzing the data. For example, there is no unique way to determine the power law exponent of
the avalanche size distribution. This variability is also observed in experiments, and poses a challenge for
theorists.
doi:10.3389/conf.neuro.06.2009.03.339

III-60. How the optic nerve allocates space, energy capacity and information
Janos Perge
Kristin Koch
Robert Miller
Peter Sterling
Vijay Balasubramanian

JPERGE @ RETINA . ANATOMY. UPENN . EDU
KRISTINHENS @ GMAIL . COM
RFM @ UMN . EDU
PETER @ RETINA . ANATOMY. UPENN . EDU
VIJAY @ PHYSICS . UPENN . EDU

University of Pennsylvania

COSYNE 09

259

III-61
Axons in the mammalian central nervous system exhibit a broad range of diameters. The thinnest, arising
from smallest neurons, are 0.2µm, approaching the limit set by channel noise. The thickest, arising from
the largest neurons, reach 20µm. In many tracts the axon diameters are highly skewed–thin axons are
numerous and thick ones are rare. This distribution, a basic feature of brain structure, has never been
explained. The standard explanation for why some axons are thick is that higher conduction velocities are
needed to reduce conduction times. For example, the Ia fiber, the afferent limb of a long feedback loop,
conducts impulses from muscle spindles up to 120 m/s. Were it generally true that conduction time sets
axon diameter (Wang et al., 2008), thick fibers might predominate in long tracts and thin fibers in short ones,
but this is not so. Further, in many tracts the gain in conduction time for thick fibers is less that the spike
timing jitter. Even if increased conduction velocity could explain why certain axons are thick, it does not
explain why mostly they are thin. One need is to conserve space: fiber volume increases with the square of
the radius, so the thickest axon occupies the same volume as 10,000 of the thinnest. Therefore, a brain with
mostly thin axons can make more connections (Wen&Chklovskii, 2005). Another need is to save energy:
action potentials consume 35% of the cortical energy budget (Attwell&Laughlin, 2001). So, if thick axons
used disproportionately more energy, this would be a reason to use them sparingly. Thus we quantified
how space, energy capacity, and firing rate are matched in a central tract, the optic nerve. We studied
guinea pig, for which spike rates to naturalistic stimuli were known for a large population of cell types. We
found that astrocytes use nearly 30% of the space and more than half of the mitochondria, establishing
their significance for the brain’s budgets. Axons are mostly thin, the distribution peaking at 0.7µm (near the
limit set by channel noise) with a long tail toward larger diameters that require 10-fold more space. Energy
capacity (mitochondrial volume) rises quadratically with diameter, by 25-fold across the span 0.5-1.5µm
(95% of the fibers). Finally, the distribution of mean firing rates matches the distribution of axon calibers.
This implies a law of diminishing returns: twice the rate requires more than twice the space and energy
capacity, thus constraining the optic nerve to send most information at low rates over fine axons. Optimizing
the energy capacity per spike predicts the most common axon diameter and firing rate. Since thicker optic
axons reduce spike travel time to cortex by less than the timing jitter, their real purpose may be to supply
larger terminal arbors that support higher information transfer rates. Attwell, D., and Laughlin, S.B. (2001).
J Cereb Blood Flow Metab 21, 1133-1145. Wang, S.S.H., et al. (2008). J Neurosci 28, 4047-4056. Wen,
Q., and Chklovskii, D.B. (2005). PLoS Comput Biol 1, e78.
doi:10.3389/conf.neuro.06.2009.03.164

III-61. A neural circuit to read out the temporal population code
Andre Luvizotto
Cesar Renno-Costa
Paul F. M. J. Verschure

ALUVIZOTTO @ IUA . UPF. EDU
CCOSTA @ IUA . UPF. EDU
PAUL . VERSCHURE @ IUA . UPF. EDU

SPECS - Universitat Pompeu Fabra
The microcircuits of the cerebral cortex appear to implement a rather uniform circuit template or canonical
microcircuit that is characterized by dense local and sparse long-range connectivity. In earlier work we
have proposed that dense excitatory local connectivity plays a specific role in the rapid and robust transformation of spatial stimulus information into a, so called, temporal population code[4]. We have shown that
this temporal population code or TPC provides for a high-capacity encoding and can generalize to realistic
tasks such as the generation of place cells. Since this proposal a number of physiological observations in
the mammalian visual [1] and auditory cortex have provided direct support for the notion that the temporal
dynamics of the population response in primary sensory areas can serve as a substrate for stimulus encoding. If the TPC plays a role in encoding global stimulus features in a compact temporal representation
it is relevant to understand what its key coding features are and how these features can be decoded. Here
we present a biologically-plausible neural circuit able to read out the temporal population code into a non-

260

COSYNE 09

III-62 – III-63
redundant and dense representation based on the, so called, wavelet decomposition [2]. First we perform
a systematic investigation among the main orthogonal wavelet families - Daubechies, Symlet and Coiflet in order to determine the more suitable basis for the signal decomposition. As a classification criterion we
maximized the classification performance while minimizing the required data. The results are evaluated using a correct classification ratio (CCR) [3]. We evaluated the CCR of our model using standardized stimulus
sets used in previous studies. We found that a wavelet based decoding could improve the CCR by 13%
while reducing the required data with a factor of 9 as compared to the linear classification metric used in
previous studies. Subsequently we show that this compact wavelet description can be directly captured in a
readout network of excitatory and inhibitory neurons that transforms the TPC into a local representation of
the different stimulus classes. Hence, these results show that the TPC is constructed around a small number of coding components that can be well described by wavelets and that the same description lends itself
well for a direct neuronal implementation. Our study suggests that sensory processing hierarchies might
well consist of sequential operations where spatio-temporal transformations at lower levels form compact
encodings of global stimulus features that transmitted to higher levels using minimal bandwidth and that are
decoded through wavelet-like filters at these higher levels. Acknowledgments We thank Armin Duff, Andrea
Giovannucci and Marti Sanchez for helpful discussions. References [1] Andrea Benucci, Robert A Frazor,
and Matteo Carandini. Standing waves and traveling waves distinguish two circuits in visual cortex. Neuron,
55(1):103–117, 2007. [2] Stéephane Mallat. A Wavelet Tour of Signal Processing. Academic Press, 1998.
[3] J Victor and K Purpura. Estimation of information in neuronal responses. Trends Neurosci, 22(12):543,
1999. [4] Reto Wyss, Peter Konig, and Paul F M J Verschure. Invariant representations of visual patterns in
a temporal population code. PNAS, 100(1):324–329, 2003.
doi:10.3389/conf.neuro.06.2009.03.202

III-62. Withdrawn
III-63. Correlation transfer in neuronal populations
Jianfu Ma
Kresimir Josic
Aditya Barua
Robert Rosenbaum
Fabien Marpeau

MAJF @ MATH . UH . EDU
JOSIC @ MATH . UH . EDU
ADITYA @ MATH . UH . EDU
ROBERTR @ MATH . UH . EDU
MARPEU @ MATH . UH . EDU

University of Houston
Correlated activity in neural tissue can significantly impact the information carried by a population of neurons. However, there are relatively few analytical results that provide a mechanistic understanding of how
correlations are generated and propagated. We start by examining this question using the integrate and
fire model which has inspired many developments in theoretical neuroscience. In the second part of the
presentation we demonstrate these results using new numerical methods for the simulation of networks of
stochastic integrate and fire neurons. These methods are several orders of magnitude faster than typical
Monte Carlo simulations. Outputs of a population of neurons are typically pooled to form the input to cells
downstream. We provide simple analytical results that describe this situation and show that correlations can
be propagated in a counterintuitive manner: Small correlations in the population can translate into large input correlations after such pooling. On the other hand, an increase of correlations within a populations can
decrease correlations between populations. It has recently been observed that the transfer of correlations
from input currents to output spike trains depends on the firing rate in neuron models and experiments in
vitro. Over rapid time scales, correlation transfer increases with both spike time variability and rate; the

COSYNE 09

261

III-64
dependence on variability disappears at large time scales. We show that the behavior of the perfect integrate and fire (PIF) model is quite different: correlations are transferred perfectly over large windows. We
give a full description of correlation transfer in PIFs, and provide an intuitive understanding of how crosscorrelograms are transformed in networks of such neurons. Although the PIF preserves correlations over
long time windows, it tends to “smear out” the cross-correlogram, due to thresholding.This analysis shows
that the decorrelation typically observed in a feedforward configuration of more realistic neurons results
both from the existence of a firing threshold and “memory loss” induced by the leak. Therefore, the peculiarity of the PIF model can be used to provide an intuitive understanding of the behavior of more complex
models. To examine when these results apply to more complex models requires numerical simulations. We
present a fast and accurate finite volume method to approximate the solution of the Fokker-Planck equation
that models the multivariate density of the subthreshold voltages of stochastic integrate and fire neurons.
The discretization of the boundary conditions offers a particular challenge, as standard operator splitting
approaches cannot be applied without modification. In comparison to Monte Carlo methods, the present
approach offers improved accuracy, and decreases computation times by several orders of magnitude.
doi:10.3389/conf.neuro.06.2009.03.311

III-64. Robust resolution of neuronal population dynamics on a single
trial basis
Kai Miller
Bharathi Jagadeesh
Adam Hebb
Jeffrey Ojemann
Rajesh Rao

KJMILLER @ GMAIL . COM
BJAG @ U. WASHINGTON . EDU
AOHEBB @ AOH . CA
JOJEMANN @ U. WASHINGTON . EDU
RAO @ CS . WASHINGTON . EDU

University of Washington
Electrocorticographic recording allows a measure of population scale ( 500,000 neurons) activity from the
brain surface of humans. While the dynamics of the response of single neurons, or small populations of
neurons, to category specific stimuli have been examined, the dynamics of entire populations in response
to single stimuli have not. We sought to characterize the response of entire neuronal populations, in this
setting, using two categories of visual stimuli (faces and houses) with 6 human subjects. To do this, we
implemented a decoupling process that extracts broadband, power-law (noise-like), population activity from
the power spectral density of individual electrodes. On a trial-by-trial basis, the projections of this decoupling process to individual stimulus presentations are extremely robust, and are appropriately classified with
nearly perfect accuracy (mean of 98%, range 92-100%). From the projection of face stimuli, house stimuli,
and inter-stimulus interval periods in specific electrodes, category specific areas are robustly segregated
from one another. The spatial distribution is consistent across subjects, with face-specific regions consistently lateral to house-specific regions, a cortical representation similar to that characterized by fMRI. These
broadband changes are projected to the dynamic spectrum to obtain a continuous timeseries (C1(t)), and
their dynamics are explored. Based on previous work, we propose that this represents a strong estimate
of neuronal population activity on a 10-20ms timescale. The time-dependence of this activity is highly
conserved, with a mean latency (across 6 subjects) of 210ms for faces (in an appropriate electrode) and
223ms for houses (in an appropriate electrode). While this activity is specific for single presentations of
different categories, in different brain regions, the population response is more exaggerated for face area
activity. Our results, illustrated in the attached supplement, demonstrate that different classes of stimuli
can be resolved from one another on a single-trial basis using broadband spectral change. Furthermore, a
continuous extrapolation of this change, in different areas, is specific for different classes of stimuli, so that
variable latency and duration of cortical activity in category-specific response can be quantified on a single

262

COSYNE 09

III-65 – III-66
trial basis.
doi:10.3389/conf.neuro.06.2009.03.297

III-65. Change-based inference in attractor nets: Linear analysis
Reza Moazzezi
Peter Dayan

REMO @ GATSBY. UCL . AC. UK
DAYAN @ GATSBY. UCL . AC. UK

Gatsby Computational Neuroscience Unit, UCL
A conventional view of information processing by line (manifold) attractor networks holds that they represent processed information by the identity, and/or location (on a null-stable manifold), of the attractor state
to which they converge [1,2]. Subsequently, a readout mechanism (which we call attractor-based readout)
performs decoding by identifying the converged state of the network. Although this method has been successfully applied to a variety of tasks, including orientation estimation, cue integration and decision making,
there is little evidence for attractor states in cortical networks. Neurons in sensory cortical areas rarely
exhibit persistent activity in natural environments, and the firing rates of most apparently persistently-active
neurons in prefrontal cortical areas also change systematically over time. We have recently suggested a
new computational view of attractor networks, which involves reading information from the early portion
of the trajectory of their states as they evolve towards their attractors. This change-based readout makes
decisions based on the way a statistic of the state of the network changes over time [3]. We showed
that this method can perform nearly as well as an ideal observer in a model visual hyperacuity task, and
demonstrated its additional computational benefits such as the possibility of automatic invariance to certain
irrelevant input dimensions. Here we provide theoretical and empirical evidence that links change-based
and attractor-based inference. We design a network that performs two tasks: a discrimination task (solved
by change-based readout) that involves deciding if a low contrast target bar is to the left or right of a high
contrast carrier bar, and an estimation task (solved by attractor-based readout) that involves determining
the location of the carrier in the absence of the target. Both tasks and the network are designed such that
linearization is an excellent approximation. We first show that a necessary condition for the network to be
near optimal for discrimination via change-based readout is for attractor-based readout to be sub-optimal
for estimation and, through the linearization, show how the network performs the two tasks. Then, we show
that although the network is sub-optimal at both tasks, it is nevertheless near optimal in both cases. References 1. Zhang, K. Representation of spatial orientation by the intrinsic dynamics of the head-direction cell
ensemble: A theory. Journal of Neuroscience 16, 2112-2126 (1996) 2. Seung, H. S. How the brain keeps
the eyes still. Proceedings of the National Academy of Sciences USA 93, 13339-44 (1996) 3. Moazzezi, R.,
Dayan, P. Change-based inference for invariant discrimination. Network: Computation in Neural Systems
19, 236-252 (2008)
doi:10.3389/conf.neuro.06.2009.03.020

III-66. Bayesian Population Decoding of Spiking Neurons
Sebastian Gerwinn
Jakob Macke
Matthias Bethge

SGERWINN @ TUEBINGEN . MPG . DE
JAKOB @ TUEBINGEN . MPG . DE
MBETHGE @ TUEBINGEN . MPG . DE

MPI for Biological Cybernetics

COSYNE 09

263

III-67
Sequences of action potentials are believed to be the basis for information transmission in populations of
nerve cells. In these sequences, information about sensory stimuli, behavioral variables or endogenous
signals is encoded. Many sensory inputs change continuously in time and have variations across a large
range of different time scales. Action potentials, however, are discrete events in time, and depend on the
stimulus in a stochastic manner. Most often, simple linear summations over spikes such as peri-stimulus
time histograms or linear filtering schemes are used as read-out of spiking neural populations. How information about continuous, time-varying stimuli can be extracted from the temporal information encoded in
the sequence of interspike intervals, however, is not well understood. In this work we present an approximate Bayesian decoding scheme for extracting information about a stimulus—or other continuous-valued
inputs—from spike trains. We use this decoding rule to analyse in how far encoding parameters affect the
reconstruction and how to adapt them to the stimulus statistics in a favorable way. We describe the noisy
generation of spikes in the neural population in response to a continuous stimuli using a probabilistic neuron
model. Specifically, we use a leaky integrate-and-fire neuron model which is driven by a Gaussian process.
The neural noise is modeled by a stochastic threshold which is set to a new random value after each spike.
The description of spike generation as a renewal process is included in this model class as a special case
in the limit of an infinitely large time constant. In particular, when the exponential distribution is chosen
for the threshold noise, the spike generation process equals an inhomogeneous Poisson process. Thus,
it is straightforward to gradually adjust the reliability of the spike generation process in this model class.
Thus, we have specified a probabilistic model for both the stimulus ensemble and the neural encoding.
This allows us to decode the stimulus from the neural responses in a Bayesian framework. In particular,
we seek to calculate the posterior probability distribution over the stimulus given an observed sequence of
spikes. Calculating the posterior in high dimensions is a challenging problem for which we are applying
approximate inference techniques. Our results show that the approximate Bayesian decoding algorithm is
able to reconstruct a time varying stimulus. We also investigate the scaling of the reconstruction error with
increasing number of observations. The Bayesian decoding algorithm does not only estimate the posterior
mean, but also the posterior variance, a measure of residual uncertainty over the stimulus. Such uncertainty
information is not available to simple regression decoders. Having access to the posterior uncertainty also
enables one to ask questions about the reliability and robustness of the neural code. Additionally, it allows
one to ask the question of whether encoding parameters are tuned to the statistics of the input in a manner
that reduces the residual uncertainty about the stimulus.
doi:10.3389/conf.neuro.06.2009.03.026

III-67. Contrast responses of P-cells and V1 neurons are optimized for
a winner-takes-all encoding
Robert Martin1,2
Yoav Tadmor3
Klaus Obermayer1,2

RM @ CS . TU - BERLIN . DE
YOAV. TADMOR @ NCL . AC. UK
OBY @ CS . TU - BERLIN . DE

1

Berlin Institute of Technology
Bernstein Center for Comp Neurosci Berlin
3
Newcastle University
2

The principle of histogram equalization/infomax predicts neuronal response properties of contrast sensitive
neurons from the distribution of contrasts in natural images. This has been successfully demonstrated
for the fly large monopolar cell [1], cat X- and Y-retinal ganglion cells (RGC) and LGN neurons, cat V1
neurons, and macaque retinal and LGN M-cells [2,3]. However, responses of macaque P-RGC and -LGN
neurons and V1 neurons are less sensitive to contrast than histogram equalization predicts. Here, we
determine the distribution of contrasts RGC, LGN neurons and V1 neurons encounter in natural images.

264

COSYNE 09

III-68
Using 70 Difference-of-Gaussian (DoG) and 80 Gabor contrast operators, both biologically plausible and
representative of the range of macaque neurons in LGN and V1, we sample a set of calibrated greyscale natural images [4]. The full contrast distribution (each position sampled by all contrast operators)
reconfirms the above-cited results regarding histogram equalization. However, sampling each location only
with the contrast operator that shows the strongest response at this location (for each, DoG and Gabor
operators separately) produces a contrast distribution that predicts the contrast response function of both,
macaque P- and V1 neurons. We then compare the performance of the histogram equalization-based
(referred to as “M-like”) encoding strategy with that of a top-response-based (“P-like”) strategy, for the
stages of LGN contrast coding and of V1 contrast coding. Using both, mutual information and rate distortion
theory we find that for each individual neuron, M-like encoding outperforms P-like encoding with respect
to the information conveyed about natural image contrast. However, considering the mutual information
between neurons with similar receptive field characteristics (e.g., neighboring preferred spatial frequencies)
shows that P-like encoding is accompanied by a pronounced decrease in the mutual information between
different output channels. Taken together these findings suggests that macaque P- and V1 neurons employ
a contrast coding strategy intrinsically different from M-cells and cat visual neurons. Unlike the latter, P and
V1 contrast responses are not optimized for maximizing the contrast information transmitted by individual
cells, but also to reduce the redundancy between different transmission channels, thus providing a trade-off
between the desire of maximizing the transmitted information and reducing the redundancy at the population
level. In doing so, their contrast response is in fact optimized for encoding the contrast of spatially optimal
features. Such a strategy would imply a Winner-Takes-All-like encoding to operate at the readout stage. And
indeed, we can also show that the lower contrast sensitivity of the P-like encoding aids any winner-takes-all
mechanism: For both, LGN and V1, the ratio between highest and second-highest response in the P-like
encoding is substantially larger than in the M-like encoding. [1] Laughlin, 1981 Zeitschrift für Naturforschung
36: 910 – 912 [2] Tadmor and Tolhurst, 2000 Vision Research 40: 3145 – 3157 [3] Clatworthy et al, 2003
Vision Research 43: 1983 – 2001 [4] van Hateren and van der Schaaf, 1998 Proceedings of the Royal
Society 265: 359-366
doi:10.3389/conf.neuro.06.2009.03.076

III-68. Sampling and Optimal Cue Combination during Bistable Perception
Ruben Moreno-Bote
David Knill
Alex Pouget

RMORENO @ BCS . ROCHESTER . EDU
KNILL @ CVS . ROCHESTER . EDU
ALEX @ CVS . ROCHESTER . ED

University of Rochester
When looking at a moving ball, psychophysics experiments suggest that we represent all directions, along
with their probabilities, even though we appear to perceive only one direction. This type of probabilistic
representations allows behavior to be optimal given the uncertainty inherent to our sensory stimuli. But if
the brain represents multiple interpretations in the form of probability distributions, why do we get a sense
of unitary perception? Is this a mere illusion or the result of an active process, such as sampling, that
collapses probability distributions onto single interpretations? In this study, we take advantage of bistable
perception and cue combination to explore the possibility that perception is the result of a sampling process.
The Necker cube is a well known example of a bistable percept. In this case, perception is unitary (only
one interpretation of the Necker cube is perceived at any given time), but it is bistable, in the sense that the
interpretation changes from time to time. The source of this bistability is not fully understood, although it has
been proposed to arise from a sampling process of a bimodal probability distribution which peaks sharply at
the two interpretations of the stimulus. To test this idea, we employed a stimulus made of two superimposed

COSYNE 09

265

III-69
drifting gratings, which is ambiguous as to which grating is moving in front and which one is moving behind.
By varying the spatial frequency, the speed or disparity of one grating, it is possible to bias in a gradual
manner the fraction of time that a particular grating is perceived as being in front. We asked subjects to
report the spontaneous alternations in depth ordering over a one minute period, and measured the fraction
of dominance time f for each percept. The key question is how the perceived depth ordering changes
when two cues (e.g. spatial frequency and speed) are varied simultaneously. If the brain combines the two
cues optimally, the probability distribution over depth ordering given both cues should be the product of the
probability distributions given each cue separately. Moreover, if the brain samples this posterior distribution
(or any arbitrary power of it), then the fraction of dominance of a particular depth ordering when two cues
are manipulated simultaneously should follow f1,2 = f1 f2 / ( f1 f2 + (1- f1) (1- f2) ), where f1 (resp. f2) is the
fraction of dominance of one particular depth ordering when cue 1 (resp. cue 2) is manipulated alone We
report that the experimental confirms this prediction, and cannot be described by other candidate models,
such a cue vetoing or “stronger-cue-take-all” strategies. Finally, we show that a simple neural model is able
to account for this behavior as long as neurons simply sum their inputs. Interestingly, a simple sum is also
the combination rule that ensures optimal cue combination given the statistics of spike trains in the cortex.
Therefore, bistable perception appears to be the result of a sampling process of the posterior distributions
obtained from optimal cue integration.
doi:10.3389/conf.neuro.06.2009.03.098

III-69. Hierarchical novelty-familiarity representation in the visual cortex
Boris Vladimirskiy
Walter Senn
Robert Urbanczik

VLADIMIR @ CIMS . NYU. EDU
SENN @ PYL . UNIBE . CH
URBANCZIK @ PYL . UNIBE . CH

University of Bern
Olshausen and Field (1996, 1997) and Rao and Ballard (1999) have shown that the statistics of natural
images can explain some receptive field (RF) properties of simple cells in V1. However, only small patches
of images were used, with many cells reading out multiple statistics from each RF. Furthermore, no attempt
was made to evaluate how well the read-outs thus obtained encoded the stimuli. Predictive coding used by
Rao and Ballard is an instance of a wider class of generative Bayesian models that has been suggested to
serve as an organizing principle for the entire cortical hierarchy (Lee and Mumford, 2003; Friston, 2005).
Thus, we investigate here how good predictive coding actually is at coding whole images using a natural
topographic connectivity, where no two cells have exactly the same RF, but adjacent cells’ RF’s have strong
overlaps. In predictive coding, feedback from higher areas carries expectations of lower-level activity (familiarity signal), whereas the feedforward (novelty) signals carry discrepancies between the expectations and
the stimuli. Visual recognition becomes an iterative process relaxing to a solution that matches experience
with sensory input. We use two interconnected populations of neurons, one coding for the familiarity, and
the other, for novelty. These form one of multiple levels in a hierarchical processing structure. In contrast
to Rao and Ballard, in our model top-down effects are confined to a single level. We suggest that this
limited feedback is preferable for biologically compatible fast recognition. We use 1000 natural images (Van
Hateren and van der Schaaf, 1998) for unsupervised learning of the synaptic efficacies of two visual processing levels. The coding performance is then evaluated on a set of 200 different images from the same
database by reconstructing the stimulus based on the internal code in our generative model and directly
comparing the prediction to the actual image. Despite a compression factor of 4 for each level, the image
reconstruction quality on the test images is quite good and strongly exceeds that of local averaging, implying
that the learning results in the extraction of features characteristic of the set of natural images as a whole.

266

COSYNE 09

III-70
With our model, we have found that the extra-classical RF effect of endstopping can arise due to the topographic connectivity from interactions within the first processing level without the need for feedback from the
next level, as in Rao and Ballard. Furthermore, the effective RF’s after learning resemble those of simple
cells in V1 and learning with the topographic map leads to a global organization of the RF’s. Finally, the
proposed architecture allows for the simultaneous, but separate, representation of familiarity and novelty in
the visual cortex. The novelty signal could produce read-out to higher processing centers, activating them if
a localized novel signal, such as a predator in a serene environment, suddenly appears in a familiar image.
Thus, our implementation of predictive coding could be an effective way for the visual system to combine
fast hierarchical visual representation with the interaction with higher information-processing areas in the
brain.
doi:10.3389/conf.neuro.06.2009.03.096

III-70. A sparse coding model with “imperfect” feed-forward circuitry
William Coulter
Friedrich Sommer

WCOULTER @ BERKELEY. EDU
FSOMMER @ BERKELEY. EDU

University of California Berkeley
Although simple receptive fields are approximately outlined by a small set of highly specific geniculocortical
afferents (Reid and Alonso, 1995, Alonso et al. 2001), the feed-forward wiring is often only an imperfect
match of the receptive field, suggesting that neural response properties are also shaped by intracortical
processing. While existing sparse coding network models successfully explain the emergence of simple
receptive fields, they predict that receptive fields are shaped almost exclusively by feed-forward connections,
e.g., (Olshausen and Field, 1996, Rehn and Sommer, 2007). Here we ask if models principled by efficient
coding can reproduce response properties from real neurons even though the feedforward weights in the
model match the measured receptive fields only imperfectly. We present a novel neural network model for
sparse coding based on compressed sensing, a computational principle recently developed in engineering
for data compression (Candès and Romberg, 2006). The model exhibits smooth and more realistic simple
cell receptive fields than its ‘imperfect’ feed-forward connections would predict, compare the two panel of
Fig. 1 of the supplement. Thus, the model demonstrates that efficient coding schemes exist in which
feedback connections are critical for shaping the receptive fields. We use simulation experiments with our
model to characterize deviations between feed-forward circuitry and measured receptive fields. Alonso
JM, Usrey WM, Reid RC (2001) Rules of Connectivity between Geniculate Cells and Simple Cells in Cat
Primary Visual Cortex. J. Neurosci. 21(11):4002-4015. Candès E and Romberg J (2006) Quantitative
robust uncertainty principles and optimally sparse decompositions. Foundations of Comput. Math. 6(2),
pp. 227 – 254. Olshausen BA and Field DJ (1996) Emergence of Simple-Cell Receptive Field Properties
by Learning a Sparse Code for Natural Images. Nature, 381: 607-609. Reid RC and Alonso JM (1995)
Specificity of monosynaptic connections from thalamus to visual cortex. Nature 378:281-284. Rehn M and
Sommer FT (2007) A network that uses few active neurones to code visual input predicts the diverse shapes
of cortical receptive fields. J. Comp. Neurosci. 22 (2) 135-146.
doi:10.3389/conf.neuro.06.2009.03.340

COSYNE 09

267

III-71

III-71. Comparison between decoding algorithms in open-loop and
closed-loop performance
Shinsuke Koyama1
Steven Chase
Andrew Whitford2
Meel Velliste2
Andrew Schwartz
Robert Kass1
1
2

KOYAMA @ STAT. CMU. EDU
SCHASE @ ANDREW. CMU. EDU
ASW 35@ PITT. EDU
MEV 3@ PITT. EDU
ABS 21+@ PITT. EDU
KASS @ STAT. CMU. EDU

Carnegie Mellon University
University of Pittsburgh

Recent developments in experimental technology allow us to record neural activity from ensembles of motor
cortical neurons in real time. When coupled with an appropriate decoder, the activity of these neurons can
be used to establish a brain-computer interface, and directly drive the motion of, for example, a cursor on
a computer screen. Many decoding algorithms have been proposed for this purpose; choices range from
the simple population vector algorithm (PVA) through the optimal linear estimator (OLE) to various versions
of Bayesian decoders. The PVA characterizes each neuron’s activity by preferred direction and performs
optimally when the tuning functions are linear and the set of preferred directions are uniformly distributed.
To improve performance when the set of preferred directions is not uniformly distributed, Salinas and Abbott
(1998) suggested the OLE, which corresponds to least-squares estimation. The PVA may be considered a
special case of least-squares estimation when the preferred directions are uniformly distributed. Bayesian
decoders make use of a fully probabilistic model comprising 1) an observation model, which describes how
the observed neural activity relates to the time-evolving signal of interest, and 2) a state model, which describes how the signal itself evolves from one time step to the next. Bayesian decoders typically provide
the most accurate off-line trajectory reconstructions. However, since Bayesian and linear decoders differ in
several respects, it is not known which factors are critical for improving device performance. Furthermore,
it is not necessarily true that improvements (or deficits) in off-line reconstruction will translate into improvements (or deficits) in on-line control, as the subject might compensate for the specifics of the decoder in
use at the time. In this study, we assess the performance of three decoders (the PVA, OLE, and the LGF,
a type of Bayesian decoder) in three separate regimes: off-line simulated data, off-line reconstruction of
experimental data, and on-line in a brain-control task. By comparing the performance of the three decoders
under each regime, it is possible to attribute performance differences to particular modeling assumptions.
Specifically, we can assess which of the following four algorithmic differences has the largest effect on control: the assumption of uniformity in the set of preferred directions, the assumption of linearity in the tuning
curves, assumptions about constant spike count variance, or differences in the way the cursor trajectories
are smoothed. In off-line reconstruction, we find that assumptions about uniformity in the set of preferred
directions and the way the cursor trajectories are smoothed have the most impact on decoder performance;
surprisingly, assumptions about tuning curve linearity and spike count variance play relatively minor roles.
In on-line control, subjects compensate for directional biases caused by non-uniformly distributed preferred
directions, leaving cursor smoothing differences as the largest single algorithmic difference driving decoder
performance. These results may provide a helpful interpretation of when the off-line performance of a
certain decoder will translate to on-line control.
doi:10.3389/conf.neuro.06.2009.03.150

268

COSYNE 09

III-72

III-72. Correlated physiological and perceptual effects of noise in a
tactile stimulus
Armin Lak1
Ehsan Arabzadeh2
Justin Harris
Mathew Diamond1
1
2

LAK @ SISSA . IT
EHSAN @ UNSW. EDU. AU
JUSTINH @ PSYCH . USYD. EDU. AU
DIAMOND @ SISSA . IT

International School for Advanced Studies
UNSW

Comparing neuronal firing parameters with the perceptual report for the same stimuli can be a powerful
strategy for uncovering the brain’s encoding and decoding mechanisms. The work reported here utilizes
that strategy, but with the twist that the neuronal activity is recorded from the whisker-region of rat cortex (barrel cortex) while the perceptual report comes from human subjects. The stimuli in both cases are
trains of skin deflections. If the predictions made from the barrel cortex data set are borne out in tests
of human tactile perception, we can propose general coding principles that extend across species. We
first investigated the responses of neurons in anesthetized rat barrel cortex to trains of whisker deflection,
where each train had a periodic temporal structure but was characterized by either a constant-amplitude
across all deflections or else variable amplitude (“amplitude noise”). Stimulus trains containing amplitude
noise were matched with constant-amplitude trains for mean amplitude but included deflections smaller and
larger than the mean. Cortical extracellular responses – collected from the middle layers – were equivalent for constant and noisy stimulus trains with frequencies up to 10 Hz. Above 10 Hz, amplitude noise
led to a larger response magnitude as well as more prominent firing synchrony. The effects of amplitude
noise are thus remarkably similar to those reported earlier for temporal noise: stimulus trains with an irregular sequence of interdeflection intervals evoke a larger and more synchronous cortical response than
do frequency-matched periodic stimulus trains (Lak et al., Cerebral Cortex, 2008). The Romo laboratory
has shown that in primates the perceived intensity of a train of deflections is correlated with somatosensory cortical firing rate. We predicted therefore that the presence of frequency and amplitude noise would
increase response magnitude in human subjects, and that this would lead to an increase in perceived intensity, as compared to matched constant-amplitude or constant-frequency stimuli. We tested this prediction in
a series of psychophysical experiments. For each subject, the threshold for detection of amplitude and frequency noise was estimated using a 2-interval forced choice (2AFC) noise detection task. A sub-threshold
level of noise was used in the subsequent task. In a standard staircase procedure, subjects were asked to
discriminate the intensity of pairs of vibrations delivered to their fingertip in a 2AFC paradigm in which one
of the vibrations always had either amplitude or frequency noise while the other vibration did not. The key
finding is that, although subjects were unaware of the noise, its presence still caused them to overestimate
vibration intensity. Psychophysical effects were comparable – in the magnitude and direction of perceptual bias – with those predicted if the physiological effects of stimulus noise seen in barrel cortex were to
generalize directly to humans. The main conclusions are that neurons in rat barrel cortex are “tuned” to
respond in a different way to stimulus trains characterized by temporal and amplitude unpredictability, and
that physiological phenomena from barrel cortex can be directly transferred to human tactile system to test
hypotheses of perceptual mechanisms.
doi:10.3389/conf.neuro.06.2009.03.278

COSYNE 09

269

III-73 – III-74

III-73. Correlation-rate relation enhances information throughput in
layer networks of spiking neurons
Brent Doiron1
Jaime de la Rocha2
Eric Shea-Brown3
Kresimir Josic4
Alex Reyes

BDOIRON @ PITT. EDU
JAIME . DELAROCHA @ RUTGERS . EDU
ETSB @ WASHINGTON . EDU
JOSIC @ MATH . UH . EDU
REYES @ CNS . NYU. EDU

1

University of Pittsburgh
Rutgers University
3
University of Washington
4
University of Houston
2

Determining the impact of correlated activity on coding in neural ensembles is a central challenge for systems neuroscience. On the one hand, correlated activity produces redundancy across a population, limiting
the overall coding capacity of a neural ensemble (Zohary et al., 1994). On the other hand, correlated activity of a pre-synaptic population increases the transfer gain to upstream neurons (Salinas and Sejnowski,
2001). Making use of the beneficial aspects of correlation, while mitigating the deleterious side, is a difficult task. Recently, we have shown in a very general setting that pairwise spike train correlation increase
with firing rate (de la Rocha et al., 2007). We present a combination of in vitro experiments and modeling
that demonstrate how the correlation-rate relation drastically enhances the information throughput across
a two-layer feedforward network of spiking neurons. Specifically, we study a balanced excitatory/inhibitory
projection from layer one to layer two. The transfer between layers is fluctuation driven, where synchrony in
the first layer primarily drives the response in the second layer. This architecture and dynamic are commonly
observed in thalmo-cortical and cortico-cortical systems. Using both model networks and dynamic-clamp
in vitro experiments we show that when correlation co-varies with firing rate the layer two response gain
can be large with only moderate correlation between layer one neurons. The combination of high gain and
moderate correlation increases the information transfer (Fisher) across layers compared to the case when
correlation and rate do not co-vary with one another. Further, while correlation between neurons in layer
one always decreases the information coded by layer one, the information coded by layer two is maximized
at a nonzero correlation in layer one. In sum, the spike transfer nonlinearity which creates a correlationrate relationship (de la Rocha et al., 2007) and layer two sensitivity to synchronous inputs (Salinas and
Sejnowski, 2001) combine to create an effective coding scheme in layered networks. Funding: NSF, NIH,
Burroughs Welcome. Zohary, Shadlen, Newsome; Nature 370: 140-143, 1994. Salinas and Sejnowski;
Nat. Rev. Neurosci. 2: 539-550, 2001. de la Rocha, Doiron, Shea-Brown, Josic, Reyes; Nature 448:
802-805, 2007.
doi:10.3389/conf.neuro.06.2009.03.184

III-74. Map Dynamics for Rhythmically Perturbed Model Neurons
Jan Engelbrecht
Kristen Loncich
Renato Mirollo

JAN @ BC. EDU
LONCICH @ BC. EDU
MIROLLO @ BC. EDU

Boston College
We explore universal characteristics of how small perturbing rhythms modify a model neuron’s firing pattern.
It is well known that a Hodgkin-Huxley model fires periodically when stimulated with a constant current,

270

COSYNE 09

III-75
corresponding to a very attractive limit cycle. We consider such a model of a class-1 neuron and show that
upon introducing a small perturbing current the dynamics rapidly collapses onto a 2-dimensional subspace
in which the neuron’s state may subsequently evolve more slowly and where successive spike times are
related by a 1-dimensional map t n+1 = F(t n). While such a description has been studied in detail for
integrate-and-fire models, it is surprising that more complicated gating-variable models simplify to such a
1-d description and furthermore quite suggestive how quickly this map dynamics develop for rather modest
rhythms. The expected entrainment (phase-locking) of the spike times to the rhythm is described in terms of
a steady evolution to stable fixed points of the map. Outside the entrainment regions, bottleneck phenomena
describe repeated phase skip/advance evolution of spike times. We further demonstrate the stability of this
picture to the introduction of noise and show how the perturbing rhythm’s influence increases with rhythm
period. We argue that characterizing neuron spikes in terms of a return map should also hold for pyramidal
cells subject to periodic stimuli and may be readily explored in a whole-cell slice recording setting.
doi:10.3389/conf.neuro.06.2009.03.250

III-75. Modeling adaptation in the auditory cortex to causally link neural synchrony to tinnitus
Michael Chrostowski
Suzanna Becker
Ian Bruce

CHROSTM @ MCMASTER . CA
BECKER @ MCMASTER . CA
IBRUCE @ IEEE . ORG

McMaster University
Tinnitus, the phantom perception of sound in the absence of an external stimulus, is widely thought to persist because of lasting changes in neural activity arising in the auditory cortex after hearing loss. Masking
sounds presented for a set duration and then turned off, can reduce the severity of the tinnitus percept temporarily, a phenomenon known as residual inhibition (RI) [1]. The underlying mechanism for RI is not known;
understanding it could provide insight into how changes in neural activity in the auditory cortex lead to persistent tinnitus. We propose that RI may be caused by long-term adaptation in the primary auditory cortex.
We have developed a network-level spiking model of the primary auditory cortex that includes tonotopically
arranged excitatory and inhibitory neural units. The hearing loss region is represented as one where input
firing rates are decreased. Our model includes a homeostatic plasticity mechanism that strengthens the
weights on excitatory connections to the deafferented neurons. The novel contribution here is the addition
of an adaptation mechanism to account for RI. The transient decrease in tinnitus perception that occurs
after masker stimulus presentation is accounted for by adaptation that works on timescales as long as
tens of seconds and has a similar effect on firing rates as has been found experimentally [2]. We simulate experimental findings regarding appropriate stimulus bandwidth and duration for RI in order to validate
our model. Additionally, spontaneous firing rates and synchrony in neural firing before and after stimulus
presentation are analyzed to shed light on their potential causal link to tinnitus. Our simulations indicate
that long-term adaptation in excitatory neurons of the primary auditory cortex can lead to a suppression in
spontaneous firing rates that has a similar recovery timescale (15-45 seconds) as that of masker-induced
tinnitus suppression. Furthermore, cross-correlation values between neural units in the hearing loss region
are noticeably lower (average of 26.2%) 10 seconds after a 30-second masker stimulus is simulated. Before
the stimulus, our model shows enhanced neural synchrony in the hearing loss region, which has been found
empirically [3]. Thus, the masker presentation reduces firing synchrony to levels that are seen in the normal
hearing regions of our model. Studies where an auditory masker was shown to temporarily suppress the
perception of tinnitus indicate that neural activity underlying the tinnitus percept is being transiently affected
by the masker stimulus. We show that long-term adaptation is a plausible candidate mechanism underlying
this temporary suppression. Furthermore, our simulations show that, with the presence of adaptation, the

COSYNE 09

271

III-76
enhanced neural synchrony seen with induced hearing loss is temporarily reduced by an appropriate auditory masker. This supports the argument that there is a causal link between enhanced neural synchrony
and the perception of tinnitus. References [1] Roberts, L.E., Moffat, G., Bosnyak, D.J. (2007). Acta OtoLaryngologica, 126:27-33. [2] Gourevitch, B. & Eggermont, J.J. (2008). European Journal of Neuroscience,
27:3310-3321. [3] Seki, S. & Eggermont, J.J. (2003). Hearing Research, 180:28-38.
doi:10.3389/conf.neuro.06.2009.03.241

III-76. The influence of theta (4-8Hz) structure in vocalizations on neural activity in auditory cortex
Hjalmar Turesson
Chandramouli Chandrasekaran
Asif Ghazanfar

TURESSON @ PRINCETON . EDU
CHANDRAM @ PRINCETON . EDU
ASIFG @ PRINCETON . EDU

Princeton University
Animal vocalizations are modulated both spectrally and temporally. The spectral content has the capacity
to carry indexical cues such as the identity, gender, size and age of the signaler. For example, primates
can use formant frequencies in species-specific vocalizations to estimate the body size of a conspecific.
Temporal modulations of the spectral envelope are predominantly slow, at rates less than 10Hz, but their
functional significance (if any) is less clear. We measured the power spectrum of the slow amplitude envelope of different macaque vocalizations and found them modulated in a 4-8Hz band. This suggested
to us that the 4-8Hz amplitude modulation was matched to the 4-8Hz theta band that is often seen in the
local field potential (LFP) throughout the neocortex. What, if any, is the influence of these slow temporal
modulations in macaque vocalizations on neural activity in the primary auditory cortex? We hypothesized
that the vocalizations’ 4-8Hz envelope modulations improve processing of the spectral structure. To test
this, we exploited the fact that macaques spontaneously discriminate between vocalizations whose amplitude envelopes are identical, but whose spectral structure, in the form of formant frequencies, are different.
Through low-pass filtering of the temporal envelopes with cutoffs at 3 and 10 Hz, we could test what influence the acoustic theta band had on neural responses to vocalizations with different formant frequencies.
This produced three conditions: unfiltered, 10Hz low-pass filtered and 3Hz low-pass filtered. Neural responses were recorded from primary auditory cortex in behaving monkeys. Classification based on spiking
activity was done using receiver operating characteristics (ROC) analysis. Additionally, we analyzed the
phase and analytic amplitude in different frequency bands of the LFP in order to examine how the vocalization’s theta envelope may influence neural activity. We found that classifier performance was significantly
worse under the 3Hz low-pass filtered condition than under the unfiltered and 10Hz low-pass filtered conditions. This suggests that the slow temporal modulations have a functional significance and its significance
is that it allows optimal processing of the spectral envelope. Analysis of the theta band in the LFP showed
that the distribution of phase values is clearly non-uniform with a population Rayleigh statistic at 0.5. A shift
from uniform to non-uniform distribution of phases across trials suggests that the phase of the theta oscillation is reset by the vocalization. This provides a means by which the vocalization can influence ongoing
neural activity. In the gamma band, we found that its power is strongly modulated by the phase of theta
oscillations. Together, these findings show that vocalizations reset the phase of the theta oscillation which
in turn influences the power of gamma activity. This then influences how spiking activity represents the
spectral structure of vocalizations.
doi:10.3389/conf.neuro.06.2009.03.071

272

COSYNE 09

III-77 – III-78

III-77. Control of single neuron activity by global network dynamics in
auditory cortex
Carina Curto1,2
Shuzo Sakata3
Stephan Marguet
Kenneth D. Harris

CURTO @ COURANT. NYU. EDU
SHUZO. SAKATA @ GMAIL . COM
STEPHAN @ MARGUET. BE
KDHARRIS @ ANDROMEDA . RUTGERS . EDU

1

Courant Institute, NYU
CMBN, Rutgers
3
Cntr for Molecul and Behav Neurosci, Rutgers
2

The electrical activity of neurons reflects an interplay of sensory input and the brain’s own internal dynamics. In sensory cortices, neuronal firing correlates well with characteristics of sensory stimuli; on the other
hand, each cortical neuron receives the vast majority of its inputs from other cortical neurons, suggesting
that spiking is largely controlled by the activity of the local network. This viewpoint is supported by simultaneous recordings of multiple cells, which show synchronized patterns of spontaneous activity beyond
those imposed by the structure of the sensory stimulus. In prior work, we found that the global dynamics
of population activity in rat auditory cortex is well approximated by a low-dimensional self-exciting system
model, and that the variations in population dynamics with cortical state can be captured by changes in
the parameters of this model. Here we investigate the relationship of electrophysiologically identified single
neurons to global population dynamics in this context. In the model, population activity is characterized
by a pair of ’mean field’ variables, v and w, that reflect the average population firing rate and integrated
recent past activity, respectively. To predict the spike times of individual neurons from population activity,
we use an approach analogous to the computation of hippocampal ”place fields”, by predicting the mean
firing probability of each neuron as a function of position in the v-w plane, which we term the ’activity field.’
We find that activity fields for different neurons are quite localized in distinct regions of the v-w plane. We
conclude that, while the mean activity of a column is well approximated by a simple low-dimensional dynamical system, individual neurons show widely different relationships to this dynamics. For any neuron,
activity fields computed from spontaneous and sensory-evoked activity were similar, further suggesting that
the structure of sensory responses reflects the same network dynamics that shape cortical spontaneous
activity. This work is supported by NIH Grants MH073245 and R01DC009947. CC was also supported by
a Courant Instructorship.
doi:10.3389/conf.neuro.06.2009.03.152

III-78. The predictive power of spectrotemporal receptive fields at the
single spike train level.
Joseph Schumacher
David Schneider
Sarah Woolley

JS 2880@ COLUMBIA . EDU
DMS 2159@ COLUMBIA . EDU
SW 2277@ COLUMBIA . EDU

Columbia University
Auditory neurons encode the acoustic features of complex sounds according to tuning properties that make
them particularly responsive to specific spectral and temporal cues. Reverse correlation models of tuning
such as spectrotemporal receptive fields (STRFs) estimate the spectral and temporal tuning properties of
neurons from responses to complex sounds. The accuracy of a STRF in capturing the response properties
of a neuron is assessed by using the STRF to predict responses to novel sounds and comparing those

COSYNE 09

273

III-79
predictions to peristimulus time histograms (PSTHs) of the actual responses. A PSTH averages the neural
response over multiple trials and has lower temporal precision than individual spikes. Because an animal
can identify a complex sound after hearing it only once, neural responses at the single spike train level, as
opposed to averaged over many trials, may be highly relevant to perception. Therefore, accurate models
of neural tuning should predict neural responses to a sound at the single spike train level. We asked how
accurately STRFs predict individual responses to complex communication sounds. Songbirds reliably discriminate among the unique songs of individual birds. Songbird auditory midbrain neurons reliably produce
spike trains that share precise temporal structure across trials. These neurons encode spectrotemporal
features of songs, and their STRFs predict responses at the PSTH level well. We tested the accuracy of
STRFs in predicting neural responses at the single spike train level. We recorded the responses of single
auditory midbrain neurons in male zebra finches to song from 20 other males. We modeled single spike
trains from those songs using a linear-nonlinear-Poisson cascade model (LNP). Using an inhomogeneous
Poisson process, simulated spike trains were generated based on a STRF convolved with a song spectrogram. Simulated spike trains share temporal structure with actual spike trains, but are less reliable across
trials and are therefore less temporally precise. We quantified the temporal precision of spike trains across
the actual and simulated trials, and between actual and simulated spike trains from the same songs. The
k-means neurometric applies the k-means clustering algorithm to Euclidean distances between spike trains
as defined in the van Rossum spike timing metric. Spike trains were sorted into k groups, where k is the
number of unique stimuli used to generate spike trains. Accuracy in discriminating among songs was calculated as the percentage of spike trains that cluster into the appropriate group. As a population, actual
responses of MLd neurons to songs demonstrate highly accurate k-means discriminability. STRF-based
LNP simulations typically fail to capture the stimulus discriminating capacity of actual neurons. This is
largely due to the temporal imprecision generated by the STRF-based model. Simulated spike train clusters were more scattered than actual spike trains, and clusters were generally closer under the simulated
condition. The deficiency of the STRF-based model may be due in part to an inability to resolve rapid
temporal features of the stimulus. We propose methods for supplementing and improving the STRF-based
LNP model, such as incorporating parameters that capture the neural response to amplitude modulations
within a stimulus.
doi:10.3389/conf.neuro.06.2009.03.245

III-79. Direction-dependence of inter- and intra-vibrissa adaptation in
rat primary somatosensory cortex
Ulf Knoblich1,2
Christopher Moore2
1
2

KNOBLICH @ CSAIL . MIT. EDU
CIM @ MIT. EDU

McGovern Institute for Brain Research
Massachusetts Institute of Technology

Sensory adaptation has been reported across multiple modalities and species, however its mechanisms
are still poorly understood. Primary somatosensory cortical neurons in rats respond to stimulation of their
primary vibrissa and several surround vibrissae. In addition, these neurons exhibit direction preferences.
Preliminary data indicate that this direction tuning becomes more pronounced with increased stimulus frequency. We performed whole-cell in vivo recordings in layers 2/3 and 4 of rat primary somatosensory cortex
while stimulating the primary and up to two surround vibrissae in multiple directions at varying frequencies.
Our results indicate that intra-vibrissa adaptation is strong and direction-dependent, i.e. if the same vibrissa is deflected repeatedly, the suppression of the subsequent stimulus response depends strongly on
the directional similarity between the two stimuli. In contrast, inter-vibrissa suppression depends on the
strength of the preceding stimulus response but does not show directional dependence. Intracellular in

274

COSYNE 09

III-80
vivo recording provides unique access to the synaptic currents underlying these apparently distinct forms
of tuning modulation, providing substantial constraints on the functional connectivity of rodent primary somatosensory cortex. We are combining this data with realistic biophysical modeling to delineate underlying
biological mechanisms, and to examine the computational impact of these distinct dynamics.
doi:10.3389/conf.neuro.06.2009.03.003

III-80. Fast and dynamic odor encoding in rat olfactory cortex during
odor discrimination task
Keiji Miura1,2
Zachary Mainen3,4
Naoshige Uchida2

KMIURA @ MCB . HARVARD. EDU
ZACH . IGC @ GMAIL . COM
UCHIDA @ MCB . HARVARD. EDU

1

Japan Science and Technology Agency
Harvard University
3
Champalimaud Neuroscience Programme
4
Instituto Gulbenkian de Ciencia
2

Active movement of sensory organs plays an important role in many sensory modalities. In olfaction, animals sniffing frequency shifts dynamically depending on animals’ behavioral states or task demands. Previous behavioral studies from our group showed that rats can achieve highly accurate odor discrimination
with a single sniff, suggesting that sniffing is a unit of odor encoding (Uchida and Mainen, 2003). However,
very little is known about how active sniffing affects odor encoding by neurons, because most neurophysiological studies in mammalian olfaction have been performed in anesthetized animals. In this study, we
wished to address how odor representations develop over sniff cycles and how these representations relate
to behavioral performance. We focused on the olfactory cortex because it is closer to behavioral output
than the olfactory bulb and therefore we expected that it may be easier to establish direct links between behavioral performance and neuronal activity. We recorded neural activity from individual cells in the anterior
piriform cortex of rats using a multi-electrode recording technique while they performed a two-alternative
odor discrimination task. The rats were trained to sample an odor at a central odor port and to go left or
right to a choice port to obtain water rewards. In each session, at least six odors were used, with half
of them assigned to either the left or right reward port. Simultaneously, respiration was monitored using a
temperature sensor implanted in the nostril. Olfactory cortical neurons showed robust and transient odor responses with specific latencies that were tightly locked to sniff cycles. Many neurons showed odor-specific
adaptation or facilitation at the second sniff. We then examined how the ensemble activity evolved across
sniffs. Using principal component analysis, it could be seen that ensemble activity followed odor specific
trajectories. Interestingly, the ensemble activity in the first sniff was significantly different from that in the
second sniff. This contrasts with the highly similar trajectories repeated over multiple sniff cycles in previous
studies in anesthetized animals. We next quantified information content within each sniff cycle by asking
how well an ideal observer can discriminate six odors based on the neural activity. To do this we used
several different pattern classification methods including support vector machines. This analysis showed
that the discriminability of odors developed very rapidly within the first sniff and that the second sniff provided less information. Using the combined spikes during the first and second sniffs did not improve odor
discriminability significantly compared to using only spike during the first sniff. In summary, an informative
odor representation in the olfactory cortex of behaving rats appears to develop very rapidly within a single
sniff and is not substantially augmented odor representation in the second sniff. These observations may
help to explain the behavioral observation that multiple sniffs in the odor sampling port does not result in
increased discrimination performance. This study suggests that olfactory processing during active sniffing
favors rapid processing over prolonged temporal integration during an odor discrimination task. Uchida, N

COSYNE 09

275

III-81
and Mainen, Z.F. (2003) Nature Neurosci. 6:1224-9.
doi:10.3389/conf.neuro.06.2009.03.079

III-81. Radial orientation and direction biases in the response of human visual cortex to kinetic contours
Colin Clifford
Damien Mannion
Scott McDonald

COLINC @ PSYCH . USYD. EDU. AU
DAMIENM @ PSYCH . USYD. EDU. AU
SCOTTM @ PSYCH . USYD. EDU. AU

School of Psychology, University of Sydney
It has recently been observed using fMRI that radial orientations produce higher activity in retinotopic regions of visual cortex (Sasaki, Y. et al. (2006) Neuron 51, 661-670). Here, we report analogous biases for:
(1) the axis of motion (horizontal or vertical) of unoriented textures; (2) the orientation of “kinetic” contours
defined solely by the borders between strips of texture moving in opposite directions. fMRI at 3T was used
to measure the BOLD signal (1.5mm cubic voxels; TR = 3s.) in the visual cortex of six human subjects
viewing kinetic contour stimuli presented in 15-second blocks. Stimuli were composed of 0.6◦ strips of
spatial white noise texture presented in an annulus 1-6◦ in radius. The texture in alternate strips moved in
opposite directions (left-right or up-down) at 3◦ /s. The strips themselves were static and tilted 45◦ either
left or right from vertical. A systematic pattern of bias across the cortical surface was evident in response
to the axis of motion of the textures when data were pooled across the orientation of the kinetic contours.
Comparison with flat maps of the visual field representation obtained from phase-encoded retinotopic analysis revealed that representations of the vertical (horizontal) meridian tended to respond more strongly to
vertical (horizontal) motion. Specifically, a bias for horizontal motion was evident within V1 and along the
borders between V2 and V3; a bias for vertical motion was apparent along the borders between V1 and
V2 and at the extremes of V3. Pooling data across the axis of motion of the textures revealed a different pattern of bias: the response to a given orientation was stronger in the quadrants of the visual field
for which contour orientation was predominantly radial. Specifically, the response to leftward (rightward)
tilted gratings tended to be stronger in the dorsal part of the left (right) visual cortex and the ventral part
of the right (left) visual cortex. Quantitative analysis of the variation in the proportion of voxels preferring a
particular axis of motion as a function of the meridian angle of the visual field representation (in 10◦ bins)
revealed a radial direction bias significant across subjects at p < 0.05 or better in each of areas V1, V2
and V3. Similar analysis showed a radial bias for the orientation of kinetic contours at p < 0.01 in each of
these areas. Radial orientation bias likely results from a disproportionate number of neurons being tuned
to radial orientations. The existence of radial biases for the orientation of kinetic (“non-Fourier”) as well as
luminance-defined (“Fourier”) contours could reflect processing by cue-invariant orientation mechanisms
or by distinct neuronal populations subject to similar anisotropies in orientation tuning. The radial bias for
motion might be related to the phenomenon of “motion streaks” whereby temporal integration by the visual
system introduces oriented blur along the axis of motion. We speculate that all the observed forms of radial
bias reflect a single underlying anisotropy in the way (Fourier and non-Fourier) stimulus energy is processed
across the visual field.
doi:10.3389/conf.neuro.06.2009.03.016

276

COSYNE 09

III-82 – III-83

III-82. Bayesian estimation of orientation preference maps
Jakob Macke1
Sebastian Gerwinn1
Leonard White2
Matthias Kaschube3,4
Matthias Bethge1

JAKOB @ TUEBINGEN . MPG . DE
SGERWINN @ TUEBINGEN . MPG . DE
WHITE 033@ MC. DUKE . EDU
KASCHUBE @ PRINCETON . EDU
MBETHGE @ TUEBINGEN . MPG . DE

1

MPI for Biological Cybernetics
Duke University
3
Princeton University
4
Lewis-Sigler Institute
2

Neurons in the early visual cortex of mammals exhibit a striking organization with respect to their functional
properties. A prominent example is the layout of orientation preferences in primary visual cortex, the orientation preference map (OPM). Functional imaging techniques, such as optical imaging of intrinsic signals
have been used extensively for the measurement of OPMs. As the signal-to-noise ratio in individual pixels
if often low, the signals are usually spatially smoothed with a fixed linear filter to obtain an estimate of the
functional map. Here, we consider the estimation of the map from noisy measurements as a Bayesian
inference problem. By combining prior knowledge about the structure of OPMs with experimental measurements, we want to obtain better estimates of the OPM with smaller trial numbers. In addition, the use
of an explicit, probabilistic model for the data provides a principled framework for setting parameters and
smoothing. We model the underlying map as a bivariate Gaussian process (GP, a.k.a. Gaussian random
field), with a prior covariance function that reflects known properties of OPMs. The posterior mean of the
map can be interpreted as an optimally smoothed map. Hyper-parameters of the model can be chosen by
optimization of the marginal likelihood. In addition, the GP also returns a predicted map for any location,
and can therefore be used for extending the map to pixel at which no, or only unreliable data was obtained.
We also obtain a posterior distribution over maps, from which we can estimate the posterior uncertainty of
statistical properties of the maps, such as the pinwheel density. Finally, our probabilistic model of both the
signal and the noise can be used for decoding, and for estimating the informational content of the map.
doi:10.3389/conf.neuro.06.2009.03.310

III-83. Distinct laminar zones of coherent local field potentials in monkey V1
Alexander Maier1,2
Geoffrey Adams3,4
Christopher Aura5
David Leopold

MAIERA @ MAIL . NIH . GOV
GEOFF . ADAMS @ GMAIL . COM
CJAURA @ GMAIL . COM
LEOPOLDD @ MAIL . NIH . GOV

1

National Institutes of Health
UCNI, LN, NIMH
3
Duke University Medical Center
4
National Institute of Mental Health
5
University of Alabama at Birmingham
2

Cortical field potentials are thought to arise from the spatiotemporal summation of diverse neuronal events
such as synaptic potentials, calcium spikes, after-discharges and summed action potentials. LFP thereby
reflects aspects of population activity which might go undetected in single unit recordings. Here we report

COSYNE 09

277

III-84
that the laminar profile of LFP in primary visual cortex (V1) of the macaque monkey exhibits a striking
compartmentalization into two distinct domains. We sampled the laminar structure of V1 field potentials
using linear multicontact electrodes in monkeys (1) performing a simple fixation task and (2) resting in
a dark room. In both cases, the root mean square magnitude of LFP in the gamma range (50-100 Hz)
was significantly (p < 0.001) higher in the more superficial compartment (layers I-IV: 3.2 &#956;V +- 0.11
&#956;V SEM between sessions) than in the deeper one (layers V&VI: 2.5 &#956;V +- 0.08 &#956;V SEM).
Coherence analysis revealed an abrupt transition between the two domains within a 100 micron distance.
Using current source density (CSD) analysis, we determined this transition to be situated between layers IV
and V. Additional analysis demonstrated that these distinct laminar zones cannot be explained by the wellknown sign inversion of LFP caused by afferent thalamic input to the layer IV. These results reveal that the
profile of cortical LFP coherence across the cortical thickness differs from the monotonic profile measured
tangentially to the cortical surface (e.g. Leopold et al. 2003). This functional dichotomy within a cortical
column has potential implications for understanding the inputs and outputs of the cortical microcircuit, as
well as the factors that dictate macroscopic measures of brain activity, such as the BOLD fMRI response.
doi:10.3389/conf.neuro.06.2009.03.042

III-84. Spatially periodic activity without periodic connectivity requires
an inhibition-stabilized network
Daniel Rubin
Kenneth Miller

DBR 2107@ COLUMBIA . EDU
KEN @ NEUROTHEORY. COLUMBIA . EDU

Columbia University
Length- and width-tuning experiments in V1 have revealed neurons that exhibit nonmonotonic changes in
response with increasing stimulus size. Some neurons show two peaks of firing rate as a function of stimulus size (De Valois et al. 1985), and more generally, length-tuned cells show similar nonmonotonicity of
their inhibitory synaptic conductance (Anderson et al. 2001). While it has been proposed that the latter
arises from multiple, distinct sources of inhibitory input, an alternative explanation is that the cortical network drives a periodic pattern of inhibitory-cell firing rate over retinotopic space. Adini, Sagi, and Tsodyks
(1997), in the context of a model of psychophysical results, noted that if a periodic firing pattern is driven by
net inhibitory interactions, the boundaries of the stimulated region will have the least inhibition and hence
the highest firing rates. Given oscillations of a fixed period, the phase of the center of the stimulated region
changes with stimulus size, leading to oscillations of its response with stimulus size. To better understand
what sort of cortical network could produce such an effect, we constructed a linear firing-rate model of V1
comprised of a one-dimensional recurrent network of excitatory and inhibitory neuronal populations. We
assume, as in cortical networks, that inhibitory connections are localized while excitatory are longer range.
Using a combination of both analytic and numeric modeling techniques, we find that in order for inhibitory firing rates in a generic network with aperiodic connectivity to oscillate as a function of space, several specific
requirements must be met. The most significant of these is that the network must be an “inhibition-stabilized
network” (ISN), meaning that excitatory to excitatory connectivity is strong enough to make the network unstable by itself, but the network is stabilized by feedback inhibition. Substantial evidence exists that cortical
circuits have strong recurrent excitation balanced by strong feedback inhibition. We with others have shown
more specific evidence that V1 functions within this regime, based on the experimental observation that
surround suppression of V1 neurons evokes a paradoxical decrease in inhibitory conductance (Ozeki et al.,
submitted); assuming the surround stimulus drives increased external input to the suppressed region, this
can only occur if the network is an ISN (Tsodyks et al. 1997). Our analysis leads to a number of testable
predictions. It predicts that excitation and inhibition should actually oscillate at different spatial frequencies,
and more specifically, that the excitation should always oscillate at a higher frequency than inhibition. Fur-

278

COSYNE 09

III-85
thermore there can be an oscillation of excitatory cells alone, but not of inhibitory cells alone. This model
also predicts that the paradoxical response mentioned above should carry over into the frequency domain:
for low temporal-frequency stimuli, as stimulus spatial frequency is varied, the response phase of inhibitory
neurons vs. excitatory should jump by 180◦ at a critical spatial frequency.
doi:10.3389/conf.neuro.06.2009.03.045

III-85. A dynamic receptive field of a motion-sensitive neuron in the
fly
Franz Weber1
Christian Machens2
Alexander Borst1
1
2

WEBERF @ NEURO. MPG . DE
CHRISTIAN . MACHENS @ ENS . FR
ABORST @ NEURO. MPG . DE

Max Planck Institute of Neurobiology
Ecole normale superieure Paris

In the blowfly, Calliphora vicina, visual motion is encoded by 60 so-called lobula plate tangential cells
(LPTCs). The receptive field properties of the LPTCs have been characterized previously using locally
presented stimuli [1]. These experiments allowed calculating the preferred motion direction of LPTCs at
various positions in the visual field. The resulting static receptive fields describe the spatial arrangement of
motion sensitivities, but disregard the dynamics of the neural response. On the other hand, purely temporal
aspects of motion processing have been studied in experiments where H1 was stimulated with horizontally
moving gratings with a white-noise velocity profile [2]. Temporal filters mapping the stimulus velocity onto
the neural response give insight into the dynamic features of H1, however neglect the spatial dimension of
the receptive field. In this study, we addressed both the spatial and dynamic features of the receptive field
of H1 in a combined approach. For this purpose, we recorded the activity of H1 extracellularly while presenting global white-noise motion to the fly. More specifically, the stimulus comprised an arbitrary number
of randomly moving circles, and the position of each circle followed a Wiener process, ensuring that the
motion is spatially uncorrelated. For the computation of the receptive field, we pre-processed the stimulus
by feeding it through a 2D array of motion detectors of the correlation-type, thus emulating the elementary
motion detectors presynaptic to H1 [3]. Finally, we estimated a linear kernel by regressing the vertical and
horizontal motion predictions of the detector array against the firing rate of the neuron. The kernel can be
viewed as a dynamic receptive field which we represented as a spatio-temporal vector field. A single vector
is composed of a horizontal and vertical kernel weight, describing the motion sensitivity of H1 at a particular
location and time point. Evaluation of the receptive field using cross-validation revealed that it predicts more
than 50% of the response power. Singular value decomposition allows separating the kernel into a temporal
and spatial component. The latter is comparable to the static receptive field as found by [1], showing that
the strength and direction of the motion sensitivities are dependent on the location within the visual field.
H1 responds to motion within a wide spatial range, however the receptive field exhibits strongest sensitivity
in a fronto-lateral region. The temporal component extends over about 80 ms and represents a biphasic
filter with a slight inhibition preceding a large excitatory phase. In general, thus, the spatial and dynamic
properties of the receptive field are consistent with results from previous studies. However, we found that
the spatial structure of the vector field differs for local compared to global stimulation: When displaying a
dense stimulus, the receptive field is significantly sharper than for a sparse stimulus. References [1] Krapp,
H. et al. (1996) Nature, 384:463-466. [2] Bialek, W. et al. (1991) Science, 252:1854-1857. [3] Borst, A. et
al. (1995) J Comput Neurosci, 2:5-18.
doi:10.3389/conf.neuro.06.2009.03.343

COSYNE 09

279

III-86 – III-87

III-86. Estimating surface structure and eye position from local derivatives of the disparity vector field
Jenny Read1,2
1
2

J. C. A . READ @ NCL . AC. UK

Institute of Neuroscience
Newcastle University

It has been known since Helmholtz that vertical disparities affect the depth perception caused by horizontal
disparities. The most famous demonstration of this is in Ogle’s induced effect, where a vertical magnification
of one eye’s image results in the perception that a frontoparallel surface is in fact slanted (i.e. rotated about
a vertical axis). Explanations of this illusion fall into two broad classes. Gaze angle theories (e.g. Mayhew
& Longuet-Higgins 1982, Nature 297: 376) explain the illusion as stemming from a misestimate of gaze
direction. If the eyes were looking off to one side, then the horizontal disparity specifies that the surface
is at an oblique angle to the line of sight, and it is therefore perceived as slanted. Local heuristic theories
(e.g. Rogers & Bradshaw, 1993 Nature 361:253) deduce surface slant directly from the ratio of the size of
line elements in the two eyes. These horizontal and vertical size ratios are equivalent to the first derivative
of horizontal disparity with respect to horizontal position, and of vertical disparity with respect to vertical
position. However, the derivations relating surface slant to horizontal and vertical size ratios have only
been given for restricted eye positions, e.g. assuming zero elevation and/or torsion, and for special scene
structure, e.g. line elements perpendicular to the gaze plane. It has not been made explicit how non-zero elevation would be dealt with in practice, or how the brain would know the orientation of line elements without
reconstructing the visual scene. Here, I show that the local-heuristic approach can be extended to general
eye positions and surface structure by including both first and second derivatives of the two-dimensional
disparity vector field. This enables the brain to reconstruct eye position (including gaze azimuth, elevation,
convergence, vertical vergence error, torsion and cyclovergence), surface distance and local surface structure (including slant, tilt and curvature). This is mathematically just the method of Longuet-Higgins (1981,
Nature 293: 133), which enables camera position to be reconstructed from sets of 8 corresponding points.
However, psychophysical evidence suggests that the brain computes this reconstruction locally, since we
are able to perceive simultaneously, at different places in the visual field, surface slants corresponding to
mutually inconsistent eye positions (Kaneko & Howard, 1997, Vision Res, 37:2871). This fact, together with
the observation that the induced effect does not cause the surface to be perceived off to one side, suggests
that only the surface parameters (distance, slant, tilt and curvature) are made explicitly available from this
local analysis, with the eye-position parameters remaining as hidden variables. The present formulation
shows that gaze-angle and local-heuristic theories of vertical disparity, though often presented as competitors, are in fact essentially the same. Local-heuristic theories, too, depend on an implicit misestimate of
gaze direction.
doi:10.3389/conf.neuro.06.2009.03.074

III-87. A Feed Forward-Lateral Inhibition Dynamical Model of Predictive Remapping of the Tilt Aftereffect
Arian Ashourvan
Joshua Brown
Nicholas Port

AASHOURV @ INDIANA . EDU
JWMBROWN @ INDIANA . EDU
NPORT @ INDIANA . EDU

Indiana University
The visual world is explored by saccadic eye movement, which usually occurs two or three times every

280

COSYNE 09

III-88
second. However, our perception of visual space is continuous without perceiving stable objects as moving.
The “remapping hypothesis” suggests that visual processing is altered at both the current fixation position
and the future eye position prior to a saccade; therefore enabling us to maintain a stable perception of
visual space (Duhamel et al., 1992). Recently, Melcher (2007) has shown the tilt aftereffect is predicatively
remapped before a saccade, presumably through corollary discharge (Sommer & Wurtz, 2006). In this study
we built a dynamical systems neural network model of remapping of visual space using the task of Melcher
(2007) as the input to model and the behavioral output of his subjects as the output of the model. The
heart of the model is based on the recent MT neurophysiology findings of Kohn and Movshon (2004). They
found that prolonged exposure to a stimulus, which causes adaptation, reduces cortical responsiveness and
causes changes in the tuning function of MT cortical neurons. Using a dynamic systems neural network
model with direct excitatory connections and lateral inhibitory connections, our model was able to create
changes in the tuning functions of the cortical neurons after adaptation, consistent with reported empirical
results. Finally, using the vector sum of the neural population output as the perceived direction and following
a period of adaptation in the initial location, the model is capable of predicting the adaptation aftereffect in
the future location prior to the onset of a saccadic eye movement.
doi:10.3389/conf.neuro.06.2009.03.094

III-88. A model for the distortions of space and time perception during
saccades
Guido Marco Cicchini1
Paola Binda1
Maria Concetta Morrone2
1
2

G . CICCHINI @ HSR . IT
P. BINDA 1@ TISCALI . IT
CONCETTA . MORRONE @ INPE . UNIPI . IT

Universit Vita-Salute San Raffaele Milan
Pisa University and IRCCS Stella Maris

Perception of space and time undergoes strong distortions at the time of saccadic eye movements. Spatial
distances are compressed towards the saccadic target while temporal intervals are underestimated [1-3].
We simulated these effects in a two-stage model where visual stimuli are encoded within a spatiotopic map
and decoded by matching activation patterns with a set of templates learned in steady-fixation conditions.
The retinotopic input to brief stimuli is given by an impulse response function that peaks after 25 ms and
decays to half peak value in 65 ms, extending over the duration of the saccade. The input propagates within
the spatiotopic layer with a dissipative automata whose maximum spreading velocity of about 1◦ /ms. At the
encoding stage, recalibration of the spatiotopicity for a shift of gaze is obtained by correcting the retinotopic
input with an internal signal representing eye position (corollary discharge), which starts 60 ms before the
saccade and is complete at saccadic offset. The recalibration biases the automata spreading function
parallel to the direction of the saccade, without altering the decoding stage. A given external stimulus elicits
the same response in the map when presented long before or long after a saccade. In the proximity of a
saccade, when remapping and/or retinal slip occur, activation patterns are unusual and smeared in both
space and time. The second stage of the system decodes the activation of the spatiotopic map, determining
the location and timing of the stimulus as well as the relative spatial and temporal distance of a couple of
stimuli. The system performs veridically both for space and time localization in fixation conditions. However,
at the time of saccades, the classifier is faced with unusual activation patterns causing it to distort both
spatial and temporal metrics. The best match for a couple of peri-saccadic stimuli results in a compression of
distances of about 50% both in space and time, consistently with psychophysical data. Absolute perceived
time is also altered, so perisaccadic targets are delayed by about 100 ms; time momentarily stops or even
inverts at the onset of remapping, again simulating well the perceptual data. The model is physiologically
plausible given that remapping signals are very common in associative and parietal cortex and spatiotopic

COSYNE 09

281

III-89
cortical maps have been described in VIP and area V6 of the monkey [4-6]. The model demonstrates
that localization is achieved by a simultaneous optimal decoding of space and time. When this strategy is
applied at the time of saccadic recalibration, the price is a transient distortion of the metrics for perisaccadic
stimuli. References: [1] J. Ross, M.C. Morrone, and D.C. Burr. Nature, 384: 598 (1997) [2] M.C. Morrone,
J. Ross and D.C. Burr. Nat Neurosci, 8: 950 (2005) [3] P. Binda, D.C. Burr and M.C. Morrone. Perception
36S:112 (2007) [4] JR. Duhamel, C.L. Colby, M.E. Goldberg. Science. 255:90 (1992) [5] JR. Duhamel et
al. Nature 389:845 (1997) [6] C. Galletti, P. Battaglini, P. Fattori. Exp. Brain Res. 96:221 (1993) Funded by
EU – 6th & 7th Framework Programme – “MEMORY” and “STANIB”
doi:10.3389/conf.neuro.06.2009.03.349

III-89. Local fourth-order statistics in natural scenes predict the saliency
of synthetic textures
Gasper Tkacik1
Jason Prentice1
Jonathan Victor2
Vijay Balasubramanian1
1
2

GTKACIK @ SAS . UPENN . EDU
JPRENTIC @ SAS . UPENN . EDU
JDVICTO @ MED. CORNELL . EDU
VIJAY @ PHYSICS . UPENN . EDU

University of Pennsylvania
Weill Medical College of Cornell

A recurring theme in early visual processing is the interrelationship between neural computations and natural scene statistics. For example, it is well-known that natural scenes have strong second-order correlations;
retinal center-surround processing can be viewed as a mechanism for removing them, thereby enabling efficient encoding. Natural scenes also have high-order correlations, and they are not removed by retinal
filtering. Although such correlations are critical to defining features and objects, little is known about higher
order statistical features of scenes and their relation to visual mechanisms. To examine the role of higherorder correlations, Victor and Conte have measured visual responses to synthetic black and white textures
in which pairwise and three-point correlations are absent, but which contain a tunable amount of fourthorder correlation. These textures are constructed from statistically correlated ”gliders” – groups of four
pixels arranged in a particular geometric shape. Certain gliders generated textures for which there was
a strong psychophysical and visual evoked potential (VEP) response, compared to a texture of random
pixels; we refer to these as “visible gliders.” Other gliders generated textures for which the distinguishing
psychophysical and VEP response was absent; we refer to these as the “invisible gliders.” Since these studies showed selective sensitivity to specific high-order correlations, we asked whether this selectivity could
be predicted by the extent to which these correlations were informative about natural scenes. To address
this, we first whitened and binarized images from a new high resolution database of natural images from a
baboon habitat in Botswana, and then tested how much of their structure was explained by the presence of
“visible” and “invisible” gliders. To do this, we picked different four-pixel glider shapes (e.g. four pixels arranged into a 2x2 square shape, a cross, a tee-shape etc), from both the “visible” and the “invisible” group,
and scanned each glider shape across image blocks. At each position in the image block we examined
the binary pattern of 4 pixels arranged in the chosen glider shape; the tally of these patterns resulted in a
distribution over 2ˆ4=16 possible binary patterns in an image block. Finally, we used the maximum-entropy
formalism to decompose the resulting distribution into contributions of 1st order luminance bias, 2nd, 3rd,
and 4th-order correlation, and examined the scaling of the contributions with the size of image blocks. We
found that for sufficiently large image blocks, the ensemble-average fourth-order contribution is significantly
greater than zero for the “visible” gliders. In contrast, for all invisible gliders, the same measure is indistinguishable from zero. The result was robust to different methods of whitening, binarizing, and rescaling the
natural images, and does not arise in synthetic Gaussian noise images. Lastly, we examined the spatial

282

COSYNE 09

III-90
correlations between glider patterns in natural scenes and found evidence that local statistical signatures
that we see arise from features such as edges and lines. Our results suggest that the visual system’s selective sensitivity to high-order correlations corresponds to their prevalence in nature. Victor & Conte (1991)
Vision Res.31(9):1457-88
doi:10.3389/conf.neuro.06.2009.03.134

III-90. Population coding of ground truth motion in natural scenes in
the early visual system
Garrett Stanley1
Michael Black2
J.P. Lewis3
Gaelle Desbordes1
Jianzhong Jin4
Jose-Manuel Alonso4

GARRETT. STANLEY @ BME . GATECH . EDU
BLACK @ CS . BROWN . EDU
NOISEBRAIN @ GMAIL . COM
GAELLE . DESBORDES @ GATECH . EDU
JJIN @ SUNYOPT. EDU
JALONSO @ SUNYOPT. EDU

1

Georgia Institute of Technology
Brown University, Dept. of Computer Science
3
Massey University
4
State University of New York
2

As we move through our visual environment, the spatial and temporal pattern of light that enters our eyes
is strongly influenced by the properties of objects within the environment, their motion relative to each
other, and our own motion relative to the external world. Quantifying the distributed neural representation of
luminance and motion in the early visual pathway is a critical step in understanding how scene information
is extracted and prepared for processing in higher visual centers. We argue that it is important to model
neural population responses to visual scenes with the rich complexity of the natural visual world. Current
natural scene movies provide complex scene statistics but the uncontrolled nature of these stimuli limits their
usefulness for understanding the visual code. Consequently, we have developed new image sequences of
naturalistic scenes using computer graphics methods in which we know important physical properties of
the scene including the camera motion, depth of objects, reflectance, illumination, etc. Additionally, we
have recorded the activity of a large population of single neurons in response to these movies using a
dense sampling of cells in the visual thalamus. Preliminary analyses reveal that thalamic responses and
thalamic correlated firing depends on several factors including the properties of the scene, the properties of
the thalamic neurons, and the geometric arrangement of the thalamic receptive fields. Manipulation of the
distribution of speed and direction of motion within the scene directly modulates the correlated firing across
cell pairs in a non-trivial manner that appears to depend on more than just their axis of alignment. For some
cell pairs, the structure of the spike cross-correlation was highly predictive of the stimulus manipulation (less
precise for slower stimulus, shift in latency for reverse direction of motion). From a decoding perspective,
therefore, these cases provide evidence for the possibility of decoding some aspects of the motion from
the timing of firing activity. Although the “aperture problem” usually leads to multiple solutions to the local
decoding of speed and direction of motion, decoding the activity of a population of neurons covering a larger
portion of the visual scene could in principle overcome this limitation. However, measures across the entire
sample suggest that the decoding process is much more complex and has to take into consideration the
functional properties of each neuron such as latencies and temporal filtering characteristics in addition to
the geometric relationship of the receptive fields. Taken together, our preliminary analyses suggest that
controlled manipulation of the properties of the scene can provide a rich characterization of the functional
diversity of the thalamic representation of the dynamic natural scene, which ultimately serves as the neural
code entering primary visual cortex.

COSYNE 09

283

III-91
doi:10.3389/conf.neuro.06.2009.03.133

III-91. Speed sensitive excitation shapes the tuning of a collisiondetecting neuron
Peter Jones1
Fabrizio Gabbiani1,2
1
2

PWJONES @ CNS . BCM . TMC. EDU
GABBIANI @ BCM . EDU

Baylor College of Medicine
Rice University

Motion detection, an ubiquitous computation performed by visual systems, is used to guide a range of vital
behaviors, including escape from threats. The Lobula Giant Movement Detector (LGMD), an identified
locust neuron in a neural pathway important for escape responses, has been found to be sensitive to stimulus speed but only weakly directionally selective. Both speed sensitivity and direction selectivity together
are hallmarks of motion detection, thus we are interested, in this system, in how speed sensitivity alone is
generated. We investigated a mechanism that relies solely upon the activation time-course of peripheral
receptors as a speed signal by recording intracellularly at several stages along the visual processing pathway. We found that photoreceptor responses carry information about the speed of a moving edge in the
slope their of membrane potential deflections, and that these responses can be well mimicked by stationary luminance changes of varying rate at single facets. Immediately post-synaptic to photoreceptors, large
monopolar cells (LMCs) appear to transform variation of response slope to variation in response magnitude
and timing, consistent with a high pass filtering of photoreceptor responses. LGMD responses to the same
stimuli showed similar modulation in both magnitude and timing, showing that the motion speed sensitivity
of the LGMD at least partially arises from changes in activation speed of individual photoreceptors. Since
the LGMD integrates input originating from a large number of facets on the compound eye ( 7,500), we
are building a model to extrapolate from the modulation observed in single facet inputs to the magnitude
and timing of excitation received from the population of inputs during more complex visual stimuli. We further investigated if these single facet speed signals could help tune the neuron to certain classes of stimuli.
Looming stimuli, generated by objects approaching on a collision-course with the animal, most robustly activate the LGMD. Since both angular speed and the rate of luminance change at single facets increase during
such stimuli, the dependence of latency on luminance change rate could help tune the LGMD to looming
stimuli by synchronizing the excitatory input into the cell. We tested this idea by presenting pseudo-looming
stimuli that excited exactly 45 targeted facets ( 9x45◦ ) in a precise spatio-temporal pattern that was either
consistent with a looming stimulus edge or temporally shuffled to redistribute the speeds experienced by
individual facets. We find that the pseudo-looming stimuli evoked tightly clustered spike responses, resulting in high instantaneous firing rates. The shuffled stimuli, however, evoked lower peak firing rates but often
evoked similar numbers of spikes. These results indicate that motion speed dependence imparted by the
activation rate of photoreceptors plays a role in tuning the LGMD to looming stimuli.
doi:10.3389/conf.neuro.06.2009.03.159

284

COSYNE 09

III-92 – III-93

III-92. Reading the mind’s eye: Decoding object information during
mental imagery from fMRI patterns
Thomas Serre1,2
Leila Reddy3,4
Naotsugu Tsuchiya5,6
Tomaso Poggio1
Michèle Fabre-Thorpe3,4
Christof Koch5

SERRE @ MIT. EDU
LEILA . REDDY @ GMAIL . COM
NAOTSU @ GMAIL . COM
TP @ AI . MIT. EDU
MICHELE . FABRE - THORPE @ CERCO. UPS - TLSE . FR
KOCH @ KLAB . CALTECH . EDU

1

Massachusetts Institute of Technology
McGovern Institute for Brain Research
3
Université de Toulouse; UPS; CerCo
4
CNRS; CerCo; Toulouse, France
5
California Institute of Technology
6
Japan Society for the Promotion of Science
2

Several studies have shown that category information for visually presented objects can be read out from
fMRI patterns of brain activation spread over several millimeters of cortex. What is the nature and reliability
of these activation patterns in the absence of any bottom-up visual input, for example, during visual imagery? Previous work has shown that the BOLD response under conditions of visual imagery is lower and
limited to a smaller number of voxels, suggesting that activation patterns during imagery could be substantially different from those observed during actual viewing. We here ask first, how well category information
can be read-out for imagined objects and second, how the representations of imagined objects compare
to those obtained during actual viewing. These questions were addressed in an fMRI study in which we
used four categories – faces, buildings, tools and fruits – in two conditions. In the V-condition, subjects
were visually presented with the images and in the I-condition they imagined them. Using pattern classification techniques we found, as previously, that object category information could be reliably decoded in the
V-condition, both from category selective regions (i.e. FFA and PPA), as well as more distributed patterns
of fMRI activity in object selective voxels of ventral temporal cortex. Good classification performance was
also observed in the I-condition indicating that object representations in visual areas in the absence of any
bottom-up signals provide information about imagined objects. Interestingly, when the pattern classifier
was trained on the V-condition and tested on the I-condition (or vice-versa), classification performance was
comparable to the I-condition suggesting that the functional patterns of neural activity during imagery and
actual viewing are surprisingly similar to each other. In addition we found that the performance of such
classifier could be correlated with the reported vividness of individual subjects. This suggests that people
with more vivid images are those able to accurately re-activate the patterns of neural activity from neural
populations associated with the representation of specific objects. Overall these results provide strong constraints for computational theories of vision and suggest that, in the absence of any bottom-up input, cortical
back-projections can selectively re-activate specific patterns of neural activity.
doi:10.3389/conf.neuro.06.2009.03.274

III-93. A Gamma-phase Model of Receptive Field Formation
Dana Ballard

DANA @ CS . UTEXAS . EDU

University of Texas
Rate coding is a ubiquitous model of cortical signaling but is it the cause of computation or a correlate?

COSYNE 09

285

III-94
Criticisms of the causal model cite difficulties in achieving the precision and the speed of computation, but
we argue that there is an additional constraint that arises from Bayesian models of RF formation3. Think
of the problem representing a distribution of inputs amongst cells with overlapping RFs. When a spike
sent, if it is to be included in a pool that causes the synaptic input to be modified, then it should be so
classified probabilistically according to a competition between its neighbors. This is a standard feature of
the Expectation Maximization algorithm. If this is not done, errors in the RF structure will result. However the
accumulation of statistics in this process is slow. It might be suitable for RF formation but it is unsuitable for
communicating the values of variables used in the overarching algorithm quickly. Thus it would be beneficial
to somehow have two processes, one a slow one that accumulates statistics and another a fast one that
signals values. This can be done if the cortex uses gamma-phase coding to signal analog values1 and spike
selection to modify the RF. The gamma signal is used as a timing reference and timing delays represent
numerical values, with spikes closest in time to the timing reference being the highest value. This strategy
has the additional virtue of being compatible with spike-timing dependent modification of synaptic strength.
However gamma-phase coding seems incompatible with a large amount of single cell recording data. If the
cortex is using a timing reference2, why does the output of cells appear Poission and mimic receptive field
organization? Our claim is that the probabilistic structure used to update receptive fields can be thought of
as the routing used in the overlying computation. Simply, the computation being done can take alternate
routes that are chosen probabilistically, but for each neuron on the route that is chosen to send a spike, the
value sent can be signaled quickly and accurately using gamma-phase coding. The cells appear Poisson
since they are chosen randomly and they reflect RF statistics since those statistics govern their selection.
We illustrate these points using adapting the classical sparse-coding model of RF formation between LGN
and striate cortex3 to use gamma phase coding. The stunning demonstration is that the representation of an
image is stable even though from moment to moment alternate choices of RFs are being used to represent
it. We further demonstrate that the probabilistic routing that results has the consequence that a neuron’s
receptive field, when measured in the classical way with the peristimulus histogram, recapitulates the RF
statistics. Finally we show that gamma-phase coding may be more ubiquitous than currently believed as it
can be difficult to detect if probabilistic routing is used. Supported by NIH Grant R01RR009283 1. Fries et
Trends in Neurosciences 9, 2007 2. Womelsdorf et al Science 316, 2007 3. Olshausen and Field Vision
Research 37, 1997
doi:10.3389/conf.neuro.06.2009.03.275

III-94. Complex cells in the early visual cortex have multiple disparity
detectors in the 3D binocular RFs
Kota Sasaki1
Yuka Tabuchi
Izumi Ohzawa1,2
1
2

KOTA @ FBS . OSAKA - U. AC. JP
YUKAKUMA 3@ GMAIL . COM
OHZAWA @ FBS . OSAKA - U. AC. JP

Osaka University
CREST JST

The slight differences of the two retinal images provide the stereoscopic cue for the perception of the 3D
space. These differences are called binocular disparity, and modulate the firing rate of many neurons in
the early visual cortex. Disparity selectivity of visual neurons is described most comprehensively by the
binocular receptive fields (RFs), which represent how inputs from the two eyes interact to elicit neuronal
responses. To date, binocular RF profiles have been investigated for neurons in the early visual cortex in
the 2D plane. However, binocular RFs are inherently a 3D entity in space. Here we examined binocular
interaction in the 3D space to elucidate mechanisms to detect binocular disparity signals and further pooling of these detectors to comprise the whole binocular RFs in single neurons in the early visual cortex.

286

COSYNE 09

III-95
We recorded from neurons in the early visual cortex of anaesthetized and paralyzed cats while they are
stimulated by uncorrelated dichoptic 2D dynamic noise stimuli. We used a spike-triggered analysis to examine structure of the 3D binocular RFs. Spike-triggered stimuli for both eyes were picked up, and were
taken apart into thin strips tilted along the X axes (orthogonal to optimal orientation) at Y positions (parallel
to optimal orientation). They were multiplied to yield interaction terms between the stimulus strips. The
interaction terms were summed for all spike-triggered stimuli to obtain a binocular interaction map in the X
dimensions between the pair of the Y positions. This computation was applied for all pairs of Y positions
between the left- and right-eye stimuli. For a subset of complex cells, we found no binocular interaction in
the RFs between the distant pairs of Y positions between the two eyes even though the both positions were
well within the RFs. This means that the detectors for binocular disparity cover a limited portion in the RF
and that they are pooled in space to construct the whole RF. Sixteen out of 34 complex cells showed extensive spatial pooling of disparity detectors (RF size / detector size > 1.5). Simple cells generally showed
hardly any pooling of the underlying detectors (comparison of the degree of spatial pooling between the
populations of simple and complex cells; Wilcoxon rank sum test, p < 0.0001). The degree of spatial pooling was also related to binocularity of cells; neurons with balanced inputs from the two eyes tended to show
a large degree of spatial pooling (Wilcoxon rank sum test, p < 0.0005). However, the degree of spatial
pooling was not related to optimal orientation, spatial frequency, and the size of detectors. For those complex cells showed extensive pooling of detectors for binocular disparity, tuning curves to binocular disparity
were invariant along the Y axis. These results indicate that a substantial proportion of complex cells detect
constant binocular disparities in the RFs by collecting inputs only from many antecedent neurons that prefer
identical disparities at different positions. This work was supported by Grant-in-Aid for Scientific Research
18020017, 13308048, CREST Yoshioka Project, and Global COE.
doi:10.3389/conf.neuro.06.2009.03.167

III-95. FEF neural responses during uninstructed, optimal eye movements in a free-viewing scan task
Theresa Desrochers
Ann Graybiel

TMD @ MIT. EDU
GRAYBIEL @ MIT. EDU

Massachusetts Institute of Technology
The majority of primate electrophysiological experiments begin gathering neural data only after the animal
is well trained. In this experiment we recorded from the frontal eye fields (FEF), prefrontal cortex (PFC),
and caudate nucleus (CN) of two monkeys throughout the acquisition of a novel free-viewing scan task. In
this task monkeys were required to maintain their eye position in the space occupied by a 4- or 9-target
(depending on the stage of training) grid of dots until, by freely making saccades in that space, they fixated
on or passed through the target randomly chosen to be rewarded. We previously reported that monkeys
form habitual patterns of eye movements without explicit shaping or instruction of any kind (Feledy, et al.
Cosyne 2007). Furthermore, we found that these patterns of eye movements, or scan paths, gradually
evolve to be more optimal through training where optimality was defined as the shortest distance and time
to cover all the grid targets in a loop. Here we focus on recording results from a single cortical area, FEF,
obtained at the end of training. Because of the free-viewing nature of this task, we first categorized FEF units
according to those properties found in the FEF of over-trained monkeys performing single saccade tasks.
We found similar kinds of general responses, as defined by modulation of mean firing rate, e.g. direction
selectivity and visual selectivity. In addition, we found units that had more complex firing properties that may
be novel and could indicate as yet undocumented properties of FEF units that emerge with uninstructed
learning of a naturalistic task. This work is supported by: NEI EY12848, Friends of the McGovern Institute
Graduate Fellowship, NDSEG Fellowship

COSYNE 09

287

III-96 – III-97
doi:10.3389/conf.neuro.06.2009.03.177

III-96. Contrast dependence of spatial frequency tuning in macaque
LGN
Sach Sokol
Neel Dhruv
J Anthony Movshon

SACH @ CNS . NYU. EDU
N . DHRUV @ UCL . AC. UK
MOVSHON @ NYU. EDU

New York University
The classical model of visual neurons in the lateral geniculate nucleus (LGN) holds that their receptive fields
are composed of a small center mechanism and larger surround mechanism, each Gaussian in sensitivity
distribution, and that responses are determined by the difference in activation of center and surround. This
difference of Gaussians (DoG) model predicts that, when tested with drifting sinusoidal gratings of different
spatial frequency, the neuron’s spatial frequency tuning can be modeled as the difference of two Gaussians.
Many studies have confirmed that this model provides a good account of response data obtained at high
contrast, or of contrast sensitivity data obtained by measuring gain at low contrast. The DoG model assumes that the underlying receptive field mechanisms are fixed in size, and therefore predicts that spatial
frequency tuning curves measured at different contrasts should be scaled replicas of one other. We examined this question by recording extracellularly from neurons in the LGN of anesthetized macaques and
measuring their responses to large gratings of optimal drift rate that varied over a wide range of spatial
frequency and contrast. The shapes of spatial frequency tuning curves varied with contrast for most M- and
P-cells we studied. In particular, as stimulus contrast increased, responses to high spatial frequencies grew
more rapidly than responses to low spatial frequencies, resulting in an overall rightward shift in the tuning
curve and a concomitant increase in the peak frequency. We fit the tuning data for each contrast with a
separate DoG and examined the model parameters. As contrast increased, the size of both the center and
surround mechanisms decreased systematically with increasing contrast, while the relative gain of the two
mechanisms remained roughly constant. A related analysis of LGN cells in the cat (Bonin et al., J Neurosci,
2005) proposed a third suppressive mechanism, large in spatial extent, to account for contrast dependent
spatial summation. Such a mechanism might be at work in our M-cells, but cannot account for the contrastdependent changes in P-cells, which show no signs of a suppressive surround. Our data are consistent
with a modified DoG model in which both the center and surround sizes decrease as a function of contrast.
This could be conveniently explained by a contrast dependent change in the scale of spatial integration of
a single mechanism, presumably in retina, whose signals contribute to both center and surround.
doi:10.3389/conf.neuro.06.2009.03.174

III-97. Decoding sequences of population responses in visual cortex
Andrea Benucci
Laura Busse
Steffen Katzner
Matteo Carandini

ANDREA @ CARANDINILAB . NET
L . BUSSE @ UCL . AC. UK
S . KATZNER @ UCL . AC. UK
MATTEO @ CARANDINILAB . NET

University College London
Visual stimulation evokes activity in a large population of neurons in visual cortex. Stimulus properties are

288

COSYNE 09

III-98
encoded by rapidly changing response patterns. How should these patterns of population responses be
decoded to identify individual stimuli and sequences of stimuli? Fifteen years ago, Földiák [1] proposed
that population responses can be decoded by a simple Bayesian observer that takes into consideration the
typical covariance of activity between different neurons. However, to date this covariance matrix is only
beginning to be investigated, and this proposed decoding has been attempted neither for single stimuli
nor for stimulus sequences. We recorded population responses from area 17/18 in anesthetized, paralyzed cats. Using a combination of voltage-sensitive dye imaging (VSD) and multi-electrode array recordings (Utah probes) we acquired neural responses both sub-threshold (membrane potential responses) and
supra-threshold (spiking responses). Responses were elicited by sequences of full-field gratings whose
orientation changed rapidly (33Hz) and randomly [2]. The remaining stimulus parameters were kept constant, thus changes in neural responses could only be attributed to changes in stimulus orientation. We
then asked whether a Bayesian observer could decode such activity and correctly predict the sequence of
stimulus orientations. Neurons were grouped according to their preferred orientations, and for any given
stimulus at any given time their responses provided an unbiased estimate of the stimulus orientation on a
trial-by-trail basis. We evaluated the performance of the decoder when the covariance matrix was taken
into consideration, both for the spiking responses as well as for the VSD responses. As expected [3], this
performance was significantly degraded when responses were considered independent from each other.
Our results indicate that a Bayesian decoder can accurately predict the sequence of stimulus orientations
with millisecond temporal precision. These results extend previous findings showing that sub-threshold
responses simultaneously recorded from a population of V1 neurons allow one to detect the presence or
absence of a stimulus for different values of stimulus contrast [4]. In our findings, the level of precision
and reliability of the predictions is achieved both in the supra- and sub-threshold responses. However, estimates from VSD responses develop over a time window longer than those based on spiking responses.
We conclude that a Bayesian observer that has access to the covariance of activity between neurons can
efficiently decode the orientation of rapidly changing stimuli. Support: NIH EY017396. [1] Földiák P (1993)
in: Computation and neural systems (Eeckman FH, Bower JM, eds), pp 55-60. Norwell, MA: Kluwer Academic Publishers. [2] Ringach, D. L., G. Sapiro, et al. (1997). Vision Res 37(17): 2455-64. [3] Graf, A. B.
and Movshon, J.A. (2008). Program no. 514.4, Society for Neuroscience Meeting, 2008. [4] Chen, Y., W.
S. Geisler, et al. (2006). Nat Neurosci 9(11): 1412-20.
doi:10.3389/conf.neuro.06.2009.03.211

III-98. Is Sensory Processing about Detecting Abrupt Changes in the
Sensory World?
Nabil Bouaouli
Sophie Deneve

NABIL . BOUAOULI @ ENS . FR
SOPHIE . DENEVE @ ENS . FR

Ecole Normale Supérieure, Paris
One of the most salient properties of the sensory system is its propensity to respond strongly to transient
rather than to static stimuli. This marked preference for changes is found in the brain at all levels. A
variety of mechanisms are known to boost responses to higher temporal frenquencies such as short-term
synaptic depression, spike rate adaptation and feedforward inhibition. However, change detection as a
major role of sensory processing remains largely unexplored. In order to test this hypothesis, we use a
novel probabilistic framework and show quantitatively the importance of feedfoward inhibition in detecting
sudden ”appearance” (or ”desappearance”) of stimuli. Assuming a Markov dynamics of stimuli, we derive
an ”ideal observer” that computes on-line a probability of a sudden change in input firing rates (a transition).
In general, this model predicts a bi-phasic, non-linear synaptic integration consisting of a fast, transient,
excitation followed by a slower inhibition. The same stimulus that excites the cell at short delays inhibits

COSYNE 09

289

III-99
it at longer ones. However, the exact properties of this integration, in particular the strength and time
constants of excitation and inhibition depend on the temporal statistics of the stimulus and the reliability of
the input spike trains : in noisier settings, longer integrations are required to detect changes. We explore
what this general framework implies for temporal receptive fields (tRFs), contrast adaptation and spiketime precision in early visual areas. We suppose that output spikes are generated when the probability of
transition crosses a threshold, followed by a refractory period. This spike generation mechanism reproduces
the firing statistics in the retina and LGN in response to time varying stimuli. Particularly sharp peaks in
PSTH that could not be predicted directly from the linear tRFs, but instead could be obtained by adding
feedforward inhibition and refractoriness. The predicted tRFs resemble the ones of ”ON” and ”OFF” retinal
ganglion cells and LGN cells, but their shape adapts to the (variance of the) contrast of the input. At low
contrast, integration dominates, resulting in a slower, mainly excitatory tRFs. At high contrast, tRFs are
shorter and more strongly biphasic, similar to temporal derivatives. This adaptation emerges naturally from
the non-linear probabilistic equations. Moreover, we found that bursts of spikes are preceded by strong
inhibition while single spikes are not. We, next, use this model to investigate biophysical implementations
of change detection and propose a specific neural mechanism based on synaptic short-term plasticity.
We consider a small microcircuit commonly observed in many sensory areas, namely a pyramidal neuron
receiving both mono-synaptic excitation from another pyramidal ”source” cell and disynaptic inhibition from
the same source cell. If the excitatory synapse is depressing and the inhibitory one facilitating, parameters
of this biophysical system can be adjusted to approach the ideal performance. We compare this prediction
with recent experimental data.
doi:10.3389/conf.neuro.06.2009.03.351

III-99. Wireless Recording from the Cortex of a Freely Roaming Rat
Tobi A Szuts1
Vitaliy Fadeyev2
Andrew Leifer3
Wladyslaw Dabrowski4
Naoshige Uchida1
Alan Litke2
Markus Meister1

SZUTS @ FAS . HARVARD. EDU
VF @ SCIPP. UCSC. EDU
LEIFER @ FAS . HARVARD. EDU
W. DABROWSKI @ FTJ. AGH . EDU. PL
UCHIDA @ MCB . HARVARD. EDU
ALAN . LITKE @ CERN . CH
MEISTER @ FAS . HARVARD. EDU

1

Harvard University
University of California, Santa Cruz
3
Harvard Medical School
4
AGH University of Science and Technology
2

Much of what we know about brain function comes from laboratory experiments under carefully controlled
conditions. While one generally assumes that the brain performs similarly during autonomous behavior in a
natural environment, this has rarely been tested directly. Accordingly, we have developed a wireless system
that allows neural signals to be recorded from an unconstrained animal. The system consists of: a chronically implanted tetrode drive; custom electronics that sample up to 64 channels at 20,000 samples/s and
multiplex them onto a single data stream; a microwave transmitter and receiver; and a digitizing computer,
where the electrode signals are reconstructed from the multiplexed data stream. This system is used for pilot recordings from two areas in rat cortex: a premotor area and the primary visual cortex. In each case we
observe neural spike trains both under freely roaming behavior and under laboratory conditions: in an odor
discrimination task for the premotor area and controlled light stimulation for visual cortex. We compare the
statistics of population activity between the controlled and free conditions, and analyze how each neuron’s
firing is modulated during the animal’s autonomous behavior.

290

COSYNE 09

III-100
doi:10.3389/conf.neuro.06.2009.03.331

III-100. Sensory adaptation as an optimal redistribution of neural resources
Sergei Gepshtein1,2
Luis Lesmes
Ivan Tyukin
Thomas Albright
1
2

SERGEI @ SALK . EDU
LU @ SALK . EDU
I . TYUKIN @ LEICESTER . AC. UK
TOM @ SALK . EDU

The Salk Institute, USA
RIKEN, BSI, Japan

Introduction. It has been proposed that sensory adaptation optimizes visual sensitivity to properties of the
variable environment (Sakitt and Barlow, 1982; Wainwright, 1999). On this view, motion adaptation is expected to improve the ability to perceive motion at the adapting conditions. Yet experimental evidence of
motion adaptation has been controversial. In speed adaptation, for example, sensitivity to adapting speeds
can either increase or decrease; it can also change for speeds very different from the adapting speed
(Krekelberg, van Wezel, and Albright, 2006). We presently test a normative theory of motion adaptation
which implements the premise that motion adaptation improves the ability to perceive motion in a new environment but which also predicts that (a) sensitivity to motion must deteriorate at some adapting conditions,
and (b) adaptation-induced changes are global so increments and decrements of sensitivity are expected
also away from the adapting conditions. Theory. According to a new normative-economic theory of motion
perception (Gepshtein, Tyukin, and Kubovy, 2007; www.journalofvision.org/7/8/8/), spatiotemporal sensitivity manifests an optimal allocation of limited resources (such as motion-sensitive cells) to conditions of
visual stimulation. The allocation is optimal in two respects. First, it balances the errors of estimating
stimulus location and stimulus content, satisfying the uncertainty principle of measurement (Gabor, 1946).
Second, it places more resources at the conditions where the resources are more likely to be used, by
taking into account the statistics of visual stimulation. The theory predicts that motion adaptation should
induce a characteristic pattern of changes in the spatiotemporal contrast sensitivity function (Kelly, 1979),
forming well-defined foci of increased and decreased sensitivity across a map of sensitivity to spatial and
temporal frequencies of stimulation. Experiments. We tested the predictions by measuring human contrast
sensitivity over a large range of spatial and temporal frequencies (0.25-8 c/deg and 0.5-32 Hz). The observers viewed drifting luminance gratings of variable contrast and discriminated the direction of motion.
We varied the statistics of motion speed: In some blocks of trials low speeds were more common than high
speeds, and in other blocks high speeds were more common than low speeds. To rapidly measure the entire
contrast sensitivity function in both statistical contexts, we used a novel adaptive procedure that combined
Bayesian adaptive inference with a trial-to-trial information-gain strategy (Lesmes, Lu, Baek, and Albright,
2008; www.journalofvision.org/8/6/939/). We compared the spatiotemporal sensitivity functions obtained in
the different statistical contexts and found that sensitivity changed similar to our predictions. The changes
were global and they formed foci of increased and decreased sensitivity, so the map of observed changes
was similar to the map of predicted changes. Conclusions. These findings support the normative-economic
theory and the view that motion adaptation amounts to reallocation of neural computational resources in
the visual system: The allocation of sensitivity takes into account both the uncertainty principle of measurement and the statistics of stimulation. Since computational resources of the visual system are limited,
improvement of sensitivity to some stimuli is accompanied by deterioration of sensitivity to other stimuli.
doi:10.3389/conf.neuro.06.2009.03.336

COSYNE 09

291

A–B

Author Index

Author Index
Abbott L., 79, 97
Abe H., 59
Achuthan S., 177
Acker C., 237
Acuna D., 146
Adams G., 277
Adams R., 109
Adelman T., 95
Adolphs R., 55
Aertsen A., 88
Ahmadian Y., 38, 102, 104
Ahrens K., 210
Ahrens M., 192, 193
Albert M., 151
Albright T., 131, 291
Alonso J.-M., 198, 283
Alvarez G., 35
Amari S.-i., 182
Amin N., 49
Anen C., 226
Antolik J., 203
Aoki T., 66
Aonishi T., 52
Aoyagi T., 66
Arabzadeh E., 269
Araya R., 134
Ardid S., 136
Armstrong K., 92
Asari H., 255
Ashourvan A., 280
Assad J., 83
Assisi C., 172
Aubie B., 111
Aura C., 277
Axel R., 23
Ayaz A., 206
Baccus S., 153, 185, 203
Baker J., 93
Balasubramanian V., 125, 188, 259, 282
Ballard D., 285
Banerjee A., 186
Barbieri R., 71
Bargmann C., 32
Barreiro A., 233

292

Barrett D., 253
Barua A., 261
Battaglia F., 158
Bays P., 217
Bazhenov M., 122, 172
Beck C., 129
Beck J., 71
Becker S., 111, 166, 240, 271
Bednar J., 203
Behabadi B., 212
Beierholm U., 165, 226
Benayoun M., 259
Benda J., 115
Benson C., 225
Benucci A., 128, 208, 288
Berens P., 119
Berger T., 153, 175
Berkes P., 40
Bernacchia A., 68
Berzhanskaya J., 210
Bessière P., 232
Bethge M., 119, 263, 277
Billings S., 120, 148
Binda P., 281
Black M., 283
Blackman R., 216
Blaha L., 124
Blei D., 28
Bomash I., 32
Bonin V., 132
Borgers C., 181
Born R., 122, 126
Borst A., 36, 279
Bosman C., 70
Bossaerts P., 226
Bouaouli N., 289
Bouchain D., 152
Bouecke J., 233
Bourjaily M., 84, 180
Boyden E., 101, 181
Bradley D., 92
Brainard M., 250
Bremmer F., 127, 131
Briggman K., 173
Briggs F., 199

COSYNE 09

Author Index
Brody C., 154
Bromberg-Martin E., 248
Brown E., 37, 71, 182, 201
Brown J., 280
Bruce I., 271
Buerck M., 81
Buesing L., 161
Buice M., 69
Buonomano D., 243
Burak Y., 123
Burbank K., 156
Bureau I., 113
Burkitt A., 81
Busse L., 128, 208, 288
Byrne P., 240
Bányai M., 242
Cadieu C., 94, 256
Cai X., 141
Callaway E., 53, 214
Canavier C., 176, 177
Carandini M., 128, 208, 288
Cardin J., 91, 96
Carlen M., 91
Carlsson G., 78
Carmena J., 250
Cash S., 100
Casile A., 72
Cassanello C., 27
Cerqueira J. J., 221
Cessac B., 66
Chafee M., 216
Chance F., 206
Chandrasekaran C., 272
Chang S., 92
Chase S., 268
Chastain E., 87, 143
Chen Z., 37
Chiappe* E., 95
Chichilnisky E., 38, 104
Chow B., 101
Chow C., 69
Christianson G. B., 112
Chrostowski M., 271
Churchland M., 92
Cicchini G. M., 281
Clark A., 92
Clifford C., 276
Clopath C., 161
Coca D., 120, 148
Coen-Cagli R., 123
Cohen M., 34, 92

COSYNE 09

C–D
Coleman T., 117
Collins A., 142
Compte A., 136
Conway B., 126
Cooper B., 87
Corrado G., 92
Costa R., 86, 168, 221
Cotton R. J., 63, 119
Cottrell G., 77, 240
Coulter W., 267
Cowan J., 69, 259
Cowell R., 240
Crum P., 113
Cunningham J., 92, 237
Curtis J., 138
Curto C., 178, 273
Cury K., 114
Dabaghian Y., 78
Dabrowski W., 290
Dale C., 256
Dall’Asén A., 87
Dan Y., 24, 31
Darvas F., 256
Davey N., 109
Daw N., 60, 87, 140
Dayan P., 60, 140, 224, 263
de la Rocha J., 26, 270
de Polavieja G. G., 51
Deadwyler S., 153
Dean H., 186
Deco G., 178, 244
Dehghani N., 100
Deisseroth K., 25, 91
DeMarse T., 229
Deneve S., 61, 187, 289
Denk W., 173
Desbordes G., 198, 283
Desrochers T., 287
Dhruv N., 202, 288
Diamond M., 159, 269
Dias-Ferreira E., 221
Dimitrov A., 195
Doiron B., 179, 270
Dolan R., 61, 140
Dorrn A., 85
Drewes J., 206
Droulez J., 232
Drover J., 228
Drugowitsch J., 56
Drysdale P., 67
Du J., 215

293

E–H
Duque A., 26
Ecker A., 119
Eglen S., 163
El-Shamayleh Y., 202
Elliott T., 194
Engel T., 139
Engelbrecht J., 270
Erdi P., 242
Escobar G., 82
Euler T., 173
Fabre-Thorpe M., 285
Fadeyev V., 290
Fairhall A., 101
Falconbridge M., 244
Famulare M., 101
Fares T., 171
Faugeras O., 66, 90
Faure P., 111
Felsen G., 157, 222
Ferguson M., 144
Fernandez F., 213
Ferrera V., 27
Field D., 151
Fiser J., 40
Fisher D., 126
Fitzgerald J., 83, 238
Fleischer F., 72
Flister E., 130
Florian R., 246
Frank L., 30, 78
Frank M., 35
Freedman D., 83, 103
Friederich U., 148
Fries P., 70, 105
Friston K., 61
Froemke R., 85
Frost W. N., 75
Fukai T., 134, 230
Gabbiani F., 284
Gallant J., 33
Ganguli K., 250
Ganguli S., 73
Gao D., 174
Gao J., 223
Gastpar M., 74
Gavornik J., 167, 249
Gegenfurtner K. R., 206
Gepshtein S., 291
Gershman S., 28
Gerstner W., 44, 161

294

Author Index
Gerwinn S., 263, 277
Gewaltig M.-O., 162
Ghazanfar A., 189, 272
Giese M., 72
Gill P., 74
Gilson M., 81
Giovannucci A., 99, 170
Gjorgjieva J., 163
Glascher J., 60
Glimcher P., 62
Goard M., 31
Gold J., 29, 247
Goldman M., 126
Goldreich D., 194
Goller F., 87
Grabska-Barwinska A., 197
Grattan L., 62
Graupner M., 175
Gray C., 93
Gray R., 67
Graybiel A., 81, 287
Green C. S., 225
Gremel C., 168
Griffiths T., 190
Gruen S., 182
Gruendlinger L., 235
Grzywacz N. M., 157
Grünewälder S., 108
Guentuerkuen O., 197
Guetig R., 73, 106
Guillaume M., 257
Gutkin B., 175, 187
Görgen K., 70
Hackett T., 113
Haider B., 26
Hairston W. D., 157
Halgren E., 100
Hamilton L., 194
Hampson R., 153
Han S., 174
Han X., 101, 181, 240
Harper N., 98
Harris J., 269
Harris K., 192
Harris K. D., 26, 273
Hartmann T., 131
Hasenstaub A., 53, 214
Haslinger R., 201
Hauser F., 152
Hebb A., 262
Hehrmann P., 98

COSYNE 09

Author Index
Heilman K., 236
Heinen S., 54
Helmstaedter M., 173
Hemmert W., 52
Herikstad R., 93
Herreros-Alonso I., 99, 170
Hikosaka O., 248
Hill E., 75
Histed M., 132
Hoffman K., 189
Hohl S., 199
Hollender L., 26
Holy T., 254
Hosoya T., 107
Hosseini P., 92
Houillon A., 232
Howard M., 55
Huang E., 218
Huang W., 36
Huerta R., 89
Hugues E., 178, 244
Huh D., 169
Huhn Z., 242
Husain M., 217
Huth A., 256
Hyvärinen A., 117
IIDA M., 52
Ishibashi K., 107
Ishii T., 107
Isomura Y., 134
Issa E., 113
Ito I., 45
Itskov P., 159
Itskov V., 178
Jacobs R., 196
Jadi M., 216
Jagadeesh B., 262
Jain R., 212
Jain S., 216
Jancke D., 197
Jaramillo S., 55
Jayaraman V., 95
Jazayeri M., 57
Jin J., 198, 283
Jin X., 86, 221
Johnson A., 221
Jones A., 210
Jones P., 284
Jones S., 96
Joshi P., 165

COSYNE 09

I–L
Josic K., 261, 270
José J., 250
Joukes J., 207
Juusola M., 51, 120, 148
Kahana M., 240
Kanan C., 77
Karabalin R., 215
Kaschube M., 204, 277
Kass R., 268
Kastner D., 185
Katahira K., 107
Katzner S., 128, 208, 288
Kawasaki H., 55
Kazama H., 191
Kepecs A., 143, 227
Kersten D., 225
Kesseli J., 252
Khan A., 54
Kim I.-J., 205
Kimchi E., 145
Kiss T., 242
Knill D., 265
Knoblauch A., 162
Knoblich U., 91, 253, 274
Kobayashi R., 133
Koch C., 138, 215, 285
Koch K., 259
Koechlin E., 142
Koepsell K., 94, 256
Kohn A., 92, 123
Kopec C., 154
Kopell N., 181
Kording K., 150
Kovach C., 55
Koyama S., 268
Kreiman G., 103, 156
Krekelberg B., 127, 131, 207
Kremkow J., 257
Krueger K., 224
Kulkarni J., 38
Kumbhani R., 202
Körner E., 162
Körner U., 162
Köster U., 117
Lak A., 269
Lalor E., 102
Latham P., 71, 150, 171, 253
Lau H., 64, 218
Laubach M., 145, 258
Laurent G., 184, 185, 215

295

M

Author Index
Lauritzen T., 137
Law J., 203
Leahy R., 256
LeBeau F., 181
Lee D., 59, 141
Leifer A., 290
Lein E., 210
Lengyel M., 40, 241
Leopold D., 277
Lerchner A., 171
Lesica N., 198
Lesmes L., 291
Lewis J., 283
Li C., 24
Li J., 42
Li M., 101, 181
Lima B., 37, 105, 201
Linden J., 112, 192, 193
Lindgren J., 117
Lisberger S., 42, 199
Litke A., 290
Liu J., 204, 243
Liu Y., 47
Lo C.-C., 220
Lochmann T., 187
Logothetis N., 189
Loncich K., 270
Longtin A., 115
Lott G., 95
Louie K., 62
Luk C.-H., 219
Luthman J., 109
Luvizotto A., 260
Ma J., 261
Ma R., 117
Ma W. J., 36
Machens C., 252, 279
Macke J., 119, 263, 277
MacLeod D., 244
Maex R., 109
Maier A., 277
Maier J., 98
Mainen Z., 219, 222, 227, 275
Maler L., 115
Maloney L., 131
Malpeli J., 117
Mamelak A. N., 77
Maniscalco B., 64, 218
Mannion D., 276
Marguet S., 273
Markram H., 175

296

Marmarelis V., 153
Marpeau F., 261
Martin R., 264
Masmanidis S., 215
Masquelier T., 244
Matsumoto M., 248
Maunsell J., 34
McAlpine D., 98
McClelland J., 223
McCormick D., 26
McDonald S., 276
McPeek R., 54
Mechler F., 116
Meier P., 130
Meister M., 123, 205, 255, 290
Mel B., 135, 161, 212, 216
Meletis K., 91
Melo I., 221
Memoli F., 78
Mendel J. M., 157
Mendez J., 87
Menz M., 203
Mercado III E., 193
Merzenich M., 193
Meyers E., 103
Middleton J., 115
Miller C., 58
Miller E., 23, 103
Miller K., 245, 262, 278
Miller P., 84, 180
Miller R., 231, 234, 259
Mintz M., 99
Mirollo R., 270
Mishchenko Y., 229
Mitchell J., 138
Miura K., 275
Moazzezi R., 263
Mohan A., 76
Moldakarimov S., 122
Monaco J., 79
Montague P. R., 50
Moore C., 91, 96, 274
Moore T., 92
Moore-Kochlacs C., 75
Moreno-Bote R., 56, 265
Morrone M. C., 281
Morvan C., 131
Movshon J. A., 92, 202, 288
Munro S., 74
Murakami M., 219
Murray I., 157
Mutch J., 253

COSYNE 09

Author Index
Nagarajan V., 76
Nakahara H., 248
Nakamura K., 248
Narayan A., 184
Narayanan N., 145, 258
Nassar M., 29
Natarajan R., 157
Navalpakkam V., 138
Nawrot M., 88
Nemenman I., 155
Neuenschwander S., 37, 105, 201
Neumann H., 129, 164, 233
Newsome W., 92, 223
Ng S. W. B., 197
Nguyen D., 71
Nikolaev A., 51
Nirenberg S., 32
Niv Y., 28
Norcia A., 231, 234
Nowicki D., 160
Nowotny T., 89, 147
O’Doherty J., 60
Obermayer K., 108, 121, 264
Ohiorhenuan I., 116
Ohzawa I., 286
Oizumi M., 107, 191
Ojemann J., 262
Okada M., 52, 107, 191
Olshausen B., 239
Omori T., 52
Ong C.-y., 45
Onken A., 108
Oostenveld R., 70, 105
Orban G., 40
Orduna I., 193
Oswald A.-M., 179
Otte S., 53
Oviedo H., 113
Ozuysal Y., 153
OKane C. J., 51
Packer A., 146
Palm G., 152
Pandarinath C., 32
Paninski L., 38, 102, 104, 146, 149
Pantazis D., 256
Papadopoulou M., 185
Parga N., 26
Park C., 251
Park I., 229
Pennartz C., 158

COSYNE 09

N–R
Perge J., 259
Perin de Campos R., 175
Perona P., 138
Perrinet L., 65, 257
Pesaran B., 186
Petersen C., 46
Pfau D., 149
Pfister J.-P., 241
Pillow J., 38, 102, 104
Pipa G., 37, 201
Pitkow X., 118, 149
Poggio T., 103, 253, 285
Poizner H., 236
Poo M.-M., 24
Port N., 280
Pouget A., 56, 71, 265
Prentice J., 125, 188, 282
Price N., 126
Principe J., 229
Purpura K., 116
Qian X., 101, 181
Quartz S., 226
Rahnev D., 218
Ramachandra C., 212
Raman B., 45
Rangel A., 27, 138
Rao M., 229
Rao R., 143, 183, 262
Rapela J., 157
Raudies F., 164, 233
Raymond J., 43
Read J., 280
Reddy L., 285
Redish A. D., 221
Reid R. C., 132
Reinagel P., 130
Reiser M., 95
Renart A., 26
Renno-Costa C., 260
Reyes A., 26, 270
Reyes A. D., 179
Reynolds J., 138
Rickert J., 88
Riehle A., 88
Ringbauer S., 164
Rinkus G., 110
Rinzel J., 179
Robinson P., 67
Roelfsema P., 85
Roemschied F., 127

297

S–T
Roiser J., 140
Rokni U., 123
Rorie A., 223
Rosenbaum R., 261
Ross I. B., 77
Rothkopf C., 41
Rotter S., 88
Roukes M., 215
Rowekamp R., 211
Rowland J., 209
Rubchinsky L., 251
Rubin D., 278
Rudnicki M., 52
Rusu C., 246
Rutishauser U., 77
Ryu S., 92, 237
Sadek A., 215
Sahani M., 92, 98, 112, 192, 193, 237
Sakata S., 192, 273
Sanes J., 205
Sanger T., 231
Santhanam G., 237
Santhanam S., 92
Sasaki K., 286
Satoh R., 191
Savin C., 44
Schiff N., 228
Schmid A., 46, 116
Schmitzer-Torbert N., 221
Schneider D., 273
Schneidman E., 188
Scholl B., 49
Schrater P., 146, 225
Schreiner C., 85
Schumacher J., 273
Schuman E., 77
Schwartz A., 268
Schwartz O., 123
Scott B., 92
Scott S. H., 42
Sederberg A., 204
Seelig J., 95
Segev R., 39
Sejnowski T., 53, 75, 122, 214
Senn W., 266
Seo H., 141
Serre T., 285
Seymour B., 61, 140
Shadlen M., 57
Shamir M., 39
Shamma S., 48

298

Author Index
Shams L., 157, 165
Sharpee T., 47, 211, 238
Shea-Brown E., 233, 270
Shen K., 184
Shenoy K., 92, 237
Shenoy P., 143
Shi L., 190
Shimazaki H., 182
Shinomoto S., 133
Shlens J., 38, 104
Shouval H., 167, 249
Shuler M., 167
Siegelmann H., 160
Silberberg G., 175
Simmons K., 125
Simoncelli E., 38, 102, 104
Simpson G., 256
Sinclair R., 212
Singer A., 30
Singer W., 105
Singh G., 78
Smith M., 92
Smith N., 258
Smolyanskaya A., 122
Snyder L., 92
Sober S., 250
Sohal V., 25
Sohl-Dickstein J., 239
Sokol S., 288
Sommer F., 267
Somogyvari Z., 242
Sompolinsky H., 73, 106, 123
Song Z., 120
Sousa J. C., 221
Sousa N., 221
Spivey M., 144
Sprekeler H., 44
Stanley G., 198, 283
Stepanyants A., 82, 171
Sterling P., 259
Steuber V., 109
Stevenson I., 150
Stiefel K. M., 212
Stopfer M., 45
Stratton P., 69
Sugrue L., 92
Sussillo D., 97
Svoboda K., 113
Szechtman H., 166
Szuts T. A., 290
Tabuchi Y., 286

COSYNE 09

Author Index
Tadmor Y., 264
Takahashi N., 54
Takiyama K., 107
Talei Franzesi G., 181
Tanifuji M., 200
Tenenbaum J., 35, 40
Teng C.-L., 247
Teramae J.-n., 230
Theunissen F., 49, 74, 194
Thilo E., 233
Thorn C., 81
Thorpe S., 244
Tierz M., 180
Tkacik G., 125, 188, 282
Todorov E., 169
Tolias A., 119
Tolias A. S., 63
Tong M., 77
Tootoonian S., 184
Torben-Nielsen B., 212
Torres E., 236, 250
Tortell R., 223
Touboul J., 66
Toulouse T., 166
Touryan J., 157
Townsend J., 124
Toyoizumi T., 245
Triesch J., 41, 44, 165
Trommershäuser J., 206
Tsai L.-H., 91
Tschetter W., 32
Tsodyks M., 30, 106, 235
Tsubo Y., 133, 134
Tsuchiya N., 55, 285
Turaga D., 254
Turesson H., 189, 272
Turner G., 185
Tyukin I., 291
Uchida N., 114, 227, 275, 290
Ulinski P., 196
Ullman S., 200
Urbanczik R., 266
Usrey W. M., 199
van Albada S., 67
van der Meer M., 221
van Drongelen W., 259
van Hemmen J. L., 81
Van Ooyen A., 85
van Vreeswijk C., 182
Vasconcelos N., 174

COSYNE 09

U–W
Vasilaki E., 161
Vasserman G., 39
Velliste M., 268
Veltz R., 90
Verschure P. F. M. J., 99, 170, 260
Victor J., 32, 46, 116, 228, 282
Vidal-Naquet M., 200
Vidne M., 38
Vierling-Claassen D., 96
Vildavski V., 231, 234
Vinck M., 105
Vinnik E., 159
Vladimirskiy B., 266
Vogels T., 134
Vogelstein J., 146
Voges N., 65, 257
Von Der Heide R., 124
Vul E., 35
Wade A., 137, 209
Wallace E., 150, 259
Wallis J., 219
Wang H., 52
Wang J., 75, 239
Wang X., 58, 113
Wang X.-J., 68, 136, 139, 220
Wardill T. J., 51
Watling L., 85
Weber D., 256
Weber F., 279
Wehr M., 49
Wei Y., 135
Weisswange T., 41
Weng C., 198
Wenger M., 124
White J., 213, 237
White L., 277
Whitford A., 268
Whittington M., 181
Wiles J., 69
Williamson R., 193
Wilson M., 71
Wilson R., 29
Wimmer K., 121
Wissig S., 123
Wittenberg G., 80
Wojnowicz M., 144
Womelsdorf T., 70, 105
Woodman M., 176
Woolley S., 273
Worth R., 251
Wu X., 161

299

Y–Z

Author Index

Wyeth G., 69
Yanike M., 27
Yanping H., 183
Yen S.-C., 93
Yizhar O., 25
Yoshida W., 61
Yu A., 141
Yu B., 92, 237
Yuste R., 134, 146
Zador A., 55, 113
Zanos T., 153
Zariwala H., 210
Zavada A., 147
Zemel R., 157
Zeng H., 210
Zhang F., 25, 91
Zhang L., 77
Zhang Y., 205
Zheng L., 51
Ziegler L., 161
Zucca R., 99, 170
Zwingman T., 210

300

COSYNE 09

