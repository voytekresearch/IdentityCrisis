Cosyne 2008
Computational and Systems Neuroscience

Conference Program and Abstracts

28th February – 2nd March 2008
Marriot Hotel, Downtown
Salt Lake City, Utah

The annual Cosyne meeting provides a forum for the exchange of experimental and theoretical
results in systems neuroscience. Presentations are arranged in a single track, so as to encourage
interdisciplinary interactions.
The 2008 meeting consists of 13 invited talks selected by the Executive Committee, along with
20 talks and 300 posters selected from submitted abstracts by the Program Committee. Twelve of
the poster presenters will also give a 4-minute “spotlight” presentation summarizing their
submitted work. Abstracts will remain available on the internet after the meeting at
http://cosyne.org
Cosyne 2008 Organizing Committee:
•
•
•
•

General Chair: Eero Simoncelli (NYU)
Program Chair: Matteo Carandini (Smith-Kettlewell)
Workshop Chair(s): Fritz Sommer, Jascha Sohl-Dickstein (UC Berkeley)
Publicity Chair: Alex Wade (Smith-Kettlewell)

Cosyne 2008 Program Committee:
•
•
•
•
•
•
•
•
•
•
•
•

Matteo Carandini, chair (Smith-Kettlewell)
Dora Angelaki (WUSTL)
Mathew Diamond (International School for Advanced Studies, Trieste)
Allison Doupe (UCSF)
Adrienne Fairhall (U Washington)
Michael Hasselmo (Boston U)
Adam Kepecs (CSH)
Peter Latham (UCL)
Klaus Obermayer (Technische Universität Berlin)
Bijan Pesaran (NYU)
John Reynolds (Salk Institute)
Daniel Wolpert (Cambridge U)

Cosyne 2008 Executive Committee:
•
•
•
•
•

Tony Zador (CSHL)
Alex Pouget (U Rochester)
Zach Mainen (CSHL)
Eero Simoncelli (NYU)
Matteo Carandini (Smith-Kettlewell)

1

2

Swartz Foundation Travel Fellowships
The Swartz Foundation for Computational Neuroscience has generously provided travel
fellowships for students/postdocs who are primary authors of contributed abstracts.
Awards were allocated based on need, as well as program committee assessment of the
submitted abstract. Recipients were as follows:
Misha Ahrens, Omri Barak, Urs Bergmann, Tim Blanche, Sen Cheng, Tom Davidson, Joset A.
Etzel, Jeffrey Gavornik, Venkatesh Gopal, Brett Graham, Robert Guetig, Andreas Kardamakis,
Roozbeh Kiani, Nienke Korsten, Hagai Lalazar, Jeremy Lewi, Nuo Li, Andrew Isaac Meso,
Srdjan Ostojic, Elena Phoka, Nicolas Pinto, Kerstin Preuschoff, Hannes Saal, Rodrigo F.
Salazar, Cristina Savin, Lavi Shpigelman, Christian Tetzlaff, Georgios Tsirogiannis, Jan
Wiltschut, Ilana B. Witten, Philipp Wolfrum, Marc Zirnsak

Conference Support
•

Registration/Hotel arrangements: Beth Heberger, Conference and Events Office,
University of Rochester

•

Online Submissions/Reviews: Thomas Preuss, confmaster.net

•

Poster/Program cover design: Jose Acosta, Laboratory for Computational Vision, New
York University

3

N e w f ro m Th e M I T Pre s s

Dynamical Systems in
Neuroscience

The Geometry of Excitability and Bursting
Eugene M. Izhikevich
Explains the relationship of electrophysiology,
nonlinear dynamics, and the computational
properties of neurons, with each concept
presented in terms of both neuroscience and
mathematics and illustrated using geometrical intuition.
Computational Neuroscience
457 pp., 409 illus., $60 cloth

edited by Guido Dornhege, Jose del R. Millan,
Thilo Hinterberger, Dennis J. McFarland,
and Klaus-Robert Müller
The latest research in the development of technologies that will allow humans to communicate, using brain signals only, with computers,
wheelchairs, prostheses, and other devices.
Neural Information Processing series
519 pp., 150 illus., $45 cloth

Bayesian Brain

Microcircuits
The Interface between Neurons and Global
Brain Function
edited by Sten Grillner and Ann M. Graybiel
Leading neuroscientists discuss the function
of microcircuits, functional modules that act as
elementary processing units bridging single
cells to systems and behavior.
Dahlem Workshop Reports
472 pp., 18 color illus., 60 b & w illus., $55 cloth

The MIT Press

Toward Brain-Computer
Interfacing

Probabilistic Approaches to Neural Coding
edited by Kenji Doya, Shin Ishii,
Alexandre Pouget and Rajesh P. N. Rao
Experimental and theoretical neuroscientists
use Bayesian approaches to analyze the brain
mechanisms of perception, decision-making,
and motor control.
Computational Neuroscience
340 pp., 102 illus., 10 color, $54 cloth

Visit our

BOOTH

To order call 800-405-1619 • http://mitpress.mit.edu

for a 20%

DISCOUNT

HFSP Journal
F r o nt i e r s o f I nt e r d i sc i p l i na r y
R e s e a r c h in t h e L i f e S c i e nc e s

Single Molecule Imaging and Micromanipulation
Experimental and Theoretical Biology
Origin of Life
Chemical Genetics
Biomagnetism
Computational Biology
Physical Biology

http://hfspj.aip.org
info@hfsp-publishing.org

Nature Neuroscience
Nature Neuroscience provides the international
community with a highly visible forum in which
the most exciting developments in all areas of
neuroscience can be communicated to a broad
readership. A lively front half, including News &
Views, Reviews, Book Reviews and editorials
help place the primary research in context,
providing readers with a broad perspective on
the entire ﬁeld.

Nature Reviews Neuroscience
Nature Reviews Neuroscience is the leading
monthly review journal in the neurosciences.
It publishes articles that review recent progress
in brain and nervous system research. Topics
range from molecular and cellular aspects of
neuronal development and function to behaviour,
cognition and disorders of the nervous system.
Chief Editor: Claudia Wiedemann
Impact Factor: 23.054*
2/199 in Neuroscience

Editor: Sandra Aamodt
Impact Factor: 14.805*
4/199 in Neuroscience

Nature Clinical Practice Neurology
Nature Clinical Practice Neurology delivers
up-to-date developments in neurology with clear,
concise interpretations of clinical applications and
a format that allows for quick and easy reference.
Providing scientiﬁcally sound evaluations of new
studies and insightful commentaries from leading
experts, as well as in-depth reviews on topics of
current interests, readers will always be informed
of the latest ﬁndings as they relate to practical
patient care.
Editor-in-Chief: John W Grifﬁn, MD

www.nature.com/reviews/neuro

www.nature.com/ncpneuro

www.nature.com/natureneuroscience

www.nature.com/neurosci

T h e R Z 2 Z-S e r i e s P r o c e s s or
Streamlines
data acquisition

Minimizes

post-hoc analysis

Eliminates

TDT’s new System 3 Z-Series processor and preampliﬁer deliver increased
processing, throughput and input channels required to record neurophysiological
data from up to 256 channels.

data transfer bottlenecks

Increases

realizable sampling rates

• User Conﬁgurable Real-Time Processing
• Optically Isolated Direct Digital PreAmps
• Integrated Stimulus Generation
• Powerful Software Control
• High Channel Count, High Sampling Rate System

Completely

System 3 compatible

Tucker-Davis Technologies • 11930 Research Circle • Alachua, Fl 32615
Phone: 386.462.9622 • Fax: 386.462.5365 • E-mail: info@tdt.com

w

w

w .

t

d

t

.

c

o

m

Meeting schedule

Cosyne 2008

Thursday, 28 February
Time

Page

4.00pm

Registration desk opens

6:00pm

Welcome reception, including cocktails and buffet (funded by the Swartz
Foundation)

7:25pm

Keynote address, Models of Visual Recognition in the Ventral System,
Tomaso Poggio, MIT

8

8:30pm

Poster session I (100 posters)

9-108

Friday, 29 February

7.30am

Continental breakfast

8.30am

Neural Basis of Reach Preparation , Krishna Shenoy, Stanford University
(invited)

109

9.15am

Parietal Reach Region Cell Classes Have Complementary Planning
Responses, EB Torres and RA Andersen

110

9.30am

The Energetic Cost of Fast Spiking, AR Hasenstaub, SL Otte, EM
Callaway, and TJ Sejnowski

111

9.45am

Timing Based on Stochastic Neural and Sensory Processes, MB Ahrens
and M Sahani

112

10.00am Refreshment break
Encoding and Processing of Primary Sensory Variables by the Rat
10.30am Vibrissal/Trigeminal System, Mitra Hartmann, Northwestern University
(invited)
Cortical Activity Influences the Efficacy of Geniculocortical
11.15am
Communication, F Briggs and WM Usrey

113
114

11.30am

Entorhinal Grid Cell Responses as Basis Functions for Spatial
Reinforcement Learning, NJ Gustafson and N Daw

115

11.45am

Hippocampal Coding of Point-Free Topology, YA Dabaghian, AG Cohn
and LM Frank

116

4

Cosyne 2008

Meeting schedule

12.00pm Lunch break (and last chance to see Session I posters)
2.15pm
3.00pm

Cerebellar Long Term Depression as a Supervised Learning Rule with
All or Nothing Character, Mitsuo Kawato, ATR Computational
Neuroscience Labs (invited)
Value Representation Via Divisive Normalization in Parietal Cortex, K
Louie, L Grattan, and PW Glimcher

3.15pm

Refreshment break

3.45pm

Associative Learning Signals in the Monkey Medial Temporal Lobe,
Wendy Suzuki, New York University (invited)

4.30pm

Spotlight presentations: Posters II-1, II-14, II-16, II-54, II-64, II-71

5.00pm

Dinner break

7.30pm

Poster session II (100 posters)

117
118

119

120-219

Saturday, 1 March

7.30am
8.30am
9.15am

Continental breakfast
Traveling Waves in Cerebellar Cortex Mediated by Asymmetric
Synaptic Connections Between Purkinje Cells Michael Häusser,
220
University College London (invited)
Analysis of Decision Making in an Insect’s Gap-Crossing Behavior Using
221
Markov Models, AA Faisal, JE Niven, and SM Rogers

9.30am

Caudate Activity in a Decision-Making Reaction Time Task, L Ding and
JI Gold

222

9:45am

Cortical Topography of Intracortical Inhibition Explains Speed of
Decision Making, HR Dinse and C Wilimzig

223

10.00am Refreshment break
10.30am

Adaptive Gain Control in the Auditory System, David McAlpine,
University College London (invited)

224

11.15am

Mechanisms for Complex Feature Selectivity in the Songbird Auditory
Forebrain, CD Meliza, Z Chi, and D Margoliash

225

5

Meeting schedule

Cosyne 2008

11.30am

Learning Transformational Invariants from Time-Varying Natural
Images, CF Cadieu and BA Olshausen

226

11.45am

Natural Experience Drives Online Learning of Tolerant Object
Representations in Visual Cortex, N Li and JJ DiCarlo

227

12.00pm Lunch break (and last chance to see Session II posters)
2.15pm

Charting the Human Posterior Parietal Cortex, Sabine Kastner, Princeton
228
University (invited)

3.00pm

A Computational Model Relating Changes in the BOLD Signal to Neural
229
Activity in Cortex, WG Gibson, L Farnell, and MR Bennett

3.15pm

Refreshment break

3.45pm

A Hierarchy of Temporal Receptive Windows in Human Cortex, David
Heeger, New York University (invited)

4.30pm

Spotlight presentations: Posters III-41, III-42, III-55, III-72, III-73, III-96

5.00pm

Dinner break

7.30pm

Poster session III (100 posters)

230

231-330

Sunday, 2 March

7.30am

Continental breakfast

8.30am

Neuronal Circuit Reconstructions to Principles of Brain Design, Dmitri
'Mitya' Chklovskii, Howard Hughes Medical Institute (invited)

9.15am
9.30am
9.45am

Neuronal Ensemble Bursting in the Basal Forebrain Encodes Salience
Irrespective of Valence, S-C Lin and MAL Nicolelis
The Hemo-Neural Hypothesis: A Proposed Role for Blood Flow in
Neuromodulation and Information Processing, CI Moore, U Knoblich, R
Cao, J Cardin, B Higashikubo, and JC Brumberg
Spatially Inhomogeneous Processing of Visual Motion by Drosophila,
MB Reiser

10.00am Refreshment break

6

331
332
333
334

Cosyne 2008

Meeting schedule

10.30am

Oscillations Organize Internally Advanced Cell Assembly Sequences,
György Buzsáki, Rutgers University (invited)

335

11.15am

Quantitative Single-Neuron Modeling: Competition 2008, R Naud, T
Berger, L Badel, A Roth, and W Gerstner

336

11.30am

Neural Mechanisms of Speech Processing: Time Warp Invariance by
Adaptive Integration Time, R Gütig and H. Sompolinsky

337

11.45am Lunch break (hotel checkout, and last chance to see Session III posters)
2.15pm
3.00pm

3.15pm

4.00pm

Olfactory Processing in a Tiny Microcircuit, Rachel Wilson, Harvard
Medical School (invited)

338

Modeling Olfactory Discrimination in Drosophila, SX Luo, R Axel and
339
LF Abbott
High-Speed Depth-Targetable Control of Genetically Defined Neurons in
Freely Moving Mammals: Technology Development and
340
Neuropsychiatry Application, Karl Deisseroth, Stanford University
(invited)
Final remarks, Eero Simoncelli

7

Thursday PM, Keynote talk

Cosyne 2008

Models of Visual Recognition in the Ventral Stream
Tomaso Poggio
Massachusetts Institute of Technology, Cambridge, MA
I will describe a class of quantitative models of the ventral stream for object recognition, which have been
developed during the last two decades from the anatomical and physiological data and which are quite
successful in explaining several physiological data across different visual areas. Surprisingly, such models
also mimic the level of human performance in difficult rapid image categorization tasks in which human
vision is forced to operate in a feedforward mode. I will also describe recent read-out data from IT for the
same complex natural images used in the psychophysics.
I will then focus on the key limitations of such hierarchical feedforward models for object recognition,
discuss why they are incomplete models of vision and suggest possible alternatives focusing on the
possible computational roles of cortical backprojections.
Relevant papers can be downloaded from: http://cbcl.mit.edu/

8

Cosyne 2008

Thursday evening, Poster session I-1

Bayesian inference accounts for the ﬁlling-in and suppression of
visual perception of bars by context
Li Zhaoping1 , Li Jingling2
1

University Colllege London, UK

2

China Medical University, Taiwan

Visual input sensitivity and object recognition are largely inﬂuenced by contextual inputs. For instance,
human sensitivity to detect a target bar is increased by the presence of colinear contextual bars which could
group with the target into a smooth contour. The neurons in the primary visual cortex (V1) also increase their
responses to a bar in the presence of colinear contextual bars. Using visual psychophysics and modeling
methods, we investigate how contextual bars bias one to perceive or not to perceive the presence of a target
bar, rather than on the input sensitivity. Unexpectedly, human observers are more likely to perceive a target
when the context has a weaker rather than stronger contrast (Fig.1B & C). When the context can perceptually
group well with the would-be target, weak contrast contextual bars bias the observers to perceive a target
relative to the condition without contexts, as if to ﬁll in the target (Fig. 1A left). Meanwhile, high contrast
contextual bars, regardless of whether they groups well with the target, bias the observers to perceive no
target (Fig. 1A middle & right, and Fig. 1C). A Bayesian model of visual inference is shown to account
for the data well, illustrating that the context inﬂuences the perception in two ways: (1) biasing observers’
prior belief that a target should be present according to visual grouping principles, and (2) biasing observers’
internal model of the likely input contrasts caused by a target bar. According to this model, our data suggest
that the context does not inﬂuence the perceived contrast despite its inﬂuence on the bias to perceive the
target’s presence, thereby suggesting that cortical areas beyond V1 are responsible for the visual inferences.
A: Detecting a weak vertical target bar in various contexts
Strong colinear context

Weak colinear context

B: Probability of seeing the target
1 in weak or no context

Non−colinear context
0.8
0.6
0.4
Colinear
Orthogonal
no context

0.2
0
−2

target

0

2

4

Target contrast Ct

6

8

10
x 10

−3

C: Probability of seeing the target
1 in strong or no context

target

0.8

Colinear
Orthogonal
no context

0.6
0.4
0.2

Probability P(C |yes ) for a low contrast
C by a target is higher in weaker context

Prior belief P(yes) for a target
is higher in colinear context

0
−2

0

2

4

Target contrast Ct

6

8

10
x 10

−3

Figure 1: A: even though the vertical target has the same low luminance contrast against background in all
three contexts, one is most likely to see a vertical target bar in the left (ﬁlling-in), and least likely in the
right (supprssion), example. B & C: human observer’s probability of seeing the vertical target bar (vs. target
contrast) is higher in weak context (B) than in strong context (C).
Acknowledgments
We thank Gatsby Charitable Foundation and BBSRC for funding, and J.A. Solomon, M.J. Morgan, and P.
Dayan for comments.

9

Thursday evening, Poster session I-2

Cosyne 2008

Towards functional and anatomical mapping of every
supragranular neuron in behaving mouse V1
Mark L. D. Andermann, Aaron Kerlin, and R. Clay Reid
Department of Neurobiology, Harvard Medical School, Boston, MA, 02115
Task-dependent modulation of visual activity has been observed using electrophysiology and fMRI in
primate thalamus and primary visual cortex (V1). However, effects appear less robust than in “higher
order” visual areas. Why is this the case? One possibility is that the diversity of interdigitated functional
circuits in V1 influence a broader range of behaviors than more specialized visual areas. In this way, the
observed weaker mean correlation with specific behaviors observed in V1 may be due to sampling from
neurons with different functional and anatomical connectivity. We suggest that concerted, task-specific
modulation of neural activity in functionally and anatomically defined sub-circuits in V1 may exert a
rapid and considerable influence on target areas and on behavioral outcome.
To test these hypotheses, we are developing a simple visual task in awake, head-fixed mice. Using in vivo
two-photon microscopy, we will simultaneously monitor calcium activity in large populations of layer 2/3
neurons in mouse V1 during an orientation discrimination task. Sub-circuits will be defined by visual
response properties, anatomical cell type (transgenic mice), and cell-cell correlations in spontaneous
activity. In this way, we can assess the task-modulation of neural responses in identified assemblies, and
their combined influence on behavioral responses. In the future, we hope to use optical techniques to
directly and selectively perturb neural activity in task-modulated local assemblies of neurons during
behavior.
In preliminary experiments, we have carried out two-photon imaging of visual responses in mouse V1
neurons during anesthesia and quiet waking. A subset of neurons were exquisitely direction selective, and
virtually all neurons were driven by visual stimulation. This suggests a greater similarity between primary
visual cortex in rodent and other mammalian visual systems than has been previously proposed.
Acknowledgements
This work was supported by the National Eye Institute and the Helen Hay Whitney Foundation.

10

Cosyne 2008

Thursday evening, Poster session I-3

A Neural System for Scale and Orientation Invariant
Correspondence Finding.
Jenia Jitsev, Yasuomi D. Sato, and Christoph von der Malsburg
Frankfurt Institute for Advanced Studies, Johann Wolfgang Goethe University
Proposing A Neural Mechanism. Transformation-tolerant recognition seems to still pose a hard challenge
for artiﬁcial vision systems, as opposed to their biological counterparts. We present a neurally-plausible
mechanism to establish local feature correspondences between object images at different scales and orientations while using and explicitly representing the information about the given transformations. The
fundamental functionality is based on a network of cortical macrocolumns supporting the Dynamic Link
Architecture (DLA, [1]). This architecture can dynamically route information ﬂowing from one domain to
another, adapting the initial all-to-all connectivity of processing units to match potential transformations.
As result, the mapping, constructed between the local features of the input domain and the model domain,
provides solution for the correspondence problem, while the transformation detection mechanism in control domain signals for encountered scale and rotation (see the ﬁgure). Both tasks are accomplished in a
distributed, parallel fashion, assisting rapid processing.
Correspondence Finding via Transformation Detection. Using
Gabor wavelets to represent local features, the system is able to detect the transformations of scale and orientation by competitive evaluation of their likelihood. This evaluation, based on computation of
transformation-speciﬁc feature similarities, is carried out by a control
column. The control column entails a number of control units, each
representing a decision favoring a speciﬁc orientation-scale combination. At the same time, the control units, whose activities also signal
to enable or disable transformation-speciﬁc connections between the two domains, guide the map formation, relying on the same competitive decision process to choose the mapping most suitable for the current
task, solving the correspondence problem. Taking advantage of a columnar population code, the mapping
between the two domains can be established in very short, neurophysiologically plausible time [2].
Result and Outlook. Using natural face photographs, we demonstrate that the system is able to cope with
a broad range of transformations in orientation and scale, even if the input is substantially corrupted by
noise. As the next step, we are aiming to decompose the processing of orientation and scale by introducing
separated control columns for each transformation type in order to improve further the competitive transformation detection and to reduce substantially the total number of the control units.
Acknowledgments
This work was supported by the EU project DAISY, FP6-2005-015803 and by the Hertie Foundation.
References
[1] Dynamic link architecture. C. von der Malsburg, The Handbook of Brain Theory and Neural Networks,
Second Edition, 365-368. MIT Press, Cambridge, Mass., 2003.
[2] Rapid correspondence ﬁnding in networks of cortical columns. J. Luecke and C. von der Malsburg, In
Proc. ICANN, LNCS 4131, 668-677. Springer, 2006

11

Thursday evening, Poster session I-4

Cosyne 2008

Decoding Frequency and Timing of Emotion Perception from Direct
Intracranial Recordings in the Human Brain
Naotsugu Tsuchiya1, Hiroto Kawasaki2, Matthew A. Howard, III2 and Ralph
Adolphs1
1

California Institute of Technology,

2

University of Iowa

How do regions of higher-order visual cortex represent information about emotions in facial expressions?
This question has received considerable interest from fMRI, lesion, and electrophysiological studies. The
most influential model of face processing argues that static aspects of a face, such as its identity, are
encoded primarily in ventral temporal regions while dynamic information, such as emotional expression,
depends on lateral and superior temporal sulcus and gyrus [1,2]. However, supporting evidence comes
mainly from clinical observation and fMRI, both of which lack temporal resolution for information flow.
Recently, an alternative theory has been proposed which suggests that common initial processing for both
aspects occurs in the ventral temporal cortex [3]. To test these competing hypotheses, we studied
electrophysiological responses in 9 awake human patients undergoing epilepsy monitoring, in whom over
120 sub-dural electrode contacts were implanted in ventral temporal (including fusiform face area, FFA)
and lateral temporal (including superior temporal sulcus, STS) cortex. The patients viewed static and
dynamic facial expressions of emotion while they performed either a gender discrimination or an emotion
discrimination task. To analyze broadband field potentials (sampled at 2KHz) recorded concurrently from
the many implanted electrodes, we used a decoding method that quantified the information about the
facial stimulus that is available from the time-varying oscillatory activity of neurons. We estimated the
stimulus-induced oscillation from a time-frequency spectral analysis using a multi-taper method. This
time-frequency representation of the response was then subjected to a multivariate decoding analysis. We
trained a regularized least square classifier on 70% of the trials and tested its classification performance
on the remaining 30%. At each time step, information was combined either across channels to obtain a
time-frequency decoding map or across channels and frequencies to obtain the time course of decoding
performance. Our analysis revealed that ventral temporal cortex (including FFA) rapidly categorizes faces
from non-face objects within 100ms. Most information was contained in the high-gamma band range
(50-150 Hz). We found that ventral temporal cortex represents emotion in dynamic morphing faces more
quickly and accurately than lateral temporal cortex (including STS), which is consistent with a newer
alternative theory3. Finally we found that the quality of represented information in ventral temporal
cortex is substantially modulated by task-relevant attention.
Acknowledgments
N.T. is supported by Japan Society for the Promotion of Science (JSPS). H.K. is supported by NIH (R03
MH070497-01A2). M.H. is supported by NIH (R01 DC004290-06). R.A. is supported by the James S.
McDonnell Foundation.
References
[1] V. Bruce and A. Young, in British journal of psychology (London, England : 1953) (1986), Vol. 77 (
Pt 3), pp. 305.
[2] J. V. Haxby, E. A. Hoffman, and M. I. Gobbini, in Trends Cogn Sci (Regul Ed) (2000), Vol. 4, pp.
223.
[3] A. J. Calder and A. W. Young, in Nat Rev Neurosci (2005), Vol. 6, pp. 641.

12

Cosyne 2008

Thursday evening, Poster session I-5

Concurrent increases in selectivity and tolerance produce constant
sparseness across the ventral visual stream
Nicole C Rust and James J DiCarlo
McGovern Institute for Brain Research, MIT
Neural coding schemes that minimize the number of neurons activated at any one time (or equivalently
maximize “sparseness”) are thought to be both metabolically and computationally efficient [reviewed by
1]. But does sparseness increase as signals propagate through the cortex? To investigate this question, we
compared the response properties of neurons at different stages along the pathway supporting object
recognition, the ventral visual stream. Specifically, we recorded the responses of neurons in a mid-level
visual area (V4) and a high-level visual area (anterior inferotemporal cortex, IT) to a large set of natural
images while monkeys performed an object detection task. We found that the distributions of sparseness
values in V4 and IT were indistinguishable and that most neurons in both areas were broadly tuned.
Similarly, individual images activated the same, large fraction of neurons in V4 and IT. Thus it appears
that a coding principle is conserved at each level of processing; however, in opposition to theories of
sparse coding, the tuning we observed in mid- and high-level vision is more consistent with a broadly
distributed coding scheme [see also 2].
Example natural and scrambled images [3]

If sparseness is not changing, what is happening as signals
propagate through the visual system? We began investigating this
question by measuring tolerance to position and scale
transformations. We found that individual neuron tolerance
increased from V4 to IT, and this translated to enhanced
performance of the IT over the V4 population on a position- and
scale-invariant object recognition task. To determine whether the
neurons in each area also differ in terms of the image features that elicit a response, we presented natural
and “scrambled” images that have the same local structure but configured randomly [3]. We found that
V4 neurons responded similarly to both image sets whereas IT neurons responded much more robustly to
the natural images. Likewise, the V4 population discriminated between members of the two image sets
with similar fidelity whereas discrimination by the IT population was considerably degraded for the
scrambled as compared to the natural images. These results suggest that IT neurons are more selective
than V4 neurons in terms of the image features that drive these cells. Moreover, we found that equivalent
sparseness values were correlated with higher levels of selectivity and tolerance in IT as compared to V4.
Thus, as signals propagate through the visual system, neurons increase their selectivity for particular
image features and, at the same time, neurons increase their tolerance for the position and scale of those
features; the rates at which these two factors increase are set such that constant sparseness is maintained
at each level of visual processing. Consistent with the observation that the structure of cortex is roughly
identical regardless of where it sits in the hierarchy, we speculate that conservation of a broadly
distributed coding scheme is an optimal use of resources in equipotential cortex.
References
[1] Sparse coding of sensory inputs. BA Olshausen and DJ Field, Curr Opin Neurobiol., 14:481-7, 2004.
[2] Responses of neurons in primary and inferior temporal visual cortices to natural scenes. R Baddeley et
al., Proc Biol Sci. 264:1775-83, 1997.
[3] A parametric texture model based on joint statistics of complex wavelet coefficients. J Portilla and EP
Simoncelli, Int J Comp Vis, 40:49-71, 2000.

13

Thursday evening, Poster session I-6

Cosyne 2008

Integration of distributed one-dimensional motion signals by
macaque middle temporal cortical neurons
Andrew M. Clark and David C. Bradley
Department of Psychology, University of Chicago
Estimating the motion of objects given only the time-varying distribution of light intensity impinging
upon the retinae is a difficult problem. Computational solutions to this problem generally assume a
hierarchical fom. At the first stage, image translations are detected point-by-point within the scene. As
these local estimates are often ambiguous, that is, a family of object motions can result in identical local
samples, a second, integration, stage is required to smooth out noise and recover object velocities. While
the preponderance of psychophysical evidence supports such a two-stage model, the physiological
evidence for these models has lately been called into question. Initial reports suggested that a subset of
neurons in the cortical middle temporal (MT) area signal the veridical direction of motion of a complex
pattern (the intersection-of-constraints, or IOC direction), while in primary visual cortex (V1), MT’s
primary cortical afferent, neurons signal the one-dimensional motion of a pattern’s oriented components.
However, more recent studies have suggested that: (i) some V1 neurons signal pattern motion by
encoding the direction of two-dimensional ‘features’ (ii) MT neurons encode the vector average (VA)
rather than IOC direction in complex patterns, and (iii) MT neurons only integrate local motions when
they are spatially superimposed. Importantly, in all previous studies, stimuli contained both onedimensional contours and two-dimensional features.
To determine whether MT neurons integrate across space to compute veridical object velocities in stimuli
lacking feature signals, we measured MT neurons’ direction tuning using stimuli composed of multiple,
non-overlapping, drifting sine gratings (micro-patterns). The direction of each local grating in a micropattern was chosen by random draw from a uniform distribution (component distribution), the gratings’
temporal frequency was then set according to the IOC relationship between global and local velocities.
We generated between 3-5 unique micro-pattern stimuli for each of 16 simulated global velocities.
Component distributions were chosen such that, for each global velocity, some stimuli had identical IOC,
but different vector-average (VA), directions (incongruent condition), while others had identical IOC and
VA directions (congruent condition).
Analysis of direction tuning revealed that in no cases were responses consistent with an IOC mechanism.
That is, for all cells, preferred directions measured in the incongruent condition were shifted, relative to
preferred directions in the congruent condtion, towards the VA direction (Watson-Williams test, p<0.01).
Control experiments revealed that this shift was not dependent upon the number of component velocities,
the total number of component gratings, the size of each grating or the spatial frequency of each grating.
Furthmore, we found a significant correlation between the neurons’ pattern index (a continuous measure
of pattern direction selectivity (PDS)) and the difference in tuning bandwidths measured with single
gratings and micro-patterns. This suggests that the shift towards the VA direction is not explained by
PDS neurons becoming component direction selective (CDS) when local motions are spatially distributed.
Thus, we conclude that MT neurons do integrate spatially distributed one-dimensional motion signals, but
that this integration is inconsistent with an IOC mechanism.

14

Cosyne 2008

Thursday evening, Poster session I-7

Common neural mechanisms of intermediate shape processing
in vision and touch
Jeffrey M. Yau1, Steven S. Hsiao1, and Charles E. Connor1
1

Zanvyl Krieger Mind/Brain Institute, The Johns Hopkins University

The mammalian brain is remarkably adept at visual, tactile, and auditory pattern perception. A
fundamental question is whether common neural mechanisms for pattern processing evolved in
parallel across these separate sensory modalities. The visual and somatosensory systems confront
similar problems in processing object shape. In both systems, the brain must transform complex,
variable input patterns into compact, invariant central representations of objects. Early visual and
somatosensory cortex extract orientation (a 1st order derivative), taking advantage of the statistical
structure of small image fragments to generate a sparser, compact representation. Intermediatelevel visual cortex (V2/V4) computes curvature (a 2nd order derivative) over larger image
fragments, again taking advantage of natural image statistics to compress the representation. Is
curvature also computed at intermediate processing stages in the somatosensory system? And in
both systems, how is curvature computed from lower-level orientation signals?
In neurophysiological experiments using behaving rhesus monkeys (Macaca mulatta), we probed
the sensitivity of visual neurons from area V4 and somatosensory neurons from area 2 and SII to
a large set of visual and haptic stimuli, respectively. The stimulus sets spanned a broad range of
orientations and curvatures. Single-unit recordings showed robust responses to these stimuli with
varying degrees of shape selectivity.
We modeled the shape selectivity of individual neurons with weighted linear/nonlinear
combinations of multidimensional Gaussian tuning functions on a contour geometry domain
defined by 4 dimensions: x- and y-positions, orientation, and curvature (squashed to a range from
-1 to 1). Model parameters were fit using an iterative nonlinear least-squares algorithm, and
model complexity was constrained by the Bayesian Information Criterion.
Our analyses suggest that, in visual area V4 as well as in somatosensory areas 2 and SII,
curvature is derived from a combination of linear and nonlinear integration across multiple
component orientations. These results suggest that common mechanisms for pattern processing
evolved in parallel across the visual and somatosensory systems.

15

Thursday evening, Poster session I-8

Cosyne 2008

Contrast-dependent suppression from the “far” surround of V1
neurons: Experiments and data-driven model comparison.
J.M. Ichida1, L. Schwabe2, S. Shushruth1 and A. Angelucci1
1

Moran Eye Center, University of Utah,

2

EPFL, Brain Mind Institute

V1 cells are tuned to stimulus size and are suppressed by optimally-oriented large stimuli in the receptive
field (RF) surround. We previously proposed that modulation arising from the “far” surround (i.e. beyond
the extent of monosynaptic horizontal connections) is mediated by highly divergent and fast-conducting
feedback connections to V1 [1], and implemented this idea into a recurrent network model [2]. To isolate
the modulatory signals from the far surround, V1 cells were stimulated with an optimal grating patch
confined to the RF, surrounded by an iso-oriented annular grating of 14° fixed outer radius and an inner
radius whose size was decreased from 14° to just outside the cell’s RF size measured at low contrast. A
blank annulus was interposed between the center and surround gratings to prevent afferent stimulation of
horizontal connection neurons in the “near” surround. Using this visual stimulus, we recently showed that
the far surround can be facilitatory or suppressive depending on center stimulus contrast and surround
stimulus size [3]. Here we used the same visual stimulus to examine the contrast-dependence of far
surround suppression in V1 cells (n=70) recorded from anesthetized and paralyzed macaques. Then, we
used these measurements in order to further constrain the parameter regime of our recurrent network
model. In particular, we focused on two key parameters (the strength of the intra-areal recurrency and the
feedback inhibition), which so far have not been measured directly.
Far surround suppression was induced by gratings up to 14° away from the RF center (mean=5.7°±0.34).
We measured suppression strength (SS) as the percent decrease in the response to center-only stimulation
induced by the widest annular grating. Mean SS at high contrast was 35%±2.3 and significantly decreased
(22.2%±3.9, p<0.05) when the center stimulus contrast was lowered, and decreased even further (9.3%±
3.2) when the surround stimulus contrast was also lowered. For 87% of cells suppression was stronger at
high than at low center contrast. Far surround stimuli of identical size evoked weaker suppression of low
contrast than high contrast center stimuli, so that larger surround stimuli were required to induce
suppression of low contrast center stimuli.
We then calculated the probability of the suppression strength predicted by the recurrent network model
for different values of the two key model parameters and found that the parameter regime that can best
describe the data quantitatively includes stronger feedback excitation of inhibitory neurons than we
assumed in the initial version of the model [2]. Additionally or alternatively, stronger inhibition could
arise from direct feedforward excitation of inhibitory neurons or from surround suppression of LGN
afferents, which are missing in the current version of our model.
Supported by: NIH NSF, and Research to Prevent Blindness.
References
[1] Circuits for local and global signal integration in primary visual cortex. Angelucci et al. J Neurosci
22: 8633-8646, 2002.
[2] The role of feedback in shaping the extra-classical receptive field of cortical neurons: a recurrent
network model. Schwabe et al. J Neurosci 26: 9117-9129, 2006.
[3] Response facilitation from the “suppressive” receptive field surround of macaque V1 neurons. Ichida
et al. J Neurophys 98: 2168-2181, 2007.

16

Cosyne 2008

Thursday evening, Poster session I-9

Perception of a touch-induced visual illusion correlates with changes
of oscillatory activity in human visual and somatosensory areas
Joachim Lange, Robert Oostenveld, Pascal Fries
F.C. Donders Centre for Cognitive Neuroimaging, Radboud University Nijmegen,
The Netherlands
When a brief visual stimulus is accompanied by two brief tactile stimuli, subjects often perceive a second
illusory visual stimulus [1]. We investigated the neural mechanisms of this illusion with whole-head 151channel MEG-recordings in 22 subjects. Subjects received visuo-tactile stimulations and were instructed
to indicate the number of perceived visual stimuli while ignoring tactile stimulations. Stimulus parameters
were adjusted to obtain the illusory second flash in 50% of those trials in which stimulation conditions
permitted the illusion. We contrasted illusion and non-illusion trials and analyzed differences in spectral
power in somatosensory and occipital sensors.
In occipital sensors, the illusory percept was accompanied by a bilateral de-synchronization before
stimulus onset (-400 to -200 ms) in the alpha-band (7.5-15 Hz). Moreover, the illusion triggered enhanced
power in the gamma-band (70-130 Hz) contralateral to stimulus presentation. Interestingly, the visual
illusion is also associated with changes of oscillatory activity in somatosensory cortex, although subjects
were instructed to ignore tactile stimulation. In somatosensory sensors, the illusory percept was
accompanied by an increase of spectral power for low frequencies (5-15 Hz) around stimulus onset and a
decrease of spectral power at ~400-850 ms in the beta-band (25-30 Hz).
In somatosensory areas, decreased neuronal oscillatory activity in the beta-band has been related to
increased attention to tactile stimulation [2]. Enhanced activity in the alpha-band correlates with better
performance in tactile detection tasks [3]. Also, pre-stimulus de-synchronization in the occipital alphaband reflects similar processes in visual areas [4]. We therefore hypothesize that the observed effects for
illusion trials relate to higher processing capabilities of somatosensory and visual areas. This may lead to
enhanced inter-sensory information transfer. Higher attention to tactile stimulation in illusion trials may
thus enhance the impact of somatosensory gamma-band activity on visual areas. These findings provide
new insights in the dynamic interactions between different sensory areas during multimodal integration.
Acknowledgments
This research is supported by the German Research Foundation (J.L.), The Netherlands Organization for
Scientific Research, the Volkswagen Stiftung and the European Science Foundation (P.F.).
References
[1] Touch-induced visual illusion. Violentyev A, Shimojo S, Shams L. Neuroreport. 13;16(10):1107-10.
2005 Jul
[2] Tactile spatial attention enhances gamma-band activity in somatosensory cortex and reduces lowfrequency activity in parieto-occipital areas. Bauer M, Oostenveld R, Peeters M, Fries P. J Neurosci.
11;26(2):490-501. 2006 Jan
[3] Prestimulus oscillations enhance psychophysical performance in humans. Linkenkaer-Hansen K,
Nikulin VV, Palva S, Ilmoniemi RJ, Palva JM. J Neurosci. 10;24(45):10186-90. 2004 Nov
[4] Anticipatory biasing of visuospatial attention indexed by retinotopically specific alpha-band
electroencephalography increases over occipital cortex. Worden, M.S. et al. J. Neurosci. 20 RC63, (2000)

17

Thursday evening, Poster session I-10

Cosyne 2008

Interactions between chromatic signals in human visual cortex
measured with high-density source-imaged EEG
Alex R. Wade1 and Anthony M. Norcia1
1

Smith-Kettlewell Eye Research Institute

When measured in the primate LGN, signals in the opponent chromatic and achromatic pathways
show relative independence. This independence is reduced in primary visual cortex and higher
visual areas do not appear to contain unique populations of cells tuned to the cardinal directions
of MacLeod-Boynton space.
The way in which chromatic and achromatic signals are combined and processed in early visual
cortex is not well understood. In this study we used high-density source-localized EEG combined
with a steady-state VEP (SSVEP) design that facilitates a frequency-domain analysis to measure
responses to combinations of contrast-reversing chromatic and achromatic gratings in
retinotopically-defined visual areas. Using this paradigm, we identified stimulus-driven
components of the frequency-domain responses in four visual areas: V1, V3a, hMT+ and V4.
The input frequencies were chosen carefully so that it was possible to identify two classes of
signal in the resulting source-localized cortical current density signals: Integer multiples of the
input frequencies or ‘self terms’ result from cortical computations that operated independently on
each chromatic or achromatic channel. In contrast, sums and differences of the input frequencies
(‘intermodulation terms’) indicate the presence of non-linear interactions between the those
inputs.
Our initial data indicate that the cortical signals evoked by a combination of S-cone isolating and
(L-M)-cone isolating gratings have relatively strong self terms in primary visual cortex
indicating that the computations giving rise to these signals treat them as approximately
independent inputs at this stage. Higher areas have significantly stronger intermodulation terms
consistent with the idea that the chromatic channels are combined at a non-linearity in early
visual cortex. We compare the distribution of self- and intermodulation terms- in these areas to
those predicted by some simple models of cortical color processing.

18

Cosyne 2008

Thursday evening, Poster session I-11

Modeling the influence of local network activity on neuron
spiking responses in primary visual cortex
Kilian Koepsell, Timothy J Blanche, Nicholas Swindale, Bruno A Olshausen
The de facto standard for modeling neural responses to visual stimuli is a cascade of linear and point-wise
nonlinear processing stages. Despite the success of these models in accounting for part of the single neuron firing rate, they generalize poorly and are less successful at predicting neuronal responses to naturalistic stimuli [1]. Notwithstanding the difficulty of fitting the many parameters of these models, other limitations are more fundamental: they are only designed to capture responses that are time-locked to the stimulus; they ignore sources of trial-to-trial variability, regarding them as noise; and they do not explicitly
model interactions between neurons. Yet the activity of single neurons is modulated by not only the
stimulus, but also ongoing local network dynamics, even in primary sensory cortices.
Here we seek to improve the performance of predictive models by explicitly including the activity of
other neurons, including meso-scale activity as reflected in the local field potential (LFP), and by fitting
single-trial spike trains rather than trial-averaged rates. Using the generalized linear model (GLM)
framework [2], the standard feed-forward spatiotemporal receptive field, spiking history, activities of
other local neurons, and the influence of the network activity (LFP) can be included as factors modulating
the spiking probability. Since single neuron spiking activity is strongly phase-locked to the band-passed
LFP in various frequencies [3], we includedhere a factor of the form:

λ(t|α, β) = exp



αi cos(ϕi (t)) + βi sin(ϕi (t))

(1)

i

that multiplies the spiking probability depending on the phases i of the LFP at frequencies fi. We fit the
parameters i and i to data recorded with high-density silicon electrode arrays comprising simultaneous
recordings of 100+ neurons spanning all cortical layers in anesthetized cat primary visual cortex [4]. Brief
5s white noise and natural scene movie segments were presented 25-75 times (50Hz frame rate, 200Hz
refresh). Complex Morlet wavelets were used to obtain an instantaneous measure of LFP phase in frequency bands from 1-150Hz.
The addition of the LFP component (1) to the model significantly improved the explained variance (r2) of
the average firing rate. More importantly, the model captured the spike timing structure on individual trials, as measured by an increase in the likelihood estimate. These encouraging preliminary results suggest
that this type of modeling will give a fuller and more quantitative understanding of the relations between
single neurons and network-level dynamics in the primary visual cortex.
[1] Olshausen BA, Baker J, Yen CS, Gray CM. (2004) Receptive field models fail to predict responses of
V1 neurons to natural movies. Society of Neuroscience, San Diego.
[2]Truccolo W, Eden UT, Fellows MR, Donoghue JP, Brown EN. (2005) A Point Process Framework for
Relating Neural Spiking Activity to Spiking History, Neural Ensemble, and Extrinsic Covariate Effects.
J Nphys. 2005 93(2):1074-89.
[3] Blanche TJ & Koepsell K. (2007) Spike timing precision and the influence of cortical dynamics. COSYNE, Salt Lake City, Utah.
[4] Blanche TJ, Spacek MA, Hetke JF, Swindale, NV. (2005) Polytrodes: high-density silicon electrode
arrays for large-scale multiunit recording. J NPhys. 93(5):2987-3000.

19

Thursday evening, Poster session I-12

Cosyne 2008

Temporal Encoding of Visual Space during Fixational Instability
Martina Poletti1 and Michele Rucci1
1

Active Perception Laboratory, Boston University, Boston, MA 02215.

Under natural viewing conditions, saccades separate brief periods of visual fixation. The term “fixation”
is misleading, however, as the eye is not at rest in the intervals between saccades. During these periods,
fixational eye movements, including small saccades and drifts, modulate the spatiotemporal input to the
retina. It has long been questioned whether fixational modulations of luminance have visual functions.
As postulated by the dynamic theories of visual acuity early last century and by recent theoretical
proposals [1-2], fixational modulations might encode spatial information in the temporal domain.
We have proposed that fixational eye movements are part of a scheme of acquisition of visual information
that enables efficient representations of natural stimuli [2]. According to this theory, fixational eye
movements contribute to the decorrelation of the responses of retinal ganglion cells (RGCs) over multiple
fixations. Recent psychophysical experiments have confirmed some of the predictions of this theory.
These experiments have shown that fixational eye movements improve the discrimination of high spatial
frequency gratings masked by low-frequency noise, but do not help in the discrimination of lowfrequency gratings masked by high-frequency noise [3].
Here, we examine the impact of fixational modulations of luminance on retinal activity within a single
fixation. Spatiotemporal filters designed on the basis of neurophysiological data modeled the responses
of parvocellular ganglion cells in the macaque’s retina. Modeled neurons were exposed to the same
visual input experienced by subjects in our experiments, i.e. the spatiotemporal signals resulting from
viewing stimuli during oculomotor activity. We show that synchronous modulations in RGC responses
resulting from fixational instability are consistent with psychophysical results. During presentation of
high-frequency gratings, oculomotor activity influenced the correlation between pairs of RGCs in a way
that depended on the relative alignment of the receptive fields of the two considered cells. Cell responses
were strongly correlated only when their receptive fields were aligned parallel to the grating’s orientation.
Such a dependence on receptive-field alignment was instead absent during viewing of low-frequency
gratings. That is, in keeping with the subjects’ reports, fixational eye movements synchronously
modulated arrays of RGCs parallel to the grating’s orientation during viewing of high-frequency gratings
masked by low-frequency noise, but not during presentation of low-frequency gratings masked by highfrequency noise. Changes in the structure of correlated activity occurred without affecting average firing
rates. These results further suggest that synchronous modulations resulting from fixational eye
movements are an important component of the way visual information is acquired and encoded in the
early visual system.
Acknowledgments
This work was supported by NIH grants R03 EY015732, R01 EY18363 and NSF grant BCS-0719849.
References
[1] Figuring space by time. E. Ahissar, and A. Arieli, Neuron 32, 185-201, 2001.
[2] Fixational instability and natural image statistics: Implications for Early Visual Representations. M.
Rucci and A. Casile, Network: Computation in Neural Systems 16(2-3), 121-138, 2005.
[3] Miniature eye movements enhance fine spatial detail. M. Rucci, R. Iovin, M. Poletti, and F. Santini,
Nature 447(7146):851-854, 2007.

20

Cosyne 2008

Thursday evening, Poster session I-13

The perceptual interpretation of a moving square-wave plaid is
speed-dependent
James H. Hedges1, Alan A. Stocker1,2, and Eero P. Simoncelli1,2
1New

York University,

2Howard

Hughes Medical Institute

When visual scenes are ambiguous, the visual system must choose from multiple perceptual
interpretations. A classic stimulus that illustrates this idea is a moving plaid, composed by adding two
drifting one-dimensional gratings (left panel). Such a stimulus is typically perceived as either a single
coherently moving pattern or two transparent components sliding over one another [1]. Previous studies
have identified many factors that can influence which of these percepts occurs. But these studies typically
explored the effects of a single stimulus variable in isolation.
We performed a series of psychophysical experiments to examine the joint effects of the component speed
and the coherent pattern speed on perceptual interpretation. The coherent pattern speed is determined by
the component speed and the angle between the two gratings’ directions: vc = vp cos(/2). We used a
forced-choice experimental design in which subjects indicated their percept after a brief presentation (1.5
s) of the stimulus. We found that for a range of intermediate component speeds, the transparent
interpretation became more likely when a plaid’s pattern speed was significantly faster than its component
speed (middle panel). For both higher and lower component speeds, the percept remains coherent up to
higher pattern speeds.
We hypothesize that these behaviors arise from a competition between two visual preferences: the system
prefers a single motion to a two-motion interpretation, but the system also prefers slower speed
interpretations to faster ones. These two preferences arise naturally in the context of a Bayesian decision
model that incorporates a prior that favors slower speeds. We write the probability of the observed
stimulus, conditioned on both the interpretation (transparent vs. coherent) and the relevant velocity (of
either components or pattern, respectively) using a lognormal likelihood function derived in a previous
psychophysical study [2]. The probability conditioned on each interpretation is then multiplied by a
power-law prior for slower speeds (also derived in [2]), and integrated over all velocities to generate the
likelihoods of the interpretations, which are then compared to determine the “percept.” The perceptual
interpretations predicted by this simple model (right panel) are well-matched to the experimental data.

component

10

 

5
transparent
coherent
0 
0
5
10
component speed (deg s -1)

model
pattern speed (deg s -1)

pattern

pattern speed (deg s -1)

data (1 subject)

10

5

0
0
5
10
component speed (deg s -1)

References
[1] Adelson, E. H. & Movshon, J. A. Phenomenal coherence of moving visual patterns. Nature 300,
523-5, 1982.
[2] Stocker, A. A. & Simoncelli, E. P. Noise characteristics and prior expectations in human visual speed
perception. Nat Neurosci 9, 578-85, 2006.

21

Thursday evening, Poster session I-14

Cosyne 2008

A principle relating neuronal selectivity to local map structure in
visual cortex
Ian Nauhaus1, Andrea Benucci2, Matteo Carandini2 and Dario L Ringach1
1

University of California, Los Angeles, 2 Smith-Kettlewell Eye Research Institute,
San Francisco
There is substantial debate as to the degree to which neurons in primary visual cortex (V1) derive their
selectivity from local lateral interactions. If these interactions play an important role, then the local
neighborhood would determine not only a neuron’s preferences but also its degree of selectivity.
Previous studies have reported evidence against this prediction, indicating that the selectivity of spike
responses does not depend on location within the functional maps [1,2].
We have tested this prediction using a technique that localizes the electrodes in the orientation map with
extremely high precision [3]. In V1 of anesthetized monkey or cat, we first performed optical imaging to
acquire a map of orientation preference. We then implanted in the same patch of cortex a 10x10 electrode
array with 400m spacing to record spikes in the presence of a grating stimulus. We determined electrode
locations by finding the placement of the array which maximizes the agreement in orientation between the
electrophysiology and imaging. Tuning selectivity of each neuron was quantified by using the  of a
best-fit Gaussian. The homogeneity in the map was quantified in a region around each electrode with a
spatial scale of ~180m to approximate the dendritic field in cat and monkey V1.
The trends for the monkey and cat data show that cells located in regions of homogeneous preference
have sharper tuning. Furthermore, the data points from the two species lie along the same line. This links
the two previous observations that cat V1 neurons are more selective than those in the monkey, and that
regions of homogeneous preference are larger in cat than in monkey. These findings suggest that the
structure of the orientation map, on the spatial scale of dendritic integration, is a critical determinant of
orientation selectivity in both species, and that a common rule relates selectivity and map structure in
different species.
Acknowledgments
Supported by: NIH-EY 12816 (DLR), FA8650-06-C-7633 (DLR) and NIH-EY017396 (MC)
References
[1] Invariant computations in local cortical networks with balanced excitation and inhibition. J. Marino, J.
Schummers, D. C. Lyon et al., Nature Neuroscience 8 (2), 194 (2005).
[2] Orientation selectivity in pinwheel centers in cat striate cortex. P. E. Maldonado, I. Godecke, C. M. Gray
et al., Science 276 (5318), 1551 (1997).
[3] Precise alignment of micro-machined electrode arrays with V1 functional maps. I. Nauhaus and D. L.
Ringach, J Neurophysiol 97 (5), 3781 (2007).

22

Cosyne 2008

Thursday evening, Poster session I-15

Methods for classifying shapes of receptive ﬁelds
Martin Rehn1 , David K. Warland2 , and Friedrich T. Sommer1
1

Redwood Ctr./Helen Wills Neurosci. Inst., UC Berkeley, CA, 2 Dept. of Biol. Sci., Section
Neurobiol., Physiol. and Behavior, UC Davis, CA.

In visual cortex, it has been shown that cells that are ideal local edge detectors seem to coexist with other
types, such as cells with non-oriented receptive ﬁelds and cells with narrower spatial frequency tuning
[2]. This ﬁnding challenges the traditional notion that simple cells are just local edge detectors [3]. In
signal processing it is common to use a mixture of dictionaries to provide coding efﬁciency [4]. Each
dictionary is made up of variations of a single shape primitive that is repeated under transformations, such
as in different positions and sizes. Recently it was demonstrated in a model of visual cortex that different
classes of experimentally found receptive ﬁelds (oriented and nonoriented receptive ﬁelds) can be learned
from natural images in an unsupervised fashion [1].
Here we ask how one can detect subclasses of neural receptive ﬁelds that relate to certain shape primitives.
In general, this is a hard problem because the similarity of shapes belonging to a class cannot be assessed by
using a simple metric in the original data space. Rather, the detection of a class relies on similarities that take
the transforms into account that generated the dictionary. The current approach for ﬁnding shape primitives
was to ﬁt parametric functions, such as Gabor functions, to the receptive ﬁelds, thus mapping them into the
parameter space of those functions [1]. However, pre-deﬁned ﬁtting functions often do not capture all types
of shapes occurring in the data; for example, Gabor functions ﬁt center-surround shapes only poorly. Here
we present and compare novel approaches to analyze receptive ﬁelds with respect to shape primitives that
can overcome some of the drawbacks of the current approach.
First, we describe an extension of the existing parametric technique: We use Bayesian clustering to detect
different shape classes in the parameter space. Further, we extend the Bayesian analysis to heterogeneous
mixtures, that contain more than one functional form. For instance, we use a mixture of Gabor functions
and difference of Gaussian functions. Finally, we assess whether a class of shapes tiles the input space.
For example, we check that each class of shapes exists in all spatial locations, and possibly also in all
orientations and sizes. Second, we describe methods to analyze shapes of receptive ﬁelds that are not tied to
ﬁxed families of functions. We use non-parametric descriptions in the visual ﬁeld domain or in the Fourier
domain, rather than pre-deﬁned functions as the class prototypes. The translational, rotational and scaling
invariances are separated out from the prototype description. All methods are compared for synthetic data,
data from sparse coding models and experimental data to compare their ability to characterize different
classes of receptive ﬁelds.

References
[1] A network that uses few active neurones to code visual input predicts the diverse shapes of cortical receptive ﬁelds.
M. Rehn and F. T. Sommer, J Comput Neurosci 22(2):135–146, Apr 2007.
[2] Spatial structure and symmetry of simple-cell receptive ﬁelds in macaque primary visual cortex. D. L. Ringach, J
Neurophysiol 88(1):455–463, Jul 2002.
[3] Synaptic integration by V1 neurons depends on location within the orientation map. J. Schummers, J. Marino,
and M. Sur, Neuron 36(5):969–978, Dec 2002.
[4] Wavelet-domain approximation and compression of piecewise smooth images. M. Wakin, J. Romberg, H. Choi,
and R. Baraniuk, IEEE T Image Process 15(5):1071–1087, 2006.

23

Thursday evening, Poster session I-16

Cosyne 2008

!"#$%&'()#*!+,-./0012-301%45#$#%5-#*-6%5*#*$05$-7#$2-8+'0*(
9/$#:+,-6!0-6%:;#5+$#%5
<0#-=#->+?@-A#+54-B2%!C@-+5"-D!1+*-6E-F+&&+C
!"#$%&'#()*+,*-+./%'(%&0***12#()*2+33%4%*+,*5%6*7+&8
9:(./#"4 * : * ';%:8%&<' * ,:.#:3 * =+$%=%"(' * .:" * >&:=:(#.:33) * %"/:".% * +?& * :@#3#() * (+ * .+=;&%/%"> * 6+&>'0*
%';%.#:33)*#,*(/%*';%%./*'+?">*#'*.+&&?;(%>*@)*"+#'%A*B(*6:'*&%.%"(3)*,+?">*#"*:"*+;%"C'%(*6+&>*&%.+4"#(#+"*
(:'8*(/:(*(/%*#=;&+$%=%"(*#"*;%&.%"(:4%*.+&&%.(*>?%*(+*(/%*?'%*+,*$#'?:3*#",+&=:(#+"*#'*=:D#=:3*"+(*:(*
/#4/ * @?( * :( * #"(%&=%>#:(% * :?>#(+&) * "+#'% * 3%$%3' * E FA *G/#' * '%%=' * (+ * .+"(&:>#.( * (/% * ;&#".#;3% * +, * #"$%&'%*
%,,%.(#$%"%'' * +,(%" * ,+?"> * ,+& * =?3(#'%"'+&) * .?% * .+=@#":(#+" * H(/% * %"/:".%=%"( * >?% * (+ * +"% * '%"'+&)*
=+>:3#()*#'*3:&4%'(*6/%"*(/%*+(/%&*=+>:3#()*/:'*(/%*3+6%'(*'#4":3C(+C"+#'%*&:(#+IA*G+*@%((%&*./:&:.(%&#J%*
(/#'*:;;:&%"(*>#':4&%%=%"(*6%*&%;&+>?.%*(/%*&%'?3(*?">%&*@%((%&*.+"(&+33%>*.+">#(#+"'*:">*'/+6*(/:(*:*
"+$%3*$#'?:3*'(#=?3?'*(/:(*;&+$#>%'*+"3)*(%=;+&:3*#",+&=:(#+"*;&+>?.%'*:*@%/:$#+&*.+",3#.(#"4*6#(/0*:">*
%$%"*+;;+'#(%*(+*#"$%&'%*%,,%.(#$%"%''A*
9%*(/%"*;&%'%"(*:*K:)%'#:"*=+>%3*+,*+;(#=:3*6%#4/(#"4*+,*:?>#(+&)*:">*$#'?:3*#",+&=:(#+"*(/:(*.:"*
%D;3:#" * (/% * >:(: *,&+=* @+(/ * .+">#(#+"'A *B"*(/#'* =+>%30* 6+&>' *:&% *&%4:&>%>*:' * ;+#"(' *#"*: *.+"(#"?+?'0*
=?3(#>#=%"'#+":3*';:.%*6#(/*$:3?%'*(/:(*/:$%*(+*@%*#",%&&%>*,&+=*(/%*'%"'+&)*.?%'A*G/%*/#4/*>#=%"'#+"*#'*
:*8%)*;&+;%&()*,+&*%D;3:#"#"4*(/%*=:D#=:3*;%&,+&=:".%*%"/:".%=%"(*:(*#"(%&=%>#:(%*"+#'%*3%$%3'A*B"*
.+"(&:'(0*#"$%&'%*%,,%.(#$%"%''*&%'?3('*,&+=*(/%*K:)%'#:"*=+>%3*#"*3+6*>#=%"'#+"'A*G/%'%*&%'?3('*'?44%'(*
(/:(*:?>#(+&)C$#'?:3*';%%./*;%&.%;(#+"*=:)*,+33+6*(/%*':=%*+;(#=:3#()*;&+;%&(#%'*+,*=?3(#'%"'+&)*.?%*
.+=@#":(#+"*(/:(*6%&%*;&%$#+?'3)*+@'%&$%>*+"3)*+"*$%&)*'#=;3%*'(#=?3#A*L%$%&:3*;&%>#.(#+"'*:&%*+@(:#"%>*
,+&*=?3(#'%"'+&)*.?%*.+=@#":(#+"*6#(/*.+=;3%D*'(#=?3#A





M?>#(+&)C:3+"%* ;%&,+&=:".%* H@3?%* ')=@+3'I* :">* :?>#(+&)C$#'?:3* ;%&,+&=:".%* H4&%%"* ')=@+3'I* #"*
';%%./*&%.+4"#(#+"*:'*:*,?".(#+"*+,*:?>#(+&)*L5-A*K%/:$#+&:3*>:(:*:&%*,#((%>*6%33*@)*:*&%:3#'(#.*K:)%'C
+;(#=:3* =+>%3* +,* ';%%./* ;%&.%;(#+"* H3#"%'IA* -%>* 3#"%N =?3(#'%"'+&) %"/:".%=%"(A +G- O?33* $#'?:3*
#",+&=:(#+"A*;G B=;+$%&#'/%>*$#'?:3*#",+&=:(#+"A

30H0&0510
E F*-+''*PM0*L:#"(CM=+?&*Q0*P%:$#((*R50*S:$#((*Q20*O+D%*SS*H1TTUI*Q+*)+?*'%%*6/:(*#*:=*':)#"4V*
WD;3+&#"4*$#'?:3*%"/:".%=%"(*+,*';%%./*.+=;&%/%"'#+"*#"*"+#')*%"$#&+"=%"('A*2%&%@*2+&(%D* UN XUC
YZA

24

Cosyne 2008

Thursday evening, Poster session I-17

Statistical analysis of natural sounds.
Maria Neimark Geffen, Marcelo Magnasco
Center for Studies in Physics and Biology, Rockefeller University
A babbling brook sounds quite different than rustling leaves or swaying branches. Such textured broadband sounds are common in nature. To discriminate among them, the auditory system extracts
information from statistical regularities of these sounds. At the same time, it discards information about
the particulars of each sound iteration. We constructed a library of natural sound textures recorded at high
resolution, and performed statistical analysis to determine the dimensions in the sound space, which allow
for their low-dimensional representation. Using wavelet analysis, the sounds were decomposed into
fundamental building blocks, which could be randomly intermixed to construct novel sounds. We
computed the statistics of the individual building blocks, and the statistics of their combination in
different sound textures. This analysis was used to compose intermediate sounds: broad-band noise was
gradually morphed into distinct sound objects, by systematically changing of the statistical distribution of
the basic sound waveforms. We show that complex sounds can be generated from broad-band noise using
a small number of statistical rules. This naturalistic sound library can be used in studies of neural
encoding of higher-order statistics of sensory stimuli.

Acknowledgments
Maria Neimark Geffen, Ph.D. holds a Career Award at the Scientific Interface from the Burroughs
Wellcome Fund.

25

Thursday evening, Poster session I-18

Cosyne 2008

Perceptual sensitivity to high-frequency interaural time differences
created by rustling sounds
Katharina Kaiser and Lutz Wiegrebe
Biocenter, University of Munich, Germany
Interaural time differences (ITDs) are recruited to localize sounds in azimuth. ITDs can be extracted from
low-frequency sounds or from the envelopes of high-frequency, complex sounds. Studies of the latter
have included amplitude-modulated tones 1,2 or transposed stimuli 3,4, i.e., stimuli which carry in their
envelope information similar to the information carried by a low-frequency pure tone. In a recent study,
Bernstein and Trahiotis 5 showed that not the degree of envelope fluctuation, as quantified by the
envelope 4th moment, determines sensitivity to envelope ITDs but the envelope spectrum and with it the
interaural cross-correlation function of the auditory envelopes provide a good description of envelope
ITD sensitivity. Here we use a different class of high-frequency stimuli, namely noise stimuli generated
with different degrees of roughness which resemble natural rustling sounds. Stimuli are generated with a
Gaussian-noise carrier and an aperiodic, rectangular modulator of statistically different duty cycle. The
psychophysical results obtained with these stimuli show that ITD sensitivity increases both with
increasing roughness and increasing bandwidth of the rustling sounds. While the effect of bandwidth on
ITD sensitivity is in line with the conclusions of Bernstein and Trahiotis 5, the effect of roughness is not.
ITD sensitivity for rustling sounds that span the frequency range from 2 to 8 kHz elicited ITD thresholds
as low as 32 μs, i.e., considerably lower than reported in previous studies with transposed stimuli. The
data show that high-frequency rustling sounds provide strong temporal localization cues which the
auditory system can effectively exploit.
Acknowledgments
We thank Benedikt Grothe for helpful discussions. This work was supported by the Volkswagenstiftung
(I79/780-0 to L.W.).

References
1 Dye,R.-H.J., Niemiec,A.J. & Stellmack,M.A. Discrimination of interaural envelope delays: the effect of
randomizing component starting phase. J Acoust. Soc. Am. 95, 463-470 (1994).
2 Bernstein L.R. & Trahiotis C. Detection of internaural delay in high- frequency sinusoidally amplitudemodulated tones, two- tone complexes, and bands of noise. J Acoust Soc Am 95, 3561-3567 (1994).
3 Bernstein,L.R. & Trahiotis,C. Enhancing sensitivity to interaural delays at high frequencies by using
"transposed stimuli". J Acoust. Soc. Am. 112, 1026-1036 (2002).
4 Bernstein,L.R. & Trahiotis,C. Enhancing interaural-delay-based extents of laterality at high frequencies
by using "transposed stimuli". J Acoust. Soc. Am. 113, 3335-3347 (2003).
5 Bernstein,L.R. & Trahiotis,C. Why do transposed stimuli enhance binaural processing?: Interaural
envelope correlation vs envelope normalized fourth moment. J Acoust. Soc. Am. 121, EL23-EL28 (2007).

26

Cosyne 2008

Thursday evening, Poster session I-19

Level-distribution-related changes in IC responses: adaptation
or instantaneous non-linearity?
Phillipp Hehrmann1 , Isabel Dean2 , Misha B. Ahrens1 , Nicol S. Harper2 ,
David McAlpine2 and Maneesh Sahani1
1

Gatsby Computational Neuroscience Unit and 2 Ear Institute, UCL.

Recent work has demonstrated that non-linear systems can exhibit stimulus-statistics-driven changes in responses, even when the parameters of the underlying system remain constant. One example is a Reichardt
motion detector, which exhibits a form of dynamic gain control closely resembling the apparently adaptive
behaviour of the H1 neuron in the ﬂy visual system [1]. Neurons in the inferior colliculus (IC) of anaesthetized guinea pigs have been shown to adapt their response characteristics to the sound-level distribution
of a stimulus [2]. Speciﬁcally, the rate-level functions of these neurons change in a way that improves the
population coding accuracy around the most commonly occurring stimulus sound levels. We asked whether,
and to what extent, this stimulus adaptation in IC could be explained as an inherent effect of an essentially unadaptive but nonlinear system being probed with varying stimuli (as has been suggested for H1), or
whether it is a consequence of more slowly-acting adaptive processes in the system.
Following the former hypothesis, we ﬁrst extended a parametric model originally proposed to characterize
the responses of A1 neurons to short amplitude transients [3], so as to model the responses measured in
the IC experiments. The longest membrane time constants in the model are on the order of 10 ms, much
shorter than the duration of individual sound-level stimuli in the experiments (50 ms). Nevertheless, we
found that for appropriate parameter settings, the rate-level functions obtained from the model were indeed
sensitive to stimulus statistics, and in a way that closely resembled several key characteristics of the changes
observed in IC. We then returned to the original data to quantify, now using a non-parametric model, the
relative contributions of the instantaneous effect of changes in the probe stimuli on the one hand and long
term adaptive effects on the other. This analysis, however, revealed that short-window non-linear responses
to the statistics of the probing stimuli did not account for a substantial fraction of the observed changes.
Thus, although an established non-linear level-response model has the potential to reproduce the experimental ﬁndings, this mechanism appears not to contribute substantially to these particular adaptive changes. We
are therefore currently extending the simple parametric model to incorporate slower adaptive processes.
Acknowledgments
Supported by the Gatsby Charitable Foundation.
References
[1] Adaptation without parameter change: Dynamic gain control in motion detection. A. Borst, V. L. Flanagin and H. Sompolinsky. PNAS 102(17):6172–6, 2005
[2] Neural population coding of sound level adapts to stimulus statistics. I. Dean, N. S. Harper and D.
McAlpine, Nat Neurosci 8(12):1684–9, 2005
[3] Auditory edge detection: a neural model for physiological and psychoacoustical responses to amplitude
transients. A. Fishbach, I. Nelken and Y. Yeshurun. J Neurophysiol 85(6):2303–23, 2001.

27

Thursday evening, Poster session I-20

Cosyne 2008

Compensation for natural timing variations and time warping in
cortical discrimination of complex sounds
Cyrus P. Billimoria, Ross K. Maddox, Gilberto Graña, and Kamal Sen
Hearing Research Center and the Center for Biodynamics, Department of
Biomedical Engineering, Boston University, Boston, MA
In order to correctly classify, discriminate and recognize stimuli, sensory systems must be able to
compensate for natural variations in the stimuli. This ability to deal with variations in stimulus parameters
such as intensity, timing and speed of presentation, is known as response invariance. Previously, we have
shown that some neurons in field L (the avian analog of primary auditory cortex) in the zebra finch are
capable of intensity invariant discrimination of songs. Here we examine the ability of field L neurons to
cope with natural variations in timing in multiple renditions of songs, as well as synthetic variations in
time-warped songs. In a first set of experiments, we played multiple renditions of zebra finch songs and
recorded neural responses in field L. To quantify neural discrimination performance, we used a
classification method based on a spike distance metric (SDM). The SDM measures the distance between
spike trains that are filtered using a decaying exponential kernel, which is parameterized by a time
constant. We find a diverse distribution of invariance to timing variations in multiple renditions, with
some neurons showing highly invariant discrimination over the natural range of variation.
To further test the ability of Field L to deal with larger timing differences we modified birdsongs using
phase vocoding to speed up and slow down the songs without changing the frequency content. We tested
a range of speeds between half the normal speed to twice the normal speed, which represent much greater
changes in timing than natural timing differences present in zebra finch songs. We find that the SDM
based classification scheme used previously does not allow invariant discrimination for the larger time
warps. In behavioral experiments we find that zebra finches are able to deal with modest time-warps. We
found that using the neural data, two simple modifications to the classification scheme and SDM results
in invariant discrimination across the entire range of time warps tested. The first modification involves a
re-alignment of the spike trains in time. The second modification involves adjusting the time constant of
the exponential filter in the SDM proportionally to the warp factor.

Acknowledgments
This work was supported by National Institute on Deafness and Other Communication Disorders Grant
1R01 DC-007610-01A1.

28

Cosyne 2008

Thursday evening, Poster session I-21

Functional groups in the auditory system and the representation of
sound features along perceptual dimensions.
Frédéric Theunissen1, Sarah Woolley2, Patrick Gill1 and Thane Fremouw1
1

UC Berkeley, 2Columbia University and 3University of Maine.

Auditory perception depends on the coding and organization of the information-bearing acoustic features
of sounds by auditory neurons. We show that auditory neurons can be classified into functional groups
each of which plays a specific role in extracting distinct complex sound features. We recorded the
electrophysiological responses of single auditory neurons in the songbird midbrain and forebrain to
conspecific song, measured their tuning by calculating spectro-temporal receptive fields (STRFs), and
categorized the STRFs using a cluster analysis. We classified the neurons in to four major functional
groups in the midbrain and five major functional groups in the forebrain. Although the major groups
found in the midbrain were also found in the forebrain, the forebrain neurons showed a higher degree of
heterogeneity within each group illustrating that both inheritance and creation of response properties
occur as information ascends in the auditory processing stream. We then analyzed how conspecific songs
were represented by a population of neurons in each functional group and found a classification of sound
features in terms of their significance for fundamental acoustical percepts of pitch, timbre and rhythm.
This representation of sound features along perceptual dimensions was also made evident by comparing
the tuning of groups of neurons and the perceptual features of sound elements in the modulation spectrum
space. The modulation spectrum is the amplitude spectrum obtained from a 2d Fourier Transform of the
log spectrogram[1, 2]. The figure shows the region in the modulation spectrum occupied by sound
features that are important for different percepts (left panel) and how this space is tiled by some of the
functional groups that were found in the auditory forebrain (right panel; FBB: fast broadband, SBB: slow
broadband, WB: Wideband; SNB: spectral narrowband).

Acknowledgments
This work was supported by NIH grants DC05087 to SW and DC007293 and MH66990 to FT.
References
1.
Singh, N.C. and F.E. Theunissen, Modulation spectra of natural sounds and ethological theories
of auditory processing. J Acoust Soc Am, 2003. 114(6 Pt 1): p. 3394-411.
2.
Woolley, S.M., et al., Tuning for spectro-temporal modulations as a mechanism for auditory
discrimination of natural sounds. Nat Neurosci., 2005. 8(10): p. 1371-9.

29

Thursday evening, Poster session I-22

Cosyne 2008

Choice of Sampling Strategy in Embodied Active Sensing
Jason T. Ritt, Ethan Skowronski-Lutz, Alexis A. Bradshaw, Christopher I.
Moore
McGovern Institute for Brain Research, Massachusetts Institute of Technology
In nature, sensation is partly a behavioral process, in which organisms actively acquire and modify sensory
information through self-motion (e.g. saccades, scanning with fingertips). The choice of sampling motions,
or “strategy”, depends in general on task goals. Also important, though not as well understood, is the
dependence of sensing strategies on the physical embodiment of the sensory organ.
We are addressing this question through parallel modeling and experiment in the rodent vibrissa (whisker)
system, in the specific context of tactile discrimination of surface features. Simulations of the dynamics of
an idealized vibrissa in surface contact suggest how decision rules and control strategies may impact discrimination effectiveness. The model also suggests an explanation for divergent accounts of vibrissa texture
signals, specifically the expression of mechanical resonance. Complementing the theoretical approach, we
track contact-induced motion signals in vibrissae (via high speed videography) while recording neural activity (via multi-electrode cortical implants) of rats trained to perform texture discrimination. Given which
aspects of vibrissa motion patterns are found to strongly modulate cortical activity, the model suggests how
informative different sampling strategies would be for this encoding. For example, vibrissa motion frequency and velocity co-vary at some but not all sweep speeds; choice of sweep speed can thus preferentially
emphasize one of these motion parameters over the other. Such strategies can then be compared to empirically observed animal behavior. Despite the simplicity of the model, we are able to capture salient features
of active sensing in awake, behaving animals.

Acknowledgments
Support provided by NIH 5 F32 NS045415 (J.T.R), NIH RO1 NS045130 (C.I.M.), and NSF 0316933
(C.I.M.). J.T.R. holds a Burroughs Wellcome Fund Career Award at the Scientific Interface.

30

Cosyne 2008

Thursday evening, Poster session I-23

Predictive information in the retina
S. E. Palmer1 , M. J. Berry, II2 , and W. Bialek1
1
2

Department of Physics, Lewis-Sigler Institute for Integrative Genomics, and
Department of Molecular Biology, Princeton University, Princeton, NJ 08544

Prediction is important for almost all modes of behavior and our research focuses on how a population of
neurons implements predictive computations. We have examined how groups of retinal ganglion cells (the
output neurons of the retina) encode predictive information in their collective ﬁring patterns. The population
response is represented as a binary word, indicating a spike or no-spike from each neuron in a small time
window of size Δt. We can then ask how precisely a word at time t speciﬁes a future word at time t + Δt.
This is the predictive information in the population ﬁring. We next construct a synthetic neuron that receives
these inputs, and gives as output a single bit (spike or no-spike). We ask how this downstream neuron,
combining subgroups of its potential presynaptic cells, would learn to become maximally predictive of its
inputs, in the sense that its output spiking would carry maximal information about its input pattern at the
next time step. Can this output capture the available predictive information? What is the structure of the
algorithm that maps inputs to predictive outputs? For four-cell subgroups, we can exhaustively search all
possible deterministic rules for converting input responses into a binary output, and we use this to test our
search strategies for larger groups. We ﬁnd that the best rules can capture more than 95% of the predictive
information in the input ﬁring. Rules which capture such a large percentage of the predictive information
also retain a large amount of stimulus information about the visual world. These best rules can be reliably
learned by a perceptron model, meaning that instantiating such a coding of predictive information in the
brain might be biologically plausible.

Acknowledgments
We thank G. Tkačik, E. Schneidman, and G. J. Stephens for useful discussions. This work was supported by
NIH grant EY03878, by NSF grants IIS-0613435 and PHY-0650617, by Novartis (through the Life Sciences
Research Foundation), and by the Swartz Foundation.

31

Thursday evening, Poster session I-24

Cosyne 2008

Laplace’s Method in Neural Decoding
Shinsuke Koyama, Lucia Castellanos Pérez-Bolde, Cosma Rohilla Shalizi,
and Robert E. Kass
Carnegie Mellon University
State-space models are a promising technique for neural decoding, especially in domains like neural prostheses where the signal to be reconstructed has signiﬁcant temporal structure. The optimal estimate of the
state is its conditional expectation given the observed spike-train histories, but taking this expectation is
computationally hard, especially when nonlinearities are present. Existing ﬁltering methods, including sequential Monte Carlo, tend to be either inaccurate or slow. In this paper, we propose a new nonlinear ﬁlter
which uses Laplace’s method, an asymptotic series expansion, to approximate the conditional mean and
variance, and a Gaussian approximation to the conditional distribution of the state. This Laplace-Gaussian
ﬁlter (LGF) gives fast, recursive, deterministic state estimates, with an error which is set by the stochastic
characteristics of the model and is, we show, stable over time. We illustrate the decoding ability of the LGF
by applying it to a simulation and real-data of the cortical control of hand motion, where it delivers superior
results to sequential Monte Carlo in a fraction of the time [1].

[1] Shinsuke Koyama, Lucia Castellanos Pérez-Bolde, Cosma Rohilla Shalizi, and Robert E. Kass. Approximate methods for state-space models. In preparation, 2007.

32

Cosyne 2008

Thursday evening, Poster session I-25

Mean-ﬁeld approximations for coupled populations of generalized linear model spiking neurons with Markov refractoriness
Taro Toyoizumi, Kamiar Rahnama Rad, and Liam Paninski
Columbia University
There has recently been a great deal of interest in inferring network connectivity from the spike trains in
populations of neurons. One class of models that has proven quite useful is known in the statistics literature as the “generalized linear model” (GLM), variants of which have been applied successfully in the
hippocampus, motor cortex, retina, and cultured cortical slice. In its simplest form, this model incorporates
both stimulus-dependence terms and direct coupling terms between each observed neuron in the network;
ﬁtting the model parameters leads to an inhomogeneous (stimulus-driven), nonlinear coupled spiking model
with delay terms. Statistically speaking, the model is attractive due to its explicitly probabilistic nature,
and because ﬁtting the model parameters is surprisingly simple: under certain simple conditions, the loglikelihood function with respect to the model parameters is concave and, hence, the maximum likelihood
estimation of number of parameters can be easily performed via standard ascent methods.
Once the model parameters are obtained, we are left with an obvious question: what do we do next? One
of the key applications of such a network model is to better understand the input-output properties of the
network. For example, we would like to be able to predict the mean ﬁring response of the network given a
novel input, and to dissect out the impact of the network coupling terms on this stimulus-response relationship (e.g., how does local inhibition impact the stimulus ﬁltering properties of the network?). We would also
like to know how the correlation properties of spike trains in the network might depend on the stimulus, and
in general how correlations might encode stimulus information. We can in general study these questions via
direct Monte Carlo of the network. However, simulation of a large scale probabilistic spiking network is
computationally expensive, since we often need to draw many samples (i.e., run many simulated trials) in
order to compute the quantities of interest to the desired precision. More importantly, numerical simulation
often provides limited analytical insight into the mechanisms underlying the observed phenomena.
The goal of this study is to investigate how much of the behavior of these GLM networks can be understood
using standard analytical “mean-ﬁeld” approximations. In particular, we develop analytically-tractable approximations for the mean ﬁring rates of the network given novel stimuli, as well as the auto- and crosscrosscorrelation and input-output ﬁltering properties of these networks. These approximations are valid
when the network coupling terms are small, and lead to deterministic ordinary differential equations that are
much easier to analyze than direct Monte Carlo simulation of the network activity. However, in the case of
strong refractory effects, these mean-ﬁeld approximations become inaccurate, since the spike-history terms
in the generalized linear model must be large to induce strong refractoriness, and this pushes our approximations beyond their region of accuracy. Therefore we introduce a new model, a generalized linear model
with Markovian refractoriness. This new model has several advantages in this setting: it captures strong
refractoriness, retains all of the easy ﬁtting properties of the standard generalized linear model, and leads
to much more accurate mean ﬁeld approximations. We validate our mathematical analysis with a variety of
simulated examples.
Acknowledgments
TT was supported by the Japan Society for the Promotion on Science, a Grant-in-Aid No. 1806772 for JSPS
fellows. LP is supported by the NEI and by a Sloan Fellowship.

33

Thursday evening, Poster session I-26

Cosyne 2008

Temporal Compression Mediated by
Short-Term Synaptic Plasticity
Christian Leibold1−3,5, Anja Gundlﬁnger2,4 , Robert Schmidt1,4 ,
Kay Thurley1,6 , Dietmar Schmitz2,4 Richard Kempter1,2,4
1

Institute for Theoretical Biology, Humboldt-Universität zu Berlin
Research Center, Charité, Berlin
Department of Biology II, University of Munich
4
Bernstein Center for Computational Neuroscience Berlin
5
Bernstein Center for Computational Neuroscience Munich
6
Institute of Physiology, University of Bern

2
Neuroscience
3

Time scales of cortical neuronal dynamics range from few milliseconds to hundreds of milliseconds. In
contrast, behavior occurs on the time scale of seconds or longer. How can behavioral time then be neuronally
represented in cortical networks? Here we offer a hypothesis on how to bridge the gap between behavioral
and cellular time scales. The core idea is to use a long time constant of decay of synaptic facilitation to
translate slow behaviorally induced temporal correlations into a well-suited distribution of synaptic response
amplitudes. These amplitudes can then be transferred to a sequence of action potentials in a population of
neurons. In-vitro experiments with hippocampal pyramidal neurons and a computational model reveal that
subthreshold oscillations considerably increase the range for temporal encoding. We ﬁnd that the mutual
information between spike timing and synaptic response amplitude crucially depends on the phase of the
synaptic input.
The spike sequences that are generated by the combination of short-term synaptic plasticity and subthreshold
oscillations in a population of neurons provide temporal correlations on a millisecond time scale. These
correlations are able to induce persistent synaptic changes. As a proof of concept, we provide simulations of
a neuron that learns to discriminate temporal patterns on a time scale of seconds by a synaptic learning rule
(tempotron) with a millisecond memory buffer. We ﬁnd that, also in terms of the success of learning, the
conversion from synaptic amplitudes to millisecond correlations can be strongly facilitated by subthreshold
oscillations.
Our model provides a hypothesis on the mechanism of hippocampal reverse replay, predicting that instances
of reverse replay should be temporally correlated with synchronous readout of the synaptic memory buffer
(state of facilitation) of a large number of hippocampal mossy ﬁber synapses. This would correspond to the
synchronous activation of a large fraction of the presynaptic dentate gyrus granule cells.
Acknowledgments

This work has been supported by the Deutsche Forschungsgemeinschaft (DFG) through SFB 618 “Theoretical Biology” and GRK 1123 “Memory Consolidation” as well as Emmy Noether grants (Schm 1381/1-2,3
and Ke 788/1-2,3) to DS and RK, and the Bundesministerium für Bildung und Forschung (Bernstein Centers
for Computational Neuroscience Berlin and Munich, 01GQ0410 and 01GQ0440, respectively.

34

Cosyne 2008

Thursday evening, Poster session I-27

Faithful Representation of Video Streams with a
Population of Spiking Neurons
Aurel A. Lazar and Eftychios A. Pnevmatikakis
Department of Electrical Engineering, Columbia University
We present a model architecture for the representation of natural and synthetic video streams (movies,
animation) with a population of spiking neurons. Our focus is on ﬁnding conditions required for a faithful
representation of feature rich video content in the spike domain and on providing an algorithm that recovers
streaming video with arbitrary precision.
In our architecture a time-bandlimited and space-discretized video stream is fed to a population of neurons.
The streaming input to each neuron is ﬁltered by a spatiotemporal receptive ﬁeld whose spatial and temporal
components consist of a Gabor ﬁlter and an integrator, respectively. The spatial components form a Gabor
ﬁlterbank. Spike generation is threshold based and the resulting spike trains represent the output of the
model architecture.
We formally prove that under certain conditions video streams can be faithfully recovered from the spike
trains generated by the neural architecture and provide an algorithm for perfect recovery. The conditions
call for (i) a minimum total average spike density that is determined by the video spatial resolution (given by
the number of pixels) and frame rate (bandwidth), and (ii) a minimum number of neurons that is determined
by the maximum video spatial resolution that can be formally recovered.
We illustrate the algorithm with both natural video streams and, synthetic video streams with separable
space and time components. In the latter case we also provide an algorithm for estimating the space/time
components from the recovered signal. We show that condition (ii) can be relaxed for video streams with
low spatial bandwidth since neighboring pixels exhibit signiﬁcant correlation. We also demonstrate that
increasing the number of neurons in the architecture results in a recovery with an improved spatial resolution
(see Figure 1). Finally, we investigate whether the ablation of neurons whose receptive ﬁelds are centered
around a speciﬁc spatial domain results in a local degradation of the video stream representation. Our
presentation will be supported by extensive real-time video feeds.

Figure 1: The quality of the recovery of the spatial component for a space-time separable video stream
increases with the number of neurons. The spatial resolution of the original video stream is 51 × 51 pixels.
Acknowledgments
This work was supported by NIH grant R01 DC008701-01 and NSF grant CCF-06-35252. E. A. Pnevmatikakis was also supported by the Onassis Public Beneﬁt Foundation.

35

Thursday evening, Poster session I-28

Cosyne 2008

The Role of Stimulus Correlations for Population Decoding in the
Retina
Greg Schwartz1, Jakob Macke1,2, and Michael J. Berry II1
1Princeton

University, 2Max Planck Institute Tuebingen

All information about the visual world passes through the optic nerve, so with access to the spike trains of
a large number of retinal ganglion cells, one should be able to construct a decoding algorithm to
discriminate different visual stimuli. Despite the inherent noise in the response of the ganglion cell
population, everyday visual experience is highly deterministic. We have designed an experiment to study
the nature of the population code of the retina in the “low error” regime.
We presented 36 different black and white shapes, each with the same number of black pixels, to the
retina of a tiger salamander while recording retinal ganglion cell responses using a multi-electrode array.
Each shape was presented over 100 trials for 0.5 s each and trials were randomly interleaved. Spike trains
were recorded from 162 ganglion cells in 13 experiments. We removed noise correlations by shuffling
trials, as we wanted to focus on the role of correlations induced by the stimulus (signal correlations).

False Alarm Rate

We designed decoding algorithms for this population response in order to detect each target shape against
the distracter set of the 35 other shapes. Binary response vectors were constructed using a 100 ms bin
following the presentation of each shape. First, we used a simple decoder that assumes that all neurons are
independent. This decoder is a linear classifier. A second decoder, which takes into account correlations
between neurons, was constructed by fitting Ising models1 to the
population response using up to 162 neurons for each model.
0
10
Independent Decoder
We also constructed the statistically optimal decoder based on a
Mixture Decoder
Ising Decoder
-1
mixture model, which accounts for signal correlations.
Low Error Limit
10
-2

10
Using populations of many neurons, the optimal and Ising
decoders performed considerably better than the “independent”
-3
10
decoder. For certain shapes, the optimal decoder had 100 times
-4
fewer false positives than the independent decoder at 99% hit
10
rate, and, in the median across shapes, the performance
-5
10
enhancement was 8-fold. While the decoder using an Ising
model fit to the pairwise correlations did not achieve optimality,
0
40
80
120
160
Number of Cells (Rank Ordered)
it was up to 50 times more accurate than the independent
decoder, and 3 times more accurate in the median across shapes.
Some shape discriminations were performed at zero error out of 3500 trials using the optimal and Ising
decoders on only a subset of the recorded cells while none reached this “low error” level using the
independent decoder even on all 162 cells (see figure).

We find that discrimination with very low error using large populations requires a decoder that models
signal correlations. Linear classifiers were unable to reach the “low error” regime. The Ising model of the
population response is successfully applied to groups of up to 162 cells and offers a biologically feasible
mechanism by which downstream neurons could account for correlations in their inputs.
References
[1] E Schneidman E, Berry II MJ, Segev R, Bialek W. (2006). Weak pairwise correlations imply strongly
correlated network states in a neural population. Nature 440, 1007-1012.

36

Cosyne 2008

Thursday evening, Poster session I-29

Stimulus-Independence of Noise Correlations is Beneﬁcial for
Short-Term Population Coding in MT
Arno Onken1,2 and Klaus Obermayer1,2
1
2

Bernstein Center for Computational Neuroscience Berlin
Berlin University of Technology

The noise correlation of orientation selective populations in the middle temporal (MT) area is known to
depend on the difference in their preferred directions [1, 2]. Here, we construct a multivariate Poisson
(MVP) model of spike-counts of an orientation selective population of up to 100 MT neurons in order to
compare the impact of different correlation structures on coding at a time scale of tens of milliseconds.
In accordance with experimental ﬁndings, the model has the following constraints: (1) We assume von
Mises tuning functions with ﬁring rates between 10 Hz and 60 Hz. (2) Covariances are strictly decreasing
for increasing differences in preferred directions up to a margin with a speciﬁed mean of correlation coefﬁcients. (3) With respect to the number of hidden variables, the model complexity is minimal subject to
the ﬁrst constraints. We ﬁnd that a MVP model with only pairwise correlations cannot reproduce the mean
correlations observed experimentally; on the other hand, the constraints yield a unique higher order system
of correlations. Within this model class, near stimulus-independence of the correlation coefﬁcients produces the best ﬁt to the neurophysiological data, i.e. the dependence residue is close to the experimentally
observed one.
The model is complemented by a stimulus readout. We assess the mean square error (MSE) of optimal
decoders and compare the normalized average Kullback-Leibler divergence (ΔI/I) between the posterior
distribution of the true responses and the posterior distribution of independent responses. Since the computation of the Bayes-optimal posterior has exponential complexity, we apply an approximation based on an
orthonormal system for the likelihoods. We estimate coding capabilities within 30 ms windows for different
models in the class: one with a correlation structure that exhibits stimulus-independent covariances and thus
stimulus-dependent correlation coefﬁcients, a model with a structure that shows near stimulus-independent
correlation coefﬁcients (the best ﬁtting model), and models with intermediate structures. It is found that the
best ﬁtting model is not optimal in terms of the MSE readout performance. However, ΔI/I turns out to
be minimal for the best ﬁtting model. Furthermore, the readout performance of a MT model without any
noise correlation is better than the readout performance of any model in the investigated model class. These
ﬁndings suggest that MT optimizes the correlation structure in the encoding process to let the resulting posterior come near to the posterior of a model with independent noise. Therefore, the ﬁndings indicate that
MT cannot beneﬁt from noise correlations for short-term coding.
Acknowledgments
This work was supported by BMBF grant 01GQ0410.
References
[1] Correlated ﬁring in macaque visual area mt: time scales and relationship to behavior. W. Bair, E. Zohary,
and W. T. Newsome, Journal of Neuroscience 21(5):1676-1697, March 2001.
[2] The structure and time course of neuronal correlation in cortical motion area MT. X. Huang and S. G.
Lisberger, Society for Neuroscience Abstracts 33:715.16, November 2007.

37

Thursday evening, Poster session I-30

Cosyne 2008

Estimating multi-neuronal point-process spiking models with
partially observed data.
Jonathan W. Pillow and Peter Latham
Gatsby Computational Neuroscience Unit, UCL
Point process encoding models provide powerful statistical methods for understanding the responses of
neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early
sensory pathway, they have fared less well in deeper brain areas, where responses are more highly nonlinear
and more heavily inﬂuenced by feedback and recurrent processing. Here we propose an extension to the
point-process modeling framework by introducing a population of hidden (or “latent”) neurons whose activity can inﬂuence the responses of observed neurons, resulting in a more general and more ﬂexible model
of population responses.
The multivariate point process encoding model [1, 2], often referred to as a generalized linear model (GLM),
contains a bank of linear ﬁlters specifying the dependence of each neuron’s response on the stimulus and
the recent spiking history of itself and its neighbors. The input-output properties of individual neurons are
extremely simple, with the conditional intensity (i.e., spike rate) given by an exponential function of its net
ﬁltered input. However, the population as a whole can exhibit more interesting functional properties because
of the recurrence introduced by the coupling terms between neurons. Latent neurons endow the model with
the ﬂexibility to exhibit a richer array of functional behaviors, because nonlinear and modulatory effects that
cannot arise within the standard model can be ascribed to interactions with unobserved neurons.
Estimating this model requires an algorithm for computing the likelihood of the observed data and maximizing it with respect to the model parameters (including those governing latent neurons and their connectivity
with other neurons in the population). This is a difﬁcult problem because it requires integrating over the
probability of unobserved neurons’ responses, which is computationally infeasible under the standard point
process model likelihood. We address this problem by introducing an auxiliary point process model that
approximates the conditional probability over latent spikes given the observed spikes. This auxiliary model
can then be employed, in a variant of the “wake-sleep” algorithm, to ascend the likelihood function of the
original model parameters. We demonstrate the method on a simple toy network consisting of one observed
and one hidden neuron, and show that it out-performs a standard GLM description of the observed response.
We are currently pursuing applications to larger networks, which may provide insight into the inﬂuence of
network interactions on the encoding of stimuli in neural populations farther along in the sensory pathway.
Acknowledgments
This work was supported by the Gatsby Charitable Trust and the Royal Society USA/Canada Research
Fellowship.
References
[1] A Point Process Framework for Relating Neural Spiking Activity to Spiking History, Neural Ensemble
and Extrinsic Covariate Effects. Truccolo, Eden, Fellows, Donoghue & Brown. J Neurophysiol 93(2):1074–
1089, 2003.
[2] Maximum likelihood estimation of cascade point-process neural encoding models. L Paninski. Network:
Computation in Neural Systems 15:243–262, 2004.

38

Cosyne 2008

Thursday evening, Poster session I-31

Bayesian Restoration of Phase Response Curves and Prediction
of Stochastic Behavior of Hippocampal CA1 Pyramidal Neurons

Keisuke Ota1 , Toru Aonishi1 2 , Shigeo Watanabe3 , Hiroyoshi Miyakawa3 ,
Toshiaki Omori4 2 , and Masato Okada5 2
1
Tokyo Inst. of Tech., 2 RIKEN BSI, 3 Tokyo Univ of Pharm and Life Sci, 4 JSPS
Research Fellow, 5 Univ. of Tokyo
Recently, phase response curves (PRCs) of single neurons have been estimated by perturbation-response
experiments in vitro [1,2]. It has been elucidated that the phase description based on PRCs physiologically
measured can predict oscillatory behavior of neural networks [2]. Therefore, in-vitro measurement of PRCs
has been noticed as one of important methods which would bridge the gap between single neuron dynamics
and network dynamics. However, when estimating PRCs through in-vitro experiments, we would encounter
serious problems where PRCs had to be retrieved from noisy data.
In this study, we propose an estimation method of PRCs based on Bayesian statistics. To derive a likelihood
function of PRCs, we ﬁrst analyze a stochastic differential equation which is a model of the perturbationresponse experiment and derive a probabilistic density function describing stochastic phase motions by use
of the Fokker-Planck (FP) equation. According to Bayes’ theorem, by introducing the smoothness prior,
we obtain a posterior density function representing the reversal process for the observation of PRCs. Then,
we try to estimate PRCs by applying Maximum A Posteriori (MAP). Additionally, we analytically derive a
Marginal Likelihood, which gives optimal hyper-parameters; intensity of the white noise inherent in neurons
and parameters of the smoothness prior. Figures 1 and.2 show the result of applying our algorithm to in-vitro
data measured from a hippocampal CA1 pyramidal neuron.
By applying our algorithm to in-vitro data, we could estimate Langevin equations describing stochastic
phase motions of neurons, because probabilistic models assumed here are based on the Langevin equation
[3]. Therefore, it is possible to predict stochastic behaviors of neurons by use of the FP equation. Here we
will have an estimate on equilibrium distributions for spike-timing of hippocampal CA1 pyramidal neurons
driven by periodical perturbations.
Figure2:PRC in Hippocampal CA1 Pyramical Neuron

Figure1:Marginal Likelihood
-1740
-1745
-1750
-1755
-1760
-1765
-1770
-1775
-1780

400

300

200

25
PRC
STR

Spike Time Advance(msec)

D:smoothness parameter

500

-1.75e+003
-1.76e+003
-1.77e+003

100

10

15

20

15

10

5

0

-5

-10

-15

20

0

10

20

30

40

50

60

70

Time Since Last Spike(msec)
Perturbation(rectangle pulse) : 15pA 20msec

σ:intensity of white Gaussian noise
Maximum point is (D,ǻ


References
[1] M.Lengyel, J.Kwag and P.Dayan, Nature Neuroscience 8:1677-1683, 2005
[2] T I. Netoff, M I. Banks, A D. Dorval, C D. Acker, J S. Haas, N Kopell and J A. White, Journal of
Neurophysiology 93:1197-1208, 2005
[3] T.Aonishi and K Ota, Journal of the Physical Society of Japan 75(11):114802, 2006

39

Thursday evening, Poster session I-32

Cosyne 2008

Topological Structure of Population Activity in Primary Visual
Cortex
D. Ringach1, G. Singh2, F. Memoli3, T. Ishkanov3, G. Sapiro4 and G. Carlsson3
1

Department of Neurobiology, University of California, Los Angeles, 2Institute for
Computational and Mathematical Engineering, Stanford University, 3Department
of Mathematics, Stanford University, 4Department of Electrical and Computer
Engineering, University of Minnesota.
Information in the cortex is believed to be represented by the joint activity of neuronal populations.
Developing insights into the nature of these representations is a necessary first step in our quest to
understanding cortical computation. Here we demonstrate that fundamental questions about neural
representation can be cast in terms of the topological structure of population activity. A new method,
based on the concept of persistent homology, is introduced and validated on artificial datasets. The
technique is then applied to study the topological structure of neural activity in cell populations of
primary visual cortex that were either spontaneously active or driven by natural image sequences. Our
analyses confirm that spontaneous activity is highly structured and statistically different from noise.
Furthermore, the topological objects derived from spontaneous and driven activity have similar
distributions which are dominated by the topology of a circle and the two-sphere. This latter structure, we
postulate, corresponds to the representation of orientation and spatial frequency on a spherical surface.
Our findings shed new light on the relationship between ongoing and driven activity in primary visual
cortex and demonstrates, for the first time, that computational topology offers novel tools to tackle
fundamental questions about the representation of information in the nervous system.

Acknowledgments
This work has been supported by DARPA HR0011-05-1-0007 (GC, GS, FM, TI and GS), NSF DMS
0354543 (GC), ONR N000140310176 (GS), NSF 0309575 (GS), DARPA FA8650-06-1-7630 (GS),
14168480-32905-C (GS), NGIA HM1582-04-1-2023 (GS), ARO W911NF-07-1-0473(GS), NIH-EY
12816 (DLR), FA8650-06-C-7633 (DLR).

40

Cosyne 2008

Thursday evening, Poster session I-33

Exploring Parallelly Recorded Spike Trains
Ovidiu F. Jurju 1,2,3, Danko Nikoli 1,2, Wolf Singer 1,2, Dirk Metzler 4 and
Raul C. Murean1,2,3
1

Frankfurt Institute for Advanced Studies, Frankfurt am Main, Germany; 2 Max-Planck Institute
for Brain Research, Frankfurt am Main, Germany; 3 Center for Cognitive and Neural Studies,
Cluj-Napoca, Romania; 4 Johann Wolfgang Goethe University, Frankfurt am Main, Germany
The brain’s huge computational power relies on its distributed architecture, with a highly interconnected
network of constantly interacting individual units. Understanding the collective behavior of these units
requires methods for visualizing multi-dimensional data. The current way of viewing spike trains from
parallel recordings relies on rastergrams in which the display order of neurons can greatly influence one’s
ability to detect even the simplest spike patterns [1].
We propose a method which offers the means to interpret and visualize the complex dynamics of a set of
parallelly recorded neurons. By considering the activity of all neurons at different points in time, we were
able to describe, in a multi-dimensional space, the collective behavior of neurons. Using 3D Kohonen
Maps, we projected the multi-dimensional space to a lower dimensional (3D) space of colors. Based on
the projection, multi-dimensional vectors are coded by colors, considering their degree of similarity.
The method was applied to data recorded from cat area 17 under various stimulations conditions (gratings
shown in Fig.1). Besides visualizing the time course of collective dynamics, we also quantified the
variability of responses in different recording sessions. In addition, we provide quantitative analysis
describing the trajectory of the system in the multi-dimensional space and compare the properties of
multi-dimensional patterns with the properties of their constituent neurons: peri-stimulus time histograms,
stimulus tuning, auto- and cross-correlation analysis. The method allows fast, flexible and intuitive data
exploration, opening the way for efficient multi-dimensional analysis.

Fig.1. Activity of 26 simultaneously recorded single units from cat V1, stimulated with drifting sinusoidal
gratings. Colored lines represent individual trials and are grouped by stimulation condition

References
[1] Methods for finding and validating neural spike patterns. J.V. Toups and P.H.E. Tiesinga,
Neurocomputing 69:1362–1365, 2006.

41

Thursday evening, Poster session I-34

Cosyne 2008

The effects of correlated neural activity on single-neuron spiking
variability in the primate retina
Jonathan W. Pillow1 , Jonathon Shlens2 , Liam Paninski3 , Alexander Sher4 ,
Alan M. Litke4 , E.J. Chichilnisky2 , Eero P. Simoncelli5
1

Gatsby Computational Neuroscience Unit, UCL; 2 The Salk Institute;
3
Columbia University; 4Santa Cruz Institute for Particle Physics, University of
California, Santa Cruz; 5 HHMI, Center for Neural Science, and Courant Inst.
Math. Sciences, NYU
Visual neurons ﬁre stochastically in response to stimuli: repeated presentations of a single visual stimulus
result in spike patterns that differ markedly from trial to trial. In retinal ganglion cells, this variability places
critical limits on the ﬁdelity with which neurons can convey visual information to the brain. However,
single-neuron measurements of response variability neglect the fact that neurons in a local population exhibit
statistical dependencies in their responses, which can inﬂuence the effective variability (and hence coding
ﬁdelity) of the full population. Does the variability observed in individual neurons primarily reﬂect their
intrinsic stochastic properties or the stochastic behavior of the local network? Here we show that a model
of spiking activity in a population of macaque retinal ganglion cells can be used to account for a substantial
fraction of a single neuron’s variability in terms of variability present in the population response.
The model, an instance of generalized linear model, describes each neuron’s response as a point process
whose conditional intensity (i.e., spike-rate) is given by the exponentiated net output from a bank of linear
ﬁlters applied to the stimulus, the neuron’s recent spiking history, and the recent spiking history of other
neurons in the population. This model provides an accurate description of p(r|x), the joint probability
distribution over spike responses r given a stimulus x, which encompasses both the the stimulus-dependence
and the detailed space-time correlation structure of the population response.
We present a novel approach to understanding the relative contribution of intrinsic and network stochasticity
to the response variability of a single cell, by using the model to probe the inﬂuence of neighboring cells
in the population. Speciﬁcally, we draw samples from p(ri |rj=i , x), the distribution over the i’th neuron’s
response given both the stimulus and the responses of other neurons in the population. Averaging these
responses gives a population-conditioned PSTH, which can be considered a single-trial rate prediction of
the cell’s activity based on both the stimulus and the population activity during that trial. We show that
this model-based calculation predicts single-trial activity more accurately then the neuron’s actual PSTH
(which gives the lowest possible variance for any stimulus-based prediction). Thus, a traditional raster plot
reﬂects only a lower-bound on the precision and reliability of single neurons, and a signiﬁcant fraction of
the variability observed in such rasters can be attributed to spiking activity in the local population.
Acknowledgments
This work was supported by the Royal Society USA/Canada Research Fellowship (JP); NSF IGERT DGE03345 (JS); NEI grant EY018003 (EJC, LP, & EPS); Gatsby Foundation Pilot Grant (LP); Burroughs Wellcome Fund Career Award at the Scientiﬁc Interface (AS); US National Science Foundation grant PHY0417175 (AML); McKnight Foundation (AML & EJC); and HHMI (EPS).

42

Cosyne 2008

Thursday evening, Poster session I-35

Ramping, ramping everywhere: an overlooked timing model
Patrick Simen
Princeton University
Ramping ﬁring rates prior to behavioral responding seem ubiquitous in behavioral neuroscience, and they
are most easily explained as anticipatory in nature. While such ramping could be merely a response to
anticipations encoded in some other way, a more parsimonious explanation is that ramping itself is the
mechanism by which anticipation occurs. Here I show that a noisy, three-layer neural network can time intervals by controlling, through synaptic strength, the rate at which average activation in its middle layer rises
linearly to a threshold. This model approximates a drift-diffusion process with a single absorbing boundary
that is normally distributed across trials. The approximate normality of the threshold distribution — which
is critical to the model’s account of data from timing experiments — arises naturally as a consequence of
strong positive feedback in the model’s output layer. Hysteresis in the output layer can then account for
high-rate responding near the time of expected reward delivery in the peak interval (PI) procedure [1], as
well as for the ﬁring rate dip in dopamine-producing cells when expected rewards are omitted.
The dynamics of both the linearly ramping units and the switch-like, bistable units are determined by different values of positive feedback, encoded at each unit as a recurrent synaptic weight. This weight strength
deﬁnes a cusp catastrophe, and nearly linear ramping occurs at the bifurcation value of this parameter (i.e.,
at the cusp). Bistability occurs at all larger values of positive feedback, and hysteresis is stronger the larger
the value of this parameter. The simple structure of the model allows durations to be encoded by a single, inter-layer connection-strength from the ﬁrst, switch-like layer to the second, ramping layer. A simple
weight-change rule therefore sufﬁces to track changing interval durations robustly.
The main reason such a model has been overlooked in the psychological literature is that drift-diffusion
models with ﬁxed thresholds do not have the property of ‘scalar invariance’, in which the standard deviation
of interval estimates is a ﬁxed proportion of the estimates’ means. However, if the model’s processing
units are allocated so as to minimize the variance of its duration estimates, then scalar invariance occurs
in the limit of large allocations. The model supports a variety of temporal computations, such as timeleft and bisection, that can be easily performed by neural network mechanisms, and temporal derivative
computations [2] applied to the ramp can be used with a drift-diffusion model of decision making to account
for hyperbolic discounting of delayed rewards. This model is most closely related to that of [3], but extends
it by relating it explicitly to drift-diffusion models of decision making, by explaining threshold variability
mechanistically, and by incorporating ramping into models of several different temporal computations.
References
[1] Application of scalar timing theory to individual trials. R. M. Church, W. H. Meck and J. Gibbon, Journal
of Experimental Psychology: Animal Behavior Processes, 20:135–155, 1994.
[2] Analog VLSI and neural systems. Carver Mead, 1989, Addison-Wesley.
[3] Climbing neuronal activity as an event-based cortical representation of time. J. Reutimann, V. Yakovlev,
S. Fusi and W. Senn, Journal of Neuroscience, 24(13):3295-3303, March 31, 2004.

43

Thursday evening, Poster session I-36

Cosyne 2008

A comparison of neural spiking activity in the sub-thalamic nucleus
of Parkinson's disease patients and healthy primates
1

Sridevi V. Sarma , 2Ming L. Cheng, 2Ziv Williams, 2Rolling Hu, 1,2Emery N.
Brown, 2Emad Eskandar
1

Massachusetts Institute of Technology,

2

Masachusetts General Hospital

How neurons in humans encode information about the outside world and how this processing changes
when the brain is diseased are central questions in neuroscience and medicine. Historically,
microelectrode recordings of single-unit neuronal activity have been confined to animal preparations.
Recently, it has become possible to obtain single-unit recordings in humans undergoing deep brain
stimulation surgery. In this study, we recorded neuronal activity from the sub-thalamic nucleus (STN) of
the basal ganglia of patients with Parkinson’s disease (PD). In parallel, identical experiments were
conducted on healthy primates, providing a rare opportunity to analyze STN neuronal activity recorded in
both the disease and healthy state during the same visual guidance tasks; and, to correlate the
neurophysiology with associated behavior. Prior to this study, characterizations of STN neural activity in
PD patients were being quantified without comparisons to healthy STN neurons (e.g. [1]).
We developed point process (PP) models of STN neurons to capture neural spiking dynamics as a
function of extrinsic stimuli and the neuron’s own spiking history. A PP is a binary stochastic process
defined in continuous time and is characterized entirely by the conditional intensity function (CIF)
O (t | H t )' , which is roughly the probability of a neuronal spike in any small time interval (t , t  ' )
given the history ( H t ) of the spiking activity and that of any covariates up to time t [2]. The PP
paradigm is a probabilistic framework that is advantageous over traditional analyses which try to uncover
intrinsic and extrinsic factors on neuronal spiking activity (e.g. mean firing rates, autocorrelation
functions, inter-spike interval histograms) in that a single PP model captures the relative contribution of
history effects as well as the impact of stimuli on the probability that the neuron will spike. In addition,
we can measure uncertainty in the model parameters and measure absolute goodness-of-fit between a
model and observed data. We defined the CIF as a dynamic function of the movement direction and the
neuron’s spiking history in the preceding 150 msec. Our models quantify abnormalities in PD neural
activity such as bursting, 10-30Hz oscillations, and loss of directional plurality, which may directly relate
to motor disorders observed in PD (e.g. bradykinesia, resting tremor, rigidity). For all subjects,
abnormailities decreased after a visual stimulus or movement onset. Furthermore, we found that not being
able to anticipate visual cues degraded neuronal activity in all subjects, though not enough in healthy
primates to reach abnormal levels observed in PD activity. Correspondingly, reaction times increased
100% in PD patients and were not affected in healthy primates when visual cues could not be anticipated.
Acknowledgments
Support provided by NIDA grant DA015644 and MH59733 to E.N.B., NREF from the AANS to E.E.
References
[1] Timing and direction selectivity of subthalamic and pallidal neurons in patients with Parkinson
disease. Z.M. Williams, J.S. Neimat, G.R. Cosgrove, E.N. Eskandar, Exp Brain Res. 162(4): 407-416,
May 2005.
[2] Point Processes. D.R. Cox and V. Isham, Boca Raton, FL: CRC, 2000.

44

Cosyne 2008

Thursday evening, Poster session I-37

Representational similarity analysis – a framework for relating
computational theory to behavioral and brain-activity data
Nikolaus Kriegeskorte,1 Marieke Mur1,2, Roozbeh Kiani3, Peter Bandettini1
1

Section on Functional Imaging Methods, Laboratory of Brain and Cognition, National Institute of Mental Health
Department of Cognitive Neuroscience, Faculty of Psychology, Maastricht University
3
Department of Neurobiology and Behavior, University of Washington, Seattle, Washington, USA
2

A fundamental challenge for systems neuroscience is to
quantitatively relate its three major branches of research:
behavioral experimentation, brain-activity experimentation, and computational modeling (Fig. 1a). We suggest
relating the three branches by quantitatively comparing
representational dissimilarity matrices.
For each pair of experimental conditions (e.g. each pair of
stimuli), the representational dissimilarity matrix contains
an entry reflecting the dissimilarity of the activity patterns
associated with the two conditions. Intuitively, the
dissimilarity matrix encapsulates the information carried
by a given representation in a brain or computational
model.
Using the representational dissimilarity matrix as the
signature of each representation allows quantitative
comparisons between representations (Fig. 1b) without
the need for a spatial correspondency mapping (defining,
for example, which neuron corresponds to which unit of a
computational model). We can relate representations
measured in biological brains to representations in computational network models. We can also relate representations between different brain regions, as well as
between different individuals and species. Moreover, we
can relate different modalities of brain activity
measurement (e.g. single-cell recording and fMRI) for a
given brain representation. Our approach requires fewer
assumptions than formal information-theoretic alternatives and can handle designs with large numbers of
conditions (e.g. many stimuli).
We demonstrate this method, called “representational
similarity analysis”, by relating object representations
between monkey and human IT (measured with singlecell recording and fMRI, respectively) and several
computational models. The representational dissimilarity
matrices are simultaneously related via second-level
application of multidimensional scaling and tested for
relatedness and distinctness using randomization and
bootstrap techniques.

45

Fig. 1: The representational dissimilarity matrix
as a hub that relates different representations.
(a) The representational dissimilarity matrix can
provide an interface for relating different
representations.
Note
that
representational
dissimilarity matrix shown is based on only 4
2
conditions, yielding only (4 -4)/2=6 parameters.
However, since the number of parameters grows
quadratically in the number of conditions, the
representational dissimilarity matrix can provide an
informationally rich interface for relating different
representations. (b) This panel illustrates in greater
detail what different representations can be related
via the quantitative interface provided by the
representational dissimilarity matrix. We arbitrarily
chose the example of fMRI to illustrate the withinmodality relationships that can be established. Note
that all these relationships are difficult to establish
otherwise (gray double arrows).

Thursday evening, Poster session I-38

Cosyne 2008

Sparse decoding of neural activity in a spiking neuron model of V1
Jianing Shi, Jim Wielaard and Paul Sajda
Columbia University, New York, NY
We investigate using a previously developed spiking neuron model of layer 4 of primary visual cortex (V1) [1]
as a recurrent network whose activity is consequently linearly decoded, given a set of complex visual stimuli.
Our motivation is based on the following: 1) Linear decoders have proven useful in analyzing a variety of
neural signals, including spikes, firing rates, local field potentials, voltage sensitive dye imaging, and scalp
EEG, 2) linear decoding of activity generated from highly recurrent, nonlinear networks with fixed
connections has been shown to provide universal computational capabilities, with such methods termed liquid
state machines (LSM) [2] and echo state networks (ESN) [3], 3) in LSMs or ESNs often little is assumed about
the recurrent network architecture. However it is likely that for a given type of stimulus/input, the architecture
of a biologically constrained recurrent network is important since it shapes the spatio-temporal correlations
across the neuronal population, which can potentially be exploited efficiently by an appropriate decoder.
We conduct experiments using a two-alternative forced choice paradigm of face and car discrimination, where
a set of 12 face (Max Plank Institute face database) and 12 car grey-scale images are used [4]. All the images
(512 x 512 pixels, 8 bits/pixel) have identical Fourier magnitude spectra. The phase spectra of the images are
manipulated using the weighted mean phase method to introduce noise, resulting in a set of images graded by
phase coherence. The sequence of images are presented to the V1 model (detailed in [1]) in a block design,
where a face or car image is flashed for 50 ms, followed by interval of 200 ms in which a mean luminance
background is shown. We use a linear decoder to map the spatio-temporal activity in the recurrent V1 model to
a decision on whether the input stimulus is a face or a car. We employ a sparsity constraint on the decoder in
order to control the dimension of the effective feature space. Sparse decoding is also consistent with previous
research efforts on decoding multi-unit recording and optical imaging data.
We evaluate the decoding accuracy of the linear decoding of the activity in the V1 model and compare that to a
set of psychophysical data using the same stimuli. We construct a neurometric function for the decoder, with
the variable of interest being the stimulus phase coherence. We find that linear decoding of neural activity in a
recurrent V1 model can yield discrimination accuracy that is at least as good as, if not better than, human
psychophysical performance for relatively complex visual stimuli. Thus substantial information for superaccurate decoding remains at the level of V1 and loss of information needed to better match behavioral
performance is predicted to occur downstream in the decision making process. We also find a small
improvement in discrimination accuracy when a spatio-temporal word is used relative to a spatial-only word,
providing insight into the utility of a temporal vs. a rate code for behaviorally relevant decoding.
Acknowledgments
We thank Marios Philiastides for providing the face/car stimuli and psychophysics data. We also thank Mads
Dyrholm for helpful discussions. This work was supported by NGA HM1582-07-1-2002.
References
[1] Jim Wielaard and Paul Sajda. Extraclassical receptive field phenomena and short-range connectivity in V1. Cereb
Cortex, 16(11):1531–45, 2006.
[2] Wolfgang Maass, Thomas Natschlager, and Henry Markram. Real-time computing without stable states: a new
framework for neural computation based on perturbations. Neural Comput, 14(11):2531–60, 2002.
[3] Herbert Jaeger and Harald Haas. Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless
communication. Science, 304(5667):78–80, 2004.
[4] Marios G Philiastides and Paul Sajda. Temporal characterization of the neural correlates of perceptual decision
making in the human brain. Cereb Cortex, 16(4):509–18, 2006.

46

Cosyne 2008

Thursday evening, Poster session I-39

Multiplexing information in a single neuron’s code
Brian Lundstrom and Adrienne Fairhall
Physiology and Biophysics, University of Washington, Seattle, WA
The coding properties of a single neuron can be defined on a range of timescales. The variation in the
instantaneous probability of spike generation due to stimulus fluctuations is often well described in terms
of a generalized linear/nonlinear (LN) model [1, 2]. In such a model, the form of the linear filters and the
nonlinearity provides a functional characterization of the system’s computation. On slower timescales,
coding is described in terms of mean firing rate with respect to stimulus parameters that vary on correspondingly slow timescales, such as the local mean or variance. The relative sensitivity of the firing rate
to either the mean or the variance can be used to classify the neuron, viewed as a rate coder, as integratorlike or coincidence-detector-like [3]. Our aim in this work is to study the biophysical basis of encoding of
the mean and the variance by the firing rate, and to compare that with the coding of rapid fluctuations.
To focus on the input/output properties of the spike generating mechanism, we consider analytically
single-compartment model neurons driven by a filtered white noise input characterized by a given mean
and standard deviation. For many single-compartment neuron models, once the neuron begins sustained
firing to noiseless DC current (when the dynamical system loses stability for currents above rheobase), it
displays little sensitivity to input standard deviation. This is in contrast to some in vitro neurons [4-6] that
displayed a marked sensitivity to input standard deviation. Using reduced single-compartment models,
we show that this increased sensitivity to noise depends on the separation of timescales between
activation and inactivation as well as on the relation between subthreshold dynamics and the spiking
threshold (captured by the shape of the V-nullcline). We show an example for which sensitivity to noise is
absent above rheobase when the ratio of inactivation to activation time constants is small but noise
sensitivity is pronounced when the ratio is large.
Along with previous work [3, 7], this suggests that in terms of rate coding, a neuron may operate in three
computational regimes: it may be sensitive to DC current but not noise (above rheobase), it may be
sensitive to DC current and noise, or it may be insensitive to DC current but sensitive to noise. These
regimes constrain the form of the linear filters that describe neural computation using LN models [3, 7].
Further, in the absence of slow adapting timescales, the rating coding behavior may help to decode
instantaneous LN encoding that varies with stimulus statistics. Slow forms of adaptation may allow a
neuron to function in multiple rate coding regimes as determined by the slow adaptation variables.
However, even as firing rate adapts over seconds, e.g. after a sudden stimulus change, we have previously
shown that some stimulus parameters such as input variance may be rapidly decoded via a distributional
code of interspike intervals [8]. Thus, our results demonstrate the coexistence of coding mechanisms at
multiple timescales and some of the relationships between these coding strategies.
References
[1] Huys, Q.J., M.B. Ahrens, and L. Paninski, J Neurophysiol, 2006. 96(2): p. 872-90.
[2] Hong, S., B. Aguera y Arcas, and A. Fairhall, Neural Comput, In press.
[3] Lundstrom, B.N., et al., Neural Comput, In press.
[4] Arsiero, M., et al.,. J Neurosci, 2007. 27(12): p. 3274-84.
[5] Fellous, J.M., et al., Neuroscience, 2003. 122(3): p. 811-29.
[6] Higgs, M.H., S.J. Slee, and W.J. Spain, J Neurosci, 2006. 26(34): p. 8787-99.
[7] Hong, S., B.N. Lundstrom, and A.L. Fairhall, Submitted.
[8] Lundstrom, B.N. and A.L. Fairhall, J Neurosci, 2006. 26(35): p. 9030-7.

47

Thursday evening, Poster session I-40

Cosyne 2008

Spectral characteristics of mass activity in balanced inhibition
networks
Ingo Fründ1,2,3 , Frank W. Ohl2,3 , and Christoph S. Herrmann1,3
1

Otto-von-Guericke University, Magdeburg, Germany,
Leibniz Institute for Neurobiology, Magdeburg, Germany,
3
Bernstein Group for Computational Neuroscience, Magdeburg, Germany
2

In order to simulate the electroencephalogram (EEG) of human subjects in cognitive experiments, we apply
a four-step process. In a ﬁrst step, we design a network which is the topic of the current paper. In a second
step, we generate hypotheses about the EEG responses of humand subjects by computing the simulated
LFPs of the network. These hypotheses will be tested in a third step and reﬁnements of the model represent a
fourth step in case results did not match hypotheses. Neural networks with a balance between excitation and
inhibition [1] have been shown to display multiple useful computational properties [1, 2, 3], like contrast-gain
normalization or stabilization of response times. Here, we study the spectral properties of mass activity in a
large scale network of 10,000 leaky integrate-and-ﬁre neurons (similar to an example network in [4]). 20%
of the model neurons were inhibitory. Each neuron received synaptic input from 2% (200) of the neurons
in the network. The input neurons were selected from a neighborhood of the target neuron. The probability
to select a presynaptic neuron decreased with distance to the target neuron accoring to a gaussian proﬁle.
Single neurons displayed irregular activity and the overall activity of the networks appeared asynchronous
in rasterplots. The average current in the whole network ﬂuctuated with a characteristic spectral signature.
The power spectrum of this average current showed a 1/f characteristic. It was ﬂat at low frequencies
and had an approximate f −3 tail at high frequencies. A clear spectral peak was present, which shifted
from ≈ 20 Hz to ≈ 50 Hz as input to the network increased. The frequency of the spectral peak was
also dependent on the strength of excitatory and inhibitory synapses. In general, the frequency of the peak
increased if excitation became stronger of if inhibition became weaker. If inhibitory synapses were modeled
as changes of membrane conductance, their impact on the spectral peak was less pronounced than for current
based synapses. In a network in which excitatory synapses were modeled as current injections (e.g. from
dendrites) into the soma and inhibitory synapses were modeled as conductance changes, the relation between
inhibition and peak frequency was weakest. We conclude that EEG oscillations in the β/γ frequency range
(> 20Hz) might be related to activity in networks with a balance between excitation and inhibition.
Acknowledgments This work was supported by the Bernstein Project of the German Federal Ministry of
Education and Research (BMBF).
References
[1] Chaos in Neuronal Networks with Balanced Excitatory and Inhibitory Activity. C. VanVreeswijk and H.
Sompolinski, Science 274(5293):1724-1726, 1996.
[2] Invariant computations in local cortical networks with balanced excitation and inhibition. J. Marino, J.
Schummers, D. C. Lyon, L. Schwabe, O. Beck, P. Wiesing, K. Obermayer, and M. Sur, Nat Neurosci 8(2):
194-201, 2005.
[3] Balanced inhibition underlies tuning and sharpens spike timing in auditory cortex. M. Wehr and A. M.
Zador, Nature 426: 442-446, 2003.
[4] Neural network dynamics. T. P. Vogels, K. Rajan, and L. F. Abbott, Annu Rev Neurosci 28: 357-376,
2005.

48

Cosyne 2008

Thursday evening, Poster session I-41

Short-term modification of synaptic efficacies of the medial
prefrontal cortex in working memory task
Shigeyoshi Fujisawa, Asohan Amarasingham, and György Buzsáki
Center for Molecular & Behavioral Neuroscience, Rutgers University
Computations in neocortical local circuits are likely to be implemented by the formation and segregation
of cell assemblies. Yet, how neurons interact with each other flexibly through synaptic connections is not
well understood. Here we examined the firing patterns and the temporal relationships of mPFC neurons at
time scales of milliseconds (monosynaptic) and seconds in an odor-place matching working memory task,
using two-dimensional 64-site silicon probes which allow us to recorded ~ 100 units simultaneously. A
large percentage of neurons fired selectively in various regions of the apparatus with similar “life times”
and differentiated future left and right choices of the animal, that is, they formed goal-dependent
“assembly sequences.” Physiological characterization of the units allowed us to classify them as putative
principal cells and interneurons. Sizable fractions of both pyramidal cells and interneurons differentiated
between right and left trajectories in the maze.
To gain insight into neuronal interactions that might contribute to assembly sequences in the maze, we
took advantage of the large numbers of simultaneously recorded cells to physiologically identify recorded
neurons as excitatory or inhibitory by their short-latency temporal interactions with other neurons, and to
examine the functional connectivity among them. Importantly, monosynaptic interactions between these
populations varied dynamically in a position and goal dependent manner, which is likely independent of
the firing rates of the neurons. Spike transmission efficacies often depended on the spiking history of the
presynaptic neurons, indicating that short-term synaptic plasticity (synaptic facilitation / depression)
might contribute to the dynamic modification of functional connectivity. Furthermore, coincident firing of
two or more neurons nonlinearly increased the efficacies of spike transmission. These findings are
consistent with the hypothesis that neurons participate in transient coalitions that evolve over time,
supported by short-term plasticity of synaptic strengths between active neurons.

Acknowledgments
This work was supported by NIH (NS34994, MH54671), and the Japan Society for the Promotion of
Science.

49

Thursday evening, Poster session I-42

Cosyne 2008

The Impact of Hyperemia on Neurons and Glia
Ulf Knoblich1,2, Rosa Cao1,2, Bryan T. Higashikubo1,2, Jessica A. Cardin1,2,
Raddy L. Ramos3, Joshua C. Brumberg3, Christopher I. Moore1,2
1

McGovern Institute for Brain Research, Cambridge, MA,
MA, 3Queens College, CUNY, Flushing, NY

2

MIT, Cambridge,

Functional hyperemia, the increase in blood flow and volume spatiotemporally correlated with neural
activity, is typically considered to have metabolic or physiologic functions unrelated to information
processing. We are currently testing the hemo-neural hypothesis, the prediction that functional hyperemia
impacts neural representation. As part of this effort, we performed whole-cell in vivo and in vitro
recording of neurons and glia in rat barrel cortex. To generate hyperemia independent of prior neural
activity, an agonist that selectively targets and relaxes smooth muscle was applied transiently to the
cortical surface.
Our initial data suggest that functional hyperemia modulates membrane potential in cortical neurons and
glia. Following drug application, discrete onset depolarization events were observed in a subset of both
cell types (6/9 putative pyramidal neurons, 4/5 glia). These effects had at least two distinct time courses: a
fast effect (onset latency 1-10s) of moderate amplitude (<10mV) lasting about 10s, and a longer latency
effect (onset 10-50s) of higher amplitude (>10mV) lasting up to 100s or more. In addition, neurons
showed an increase in input resistance (avg. ~5%) following drug application.
During sensory stimulation of the primary vibrissa, induction of functional hyperemia leads to a decrease
in the total evoked conductance combined with an increase in reversal potential of the post-synaptic
potential. These findings are consistent with a reduced inhibitory conductance evoked by vibrissa
deflection, a hypothesis that was supported by estimating the excitatory and inhibitory components of the
post-synaptic potential.
These findings (depolarization, reduced inhibitory conductance and increased input resistance) support a
model in which functional hyperemia reduces the activity of a class of interneurons that provide tonic and
evoked inhibitory input to the recorded pyramidal cells. This prediction is further supported by the
anatomical interconnectivity of interneurons and vasculature in the neocortex.
The in vivo measurements were complimented by in vitro measures that showed no impact of the drug on
neural membrane potential (<1 mV standard deviation), except in instances where neurons were proximal
to vessels in the slice. Targeting neurons within 100 microns of an arteriole showed that ~30% showed
depolarization following vasomotion induced by drug application, consistent in time course with our in
vivo findings.
These data provide initial, preliminary support for the hypothesis that blood flow modulates neural
activity and, in turn, information processing. Further, in vitro data support the prediction that mechanical
interactions between vascular and neural/glial elements are one operative mechanism.
Acknowledgments
This work was supported by the McGovern Institute for Brain Research, the Schoemaker Fellowship, the
Mitsui Foundation and Tom F. Peterson.

50

Cosyne 2008

Thursday evening, Poster session I-43

Characterizing Neural Arbor Morphology Using Density
Distribution Functions
Corinne Teeter1,2 & Charles Stevens2
1

Salk Institute for Biological Studies, 2University of California, San Diego

Understanding the principles neural arbors use to distribute and
sample information is critical to understanding how neural
circuits are constructed and how they operate. Each axonal arbor
distributes information over some particular region of the brain
called its “territory” and each dendritic arbor samples information
from its particular territory.
The way each different arbor distributes/samples information is
dependent upon whether or not a piece of neurite resides at a
particular location in space so that it can make a synapse at that
location. The distribution of the arbor mass in space can be
described in a general way by a density distribution function
which defines the probability of finding a piece if neurite at a
particular location in an arbor’s territory (Figure 1). The density
distribution function for each neural arbor can be characterized by
the total length of the arbor (total length of all segments added
together) and the variance of the mass along the principle axes.

Figure 1. Arbor Mass
Distribution
Function.
Fish retinal ganglion cell
axon reconstructed with the
mass distribution function
plotted above. The height
of function is proportion to
the probability of finding a
piece of arbor at each point
in space. The amplitude of
the function is related to
the total length of the
arbor.

Fish retinal ganglion cell arbors (both axons and dendrites) were
filled with DiI, visualized under a confocal microscope and
digitally reconstructed by the Neurolucida computer program.
When the logarithm of the above parameters (total length of arbor
and variance along each principle axes) of each ganglion arbor are plotted together, it is revealed
that each arbor lies very close to a linear, two-dimensional manifold in the three-dimensional
space. The manifold demonstrates that there are special relationships between these three
parameters, suggesting that there are universal laws of arbor formation and information transfer
that constrain the parameters of the arbors within this space.
Both axons and dendrites lie on the same manifold. However, axons and dendrites can be
described by two separate distributions on the manifold. This demonstrates that although there is
a universal law constraining the relationships between these parameters, different types of
neurons can be differentiated by these parameters. This same type of analysis can be applied to
the neurons of cortex and hippocampus of all animals. The analysis will provide a basis to
describe the structure of arbors and possibly differentiate different types of arbors previously
identified or unidentified.
Acknowledgments
We thank Joseph Snyder for helpful discussions. This work was supported by Howard Hughes
Medical Institute.

51

Thursday evening, Poster session I-44

Cosyne 2008

Effective connectivity in a network of spiking cortical neurons
Aonan Tang1, Jon Hobbs1, Wei Chen1, Christopher J. Honey2, Dumitru
Petrusca3, Matthew I. Grivich3, Alexander Sher3, Alan M. Litke3, Olaf Sporns2,
John M. Beggs1
1

Indiana University Department of Physics, 2Indiana University Department of
Psychological and Brain Sciences, 3University of California, Santa Cruz Institute for
Particle Physics
The average cortical neuron makes and receives about 1,000 – 10,000 synaptic contacts. This anatomical
information suggests that local cortical networks are connected in a fairly democratic manner, with all nodes
having about the same degree. But the physical connections found in the brain do not necessarily reveal how
information flows through the network. We used transfer entropy [1] to assess effective connectivity in
cortical slice cultures placed on a 512 electrode array system (in collaboration with Alan Litke of UC Santa
Cruz). Data were binned at ~1 ms and cultures (n = 6) were active for periods exceeding 1 hr, which allowed
for accurate entropy estimation. Analysis revealed wide differences in node degrees, and pointed to the
existence of cells with high effective connectivity that acted as highly central hub nodes in the network.
Acknowledgments
This work was supported by NSF grant number 0343636 to JMB, by NSF grant number PHY-0417175 to

AML, by the McKnight Foundation to AML, by the Burroughs Wellcome Fund Career Award at the
Scientific Interface to AS, by McCormick Science Grant from Indiana University to AT.
References
[1] Measuring Information Transfer. T. Schreiber, Phys. Rev. Lett. 83: 461-464, 2000.

52

Cosyne 2008

Thursday evening, Poster session I-45

Proportional excitatory and inhibitory conductances are maintained
during gamma oscillations
Bassam V. Atallah1, 2, Massimo Scanziani1
1

Division of Biology, University of California San Diego 2Computational
Neurobiology Graduate Program
Gamma oscillations, the synchronous spiking of neurons at 30-80 Hz, occur in many cortical areas and
are thought to play a role in sensory processing. Several studies focusing on the mechanisms underlying
these oscillations demonstrate that both inhibitory and excitatory neurons in cortical networks participate
in these rhythms. Experiments also indicate that the amplitude of gamma oscillation cycles is modulated
in a behaviorally relevant manner. Here we investigate how inhibition and excitation varies during the
modulation of gamma oscillation amplitude.
Recordings were performed both in vivo and in acute hippocampal slices from 2-5 week-old rats. Gamma
oscillations were induced pharmacologically in vitro (50-250 nM kainate) and monitored using an
extracellular electrode placed in the stratum radiatum of area CA3. To directly compare the amplitude of
inhibition and excitation on a cycle by cycle basis, we recorded inhibitory postsynaptic currents (IPSCs)
in one CA3 pyramidal cell, voltage-clamped at the reversal potential for excitation, while simultaneously
recording excitatory postsynaptic currents (EPSCs) in a second neighboring pyramidal cell, voltageclamped at the IPSC reversal potential. Both excitation and inhibition occurred rhythmically with a period
of approximately 30ms. During each oscillation cycle, IPSCs occurred after EPSCs with an average
latency of 1.5ms, consistent with pyramidal cells exciting interneurons during each oscillation cycle.
Furthermore, we found that the peak amplitude of both inhibitory and excitatory conductances changes
from cycle to cycle yet their ratio remained constant. In fact, the inhibitory conductance was proportional
to the excitatory conductance within each cycle for conductances ranging over an order of magnitude (0.2
to 8 nS and 1 to 20 nS for excitation and inhibition respectively). Our data suggest that area CA3
maintains proportionality between excitation and inhibition during gamma oscillations.
Acknowledgments
We thank JS Isaacson and C Poo for their help with in vivo recording. This work was supported by the
NIH (MH71401 and MH70058).

53

Thursday evening, Poster session I-46

Cosyne 2008

Structure and Robustness of 2nd Order Maximum Entropy Models
for Large Neural Populations
Elad Ganmor1, Ronen Segev2, and Elad Schneidman1
1

Dept. of Neurobiology Weizmann Institute of Science, Rehovot, Israel,
Life Sciences Ben-Gurion University of the Negev, Beer-Sheva, Israel

2

Dept. of

One of the key difficulties in describing and analyzing the simultaneous spiking activity of large groups
of neurons is that the number of possible activity patterns of the group grows exponentially with the size
of the population. Recently it was shown that the minimal model that relies only on the observed pairwise
correlations between cells, but assumes no higher-order interactions, gives a surprisingly accurate
description of network state distribution in the vertebrate retina responding to naturalistic stimuli, and
cultured cortical networks [1]. Similar results have been reported for the primate retina and in vivo and in
vitro cortical networks. Notably these maximum entropy models [2,3] are defined by only a quadratic
number of parameters.
Using Monte-Carlo methods we estimated the 2nd order maximum entropy models for large networks of
up to 100 neurons from the salamander retina responding to natural movies, and found them to give a
successful quantitative description of these networks. We further studied several different simplified 2nd
order models and approximations and found that a proper choice of pairwise interactions allows for an
accurate description using a number of parameters much smaller than the quadratic number required by
the full model. For large networks we also found signatures of the contribution of higher-order
correlations to the behavior of the network. In order to better understand the functional architecture of
large populations we applied graph theoretical tools to explore the detailed structure of the networks [4],
and found several local sub-network structures that are over represented in the studied networks. This
may also help in further reducing the complexity of accurate models for these networks.
We also explored the effects of sampling on the success of the pairwise models. Although coincident
firing is rare relative to the firing rate of single neurons, we show that learning the pairwise model from
samples converges at a rate similar to that of the independent model. The pairwise model proved to be the
best model for windows shorter than 20 seconds. This allowed us to discriminate between different visual
stimuli based on single trial analysis.
Acknowledgments
This work was supported by the Center for Complexity Science (RS & ES), the Clore Center for
Biological Physics (ES), the Zlotowski Center for Neuroscience and The Rich Foundation (RS)
References
[1] Weak Pairwise Correlations Imply Strongly Correlated Network States in a Neural Population. E.
Schneidman, MJ. Berry 2nd, R. Segev and W. Bialek, Nature 440(7087):1007-12, Apr 2006.
[2] Information Theory and Statistical Mechanics. ET. Jaynes, Phys. Rev. 106(4):620-30, May 1957.
[3] Network Information and Connected Correlations. E. Schneidman, S. Still, MJ. Berry 2nd and W.
Bialek, Phys. Rev. Lett. 91(23):238701, Dec 2003.
[4] Network Motifs: Simple Building Blocks of Complex Networks. R. Milo, S. Shen-Orr, S. Itzkovitz,
N. Kashtan, D. Chklovskii and Alon U, Science 298(5594):824-7, Oct 2002

54

Cosyne 2008

Thursday evening, Poster session I-47

Microstimulation of visual cortex measured with two-photon
imaging: short- and long-range effects
Mark H Histed1 and R.Clay Reid1
1

Harvard Medical School, 220 Longwood Ave., Boston MA 02115
mark_histed@hms.harvard.edu, clay_reid@hms.harvard.edu
The ability to activate neural tissue via electrical stimulation has led to progress both in understanding the
brain and in treatments of neurological disorders since it was first used over a century ago. On the basic
science side, it has been widely used in vivo to study the causal connection between neural activity and
behavior. Clinical applications include treating neurological dysfunction (e.g. Parkinson’s), speeding
rehabilitation from stroke, and restoring sensation to the deaf (in cochlear implants) and blind (in
experimental visual prostheses). Despite this long history, the effect electrical stimulation has on neurons
in the living brain is still almost unknown. We have used the new technique of in vivo 2-photon calcium
imaging to study this effect.
We imaged the activity of neurons in the visual cortex of mice (N=12), rats (N=2), and cats (N=8) while
stimulating with standard microstimulation electrodes using trains of pulses at 250 Hz, from 100-1000s in
duration and current intensities from 1-100 μA. Cells were loaded with the calcium sensitive dye Oregon
Green BAPTA-1 AM, and a custom 2-photon scanning laser microscope was used for imaging.
At low currents there is a threshold for activation, typically near 10 μA. Near threshold, cells are
activated around the electrode tip. However, the pattern of activated cells is sparse, with only tens of
neurons active near the tip, but also occasional cells activated even several hundred microns away.
Higher currents activate larger numbers of cells, and at approximately 3 times the threshold (30-50 μA),
nearly all cells within a 3-600μm radius are active. Most neurons are activated due to direct
depolarization from the injected current – when excitatory synaptic transmission is blocked
pharmacologically, the response is largely unchanged, as expected from the strong depression of cortical
synapses at this frequency. Finally, in the cat visual cortex, we measured the effect of stimulation at
>1mm from the tip. Here, neurons are arranged into a map with columns of similar orientation tuning.
Stimulation of one column at 30-50μA activates similarly oriented cells further than 1mm away, without
activation of the intervening neurons of different direction preference.
These results explain why animals can detect currents down to nearly 10 μA, and demonstrate that higher
currents can result in long-range, but specific, interactions across the cortex.

Acknowledgments
We thank K. Ohki, V. Bonin, M. Andermann and J. H. Maunsell for helpful discussions. This work was
supported by the NEI.

55

Thursday evening, Poster session I-48

Cosyne 2008

Neural Network Model of Motor Cortex
Ben Dongsung Huh1, Emanuel Todorov1
1

University of California, San Diego

The role of primary motor cortex (M1) in arm movement control has been the subject of lasting and often
heated debates. Early views of low-level muscle control have been discredited by numerous correlations
with higher-level parameters related to hand kinematics. The abstract directional coding hypothesis
advanced to explain these observations has also been discredited, this time by numerous correlations with
lower-level muscle- or joint-related parameters. These seemingly contradicting evidences make it hard to
interpret M1 activity. Todorov [1] showed such contradiction can be resolved by ‘muscle-based-control’
model. In our current work, we demonstrate that neural-network model of M1 based on ‘muscle-basedcontrol’ indeed explain all the experimental findings.
We trained recurrent neural networks that directly control a simulated arm model in closed-loop. The
networks receive low-level sensory input from muscles and target information, and send control signals to
muscles. The network is trained to optimize behavioral cost that generates human-like reaching
movement. The training uses modified backpropagation-through-time algorithm to pass the cost-gradient
through nonlinear-dynamical system (arm model).
We compare the network’s population activity to various movement features (target direction, hand
velocity, external load). Preferred direction of neurons found this way matches well to M1 electrophysiology data in literature. The M1 network model, trained to control low-level muscle activation,
shows correlation between neural activity to high-level movement features, consistent to the theoretical
prediction of Todorov [1].
The network model also allows more fundamental understanding of movement generation by M1. We
perform simulation experiment by perturbing of network activity during movement, analyze the perturbed
arm movement, and suggest experimental prediction.
Acknowledgments
We thank Steve Scott for helpful discussions.
References
[1] On the role of primary motor cortex in arm movement control
Todorov E, In Progress in Motor Control III, Latash & Levin (eds), Human Kinetics (2003)

56

Cosyne 2008

Thursday evening, Poster session I-49

Movement selection and initiation in the rat superior colliculus
Gidon Felsen, Robert H. Klein, and Zachary F. Mainen
Cold Spring Harbor Laboratory
We are interested in how the nervous system uses sensory stimuli to select and initiate appropriate motor
actions. To this end, we have been studying how rats use olfactory cues to guide choices about, and to
initiate, directional movements. We hypothesized that the superior colliculus (SC) could play an
important role in these processes, since it is known to produce directional orienting movements, via
descending projections to several motor nuclei. In rats, it is not known how activity in the SC underlies
movement initiation, and whether the SC is also involved in selecting among potential movements.
In order to address these questions, we used tetrodes to record from intermediate and deep layer SC
neurons in well-trained rats performing a two-alternative choice olfactory discrimination task. In each
trial of the task, the rat first entered a centrally located odor port, triggering the delivery of an odor, and
then moved to either the left or right reward port to obtain a water reward. The odor was either a pure
compound or a mixture of two compounds. One pure odor was rewarded following a left choice and the
other following a right choice, and mixtures were rewarded according to their dominant component.
We analyzed the activity during the pre-movement epoch, defined as the 100 ms preceding the exit from
the odor port. The activity of many neurons during this epoch depended on whether the upcoming
movement was leftward or rightward, independent of whether the choice was correct or incorrect. Across
the population, higher firing rates preceded movement contralateral to the recording site (i.e., left SC
neurons preferred rightward movement). In some cells, this direction selectivity disappeared at the onset
of movement, while in other cells it persisted during the movement.
These data suggest that prospective direction selectivity in the SC is involved in the initiation of
directionally orienting movements. In order to determine whether the observed selectivity causes, or is
simply correlated with, movement initiation, we used the GABAA agonist muscimol to unilaterally
inactivate the SC in rats performing the behavioral task. If SC output is necessary for initiating
contralateral movements, we would expect inactivation to bias the rat towards ipsilateral movements.
Indeed, we found that muscimol, but not saline, delivered to the left SC biased the rat towards leftward
choices, and this bias was dosage-dependent. Thus, normal SC activity is necessary for initiating
appropriate directional movements.
We next asked whether the SC is also involved in the selection of movements, in addition to their
initiation. We reasoned that if the SC is only involved in initiating movements, its activity on trials in
which the same direction was selected should be independent of the stimulus presented. We found that
this is not the case: The strength of predictive left/right selectivity of several neurons exhibited a clear
dependence on the mixture ratio of the odor stimulus.
Together, these results suggest that, in rats, the SC is necessary for the initiation of appropriate orienting
movements and may be involved in their selection as well. Future studies will attempt to determine how
the SC interacts with other brain regions to make and act upon decisions based on sensory stimuli.
Acknowledgments
This work was supported by the Swartz Center for Computational Neuroscience.

57

Thursday evening, Poster session I-50

Cosyne 2008

Target selection for visually-guided reaching in the dorsal premotor
area during a visual search task
Joo-Hyun Song and Robert M. McPeek
The Smith-Kettlewell Eye Research Institute
Most visual scenes are complex and crowded, with several different objects competing for attention and
directed action. Thus, an understanding of the production of goal-directed actions must incorporate the
higher-level processes involved in the selection of a target stimulus from distractors.
To examine the neural substrates of target selection for visually-guided reaches, we recorded the activity
of isolated neurons in the dorsal premotor area (PMd) of rhesus monkeys. The role of the PMd when a
target must be selected from distractors is not yet fully understood. However, recent studies have
demonstrated that when two potential targets are presented for selective reaching, the PMd can
simultaneously encode the two competing movement goals during a delay period before the cue to move
[1].
Here, we investigated the role of the PMd in a reaction-time visual search task. We trained monkeys to
reach to an odd-colored target presented with three distractors. We traced the time course of
target/distractor discrimination and found that PMd neurons typically discriminated the target before
movement onset, about 150~200ms after the appearance of the search array. Discrimination in a subset of
neurons occurred at a consistent time after search array onset regardless of the latency of the reaching
movement, suggesting that these neurons are involved in target selection, as distinct from movement
production. In other neurons discrimination time depended on reach latency, suggesting that this latter
group of neurons are more involved in movement execution. These results suggest that different groups of
PMd neurons are involved in target selection and movement initiation.

Acknowledgments
We thank N. Takahashi for animal care. This work was supported by National Eye Institute grant
EY014885 to R.M. McPeek and by a R.C. Atkinson Fellowship Award to J.H. Song.
References
[1] Simultaneous encoding of multiple potential reach directions in dorsal premotor cortex. P. Cisek and
J. F. Kalaska, Journal of Neurophysiology 87:1149-1154, Feb 2002.

58

Cosyne 2008

Thursday evening, Poster session I-51

Movement Effects on Proprioception
Jeremy D. Wong1, Elizabeth T. Wilson1, and Paul L. Gribble1
1

University of Western Ontario

Recently it has been shown that the accuracy of human visual perception can be affected by movement
experience [1]. Adaptation of arm movements to force field perturbations applied to the hand altered the
visual perception of object motion. This shows that our perceptual systems may be tuned in systematic
ways based on movement training. The sense of body position, or ‘proprioception’, might undergo
similar adaptive changes following movement. Movement training may increase proprioceptive
sensitivity in parts of the body relevant to the current motor task and, in turn, facilitate further
improvements to motor performance. To our knowledge, psychophysical measurements of proprioceptive
sensitivity as a function of movement training have not been investigated.
Here, we investigated proprioceptive sensitivity using a purely proprioceptive test that avoided motor
responses, as well as any visual components of stimulus presentation or subject response. The right arm
of subjects was passively moved by a robotic manipulandum (InMotion2 Technologies, Massachusetts
USA) in a horizontal workspace. Subjects were presented with a proprioceptive reference position,
immediately followed by a proprioceptive judgment location. Subjects were required to make a 2alternative forced choice judgment about the location of their hand, relative to the reference location
(left/right, or near/far). Throughout the paradigm, vision was occluded, all movements of the limb were
passive and the velocities of all movements were randomized to eliminate speed or timing cues.
Proprioception was tested at the beginning and end of each experimental session. To test whether
perception might be enhanced following movement training, subjects were tested on their proprioceptive
sensitivity both before and after performing a movement training task involving repeated point-to-point
movements to visual targets. Preliminary results show that perceptual sensitivity following movement
training is greater relative to control subjects, and suggest a role for improved proprioceptive acuity in
movement performance.

Acknowledgments
We thank D Ostry and A Mattar for helpful discussion and comments.
References
[1] Motor force field learning influences visual processing of target motion. Brown LE, Wilson ET,
Goodale MA, Gribble PL. Journal of Neuroscience 27(37):9975-83, Sep 2007.

59

Thursday evening, Poster session I-52

Cosyne 2008

Dependence of dendritic plateau potential duration and amplitude
on form and location of synaptic input
Mark S. Goldman1,2, Keri Tochiki2, Charlene Schnobrich2, Susan Tse2, David
Tank3, and Guy Major3,4
1

University of California, Davis, 2Wellesley College, 3Princeton University, 4Cardiff University.

Neural activity that persists for multiple seconds following the removal of a triggering stimulus has been
identified as a neural correlate of short-term memory in a wide range of brain regions. Several
computational models have proposed that plateau potentials localized to dendritic subunits of a neuron are
a key mechanism underlying the maintenance of persistent neural activity. However, in several brain
areas exhibiting persistent firing, in vitro experiments testing for dendritic plateau potentials lasting many
seconds have failed to reveal their presence. Here, we explore the plateau generating capability of
dendrites by means of models and experiments, in particular how this depends on the intrinsic and
synaptic ionic conductances in the dendrite and on the anatomical location of the plateau-triggering input.
We first constructed one-compartment models of a single dendritic subunit. As in traditional models of
plateau potentials, depolarization-induced activation of inward currents such as Ca2+, NMDAR (NMDAreceptor), or persistent Na+ could lead to plateau potentials. However, other conductances that can exhibit
a negative-slope region of their I-V curve could also contribute, for example outward currents inactivated
by depolarization. Many different combinations of ionic conductances lead to similar plateau-potential
behavior in the single-compartment model, suggesting that ionic conductances can act as “basis
functions” that can be linearly combined to form the steady-state response of a neuronal compartment.
In cortical brain slice experiments, plateau potentials lasting a few hundred milliseconds can be evoked by
stimulating single terminal dendrites with brief (5 ms) focal glutamate pulses. However it is difficult to
evoke plateau potentials lasting many seconds using such stimuli. Our results suggest that this may not
reflect a fundamental limitation to plateau duration caused, for example, by inactivation of voltageactivated channels or deactivation of NMDARs. Rather, we suggest that dendritic subunits can exhibit
conditional bistability, for example, they can become bistable in the presence of tonic excitatory input
such as background synaptic or neuromodulatory activity. This allows them to produce multi-second
plateaus in response to transient inputs. In experiments, by adding a constant local background level of
glutamate using iontophoresis, we show that a multi-second plateau potential can be triggered in a layer 5
neocortical pyramidal neuron basal dendrite by a 5 ms pulse of glutamate.
We next explored how the somatic response to a dendritic plateau potential depends upon the location of
the input along a single dendritic branch. For NMDAR-mediated plateau potentials, multiple discrete
plateau potentials could be maintained along the dendrite, with activation of more proximal dendritic sites
leading to larger somatic voltage responses. In response to a sequence of inputs, the model neuron could
produce multiple levels of sustained activity corresponding to the turning on of different plateau potential
sites. Altogether, this work suggests a neuronal mechanism by which multi-level persistent neural
activity could be generated in the absence of recurrent synaptic feedback.
Acknowledgments
This work was supported by NIH grants MH069726 and MH068030, Royal Society and Wellcome Trust
083850, Princeton and Cardiff Universities, and a Wellesley College Brachman Hoffman Fellowship.

60

Cosyne 2008

Thursday evening, Poster session I-53

Binary synapses: better than expected
Amit Miller1,2 and Peter Latham2
1
2

Gatsby Computational Neuroscience Unit, UCL, UK
Interdisciplinary Center for Neural Computation, Hebrew University, Israel

A longstanding hypothesis in neuroscience is that learning involves changes in synaptic efﬁcacies. The ﬂip
side of this hypothesis is that learning one thing necessarily involves forgetting something else. The problem
of forgetting is especially severe if synaptic efﬁcacies take on a discrete set of values, something that has
been suggested for theoretical reasons (e.g. [1]) and for which there is some experimental evidence [2]. For
such discrete synapses, memories tend to fade exponentially fast, making it difﬁcult to store information for
long times.
The Cascade Model was suggested as a solution to the problem of fast forgetting [3]. In this model, synaptic
efﬁcacies still take on a discrete set of values, but each efﬁcacy has associated with it many internal states.
These internal states give the synapses multiple timescales, which can combine to produce power law, rather
than exponential, decay of memories. Because power law decay is much slower than exponential, the
Cascade Model has the potential to exhibit long memory lifetimes.
Here we show, somewhat counterintuitively, that these internal states – at least as implemented in the Cascade Model – do not exhibit memory lifetimes that are much longer than those of simple binary synapses.
(We use ”simple” to mean no internal states). Moreover, for realistic coding levels (the coding level is the
fraction of synapses modiﬁed during the formation of any one memory), simple binary synapses actually
outperform the Cascade Model. Importantly, not only do they outperform it, they do well on an absolute
scale: if the coding level scales as N −1/3 where N is the number of synapses in the network, then an upper
bound on the forgetting time scales as N 2/3 . With this scaling, a cubic centimeter of cortex using simple
binary synapses could store memories for about a year.
Our analysis suggests that there is no reason to use Cascade-like synapses for the sake of long memories.
However, it is still possible, and even likely, that the range of timescales associated with the Cascade Model,
and consequent power law forgetting, may play an important role in other aspects of memory formation,
interaction and decay.
Acknowledgments
This work was supported by the Gatsby Charitable Foundation and the Center for Neural Computation.
References
[1] Learning in neural networks with material synapses. Amit, D.J. and Fusi, S. Neural computation 6:957982, 1994.
[2] Graded bidirectional synaptic plasticity is composed of switch-like unitary events. O’Connor, D.H.,
Wittenberg, G.M., and Wang, S.S.-H. PNAS 102:9679-9684, 2005.
[3] Cascade models of synaptically stored memories. Fusi, S., Drew, P.J. and Abbott, L.F. Neuron 45:599611, 2005.

61

Thursday evening, Poster session I-54

Cosyne 2008

A learning mechanism for song selectivity in songbird nucleus HVC
Linli Wang, Dezhe Z. Jin
Pennsylvania State University,
Auditory response of HVC neurons can be highly selective and is often tuned to a specific syllable or
even a specific sequence of syllables. NIF, the primary source of auditory input to HVC, is not selective
in this way. In a recent model, we proposed that the auditory selectivity of HVC arises due to both the
connections from NIF neurons to HVC neurons and the internal synfire chain connectivity of HVC. The
drive from NIF to HVC in response to specific acoustic events works in conjunction with this internal
connectivity to cause HVC neurons to respond only to a NIF spike sequence driven by a particular
syllable or syllable sequence. Here we present a computational model showing how the specific wiring
from NIF to HVC can arise through learning. In the initial state of our model, synfire chain connectivity is
present within HVC, and each NIF neuron makes a weak connection to all HVC neurons. Subsequently,
the synaptic strength of each NIF to HVC connection is modified via a spike-time dependent Hebbian
plasticity scheme. A heard syllable consistently activates each NIF neuron at a specific time in relation to
other NIF neurons driven by the same syllable. Due to the synfire chain connectivity, the spontaneous
activity of HVC neurons tends to have a specific sequence as well, which promotes consistency in the
timings of NIF and HVC neural spikes. Repeated presentation of the syllable strengthens the connections
of NIF and HVC neuron pairs with the right timing correlation. Axon remodeling, in which all
connections other than the one with the strongest synapse are pruned, stabilizes the correct NIF to HVC
connectivity. We suggest that spike-time dependent plasticity of the synapses from NIF to HVC neurons,
axon remodeling of these connections, as well as the spontaneous sequential firings of HVC neurons, are
critical for the emergence of the song selectivity in HVC.

Acknowledgments
This work was supported by A.P. Sloan Fellowship and the Huck Institute of Life Sciences, Penn

State University.

62

Cosyne 2008

Thursday evening, Poster session I-55

Crossmodal conditioning to dynamic auditory-visual contingencies
Hanneke E.M. den Ouden1, Jean Daunizeau1, Quentin Huys2, Karl J. Friston1,
Klaas E. Stephan1
1
2

Wellcome Trust Centre for Neuroimaging, University College London,
Center for Theoretical Neuroscience, Columbia University, NY

Using fMRI and dynamic causal modelling [1], we previously showed that the brain learns fixed
probabilistic associations between simple auditory and visual stimuli, even when these stimuli are
behaviourally irrelevant [2]. During learning, visual cortex activity and auditory-visual connectivity
changed in accordance with the learning curve predicted by a Rescorla-Wagner (RW) model of
associative learning. Here, we extend our previous study by (i) dynamically changing the statistical
relationship between auditory and visual stimuli and (ii) using visual stimuli with a well-defined cortical
representation (faces, houses). Subjects performed a speeded discrimination task on rapidly presented
visual stimuli; on each trial, the visual stimulus was preceded by an auditory stimulus.
Behavioural results: Analyses of the reaction time (RT) data showed that the subjects' response latencies
to the visual stimuli depended on the predictive value of the auditory stimuli (p<0.001), demonstrating
that subjects could successfully track the underlying relationships. To investigate the dynamics of the
subjects' online estimates of the cue-outcome association, we explored two basic learning models: (i) a
classical RW model whose learning rate was obtained by maximum likelihood estimation from the
measured RTs, and (ii) a simple ideal Bayesian observer hidden Markov model (HMM) representing the
five levels of associative probabilities. Using the trial-by-trial estimates of the cue-outcome associations,
as predicted by either of these learning models, the regression analyses explained a greater proportion of
the variance in the RT data than using the true [but unknown] associations.
fMRI results: Generally, learning rate should depend on how quickly the environment is changing, i.e.
its volatility. However, volatility-dependent changes in learning rates are not accommodated by the RW
model nor by the ideal observer HMM. Therefore, we also used a three-level hierarchical Bayesian
observer model (as in [3]) to predict the subjects' online estimates of volatility. Using these volatility
estimates as regressor in a general linear model of our fMRI data revealed a significant activation of
anterior cingulate cortex (ACC; p<0.05, whole-brain corrected). Given previous results on volatility
during reward learning [3], our findings suggest a general role of ACC in representing volatility,
independently of the type of learning. In ongoing analyses we will characterize how volatility modulates
the strength of effective connectivity amongst the brain regions activated by our paradigm.
Acknowledgments
This work was supported by the Wellcome Trust.
References
[1] Friston KJ, Harrison L, Penny W. 2003. Dynamic causal modelling. NeuroImage 19: 273-1302.
[2] den Ouden HEM, Daunizeau J, Roiser J, Friston KJ, Stephan KE. 2006. Crossmodal conditioning to
dynamic audio-visual contingencies. Human Brain Mapping, Florence, Italy.
[3] Behrens TE, Woolrich MW, Walton ME, Rushworth MFS. 2007. Learning the value of information in
an uncertain world. Nature Neuroscience: 10: 1214-21.

63

Thursday evening, Poster session I-56

Cosyne 2008

Phase Precession in Single Trials
Robert Schmidt1,2 , Kamran Diba3 , Christian Leibold4 , Dietmar Schmitz1,5 ,
György Buzsáki3 , Richard Kempter1,2,5
1

Bernstein Center for Computational Neuroscience Berlin, Germany
Institute for Theoretical Biology, Humboldt-Universität zu Berlin, Germany
3
Rutgers University, Newark, New Jersey, USA
4
Department of Biology II, University of Munich, Germany
5
Neuroscience Research Center, Charité, Berlin, Germany
2

During the crossing of a place ﬁeld, the ﬁring phase of the hippocampal place cell decreases with respect
to the theta rhythm. This phase precession is usually studied on the basis of trial averages, in which data
from many place ﬁeld traversals are pooled together. However, functional hypotheses on phase precession
[1,2,3], including temporal coding, sequence learning/recall, and navigation, rely on single trials. Therefore,
we study properties of phase precession in single trials. In particular, we examine in what respects singletrial phase precession differs from trial-averaged phase precession.
We ﬁnd that phase precession in single trials is a robust phenomenon. Single trials show higher negative
correlation coefﬁcients between spike phase and animal position than expected from trial averages. This can
be explained by spike phases being not only a function of the animal position in the place ﬁeld, or the time
passed since the animal entered the place ﬁeld, but also of preceeding spike phases in the same trial.
On average over many trials, spike phase correlates better with position than with time passed since the
animal entered the place ﬁeld [2]. The stronger correlation with spatial than temporal measures in phase precession may be due to trial (mis-)alignment. In line with this idea, we ﬁnd that, in single trials, spike phase
correlates with both time and space to the same degree. Thus, in trial averages, adjustment of phase precession to running speed can preserve the correlation of spike phases with place, but distorts the correlation of
spike phases with time [4].
It is often stated that phase precession spans up to 360 degrees [1, 5]. In contrast, we ﬁnd that in single trials
the most frequent phase range is around 180 degrees. Variable phase offsets between trials and phase jitter
of spikes can account for this difference in phase range between pooled and single trials.
Acknowledgments
This work was supported by BCCN Berlin (01GQ0410) and DFG grants (Schm 1381/1-2,3; Ke 788/1-2,3;
SFB618, TP3) to DS and RK.
References
[1] W. E. Skaggs, B. L. McNaughton, M. A. Wilson and C. A. Barnes, Hippocampus 6:149-72, 1996.
[2] J. Huxter, N. Burgess and J. O’Keefe, Nature 425:828-32, 2003.
[3] M. R. Mehta, A. K. Lee, and M. A. Wilson, Nature, 417: 741-6, 2002.
[4] C. Geisler, D. Robbe, M. Zugaro, A. Sirota and G. Buzsáki, Proc Natl Acad Sci 104:8149-54, 2007.
[5] J. O’Keefe and M. L. Recce, Hippocampus 3:317-30, 1993.

64

Cosyne 2008

Thursday evening, Poster session I-57

A model for reactive plasticity following cortical deafferentation
and focal stroke
Markus Butz & Florentin Wörgötter
BCCN, University of Göttingen, Germany
It is still under debate to what extend structural plasticity in terms of synaptic rewiring are the
cause for cortical remapping after a lesion or accompanied with motor skill-learning. Recent
two-photon laser imaging studies demonstrate that synaptic rewiring is persistent in the adult
brain and is dramatically increased following brain lesions or after a loss of sensory input
(deafferentation). To study the time course of synaptic rewiring following a lesion, we
propose a novel neural network model. We use a recurrent neural network model as a vehicle
to model structural plasticity. The algorithm for an activity-dependent network formation used
here is based on the neurite outgrowth algorithm by Van Ooyen et al. [3]. As an extension of
the former model, we represent discrete axonal and dendritic elements separately in order to
study the time and spatial course of synapse formation, pruning and synaptic turnover. Axonal
and dendritic elements merge in a random fashion to synapses. If one cell reduces an axonal
or dendritic element bound in a synapse, the synapse will be erased but the opposite element
remains and can rebind with a different element to a new synapse. Model neurons increase
and decrease axonal and dendritic elements in an activity-dependent fashion. Hence, synaptic
rewiring is subject to shifts in the excitation-inhibition equilibrium. We apply this model to
experimental data on cortical remapping following senso-motory deafferentation in the
fashion of the early Merzenich work [2]. Here we show by our model that disinhibition of
neurons close to the deafferented neurons induces an increase in axonal elements (axonal
sprouting) that re-occupy the deafferented neurons. There is a gradient in the number of new
synapses formed between neighbouring and deafferent neurons caused by a fading
disinhibition with increasing distance from the deafferention. By this study we demonstrate
that maintaining network homeostatis and rebalancing deafferented neurons by synaptic
rewiring can result in post-lesioning cortical remapping. Thus, the model bridges the gap
between activity-dependent morphological changes on the neuronal level and a changing
connectivity of cortical maps on an anatomic level. Furthermore, we can influence the time
course of network reorganization by external stimulations. The positive effect of a chronic
excitatory stimulation promoting recovery saturates after 100 time steps corresponding to
about 100 days in rehabilitation but can be postponed by periodic inhibition. These theoretical
results have large consequences for neurological rehabilitation i.e. for focal stroke patients as
they can be tested experimentally and even in clinical applications.
Acknowledgements
We thank Arjen Van Ooyen, VU Amsterdam for fruitful discussions on the algorithm.
References
[1] Butz M, Lehmann K, Dammasch IE, Teuchert-Noodt G (2006) A theoretical network
model to analyse neurogenesis and synaptogenesis in the dentate gyrus. Neural Netw
19:1490-1505.
[2] Merzenich MM, Nelson RJ, Stryker MP, Cynader MS, Schoppmann A, Zook JM (1984)
Somatosensory cortical map changes following digit amputation in adult monkeys. J Comp
Neurol 224:591-605.
[3] Van Ooyen A, van Pelt J, Corner MA (1995) Implications of activity dependent neurite
outgrowth for neuronal morphology and network development. J Theor Biol 172:63-82.

65

Thursday evening, Poster session I-58

Cosyne 2008

Normalization and Conservation of Cortical Synaptic
Receptive Field Structure
R.C. Froemke, M.M. Merzenich, and C.E. Schreiner
Coleman Laboratory, Keck Center for Integrative Neuroscience, Department
of Otolaryngology, University of California, San Francisco, CA, USA
The cortex is highly plastic, and cortical synapses are rapidly modified by sensory experience or
electrical activity. Changes to synaptic strength in vivo have been found to alter the structure of
receptive fields of sensory cortical neurons, both during development and in adult animals
(Karmarkar and Dan, Neuron 2006). However reorganization of cortical receptive fields can
occur with complex dynamics over an extended period of time. Here we used in vivo whole-cell
voltage-clamp recordings to examine the dynamics of receptive field plasticity in adult rats,
focusing on how long-term synaptic modifications are coordinated across multiple inputs and
stimulus features.
Neuromodulation is required for induction of cortical receptive field plasticity. Recently we used
electrical stimulation of the cholinergic nucleus basalis to enable synaptic receptive field
modification in the primary auditory cortex of adult rats (Froemke et al., Nature 2007). Electrical
stimulation of the nucleus basalis was repetitively paired with presentation of a sensory stimulus
(a pure tone at a specific intensity and frequency). Before pairing, excitation and inhibition were
generally balanced across tone intensities and frequencies. After pairing, the balance of excitation
and inhibition was broken at the paired input due to an increase of excitation and suppression of
inhibition. These changes were specific to both the intensity and frequency of the paired tone. In
parallel, we also observed a set of change to unpaired inputs, including a reduction of excitation
at the original best frequency and intensity, albeit with a slower time course. Consequentially,
frequency and intensity tuning curves shifted in the direction of the paired input, while the total
synaptic current onto the neuron was kept roughly constant.
Experimental and theoretical studies suggest that the excitatory supression at the original best
stimuli might be due to competitive Hebbian mechanisms (e.g., by repeated post- before
presynaptic spiking and spike-timing-dependent plasticity). However, suppression at this
unpaired input was also induced when low-intensity tones, subthreshold for spike generation,
were paired with nucleus basalis stimulation. Therefore, we used receptor antagonists and
manipulation of activity levels to determine the network elements responsible for synaptic
receptive field plasticity in the adult cortex. Changes to the paired input were caused by NMDA
receptor activation, gated by disinhibition after nucleus basalis stimulation. Changes to unpaired
inputs may be due to delayed activity- and calcium-dependent heterosynaptic long-term
modifications (Royer and Pare, Nature 2003). Heterosynaptic depression at the original best
stimuli might therefore be important for conserving the general shape of cortical tuning curves,
preserving their slope and width after an initial expansion at the paired input.
Thus a cascading sequence of cellular mechanisms seem to be engaged during and after periods
of heightened attention or perceptual learning, to persistently shift the peaks of cortical tuning
curves without distending their overall structure. Furthermore, these processes actively normalize
the net input received by cortical neurons, presumably preventing saturation of net synaptic
strength and corresponding input-output relations.

66

Cosyne 2008

Thursday evening, Poster session I-59

The spike timing-dependent influence of intrinsic long-range connectivity
over post-natal topographic organization in primary visual cortex
J. M. Young1-3, W. J. Waleszczyk3,5, C. Wang3, M. B. Calford4, K. Obermayer1,2, & B. Dreher3.
1

Neural Information Processing Group, Department of Computer Science, Berlin University of
Technology, Germany. 2Bernstein Center for Computational Neuroscience, Berlin, Germany. 3Bosch
Institute, The University of Sydney, Australia. 4School of Biomedical Sciences, and Hunter Medical
Research Institute, The University of Newcastle, Australia. 5Nencki Institute of Experimental Biology,
Poland.
For over two decades, models applied to the problem of topographic map development in sensory cortex
have assumed that intrinsic long-range connectivity plays an important role in the activity-dependent
refinement of these maps [1-3]. However, there is a large body of experimental evidence demonstrating
that input provided via these connections is predominantly subthreshold (or only weakly suppressive),
and therefore unlikely to be able to significantly influence an activity-dependent process of this kind.
The objective of this study was to determine whether, during early postnatal development, activity
propagated by long-range horizontal cortical connections can induce long-term changes in the location
of a neuron’s classical receptive field. We investigated this issue by looking for receptive field location
changes among neurons in cat primary visual cortex that had their intrinsic horizontal input chronically
altered by the presence of a neighboring zone of partial feedforward input loss (created by a monocular
retinal lesion made at 8 weeks postnatal). Despite receiving normal feedforward input, these neurons
outside the deafferented zone underwent extensive changes in the location of their classical receptive
fields. Neurons inside the deafferented zone also shifted the locations of their receptive fields, and we
have previously demonstrated [4] that these shifts are consistent with changes in horizontal connectivity
(catalyzed by increases in neuronal gain) that are spike timing-dependent and inconsistent with changes
that are spike correlation-dependent. Here we applied the same model to reorganization outside the
deafferented zone, and a comparison of the simulated and in vivo receptive field shifts reveals that the
same plasticity principles underlie the connectivity changes in this region also. Our results indicate that
intrinsic horizontal cortical connections, via spike timing-dependent plasticity, do indeed have the
capacity to exert a powerful influence on receptive field location, but this capacity is kept under strict
control via a close coupling of the excitation and inhibition elicited by horizontal input. Our results,
therefore, support the fundamental assumption of topographic map development models that intrinsic
long-range cortical connectivity has the capacity to shape long-term changes in neuronal receptive field
location.
Acknowledgments
Support contributed by: BMBF 10025304 (Germany), ARC (Australia).
References
1.
Kohonen, T., Self-organized formation of topologically correct feature maps. Biological
Cybernetics, 1982. 43: p. 59-.
2.
Obermayer, K., H. Ritter, and K. Schulten, A Principle for the Formation of the Spatial
Structure of Cortical Feature Maps. Proceedings of the National Academy of Sciences of the
United States of America, 1990. 87(21): p. 8345-8349.
3.
Song, S. and L.F. Abbott, Cortical development and remapping through spike timing-dependent
plasticity. Neuron, 2001. 32(2): p. 339-50.
4.
Young, J.M., et al., Cortical reorganization consistent with spike timing- but not correlationdependent plasticity. Nature Neuroscience, 2007. 10(7): p. 887-895.

67

Thursday evening, Poster session I-60

Cosyne 2008

Unsupervised Learning in a Network of Tempotrons.
Dimitri Fisher and Misha Tsodyks
Department of Neurobiology, Weizmann Institute of Science.
Tempotron learning rule [1] for a leaky integrate-and-fire neuron (LIF) provides a powerful method for
learning and decoding time- and synchrony-based neuronal codes. Tempotron learning rule is
supervised, that is, the modification of LIF input synaptic strengths is governed by the external teaching
signal which specifies the correct output for each given input. However, in biological systems the
learning of temporal code is not necessarily supervised. In present work we design architecture and
learning rules for a small network of tempotrons. The network is capable of unsupervised learning of
temporal neuronal code and performing classification tasks. The main novel feature of the network is
that the different tempotrons provide teaching signals to each-other. Additional long-term synaptic
plasticity provides a homeostasis mechanism, preventing tempotrons from dominating one another.
Output from the tempotron layer converges onto the readout layer where final classification is
performed. We test the network on several classification tasks, and characterize the network
performance for distinct classes of spatio-temporal processes (spike sequences) with identical spikecount distributions in each channel. The network performance is highly accurate and efficient in its
utilization of neuronal spikes.

References
[1] R. Gutig and H. Sompolinsky, Nature Neuroscience 9(3), 420-428 (2006).

68

Cosyne 2008

Thursday evening, Poster session I-61

Inferring Human Visuomotor Q-functions using Inverse
Reinforcement Learning
Constantin A. Rothkopf1, Dana H. Ballard2
1

University of Rochester,

2

University of Texas at Austin

Psychophysical studies in humans have demonstrated that visuomotor behavior such as rapid hand
movements to targets are sensitive to the reward structure of the goal and are executed so as to maximize
reward1. Similarly, animal studies have shown that learning of such behaviors is driven by reward.
Evidence suggests that the neurotransmitter dopamine is involved in the process of learning visomotor
behaviors2. Moreover, reinforcement learning (RL) algorithms have been formulated that characterize
well the neuronal signals of dopaminergic neurons in response to the occurrences of stimuli associated
with rewards and the delivery of the rewards across learning3. Common to all these experiments is that the
reward structure of the task is explicitly controlled by the experimental setup. By comparing the observed
behavior with a normative model, it can be inferred how close to optimal the participants performed. But
how does this relate to every day activities in natural tasks?
It has been difficult to answer this question, because the value functions underlying such actions are not
known a priori. Similarly, it is difficult to come up with the reward functions for natural tasks from first
principles. By contrast, Inverse Reinforcement Learning (IRL) is the problem of inferring the reward or
value function underlying a task. Thus the parameters of the internal task representation have to be
recovered from the observed states of the world and the observed behavior. We present a new algorithm
for IRL, which reduces the number of parameters that have to be inferred by exploiting the structure
present in the considered tasks. The algorithm uses the fact that there are only few states that results in a
reward and that there are costs of actions, which are independent of the current external state. By
constructing a large number of such learning problems with scaled rewards and solving the corresponding
RL problem, the Q-functions are obtained for the different reward weights. It is shown that these Qfunctions can be compactly represented with a small number of basis functions. Therefore, instead of
inferring the reward for each state action pair, the inference task is now to find a much smaller number of
parameters. The algorithm estimates the Q-function from observed behavior using MCMC sampling
assuming that participants followed a stationary policy.
The algorithm was applied to data collected from human subjects navigating in a virtual environment and
executing different task combinations of approaching and avoiding objects as well as following a path.
The Q-functions underlying the human navigation through the environment are recovered for each
participant and task and the validity of the algorithm is verified by evaluating the RMS-error between the
actual trajectories taken by the participants and the ones simulated utilizing the extracted Q-values. These
results demonstrate the similarity of the reward functions used by the participants for identical tasks and
quantify the reward structure leading to the observed behavior.
References
[1] Trommershauser, J. et al. J. Opt. Soc. Am. A Opt. Image Sci. Vis. 20, 1419–1433
[2] W. Schultz, J Neurophysiol 80: 1-27, 1998
[3] W. Schultz, P. Dayan and P. Read Montague, Science, 275 1998

69

Thursday evening, Poster session I-62

Cosyne 2008

Cortical network plasticity to communication sounds in awake mice
Edgar Galindo-Leon1, and Robert C. Liu1
1

Department of Biology, Emory University

Understanding the representation of natural stimuli is one of the main goals of sensory systems research.
In audition, such stimuli range from naturally-occurring environmental sounds to those generated by
organisms themselves, like species-specific vocalizations. Studies on the encoding of such calls in
mammals have suggested that the neural response may be better synchronized across neurons tuned to
different sound frequencies than expected from the spectral-temporal structure of the sounds, particularly
if the calls are behaviorally-relevant [1,2,3]. However, these conclusions were based on data taken from
anesthetized animals, and how awake mammals respond to communication sounds is not well explored.
We investigate this by recording from auditory cortical neurons in awake, head-restrained mice while
playing back natural vocalizations. The mouse is developing into a promising model for this purpose. It
features a well-characterized ultrasound communication system between pups and adult females, thus
providing an ethological context. Only mothers and pup-experienced adults recognize the significance of
pup calls, thus making virgin females a natural control for the effects of behavioral relevance on neural
coding. Moreover, mice hold the potential for genetic dissection of underlying mechanisms, thus offering
new research opportunities. Finally, as we demonstrate, high-quality, single-unit (SU) electrophysiology
in awake mouse auditory cortex is feasible.
Our study recorded both SU’s as well as local field potentials (LFP) from the high-frequency area of the
left mouse auditory cortex using high-impedance tungsten electrodes. We employed the relatively new
concept of LFP ‘phase reliability’ (PR) as a useful method to differentiate neural network activity. We
observed significant differences in the short-latency PR between mothers and virgins at recording sites
whose best frequencies (BF) are ~25 kHz below the typical frequency of the ultrasound calls. Moreover,
the percentage of tone-excited SU’s that can also be excited by pup calls is significantly increased in
mothers, regardless of whether the BF is near or far from the call frequency. We discuss the possibility
that our results demonstrate plasticity in either the high frequency synaptic input to mid-frequency BF
sites, and/or the synchronization of spiking activity within the local neural population.
Acknowledgments
We thank Brian Kocher for technical assistance during experiments. This work was supported by NIH
grant DC008343 and the NSF Center for Behavioral Neuroscience.
References
[1] Representation of a species-specific vocalization in the primary auditory cortex of the common
marmoset: Temporal and spectral characteristics. X. Wang, M.M. Merzenich, R. Beitel and C.E.
Schreiner, Journal of Neurophysiology 74:2685-2706, 1995.
[2] Differential representation of species-specific primate vocalizations in the auditory cortices of
marmoset and cat. X. Wang and CS.C. Kadia, Journal of Neurophysiology 86:2616-2620, 2002.
[3] Auditory cortical detection and discrimination correlates with communicative significance. R.C. Liu
and C.E. Schreiner, PLoS Biology 5(7):e173, 2007.

70

Cosyne 2008

Thursday evening, Poster session I-63

Spike-Timing Dependent Plasticity of Inhibitory Cortical Synapses
Thomas Nowotny1 and Julie S. Haas2
1

University of Sussex, Brighton, UK

2

Harvard University, Cambridge, MA

The entorhinal cortex (EC) serves as an information gateway for the hippocampal formation. Most
principal neurons of the superficial EC are spiny stellate cells (SCs); these neurons collect input from the
neocortex, and form the perforant path input to the hippocampus. Information passing through layer II is
transformed by the robust intrinsic dynamics of SCs, neurons that participate in the regional theta rhythm.
Here, we show that hippocampus-bound information is also transformed by synaptic dynamics within
layer II; in particular, by plastic inhibitory synapses. We recorded inhibitory responses of SCs in
horizontal slices from young (P14-P21) rats at 34qC via whole-cell patch recording. To induce plasticity,
we repeatedly paired local presynaptic stimulations with evoked postsynaptic spikes at controlled timing
delays under pharmacological blockade of excitation. We found that after training, the strength of the
remaining inhibitory synaptic response varied as a function of the temporal relationship between the preand postsynaptic stimuli. For presynaptic stimuli preceding postsynaptic, inhibitory responses grew in
size, peaking at a delay of 10 ms. When a postsynaptic spike was evoked before a presynaptic stimulus
was delivered, the synaptic pairing weakened, again most noticeably at 10 ms of delay. A summary plot
of change in response versus pairing delay is shown below. The observed changes in synaptic strength
depend on changes in intracellular calcium concentrations. These data indicate that inhibition can play a
dynamic, activity-dependent role in both the rhythmic and information-processing functions of the EC,
and provide evidence that spike-timing-dependent plasticity is an important effect for inhibitory, as well
as excitatory, synapses.
Recent research has described the spatial receptive fields of superficial EC principal neurons as forming a
tessellating lattice. The EC is also implicated as a source for epileptic activity. For these reasons, we
sought to explore the effects of dynamic inhibitory synapses in a variety of models, aiming to understand
how plastic inhibitory synapses shape the response of single cells and of a network or layer of neurons
(below, right). We show that timing-dependent plasticity of inhibitory synapses is crucial for the selfregulation of propagated activity in a network.
.
A

ΔIPSP [norm]

2

1.5

1

0.5

0
−20

−10

0

10

20

Δ t [ms]

Acknowledgments
This work was supported by the Blasker Grant from The San Diego Foundation (JSH).

71

Thursday evening, Poster session I-64

Cosyne 2008

Decision-Making in the Presence of Others.
Debajyoti Ray1 and Peter Dayan1
1Gatsby

Computational Neuroscience Unit, UCL.

As for all social species, the success or failure of many of our actions depend critically on what other,
independent, decision-makers do. Achieving personally or socially efficient outcomes therefore requires
us to build, maintain, update and use sophisticated models of these other entities. The fields of
experimental and behavioural economics include a rich range of empirical approaches to studying such
interactive decision-making, and have revealed the importance of social utility constructs such as envy
and guilt that depend on the relationship between multiple choosers. Experiments into the neural bases of
such decisions suggest that areas of the brain associated with the ‘theory of mind’, i.e. the models we
build of others, are involved.
In order to gain a sharper picture of the computations and calculations underlying such choices, we are
marrying the sort of reinforcement learning that has been highly successful as a model of approximately
normative individual or single-agent decision-making with economic ideas about interactivity. In
particular, we treat our subjects as having different types (in our case associated with the parameters of
their individual social utility functions), playing a game with other subjects. As is common, we assume
that the subjects know their own types, but not those of the others; there is therefore incomplete
information. The theories of mind they need to build are exactly about the types of the other subjects, and
the consequences of these types for future interactions. Subjects also do not necessarily know how many
levels or orders of belief about others each subject builds.
We use approximation inference methods to solve the resulting partially observable Markov decision
problem (POMDP), and look at the qualitative classes of behaviour that emerge. Behaviours such as
probing arise in multi-round games, as subjects attempt to learn about, and take advantage of, each
others’ types. Not only are such models revealing about the nature of the interaction, but they can also be
used to generate regressors for fMRI data from appropriate experiments.
As a proof of the principles of the model, we have studied two games: a one-round Intentions to Trust
game [2] and a multi-round Investor-Trustee game [1]. Observed phenomena such as the importance of
intentions, the build-up and break-down of cooperation over the course of the multi-round game emerge
in our modeled interactions; the relative importance of different parameters for different players also
become starkly apparent.
Acknowledgments
We thank Read Montague, Brooks Kings-Casas, Philip Baldwin and Terry Lohrenz for data and
substantial discussions, and Karl Friston, Wako Yoshida and Scott Sanner for their cooperative social
utility functions. Funding was from the Gatsby Charitable Foundation.
References
[1] Kings-Casas, B, Tomlin, D, Anen, C, Camerer, CF, Quartz, SR, Montague, PR (2005) Getting to know
you: Reputation and trust in a two-person economic exchange. Science 308:78-83.
[2] McCabe, KA, Rigdon, ML, Smith, VL (2003) Positive reciprocity and intentions in trust games.
Journal of Economic Behaviour and Organization 52:267-275.

72

Cosyne 2008

Thursday evening, Poster session I-65

Graded prediction error signal drives behavioral choice biases in
rats
Adam Kepecs1, Naoshige Uchida2, and Zachary F. Mainen1
1

Cold Spring Harbor Laboratory,

2

Harvard University

Animal behavior can be effectively modified through reinforcement. Reinforcement learning theory
proposes that a quantitative prediction error—the difference between expected and observed outcomes—
drives learning, but there is limited direct evidence for this, in part because quantitative measurements of
learning increments are experimentally difficult to assay.
If reinforcement learning processes are active, they may provide on-going behavioral adjustments even in
the absence of explicit changes in behavioral contingencies that would require overt learning. For
example, during categorization tasks the decision-boundary can be learned through reinforcement but
may undergo continual updating during performance thereafter. If so, then the signature of predictionerror driven learning might be found even in steady-state behavior where it is much easier to quantify.
Here, we used an olfactory categorization task to search for such prediction error signals in trial-by-trial
updating of behavioral strategy.
Rats were trained to report the category (dominant component) of binary odor mixtures and were
rewarded either at the left or the right choice port for correct decisions. After several weeks of training
performance asymptoted, as indicated by a stable psychometric function. Discrimination accuracy for
pure odors was close to 100% but for the most difficult mixtures performance dropped to almost 60%.
This long-term stability allowed us to examine the trial-by-trial fluctuations in choices.
Interestingly, we found that animals were dynamically adjusting their decision strategy even after
extensive training. Rats systematically biased their choices based on the previous reward outcome. This
bias was only observed for difficult decisions (those near the category boundary) and its magnitude was
proportional to the difficulty of the previous choice (how far the stimulus was from the boundary). This
observation suggests that the category boundary and not side-bias was being updated. Surprisingly, choice
biases occurred even after correct decisions, which cannot be accounted for by a binary prediction error
signal. Since the actual reward was binary, the animals must have differential reward expectations on a
trial-by-trial basis.
These data can be quantitatively explained by a simple, single layer neural network model using the
“delta” learning rule [ v (expected–observed reward)]. To account for graded choice bias the reward
expectancy signal needs to be based on an estimate of either stimulus difficulty or decision confidence.
We show that an instantaneous estimate of confidence can be easily computed in this class of models
along with the choice [choice =sgn(wTx); confidence = abs(wTx)].
Our results demonstrate that animals continually incorporate reward feedback into their choice strategies
even in well-trained discrimination behaviors. Moreover, these data provide support for the notion that
reinforcement learning depends on a graded prediction signal to achieve statistically optimal behavioral
changes. The updating of well-learned behavior through such a prediction error signal implies that similar
processes underlie learning based on changing stimulus-outcome contingencies.

73

Thursday evening, Poster session I-66

Cosyne 2008

Homo economicus in visual search
Vidhya Navalpakkam1, Christof Koch1, and Pietro Perona1
1

California Institute of Technology

Humans rely on visual search to detect food, predators and mates. Decades of research in visual search behavior has
focused on the role of sensory information [1,2] such as salience of the target, amount of background clutter
quantified by number and heterogeneity of distracting objects, and recently target frequency effects [3]. In contrast,
the role of reward outcomes in visual search behavior is relatively unknown. Here we investigate how sensory
information (based on the visual representation of target present and absent displays, and target frequency) combines
with reward outcomes (points gained or lost for correct vs. incorrect responses) to influence decision making and
target detection performance in a visual search task.
We asked subjects to detect an oddly oriented line (of a known orientation) in briefly flashed pictures containing a
number of parallel lines. We measured target detection performance as a function of the target frequency (50%,
10%, 2%) and the reward scheme. We experimented with two reward schemes: in experiment A, the reward was
‘Neutral’, i.e. false alarm and miss errors were equally penalized (loss of 50 points) and correct detections and
correct rejections were equally rewarded (gain of 1 point). In experiment B, missing a target was penalized much
more seriously (-900 points) than a false alarm (-50 points), and correct detection was rewarded more (+100 points)
than correct rejection (+1 point). We call this the ’Airport’ reward scheme to mimic an airport scenario where
missing a bomb hidden in a suitcase is prohibitively expensive compared to false alarms that result in relatively
quick manual inspection.
We present four main findings: 1) Rare targets are missed even when the target is salient (a drop in detection rates
from close to 80% down to 30% as the target frequency decreases from 50% to 2%, replicating the results of [3]). 2)
Contrary to previous studies [3,4], we find a rapid and optimal influence of reward on sensory decision making and
detection rates – humans behave as reward-maximizing agents and decide whether the target is present or not based
on whichever maximizes their expected reward. Hence, the poor detection performance for rare targets can be
corrected by changing the reward scheme. 3) A quantitative model based on reward-maximization accurately
predicts human detection behavior in different target frequency and reward conditions. We use this model to
illustrate how reward schemes can be designed to obtain high detection rates for any target frequency. 4) We
conclude with a neurally plausible model of how subjects quickly learn the optimal decision criterion in different
target frequency and reward schemes. Potential applications of our findings include improving detection rates in
life-critical searches for rare targets (e.g., bombs in airline passenger bags, cancers in medical images).

References
[1] A. Treisman and G. Gelade. A feature integration theory of attention. Cognitive Psychology, 12:97–136, 1980.
[2] J Duncan and G W Humphreys. Visual search and stimulus similarity. Psychological Rev, 96:433–458, 1989.
[3] J.M. Wolfe, T.S. Horowitz, and N.M. Kenner. Cognitive psychology: rare items often missed in visual searches.
Nature, 435(7041):439–40, 2005.
[4] W Todd Maddox. Toward a unified theory of decision criterion learning in perceptual categorization. J Exp Anal
Behav, 78(3):567–595, Nov 2002.

74

Cosyne 2008

Thursday evening, Poster session I-67

Modeling the ocular following response to center-surround
stimulation using a probabilistic framework
Laurent U. Perrinet and Guillaume S. Masson
INCM-CNRS / University of Provence, 13402 Marseille Cedex 20, France
The machinery behind the visual perception of motion and the subsequent sensorimotor transformation, such
as in Ocular Following Response (OFR), is confronted to uncertainties which are efﬁciently resolved in the
primate’s visual system. We may understand this response as an ideal observer in a probabilistic framework
by using Bayesian theory [1] which we previously proved to be successfully adapted to model the OFR
for different levels of noise with full ﬁeld gratings [2]. More recent experiments of OFR have used disk
gratings and bipartite stimuli which are optimized to study the dynamics of center-surround integration. We
quantiﬁed two main characteristics of the spatial integration of motion : (i) a ﬁnite optimal stimulus size for
driving OFR, surrounded by an antagonistic modulation and (ii) a direction selective suppressive effect of the
surround on the contrast gain control of the central stimuli [3]. Herein, we extended the ideal observer model
to simulate the spatial integration of the different local motion cues within a probabilistic representation. We
present analytical results which show that the hypothesis of independence of local measures can describe the
integration of the spatial motion signal. Within this framework, we successfully accounted for the contrast
gain control mechanisms observed in the behavioral data for center-surround stimuli. However, another
inhibitory mechanism had to be added to account for suppressive effects of the surround.
Acknowledgments
This work was supported by EC IP project FP6-015879, ”FACETS”.
References
[1] Motion illusions as optimal percepts. Y. Weiss, Yair Weiss, E. Simoncelli and E. Adelson. Nature
Neuroscience, 5(6): 598–604, 2002.
[2] Dynamics of motion representation in short-latency ocular following: A two-pathways bayesian model.
L. U. Perrinet, F. Barthélemy, E. Castet, and G. S. Masson. In R. A. Carmona and G. Linan-Cembrano,
editors, Perception, volume 34 of ECVP, page 38, 2005.
[3] Dynamics of distributed 1D and 2D motion representations for short-latency ocular following. F. Barthlemy and L. U. Perrinet and E. Castet and G. S. Masson Vision Research, 2007.
Doi:10.1016/j.visres.2007.10.020

75

Thursday evening, Poster session I-68

Cosyne 2008

Modeling decision making under ambiguity
Kerstin Preuschoff1 and Peter Bossaerts2
1

University of Zurich, Switzerland

2

EPFL, Lausanne, Switzerland

Ecologic and economic theories of decision making under uncertainty emphasize the importance of correctly
evaluating expected reward and risk. Within neuroscience, the idea obtains strong support from the fact that
the human brain reﬂects both mathematical expectation of reward (mean) and risk (variance) in situations
under pure risk [1]. Pure risk is often dissociated from ”true” uncertainty or ambiguity in which probabilities
of outcomes are unknown.
In situations that involve neither risk nor ambiguity, organisms prefer higher expected rewards over lower
expected rewards. In situations that involve ambiguity lower ambiguity is generally preferred by human
decision makers, although the reverse is not uncommon.
The present study explores different models of decision making under ambiguity. In a simple gambling
task, subjects had to choose between two ambiguous gambles while their brain activity was being recorded
using functional imaging (fMRI). Preferences were modeled in terms of (i) Bayesian hierarchical priors, (ii)
a trade-off between expected payoff (at un-informed priors) and the amount of ambiguity in the available
options, and (iii) alpha-maxmin utility theory. We ﬁrst evaluate the relative power of the three different
approaches to predict subjects’ choices. Using the parameter values estimated from the behavioral data, we
subsequently determine which model best ﬁts the functional imaging data throughout the task. We focus on
brain regions that are known to be differentially activated during decision making under uncertainty [2]. Our
results show that activation during reward anticipation reﬂects separate encoding of mean probability and
variance of probability as estimated by a trade-off between expected payoff and the amount of ambiguity.

References
[1] Neural Differentiation of Expected Reward and Risk in Human Subcortical Structures. K. Preuschoff, P.
Bossaerts and S. Quartz, Neuron 51:381-390, August 3, 2006.
[2] Neural Signatures of Economic Preferences for Risk and Ambiguity. S.A. Huettel et al, Neuron 49:765775, March 2, 2006.

76

Cosyne 2008

Thursday evening, Poster session I-69

A neural representation of reward prediction error identified using
an axiomatic model
Robb B. Rutledge, Mark Dean, Andrew Caplin, Paul W. Glimcher
New York University
The midbrain dopamine neurons are thought to encode a reward prediction error signal [1, 2]. Despite
numerous electrophysiological studies in nonhuman primates finding evidence consistent with this
hypothesis, this hypothesis has not been formally tested and it has not yet been demonstrated that
dopamine neuron activity has all of the properties of a reward prediction error (RPE) system.
Dopamine neurons are known to increase their activity in response to unexpected rewards [3]. We used
this property to identify brain areas likely to reflect dopaminergic activity using functional MRI in
humans. Subjects were given an endowment of $60. In each trial, subjects pressed a button to play a
single lottery giving the subject a 50% chance of gaining $2 and a 50% chance of losing $2 of real
money. After a brief delay, the outcome was revealed. Subjects completed 128 trials and, including the
endowment, earned $24 to $96. We used a contrast of gain versus loss trials to identify regions of interest
(ROIs) in the ventral striatum and medial prefrontal cortex in 10 of 13 subjects. Based on anatomical
knowledge of dopamine neuron projections and electrophysiological results in nonhuman primates, we
hypothesized that the fMRI signal in these ROIs reflected the activity of midbrain dopamine neurons.
We then tested whether activity in these ROIs demonstrates all of the properties of a RPE system. Caplin
and Dean have recently derived an axiomatic basis for the RPE hypothesis rooted in economic theory [4].
This axiomatic model specifies three properties that are both necessary and sufficient for any RPE signal.
Briefly, the axioms will be satisfied if activity is: 1) increasing with prize magnitude, 2) decreasing with
lottery expected value, and 3) equivalent for outcomes from all lotteries with a single possible outcome.
We tested for compliance with these axioms in the ROIs defined in the functional localizer task, by
measuring neural activity in a second task, one in which subjects received monetary prizes from a set of
many different lotteries. In each trial, subjects pressed a button to select one of two visually presented
lotteries. After a brief delay, the outcome of the chosen lottery was revealed. We then used the activation
pattern at this time to test whether neural activity in the ROIs satisfies the axioms of the RPE model. In a
preliminary study of five subjects, we find that activity in these putatively dopaminergic regions satisfies
the first two axioms, but does not satisfy the third axiom. The axiomatic approach allows us to determine
to what extent the data satisfy the axioms, and to determine the nature of any departures from the model.
References
[1] A neural substrate of prediction and reward. W. Schultz, P. Dayan, P. R. Montague, Science 275:
1593-1599, 1997.
[2] Midbrain dopamine neurons encode a quantitative reward prediction error signal. H. M. Bayer and P.
W. Glimcher, Neuron 47: 129-141, 2005.
[3] Importance of unpredictability for reward responses in primate dopamine neurons. J. Mirenowicz and
W. Schultz, Journal of Neurophysiology 72: 1024-1027, 1994.
[4] Dopamine, reward prediction error, and economics. A. Caplin and M. Dean, Quarterly Journal of
Economics, In Press.

77

Thursday evening, Poster session I-70

Cosyne 2008

Neural correlates of value and probability in decision under risk
and in an equivalent motor task
Shih-Wei Wu1 and Laurence T. Maloney1
1

New York University

Uncertainty affects outcomes in all perceptual, motor and economic decision making tasks. We do not yet
know the extent to which neural representations of uncertainty in these different kinds of tasks are shared
or separate. Since humans come remarkably close to ideal performance in perception and movement
planning and are often far from optimal in decision making under risk (DMR), it is tempting to conclude
that perceptual and motor systems make better use of uncertainty information. However, there is no direct
comparison of performance in different decision modalities using tasks that are equivalent, and no firm
conclusion can be drawn. We designed a motor choice task that is precisely equivalent
$20 $24 $20
to choosing between two lotteries in DMR. We used it to replicate a classic experiment
[1] in the original “classical” form and in motor form. The experiment was designed to
test the Independence Axiom of EUT and assess non-linearities in probability
weighting functions (PWFs) and consequently we could compare PWFs across tasks.
The subject faced the same decisions in the same order in both modalities and knew
that no lotteries would be evaluated or motor tasks attempted until the end of the
experiment. We repeated parts of the experiment using fMRI to investigate the neural
correlates of value and probability and see how they differed in motor and classical
conditions. Methods. There were 3 sessions in the experiment. In the first session, subjects performed
rapid pointing to a rectangular target varying in size (384 trials). We estimated each subject’s motor
variability so that we could create motor tasks equivalent to any lottery. In the second and third sessions,
subjects faced both classical decision tasks and equivalent motor tasks. Distortions of value and
probability were estimated separately for the motor task and the classical task. Subjects did the third
session in the MRI scanner. We show a single motor lottery above. The participant has limited time to
touch it and win a reward that depends on the end point of his touch. This motor lottery is equivalent to
the lottery  p1 ,$24; p2 ,$20  with probabilities determined by the participant’s motor uncertainty
represented by black dots. On each trial in the matched motor conditions subjects saw two motor lotteries
and picked the one they would prefer to attempt without attempting it. Results. 4 naïve subjects
participated in the experiment. In 3 out of 4 subjects, the PWF was significantly closer to linear in the
motor task than the classical task. We found that activity in the striatum, the cingulated gyrus, and the
superior frontal gyrus tracked the distortions of probability for both the motor task and the economic task.
We also found that the dorsal premotor area was selectively activated in the motor task, consistent with
previous results on motor imagery but further demonstrates its involvement in the representation of
probability in motor task.
Classical

Motor
Areas correlated with classical and motor probability weighting
were color coded and shown separately. Activation in the
superior frontal gyrus and the superior parietal were shown for
classical probability. Activation in the cingulate and dorsal
premotor area were shown for motor probability.

[1] Wu & Gonzalez (1996). Management Science, 42, 1676-1690.

78

Cosyne 2008

Thursday evening, Poster session I-71

Chopper responses to amplitude-modulated tones: stochastic modelocking theory and observed spiking patterns?
Jonathan Laudanski1, Christian J Sumner2, Andrew T Wood1, Stephen
Coombes1 and Alan R Palmer2
1

Department of Applied Mathematics, University of Nottingham, UK
MRC Institute of Hearing Research, Nottingham University Section, University of
Nottingham, UK
2

Responses to amplitude-modulated pure tones have been used extensively to assess temporal
properties of neurons across the auditory system. The synchronisation to the modulation
frequency has mostly been measured by an index called vector strength. This index is based on
the distribution of spikes along the period of modulation. One obtains a low value when the
spikes occur evenly across the period and a high value when they are sharply distributed around
a single time. In the ventral cochlear nucleus, chopper units have been found to show band-pass
temporal responses at high sound pressure level and low-pass temporal responses at low level.
However, the fine structure of the responses remains uncharacterised.
Here, we show that data obtained from chopper neurons in response to amplitude-modulated tone
exhibit more complex synchronised discharge patterns, reminiscent of mode-locked states.
Theses responses can be organised around an Arnol’d tongue structure of a periodically forced
model accounting for the sub-threshold properties of the T-multipolar cells. Numerical
simulations of a stochastic version of this integrate-and-fire model give response patterns similar
to the one observed experimentally. Thus, the results tend to show that care should be taken
when considering the temporal properties of a neuron only on the basis of its vector strength.
Acknowledgments

This research was supported by a Marie Curie Early Stage Researcher Training Fellowship from
the European Commission (EC Contract No: MEST-CT-2005-020723).

79

Thursday evening, Poster session I-72

Cosyne 2008

Reliability of spike timings on pulse-coupled networks of neurons
Jun-nosuke Teramae, Tomoki Fukai
Laboratory for Neural Circuit Theory, Brain Science Institute, RIKEN
Temporal coding requires that neurons respond with high reliability to time-varying stimuli. Mainen and
Sejnowski showed in their pioneering work that isolated neurons respond to fast fluctuating stimuli
reproducibly on a trial-to-trial up to a precision of millisecond [1]. However, neurons in the brain work
collectively in their networks rather than independently. It remains unclear whether networks of neurons
still respond reliably to fluctuating stimuli.
We study the reliability of spike timing in a general class of pulse-coupled neurons receiving a fluctuating
external input [i(t),


X
i

N





F  Xi  +@ i  t   ¦ gij ¦ G t  t spike
.
j
j 1

spike

We find that the coupled neurons show a transition from reliable to unreliable responses at a critical
coupling strength. Responses of coupled neurons are different from trial to trial if coupling strength of the
network exceeds the critical strength. Reliability of spike timings is equivalent to noise-induced
synchronization between identical oscillators [2, 3]. To study the transition, we extend a theory of noiseinduced synchronization between isolated oscillators to between networks of oscillators. We employ the
phase reduction method to analytically derive the maximum Lyapunov exponent of the noise-induced
synchronized state. The critical strength is obtained as a value at which the maximum Lyapunov exponent
is equal to zero.
In the vicinity of the critical coupling strength, information encoded in spike timings can stay in the
network for a long time whereas information encoded in firing rate decay rapidly. We discuss a possible
role of the coexistence of two time scales on neuronal computation in the brain.
(a)
Fig1: raster plots of spike timings
for two different trials (red
and black). (a) g=0.0<gc.
(b) g>gc

(b)

20

20

18

18

16

16

14

14

12

12

10

10

8

8

6

6

4

4

2

0
585

2

590

595

600

0
585

590

595

600

Acknowledgments
This work was supported by Grant-in-Aid for Young Scientists (B) 50384722 and Grant-in-Aid for
Scientific Research 17022036 from the Japanese Ministry of Education, Culture, Sports, Science and
Technology.
References
[1] Reliability of spike timing in neocortical neurons. Z. Mainen and T. Sejnowski, Science
268(5216):1503-1506, Jun 1995
[2] Robustness of the noise-induced synchronization in a general class of limit cycle oscillators. J.
Teramae and D. Tanaka, Physical Review Letters 93(20):204103, Nov 2004.
[3] Noise Induced Phase Synchronization of a General Class of Limit Cycle Oscillators, J. Teramae and D.
Tanaka, Progress of Theoretical Physics Supplement 161: 360-363, 2006

80

Cosyne 2008

Thursday evening, Poster session I-73

Application of frequent episode discovery for analyzing multineuron spike train data
Debprakash Patnaik1 , P.S.Sastry2 , and K.P.Unnikrishnan3
1

Virginia Tech,

2

Indian Institute of Science,

3

General Motors R&D

Inferring useful information regarding coordinated behavior of a group of neurons from simultaneously
recorded spike train data is a challenging problem of current interest in neuroscience. Due to the availability
of large sets of such data, there is a lot of interest in developing novel data-analysis techniques that are
efﬁcient for this problem [1]. Many of the current techniques such as cross correlograms, JPSTH, Principal
component analysis etc. becomes computationally very intensive when one wishes to infer relationships
among more than a few (typically 3) neurons. The ﬁeld of temporal data mining in Computer Science is
concerned with developing efﬁcient algorithms for analyzing extremely large databases of symbolic sequential data streams [2]. In this paper we present that techniques from temporal data mining are very effective
for getting information regarding the microcircuits underlying a neural tissue by analyzing the multi-neuron
spike train data. Speciﬁcally we consider the framework of frequent episodes in temporal data mining. In
this framework, the data is viewed as a sequence of (time-ordered) events where each event is characterized
by an event type (which is a symbol from a ﬁnite alphabet) and a time of occurrence. For spike train data,
the event types would be neuron labels and event times would be the time of occurrence of the respective
action potentials. The objective is to discover repeating temporal patterns called episodes. A serial episode
is an ordered sequence of event types and a parallel episode is an unordered collection of event types. An
episode is said to occur in a slice of data if we can ﬁnd events in the data with appropriate event types that
satisfy the ordering constraint of the episode. For example, a serial episode A→B→C occurs in the data if
we can ﬁnd an A and sometime later a B and so on. There can be other intervening events in between. In
the spike train data, if there are strong excitatory connections from A to B and B to C, then we can expect
to see many occurrences of such a serial episode. Since there are other neurons also spiking, there can be
other events in between those corresponding to an episode occurrence. Similarly, a parallel episode (ABC)
would occur frequently in the data if the corresponding neurons often ﬁre synchronously. To capture many
such types of temporal patterns properly, we extend the frequent episodes framework so that one can impose
some temporal constraints on the occurrences of the episodes that are counted [3]. These constraints allow
us to take care of, e.g., synaptic delays. In this paper we show that this extended episodes framework is
well suited for discovering many patterns of interest in spike train data such as order, synchrony and synﬁre chains. We present algorithms that can efﬁciently discover serial and parallel episodes (under temporal
constraints) that occur sufﬁciently often in the data. We show through simulations on synthetic and real data
that these algorithms are very effective in unearthing interesting temporal patterns in the spike train data
involving large number of neurons.
References
[1] Multiple neural spike train data analysis: state-of-the-art and future challenges. Brown, E.N., Kass, R.E.,
and Mitra, P.P., Nature Neuroscience 7(5):456-461, May 2004.
[2] A survey of temporal data mining. Laxman, S., and Sastry, P.S., Sadhana 31(2):173-198, April 2006.
[3] Discovering patterns in multi-neuronal spike trains using the frequent episode method. Patnaik, D.,
Sastry, P.S., and Unnikrishnan, K.P., arXiv.org url http://arxiv.org/abs/0709.0566v2.

81

Thursday evening, Poster session I-74

Cosyne 2008

Working memory: somatic spikes or synaptic calcium?
Omri Barak1*, Gianluigi Mongillo2*, and Misha Tsodyks1
1

Department of Neurobiology, Weizmann Institute of Science, Israel
Laboratory of Neurophysics and Physiology, CNRS, Universite Rene Descartes,
France
*
These authors contributed equally to this work
2

In delayed match to sample experiments, monkeys are required to remember a sample stimulus for several
seconds in order to compare it to the following ‘match’ stimulus. Neurons were found which show
elevated firing rates throughout this delay period. Accordingly, current models of working memory
suggest that the persistent firing of neurons is the necessary mechanism to maintain information for
several seconds.
The standard mechanistic model for this phenomenon is the attractor neural network, where strong
recurrent connections within a group of neurons selective for the same stimulus enable this group to fire
even after removal of a transient stimulus [1]. A common mismatch between such models and
experimental results is that observed delay firing rates are usually quite low [2], whereas the models
require a high firing rate to maintain stability.
We propose a new theory, in which synapses can also effectively implement active memory function
through their short-term plasticity. In this account, the short-term synaptic variables are used as a 'buffer'
which is loaded, read-out and refreshed by neural activity. The refresh rate is low, since it depends on the
slow time constants associated with synaptic dynamics. The feasibility of this mechanism is demonstrated
both in a rate model [3] and in a biologically-plausible spiking network model. The resulting 'working
memory' system is robust, metabolically efficient, and can help explain recent electrophysiological
results.
Acknowledgments
G.M is supported by BACS consortium grant FP6-IST-027140 and BIND Marie Curie Team of
Excellence grant MECT-CT-2005-024831. O.B. is supported by the Azrieli Foundation and by the Kahn
Family Research Center for Systems Biology of the Human Cell. M.T. is supported by the Israeli Science
Foundation, Irving B. Harris Foundation and Abe & Kathryn Selsky Foundation.
References
[1] D. J. Amit, Behav. Brain Sci. 18 (1995).
[2] G. Rainer, E. K. Miller, European Journal of Neuroscience 15, 1244 (2002).
[3] O. Barak, M. Tsodyks, PLoS Comput Biol 3, e35 (2007).

82

Cosyne 2008

Thursday evening, Poster session I-75

Influence of Anatomical and Morphological Features on Temporal
Firing Patterns: A Simulation Study of Single Neurons at Nucleus of
the Solitary Tract
Jen-Yung Chen
Department of Psychology, State University of New York at Binghamton
In the computations performed by neurons in sensory systems, the temporal structure of neural activity
(spike timing) is thought to play a critical role. In the current study, the impact of the anatomical and
morphological features of dendrites on spike timing was systemically investigated in a computational
model. Hodgkin-Huxley types of neurons from medial nucleus of the solitary tract (Schild et al. 1993)
with various anatomical, morphological and biophysical features were constructed in NEURON, a
simulation environment for individual neuron or neural network modeling. Forty sets of random inputs
(20 sets at 40 Hz and 20 sets at 5Hz) were delivered to these model neurons. The temporal firing
characteristics of their responses were evaluated using metric space (Victor and Purpura, 1997) and
multidimensional scaling analyses. Results showed that: (1) Neurons can generate reliable firing patterns
even though their inputs are random. This performance was more apparent for higher frequency inputs.
(2) Neurons with different dendritic lengths and branching patterns generated similar firing patterns given
the same set of low frequency random inputs. However, when high frequencies of random inputs were
delivered to the neurons, different firing patterns were reliably generated by neurons with different
morphological features. (3) When low frequency inhibitory inputs were added, neurons changed their
firing patterns. Neurons with shorter dendrites were more sensitive to the temporal patterns of inhibitory
inputs. (4) Neurons which received clustered inputs (that is, highly concentrated within the dendritic tree),
generated more reliable firing patterns. In summary, these results showed that neurons can regulate their
temporal patterns of firing based on their anatomical, morphological and biophysical features and that
these firing patterns may play a crucial function in neural information processing and neural coding.

References
[1] Shield JH, khushalani S, Clark JW, Andresen MC, Kunze DL and Yang M, An ionic current model
for neurons in the rat medial nucleus tractus solitary receiving sensory afferent input, Journal of
Physiology, 469, pp341-363 (1993).
[2] Victor JD and Purpura KP, Metric-space analysis of spike trains: theory, algorithms and application,
Network: Computation in Neural Systems, 8, pp127-164 (1997).

83

Thursday evening, Poster session I-76

Cosyne 2008

A computational model of perisaccadic mislocalization in total
darkness using a binary eye position signal
Arnold Ziesche, Fred Hamker
Westfälische Wilhelms-Universität Münster
The phenomenon of visual stability of the world during saccades is commonly called “position constancy”.
Classically, the subtraction theory assumes a continuous extraretinal eye position signal (EEPS) which –
if it is tuned perfectly to the eye movements – compensates any saccade-induced movements of stimuli on
the retina. A phenomenon which suggests that the stipulated EEPS does not follow the eye movements
perfectly is the mislocalization of perisaccadic flashes. Experimental ﬁndings indicate that the time course
of the EEPS is much smoother than the actual eye movements [1, 2]. A major problem of the continuous
EEPS is that there has been no neurophysiological evidence for it so far.
We propose a different model for an eye position signal to explain the mislocalization of perisaccadic ﬂashes
in total darkness. The idea is that the eye position signal does not represent the continuous movement of
the eye but only its start and end positions (thus it is binary). When the eye is at rest there is an eye
position signal representing its position. At a certain time t1 before or during a saccade this signal goes off
while at another time t2 the signal for the eye position after the saccade goes on. At this point we make
no assumption whether there is an overlap of both signals (t2 < t1 ) or a gap without a signal (t1 < t2 ),
although neurophysiological ﬁndings from [3] suggest the latter. For our simulations we used a dynamic
neural network based on basis functions similar to [4] to perform the actual transformations from retinal to
head-centered coordinates. For simplicity the visual space has been modelled 1-dimensionally. We followed
the experimental procedure in [1] and tuned the timing parameters of the binary eye position signal to ﬁt the
experimental data. Furthermore, we wanted to stay compatible with [3], if possible.
In the overlap condition (t2 < t1 ) a ﬁt to the data from [1] was easily achieved. This was not possible in
the gap condition (t1 < t2 ) with a zero activity of the eye position signal in the gap. When we allowed a
small activity (10% of the normal activity) at both positions during the gap we achieved results similar to
the overlap condition while remaining compatible with the results from [3]. We conclude that a binary eye
position signal can explain mislocalization of perisaccadic ﬂashes while being more neurophysiologically
plausible than a continuous EEPS.
Acknowledgments
This work has been supported by the Federal Ministry of Education and Research grant
(BMBF 01GW0653).
References
[1] The time courses of visual mislocalization and of extraretinal eye position signals at the time of vertical
saccades. H. Honda, Vision Res. 31:1915-1921, 1991.
[2] Models of the mechanism underlying perceived location of a perisaccadic ﬂash. J. Pola, Vision Research
44:2799-2813, 2004.
[3] The proprioceptive representation of eye position in monkey primary somatosensory cortex. X. Wang,
M. Zhang, I.S. Cohen, and M.E. Goldberg, Nature Neuroscience 10:640-646, 2007.
[4] A computational perspective on the neural basis of multisensory spatial representations. A. Pouget, S.
Deneve, and J. Duhamel, Nature Reviews Neuroscience, 3:741-747, 2002.

84

Cosyne 2008

Thursday evening, Poster session I-77

Self-organization in networks of dissociated cortical neurons: an
experimental and modeling approach
Paolo Massobrio1, Valentina Pasquale2, Michela Chiappalone2, Luca
Leonardo Bologna2 and Sergio Martinoia1
1

Neuroengineering and Bio-nanoTechnology Group (NBT), Department of
Biophysical and Electronic Engineering (DIBE), University of Genova - Via Opera
Pia 11A, 16145 Genova – Italy, 2Neuroscience and Brain Technology Department,
Italian Institute of Technology (IIT) - Via Morego 30, 16163 Genova-Italy.
Understanding the ability of the human brain to effectively encode and transmit time-varying stimuli
coming from the world surrounding us is a challenge for many neuroscientists: recently, self-organized
criticality theory [1] has been applied to the study of neuronal networks, as complex systems composed
by many interacting non-linear units. In fact, a recurrent idea in the study of complex systems is that
optimal information processing is to be found near phase transitions or critical states. The appearance of
spreading electrical activity, organized in the form of so-called neuronal avalanches of local field
potentials whose distribution follows a critical power law, in acute and organotypic cortical slices
recorded by means of Micro-Electrode Arrays (MEAs) has been regarded as an evidence of selforganized criticality [2].
We applied the same approach to the analysis of the electrical activity of dissociated cortical neurons
extracted from rat embryos (E18) plated onto MEAs: since a few days after plating, they begin to
reconstruct an active network in vitro, which exhibits both spiking and synchronized bursting activity in
the mature stage of development (3rd-4th week in vitro). We found that these networks are capable of
reproducing spontaneous spiking activity which might evolve in the form of neuronal avalanches; these
cultures present a critical distribution of avalanche sizes and durations at a mature age, supporting the
hypothesis that neurons preserve the capability of self-organizing in an effective system even in vitro.
In order to clarify which factors drive in vitro neuronal networks to display critical, super- or sub-critical
behaviors, we developed a computational model of networks made of 1000 spiking neurons (both
excitatory and inhibitory) [3], by varying the anatomical connectivity. In particular, scale-free, random
and small-world architectures were considered and compared with the functional connectivity obtained by
the experimental data. As expected, we found that a critical distribution of the spiking activity
corresponds to a scale-free topology in the simulated model. In accordance with the modeling hypothesis,
scale-free-like functional connectivity maps were obtained from experimental data by means of crosscorrelation based methods in those cultures showing critical behavior.
References
[1] Self-Organized Criticality. Bak P., Tang C., Wiesenfeld K., Phys Rev A 38(1):364-374, 1988.
[2] Neuronal avalanches in neocortical circuits. Beggs J.M., Plenz D., J Neurosci 23(35):11167-11177,
2003.
[3] Simple model of spiking neurons. Izhikevich E.M., IEEE Trans Neur Net 6:1569-1572, 2003.

85

Thursday evening, Poster session I-78

Cosyne 2008

Designing Neurophysiology Experiments to Optimally Constrain Parametric Receptive Field Models.
Jeremy Lewi1 , Robert Butera1 , and Liam Paninski2
1

Georgia Tech, 2 Columbia University

http://www.lewilab.org
We show how prior knowledge about a neuron’s conditional response function can be used to select optimal
stimuli for ﬁtting a parametric model to a neuron’s response function. Our prior beliefs describe a submanifold in which we expect to ﬁnd the model’s optimal parameters. Stimuli are selected by maximizing the
expected information provided by the response about the model’s parameters [1]. The expected information is computed using the information available in previous experiments and our prior knowledge. Our
algorithm can be used to design optimal experiments for estimating high-dimensional receptive ﬁelds which
have a known parametric structure; e.g., Gabor functions in the case of V1 experiments.
We model a neuron’s response function as a Generalized Linear Model (GLM) [2]. We update a Gaussian
approximation of the posterior on the entire parameter space of the GLM after each observation. We do not
force the posterior to only have support on the submanifold given by our prior because 1) the true receptive
ﬁeld may not lie exactly on the submanifold, but rather only near it in some statistical sense, and 2) we
want to preserve a key log-concavity property of the posterior which ensures there are no local maxima.
We use our beliefs about the submanifold to choose optimal stimuli. We estimate the expected information
using models in the tangent space of the manifold at the projection of the maximum a-posteriori estimate
(MAP) of the parameters onto the manifold. Since our posterior is log-concave, the tangent space in the
neighborhood of the MAP is the set of models which are close to the submanifold and have high probability
under our posterior. For example, if we are ﬁtting a Gabor receptive ﬁeld, the projection of the MAP is
the best Gabor approximation of the MAP. To ﬁnd receptive ﬁelds which are close to the MAP and the
submanifold, we want to perturb the parameters of the Gabor function corresponding to the MAP. We can
approximate the changes in the receptive due to perturbation of the Gabor’s parameters by taking linear
combinations of the partial derivatives of the receptive ﬁeld with respect to the Gabor’s parameters. The
vector space deﬁned by these combinations is the tangent space. By projecting the posterior into the tangent
space, we can restrict ourselves to a set of receptive ﬁelds that have high probability under our posterior and
are close to the manifold at least locally. After projecting the posterior into the tangent space, we can use
the algorithm we introduced in [3] to efﬁciently optimize the stimuli.
We present simulations showing that the incorporation of prior knowledge leads to faster convergence to the
optimal parameter values. For comparison, our simulations also used maximally informative stimuli which
ignored our prior knowledge, as well as random stimuli, and typical stimuli such as drifting gradients.
Acknowledgments JL is supported by the DOE and by the NSF. LP is supported by the NEI and the Gatsby
Foundation.
References
[1] D.J.C. Mackay, Neural Computation 4, 590 (1992).
[2] W. Truccolo, et al., Journal of Neurophysiology 93, 1074 (2005).
[3] J. Lewi, et al., Advances in Neural Information Processing Systems 19 (MIT Press, 2007).

86

Cosyne 2008

Thursday evening, Poster session I-79

The basal ganglia play an important role in conditioned learning. Computational modeling has allowed
exploration of the expected responses generated by such learning. The results gained from these models
must provide biologically plausible hypotheses, and so the models must reliably map to anatomical and
physiological systems. A computational model of basal ganglia function is presented that describes the
role of dopamine acting on neurons in the dorsal striatum and the substantia nigra pars compacta to
generate a conditioned avoidance response. The model rests on the prevailing hypothesis that phasic
nigrostriatial dopamine acts as an error signal in learning that the basal ganglia use for action selection.
The model posits that assemblies of medium spiny
neurons (MSNs) are important to learning an expected
future benefit to the animal for a specific behavior, and
back-chaining of phasic dopamine production being sent
to the striatum influences the neuron assemblies as they
compete for control of observable behavior. In avoidance
conditioning, the control of behavior consists of
suppressing default behaviors that do not successfully
avoid punishment, which allows alternative, unsuppressed
behaviors to emerge sequentially over time. This process
is exemplified by mice in shuttle-box experiments of
conditioned avoidance that exhibit forage, freeze, and flee behaviors in turn, even after learning that a flee
response correctly results in avoiding punishment. The activity of dopamine producing neurons of the
substantia nigra pars compacta in the shuttle-box experiment are simulated by reinforcement learning and
a temporal difference algorithm to replicate the observed sequence of behaviors and a gradual reduction
in latency of the avoidance response. Although we demonstrate that the modeled dopamine activity can
reproduce this phenomenon, there are some physiological limitations to our current model of dopamine
acting directly on MSNs. Reward response models use drastic excitatory spikes in dopamine release as a
central impetus for behavioral changes. Similarly, avoidance response models have implicated dopamine
in generating an inhibitory response during punishment paradigms like conditioned avoidance. However,
baseline levels of dopamine are very small (~0.5 spikes/sec), which means that in excitatory systems the
change from baseline can be very large, but in inhibitory systems the change from baseline of dopamine
release is more limited (Fig 1). This limitation on changes in dopamine firing from baseline means that in
order to produce an inhibitory response of a magnitude on par with an excitatory change, either a
secondary system of inhibition exists, or there is some mechanism of amplifying the dopamine signal.
Fig 1: Magnitudes of dopamine changes from baseline
2.5

spikes/s from baseline

2

1.5

1

0.5

0

-0.5

-1

1

2

3

4

5
6
time, (x 100ms)

7

8

9

10

Since recent literature suggests a role for cholinergic interneurons in
modulating the dopamine activity on MSNs (Fig 2 [1]), we propose an
alteration to our model that includes the activity of cholinergic interneurons
in generating the expectation of future punishment that informs action
selection in conditioned avoidance. The model maintains the characteristics
of preferential action sequences and reduced response latency, and introduces
cholinergic interneurons to amplify the inhibitory signal from dopamine.
1. James M Tepper & J Paul Bolam. Functional diversity and specificity of
neostriatal interneurons. Current Opinion in Neurobiology 2004, 14:685–692

87

Fig 2: Paired Dopamine (DA)
and cholinergic interneuron
(TAN) firing responding to

Thursday evening, Poster session I-80

Cosyne 2008

On the plausibility of the discriminant center-surround hypothesis for visual saliency
Dashan Gao, Vijay Mahadevan, and Nuno Vasconcelos
Statistical Visual Computing Laboratory, Department of Electrical and Computer
Engineering, University of California San Diego
It has been suggested that saliency mechanisms play a role in perceptual organization. This work evaluates
the plausibility of a recently proposed generic principle for visual saliency: that all saliency decisions are
optimal in a decision-theoretic sense. The discriminant saliency hypothesis is combined with the classical
assumption that bottom-up saliency is a center-surround process, to derive a (decision-theoretic) optimal
saliency architecture. Under this architecture, the saliency of each image location is equated to the discriminant power of a set of features with respect to the classiﬁcation problem that opposes stimuli at center and
surround.
In a previous work, we have shown that there is a one-to-one mapping between the discriminant saliency
detector and a neural network that replicates the standard architecture of V1: a cascade of linear ﬁltering,
divisive normalization, quadratic non-linearity and spatial pooling [1].
In this work, we derive optimal saliency detectors for various stimulus modalities, including color, orientation, and motion, and show that these detectors make accurate quantitative predictions of several aspects
of psychophysics of human saliency, for both static and motion stimuli. These include some classical nonlinearities of orientation and motion saliency, and a Weber law that governs various types of saliency asymmetries, which are beyond the reach of previous saliency models. The discriminant saliency detectors are
also applied to various saliency problems of interest in computer vision, including the prediction of human
eye ﬁxations on natural scenes, motion-based saliency in the presence of ego-motion, and background subtraction in highly dynamic scenes. In all cases, the discriminant saliency detectors outperform previously
proposed methods, from both the saliency and the general computer vision literatures.

Acknowledgments
This research was supported by NSF awards IIS-0448609, and IIS-0534985.
References
[1] Decision-theoretic saliency: computational principle, biological plausibility, and implications for neurophysiology and psychophysics. D. Gao and N. Vasconcelos, submitted to Neural Computation, 2007.

88

Cosyne 2008

Thursday evening, Poster session I-81

Probability of Contiguous Sequences in Neural Data
Anne C Smith1 and Peter Smith2
1

University of California, Davis,

2

University of Keele, UK

Detailed analyses of simultaneous multiple neuron recordings have revealed that the temporal order of
neural firing corresponding to behavior is replayed during sleep [1] and reversed during periods of rest
[2]. This temporal replay is believed to be integral to the learning and consolidation process. While these
phenomena are robust, there are potential shortcomings in the analysis techniques used to quantify the
significance of the effect. The need for improved techniques is likely to increase as the number of cells
recorded increases and as the technology is applied across different species.
Here, we reduce the problem to a simple urn problem and present a set probability technique for
estimating the significance of strictly increasing sequences occurring by chance. The problem we wish to
solve can be posed quite succinctly: given a ranked order of numbers, corresponding to the temporal order
of firing of neurons, what is the probability of a obtaining by chance another sequence of j strictly
increasing numbers (possibly with a single interruption) picked from N occurring in a word of length n ?
For short sequences with no interruptions, it is possible to compute a closed form solution. For longer
uninterrupted sequences ( n t 2 j ), we derive upper and lower bounds [3]. For the situation where a short
sequence has a single interruption, it is possible to derive a closed form solution.
We compare our results with the rank-order correlation measure. An advantage of the sequence-based
technique we describe here is that it assigns greater importance to cells that fire in close temporal order at
intervals consistent with the time scale of synaptic modification. In contrast the rank-order correlation
measure can be strongly influenced by a few neurons firing in order but separated by large time intervals.

Acknowledgments
This work was supported by the Department of Anesthesiology and Pain Medicine, UC Davis.

References
[1] Influences of hippocampal place cell firing in the awake state on the activity of these cells during
subsequent sleep episodes. C Pavlides. and J Winson, Journal of Neuroscience, 1989. 9(8): 2907-2918.
[2] Reverse replay of behavioural sequences in hippocampal place cells during the awake state. DJ Foster,
and MA Wilson, Nature, 2006. 440(7084): 680-683.
[3] A set probability technique for detecting relative time order across multiple neurons. AC Smith and P
Smith, Neural Computation, 2006. 18(5): 1197-1214.

89

Thursday evening, Poster session I-82

Cosyne 2008

Learning Sparse Representations for Audiovisual Signals
Gianluca Monaci1 and Friedrich T. Sommer1
1

Redwood Center, University of California Berkeley

So far, the majority of studies on perception have focused on one single sensory modality. However, a
variety of cross-modal integration phenomena occur at various processing levels in the brain [1,2,3,4]. In
particular, there are evidences supporting the idea of low-level multisensory integration [1,2].
In this work we focus on audiovisual data and we propose a generative model for audiovisual signals based
on the “sparse coding” paradigm [4] that accounts for early fusion between audio and video modalities. An
(a) (v)
audiovisual signal s = (a(t), v(x, y, t)) is modeled as a sparse sum of audiovisual kernels φk = (φk , φk ).
Such kernels are bimodal functions made of synchronous (i.e. sharing the same temporal support) audiovideo components. Each kernel can be placed independently and arbitrarily in space and time. This yields:
s(x, y, t) = (a(t), v(x, y, t)) ≈



(a) (a)

(v) (v)

(ci,k φk (t − τi,k ), ci,k φk (x − χi,k , y − υi,k , t − τi,k )) ,

(m)

where ci,k , m = a, v, are the coefﬁcients and (χi,k , υi,k , τi,k ) the spatio-temporal translations of the ith
instance of kernel φk . Note that translation τi,k is the same for both modalities to ensure synchronicity.
We design an algorithm capable of learning sets of such audiovisual, synchronous, shift-invariant functions
(m)
by alternatively solving a coding and a learning step. In the coding part, the optimal weights ci,k and spatiotemporal translations (χi,k , υi,k , τi,k ) are found using an algorithm inspired by Simultaneous Orthogonal
Matching Pursuit method [5]. Two different techniques are compared for learning the collection of basis
functions: the gradient descent method [4] and the K-SVD algorithm [6].
To test the proposed model we build a database of synthetic data representing a “cartoon world” of audiovisual objects composed of geometric shapes and sinusoidal pulses. The codes learned on such database
represent meaningful audiovisual events present in the cartoon world. When used to encode ambiguous
motion display stimuli [3], the model qualitatively mimics audiovisual interactions in human perception.
Acknowledgments
This work was supported by the Swiss NSF through the prospective researcher grant n◦ PBEL2-118742.
References
[1] Seeing sounds: visual and auditory interactions in the brain. D. A. Bulkin and J. M. Groh, Current Opinion in
Neurobiology 16(4):415–419, 2006.
[2] Multisensory contributions to low-level, unisensory processing. C. E. Schroeder and J. Foxe, Current Opinion in
Neurobiology 15(4):454–458, 2005.
[3] Sensory modalities are not separate modalities: plasticity and interactions. S. Shimojo and L. Shams, Current
Opinion in Neurobiology 11(4):505–509, 2001.
[4] Sound alters visual motion perception. R. Sekuler , A. B. Sekuler and R. Lau, Nature 385(6614):308, 1997.
[5] Sparse codes and spikes. B. A. Olshausen, In: Probabilistic Models of the Brain: Perception and Neural Function,
R. P. N. Rao, B. A. Olshausen and M. S. Lewicki Eds., MIT Press 257–272, 2002.
[6] Algorithms for simultaneous sparse approximation. Part I and Part II. J. A. Tropp, A. C. Gilbert and M. J. Strauss,
Signal Processing 3(86):572–602, 2006.
[7] The K-SVD: an algorithm for designing of overcomplete dictionaries for sparse representation. M. Aharon, M. Elad
and A. M. Bruckstein, IEEE Trans. on Signal Processing 11(54):4311–4322, 2006.

90

Cosyne 2008

Thursday evening, Poster session I-83

Correlated activity in threshold-crossing spiking neurons
Yoram Burak1, Sam Lewallen2, and Haim Sompolinsky1,3
1

Center for Brain Science, Harvard University, 2Faculty of Arts and Sciences,
Harvard University, 3Interdisciplinary Center for Neural Computation, Hebrew
University
The temporal fluctuations of a common input are often responsible for the correlation in the activity of
neurons. Thus, the statistical properties of spike trains generated by neurons that respond to a common
input are of significant theoretical and experimental interest. We consider threshold-crossing spiking
processes as simple models for the activity within a population of neurons. Assuming that these neurons
are driven by a fluctuating input with Gaussian statistics, we evaluate several moments of the generated
spike trains. In particular, we calculate the cross correlation function of pairs of model neurons sharing
overlapping inputs, and its asymptotic behavior at small and large time separations. This correlation
function can be asymmetric in time, indicating a preference for one neuron to fire later than another one,
even if their inputs are identical. We also consider the spike-triggered average stimulus for uncorrelated
white noise stimuli, showing that it can differ considerably from the prediction of a standard rate-based
linear-nonlinear model [1]. Finally, we use this theoretical framework to investigate the degree of
accuracy by which the spike trains generated by an ensemble of such neurons can encode a shared,
smooth input signal.

Acknowledgments
We thank M. Meister for helpful discussions. This work was supported by the Swartz Foundation.
References
[1] A simple white noise analysis of neuronal light responses. E. J. Chichilnisky, Network: Comput.
Neural. Syst. 12:199-213, 2003.

91

Thursday evening, Poster session I-84

Cosyne 2008

Single Trial Multicomponent ERP Estimation via Nonparametric Entropy Minimization
Ramanarayan Vasudevan1 , Parvez Ahammad1 , Shankar Sastry1 and Ruzena
Bajcsy1
1

University of California at Berkeley

Analysis of Event-Related Potentials (ERPs) recorded during repeated presentations of a sensory stimulus
or task performance is critical in domains such as neurophysiology. The measurements are generally modeled as a dynamical interaction between signals that are relatively phase-locked to a speciﬁc event onset
and signals that are not phase-locked to the event, such as measurement noise or ongoing brain activity.
The phase-locked signal may have trial-to-trial variability in amplitude and latency and may in fact be the
superposition of multiple components with differential variability in their single trial amplitude scaling factors and latency shifts. Many ignore this fact and resort to extracting the event-related signal as an average
across the ensemble of trials (AERP). Recently, models such as differentially Variable Component Analysis
(dVCA) [1] have shown promise at accurately determining this phase-locked signal. While dVCA is more
realistic than AERP, it sufffers from two shortcomings. First, it requires an a priori knowledge of either the
number of components within the phase-locked signal or of the ratio between the phase-locked and ongoing
process signals. Second, it assumes that the trial-to-trial variability in amplitude and latency is Gaussian.
We propose an unsupervised learning algorithm to extract the
phase-locked component as a MAP estimate of the phase-locked
component given the set of observed signals. The resulting optimization proceeds by minimizing the penalized summed joint entropy across the ensemble of signals in the space of latency and
scaling parameters which effect each observation. We show the results of the algorithm when compared to dVCA in two instances in
the accompanying ﬁgure. The underlying signal generator is drawn
in red on all plots. Several example sample trials are illustrated in
the top two plots, and the results of both alignment procedures are
illustrated in the bottom two plots. In the left plots, the signal is
generated by ﬁve Gaussian components that have variable latencies
and scalings that are drawn from a Gaussian distribution. Note,
that it looks as if there are only two components. The root mean
square error (RMSE) of dVCA with two components is 21.75%,
the RMSE of dVCA with ﬁve components is 12.1%, and the RMSE of our algorithm is 8.05%. In the right
plots, the signal is generated by two Gaussian components that have variable latencies and scalings that are
drawn from a uniform distribution. Note that the RMSE of dVCA with two components is 10.63%, the
RMSE of dVCA with ﬁve components is 7.58%, and the RMSE of our algorithm is 3.36%. Our results
show that improved estimates of the phase-locked component can be achieved in a completely unsupervised
fashion, without making any parametric assumptions about the underlying signal.
1

Amplitude (arbitrary units)

Amplitude (arbitrary units)

1
0

−1
−2

Generator
Trial 17
Trial 59
Trial 192

−3
−4

−5
−0.1

0

0.1

0.2
0.3
0.4
Time (seconds)

0.5

0.5
0

−0.5

0.6

Generator
Trial 17
Trial 59
Trial 192

−1

−1.5
−0.1

0

0.1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.6

0.5

0.4

0

Amplitude (arbitrary units)

Amplitude (arbitrary units)

0.2

−0.5

−1

0

−0.2

−0.4

−1.5

−0.6

Generator
dVCA = 5
dVCA = 2
Entropy Alignment

−2
−0.1

0

0.1

0.2
0.3
0.4
Time (seconds)

0.5

Generator
dVCA = 5
dVCA = 2
Entropy Alignment

0.6

−0.8
−0.1

0.2
0.3
0.4
Time (seconds)

0.5

References
[1] K.H. Knuth et al. Differentially Variable Component Analysis: Identifying Multiple Evoked Components
Using Trial-to-Trial Variability, 2006.

92

0.6

Cosyne 2008

Thursday evening, Poster session I-85

Neural model for the visual recognition of actions
Falk Fleischer1, Antonino Casile1, and Martin Giese1,2
1
Dept. of Cognitive Neurology, Hertie Institute for Clinical Brain Research,
Tübingen, Germany 2 School of Psychology, University of Bangor, UK
The visual recognition of goal-directed movements is crucial for the learning of actions, and possibly for
the understanding of the intentions and goals of others. The discovery of mirror neurons has stimulated a
vast amount of research investigating possible links between action perception and action execution [1,2].
However, it remains largely unknown what is the real extent of this putative visuo-motor interaction
during visual perception of actions and which relevant computational functions are instead accomplished
by possibly purely visual processing.
Here, we present a neurophysiologically inspired model for the recognition of hand movements
demonstrating that a substantial degree of performance can be accomplished by the analysis of spatiotemporal visual features within a hierarchical neural system that reproduces fundamental properties of the
visual pathway and premotor cortex. The model integrates several physiologically plausible
computational mechanisms within a common architecture that is suitable for the recognition of grasping
actions from real videos: (1) A hierarchical neural architecture that extracts form and motion features
with position and scale invariance by subsequent increase of feature complexity and invariance along the
hierarchy [3,4,5]. (2) Learning of optimized features on different hierarchy levels using a trace learning
rule that eliminates features which are not contributing to correct classification results [5]. (3) Simple
recurrent neural circuits for the realization of temporal sequence selectivity [6,7,8]. (4) As novel
computational function the model implements a plausible mechanism that combines the spatial
information about goal object and its affordance and the specific posture, position and orientation of the
effector (hand). The model is evaluated on video sequences of both monkey and human grasping actions.
The model demonstrates that simple well-established physiologically plausible mechanisms account for
important aspects of visual action recognition. Specifically, the proposed model does not contain explicit
3D representations of objects and the action. Instead, it realizes predictions over time based on learned
pattern sequences arising in the visual input. Our results complement those of existing models [9] and
motivate a more detailed analysis of the complementary contributions of visual pattern analysis and motor
representations on the visual recognition of imitable actions.
Acknowledgments
Supported by DFG, the Volkswagenstiftung, and Hermann und Lilly Schilling Foundation.
References
[1] Di Pellegrino, G. et al. (1992): Exp. Brain Res. 91, 176-180.
[2] Rizzolatti, G. and Craighero, L. (2004): Annu. Rev. Neurosci. 27, 169-192.
[3] Riesenhuber, M. and Poggio, T. (1999): Nat. Neurosci. 2, 1019-1025.
[4] Giese, A.M. and Poggio, T. (2003): Nat. Rev. Neurosci. 4, 179-192.
[5] Serre, T. et al. (2007): IEEE Pattern Anal. Mach. Int. 29, 411-426.
[6] Zhang, K. (1996): J. Neurosci. 16, 2112-2126.
[7] Hopfield, J. and Brody, D. (2000): Proc Natl Acad Sci USA 97, 13919-13924.
[8] Xie, X. and Giese, M. (2002): Phys Rev E Stat Nonlin Soft Matter Phys 65, 051904.
[9] Oztop, E. et al. (2006): Neural Netw. 19, 254-271.

93

Thursday evening, Poster session I-86

Cosyne 2008

Modeling Electrocortical Activity by Integral Neural Field
Equations
S Coombes1 , N A Venkov1 , L Shiau2 , I Bojak3 , D T J Liley4 and C R Laing5
University of Nottingham, UK. 2 University of Houston, USA. 3 Radboud University Nijmegen Medical Centre, NL. 4 Brain Sciences Institute, Swinburne University of Technology, AU. 5 Massey University, NZ.
1

In many regions of mammalian neocortex, synaptic connectivity patterns follow a laminar arrangement,
with strong vertical coupling between layers. Consequently cortical activity is considered as occurring on a
two dimensional plane, with the coupling between layers ensuring near instantaneous vertical propagation.
Hence, it is highly desirable to obtain solutions to fully planar neural ﬁeld models.
The most popular Wilson-Cowan [1] neural ﬁeld models are in integro-differential equations. Recently there
has been a growing interest in neural ﬁeld models where the communication between different parts of the
domain is delayed due to the ﬁnite conduction speed of action potentials [2,3]. New progress has been
made in one spatial dimension with axonal delays, and in two spatial dimensions for models lacking axonal
delays. In both cases, integral transform techniques are exploited and a partial differential equation (PDE)
description is obtained. In this study, we address the physiologically important case of a model in two spatial
dimensions with axonal delays and to obtain an equivalent PDE model.
Our analysis avoids the so-called long-wavelength approximation that has previously been used to formulate
PDE models for neural activity in two spatial dimensions. This PDE has been shown to provide a longwavelength approximation to the underlying integral model, and equations of this type have been used in
several EEG modeling studies. Direct numerical simulations of this PDE model show instabilities of the
homogeneous steady state that are in full agreement with a Turing instability analysis of the original integral
model. We discuss the beneﬁts of such a local model and its usefulness in modeling electrocortical activity.
In particular we are able to treat “patchy” connections, whereby a homogeneous and isotropic system is
modulated in a spatially periodic fashion. In this case the emergence of a “lattice-directed” traveling wave
predicted by a linear instability analysis is conﬁrmed by the numerical simulation of an appropriate set of
coupled PDEs. Importantly we show that numerical simulations of the PDE model are in precise agreement
with the behavior predicted at the onset of a Turing instability.
Acknowledgments
This work is supported by EPSRC (SC), NSF (LS) and Australian Research Council (IB).
References
[1] A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue. H R Wilson
and J D Cowan, Kybernetik,13:55–80, 1973.
[2] Spatially structured activity in synaptically coupled neuronal networks: I. Travelling fronts and pulses.
D J Pinto and G B Ermentrout, SIAM Journal on Applied Mathematics,62:206–225, 2001.
[3] Waves and bumps in neuronal networks with axo-dendritic synaptic interactions. S Coombes, G J Lord,
and M R Owen, Physica D,178:219–241, 2003.

94

Cosyne 2008

Thursday evening, Poster session I-87

A biologically based model of the two-phase theory of classical
conditioning: The amygdala, auditory cortex and cerebellum.
Ivan Herreros-Alonso1 , Lukas Zimmerli2 , and Paul Verschure1,3
1

Laboratory of Synthetic Perceptive, Emotive and Cognitive Systems, Institut of
Audiovisual Studies, Universitat Pompeu Fabra, 2 Hocoma, Volketswil, Switzerland, 3 Catalan Institute of Advanced Studies (ICREA), Barcelona, Spain
The Polish psychologist Konorski has proposed in the early 1960ies that the associative processes underlying
classical conditioning can be separated in a fast non-speciﬁc learning systems (NLS) and a slow speciﬁc one
(SLS). In earlier work we have successfully individually modeled the roles of the amygdala, basal forebrain
and auditory cortex (AC) as an example of NLS and the cerebellum (CE) as a model of SLS 1,2 . Here we
provide a complete account of Konorski’s proposal by integrating these two systems and thus provide the
ﬁrst complete biologically-grounded computational model of the two-phase theory of conditioning.
AC learning has been simulated using a biophysically realistic learning rule, where neuron’s plasticity is
controlled locally by a spike-time dependent learning rule (STDP). The occurrence of a unconditioned stimulus (US), signaled by a burst of activity in the model amygdala/nucleus basalis, switches a bigger fraction
of synapses in the A1 into a plastic mode. As a result CS representations are enlarged, thus resulting in a
higher global response to a CS stimulus At the level of the cerebellum, CS and US once more converge.
Coincidence of these signals induce LTD at the level of the parallel ﬁber - Purkinje cell (PU) synapse. Due
to this LTD the PU will cease to ﬁre dis-inhibiting the neurons of the model deep cerebellar nucleus (DN),
triggering a conditioned response (CR) that is matched to the experimental interstimulus interval (ISI).
Putting together both models requires solving the process of information transmission between the NLS and
the SLS. The output of the AC comprises a continuous ﬂow of information, whereas for our CE model to
learn, a CS must be signaled by a discrete burst of activity. We investigate the hypothesis that the suitable
transformation can be performed by the granule cells: 1) the mossyﬁbre-granule synapse acts as a ﬁlter
allowing to pass only high amplitude signals3 . 2) the tonic and spillover inhibition observed in the CE4 are
used to interrupt the information ﬂow as soon as some action potentials are generated in direction to the PU.
The whole computational model is tested with simulated conditioning experiments and its real-time performance and ecological validity is tested using robot object avoidance tasks where the US is signaled with a
particular tone and the CR is expressed as an obstacle dependent modiﬁcation in the robot’s trajectory.
References
[1] Learning sensory maps with real-world stimuli in real time using a biophysically realistic learning rule.
M.A. Sanchez-Montanes, P. Koenig and P.F.M.J. Verschure, IEEE Trans. on Neural Networks 13:619-632,
May 2002.
[2] The cerebellum in action: a simulation and a robotics study. C. Hofstoffer, M. Mintz and P.F.M.J.
Verschure. European Journal of Neuroscience 16:1361-1376, July 2002.
[3] Integration of quanta in cerebellar granule cells during sensory processing. P. Chadderton, T.W. Margrie
and M. Hausser, Nature 428:856-860, April 2004.
[4] Tonic and spillover inhibition of granule cells control information ﬂow through cerebellar cortex. M.
Hamann, D.J. Rossi and D. Attwell, Neuron 33:625-633, February 2002.

95

Thursday evening, Poster session I-88

Cosyne 2008

How to combine old and new sensory information?
A. Aldo Faisal1, Daniel M. Wolpert1
1

University of Cambridge, Department of Engineering,
Computational & Biological Learning group

How should sensory information obtained at different points in time be combined? We propose a
simple stochastic model of how sensory estimates “age” with time. This model suggests how
asynchronously obtained information could be combined across time to form.
Our senses provide us with information about the world with a certain amount of uncertainty. By
combining sensory information from different sources we can reduce overall uncertainty when
estimating the state of the world. It was shown that humans integrate synchronously presented
sensory cues (e.g. proprioception/vision [1-4]) to form a statistically optimal estimate: each
sensory source is weighted by its uncertainty (here, the variance of the single cue estimation).
These existing studies assume that sensory information is obtained simultaneously, however, we
mostly act in natural settings where multiple sensory cues are experienced at different times and
durations.
How should information from sensory observations be combined with newer information to form
a combined estimate for immediate action? To be able to relate a sensory estimate to one
obtained at a later point in time we have to describe how the passing of time affected the earlier
estimate. We assume that the value of sensory estimation decreases over time (“aging”), due to
either random variability in the world or inside the observer, or systematic changes in the world.
On short time scales of seconds (and below) we assume that these changes will be composed of
linear changes proportional to the time passed (“linear drift”) and fluctuating changes that add up
over time (“diffusion”). We model the sensory estimator queried at a given time by a probability
distribution describing the estimated sensory value. Passing of time affects this sensory
estimate’s probability distribution in a manner described by a Fokker-Planck equation acting
upon the probability distribution (thus capturing the notion of linear drift and diffusive
fluctuations). This approach allows us to combine two sensory estimates obtained at different
points in time, by integrating over time the older estimate’s probability distribution using the
Fokker-Planck equation yielding a time-adjusted version of the old estimates probability
distribution so we can combine it with the recent sensory estimate.
We also provide an analytically tractable example for the case of Gaussian distributed estimators
and outline a sensorimotor task where we can test our assumptions of how sensory information is
combined over time.
Acknowledgments: This work was supported by the Wellcome Trust.
[1] van Beers RJ, Sittig AC & Denier van der Gon JJ (1996), Exp Brain Res 111, 253-261
[2] van Beers RJ, Sittig AC & Gon JJ (1999), J Neurophysiol 81, 1355-1364.
[3] Jacobs RA (1999). Optimal integration of texture and motion cues to depth. Vision Res 39, 3621-3629
[4] M. O. Ernst, and M. S. Banks (2002), Nature 415, 42

96

Cosyne 2008

Thursday evening, Poster session I-89

A mechanism for achieving zero-lag long-range synchronization
of neural activity.
Gordon Pipa1,2,3 , Raul Vicente1,2 , Leonardo Gollo4 , Claudio Mirasso4 , and
Ingo Fischer5
1

Department of Neurophysiology, Max-Planck Institute for Brain Research,
2
Frankfurt Institute for Advanced Studies
Frankfurt am Main, Germany,
3
(FIAS), Frankfurt am Main, Germany,
Massachusetts Institute of Technol4
ogy (MIT) and Massachusetts General Hospital (MGH), USA,
Universitat de
5
Heriot Watt University,
les Illes Balears & IFISC, Palma de Mallorca, Spain,
Edinburgh, United Kingdom.
How can two distant neural assemblies synchronize their ﬁrings at zero-lag even in the presence of nonnegligible delays in the transfer of information between them? Neural synchronization stands today as one
of the most promising mechanisms to counterbalance the huge anatomical and functional specialization of
the different brain areas. However, and albeit more evidence is being accumulated in favor of its functional
role as a binding mechanism of distributed neural responses, the physical and anatomical substrate for such
a dynamic and precise synchrony, especially zero-lag even in the presence of non-negligible delays, remains
unclear.
Here we propose a simple network motif that naturally accounts for zero-lag synchronization of spiking
assemblies of neurons for a wide range of temporal delays. We demonstrate that when two distant neural
assemblies do not interact directly but relaying their dynamics via a third mediating single neuron or population then they eventually achieve a zero-lag coherent ﬁring. Extensive numerical simulations of populations
of Hodgking-Huxley neurons interacting in such a network are analyzed. The results show that even with
axonal delays as large as 15 ms the distant neural populations can synchronize their ﬁrings at zero-lag in
a millisecond precision after the exchange of a few spikes. The role of noise and a distribution of axonal
delays in the synchronized dynamics of the neural populations are also studied conﬁrming the robustness of
this sync mechanism. The proposed network module is densely embedded within the complex functional
architecture of the brain and especially within the reciprocal thalamocortical interactions where the role of
indirect pathways mimicking direct cortico-cortical ﬁbers has been already suggested to facilitate trans-areal
cortical communication.
In summary the robust neural synchronization mechanism presented here arises as a consequence of the
relay and redistribution of the dynamics performed by a mediating neuronal population. In opposition to
previous works, neither inhibitory, gap junctions, nor complex networks need to be invoked to provide a
stable mechanism of zero-phase correlated activity of neural populations in the presence of large conduction
delays.
Acknowledgments
This work was partially supported by the Hertie Stiftung and the EC Project GABA (contract 043309).
References
[1] Zero-lag long-range synchronization via dynamical relaying.
(97):123902, 2006.

97

I. Fischer et al, Phys.

Rev.

Lett.

Thursday evening, Poster session I-90

Cosyne 2008

A Fast Algorithm to Compute First-Passage Times for Gaussian
Markov Processes
Thibaud Taillefumier1 and Marcelo Magnasco1
1

The Rockefeller University

Integrator models are widely used to simulate the stochastic activity of neurons. In such models, a random
variable which can be seen as a membrane potential or a calcium concentration, represents the internal state
of the cell. Its stochastic evolution is driven by a diffusion equation which integrates the contribution of the
neuron’s inputs. When the state variable reaches a threshold, the cell triggers an instantaneous response under the form of an action potential or vesicle release. To characterize the temporal response of the integrator
neurons, we need evaluating the distribution of these trigger events.
Formally, it amounts to computing the distribution of ﬁrst-passage times for a continuous Gaussian Markov
process with a continuous boundary. Unfortunately, even for simple problems such as the OrnsteinUhlenbeck process with a constant threshold, there exists no closed form solutions. Resorting to numerical method, the usual Euler method to simulate diffusion processes proves limited in term of accuracy
and speed of calculation. Here, we present a new fast algorithm to compute the ﬁrst-passage times of a
continuous Gaussian Markov process with an arbitrary continuous boundary.
The classical Haar construction of Brownian motion uses a binary tree of triangular wedge-shaped functions. We introduce a similar basis for continuous Gaussian Markov processes, in which the basis elements
preserve the properties of the Haar basis: all basis elements have compact support on an open interval with
dyadic rational endpoints; these intervals are nested and become smaller for larger indices of the basis element, and for any dyadic rational, only a ﬁnite number of basis elements is nonzero at that number. The
expansion in this Haar-like basis, when evaluated at a dyadic rational, terminates in a ﬁnite number of steps,
thus providing us with an exact schema to simulate sample paths.
Moreover, assuming the value of a sample path at two times, we can ﬁnd an upper-bound for the probability
of the sample path to cross the boundary between these two times. A “dichotomic search” algorithm computes the ﬁrst-passage times: at a given step, we have simulated a sample path using the basis elements up to
a certain depth of the binary tree; we estimate the probability that a ﬁrst-passage time occurs in the supports
of our basis elements at this depth; we then discard the supports which are unlikely to contain a crossing
event and deepen our expansion of the sample path in the ﬁrst of the remaining supports.
To sum up, contrary to the Euler method, we only simulate exact sample paths in time regions where a
ﬁrst-passage is likely to occur. The evaluation of spikes timing results much more efﬁcient in terms of speed
and accuracy. The amount of generated data makes it possible to envision an information study of the leaky
integrate-and-ﬁre model.

Acknowledgments
We thank Mariela Sued and Daniel Andor. This work has been supported in part by NIH under grant R01DC07294.

98

Cosyne 2008

Thursday evening, Poster session I-91

Robust Bayesian framework for modeling psychophysical tasks
Peter W. Battaglia1 and Paul R. Schrater1
1

University of Minnesota

Bayesian models of perception and action offer powerful tools for studying behavior and have achieved
notable successes when combined with experimental investigation [1]. By representing sensory noise,
motor imprecision, and internal processing strategies using parameters in a principled model, they may be
quantified and empirically measured. One major challenge facing this approach is that even for simple
behaviors, Bayesian models can be quite complex and have many parameters. Conducting a single
experiment may not distinguish between ambiguous sets of parameter values, analogous to being unable
to solve for two unknowns in one algebraic equation. Many studies make simplifying assumptions to
effectively reduce the number of unknowns, and/or conduct baseline experiments to increase the number
of effective equations. In an attempt to formalize such strategies, we propose a robust Bayesian
framework in which perception and action are explicitly separated in the model structure. This renders
many complex models experimentally tractable because multiple experiments can be integrated to jointly
infer parameter values. We apply our methodology to a set of psychophysical experiments on human
object size and distance perception to demonstrate how otherwise confounded parameters can be
measured.
Classical decision-making models [2] treat observers’ responses as functions of sensory measurements
(decision rules). In contrast, our approach assumes the brain uses sensory measurements to compute
general beliefs about the world state, which are then used to select responses. Assuming the perception
and action (decision) components of behavior are distinct is a natural and common scientific practice;
many experimental investigations implicitly make this assumption.
For example, measuring
discrimination thresholds independently to predict some composite behavior (like cue integration)
assumes stimulus discrimination is constant regardless of task.
Our framework specifies two brain processing components that lead to any psychophysical response:
using sensory data to infer a posterior distribution over world states followed by the application of a loss
function and selection of a response that maximizes the expected reward given the posterior and loss
function. The relationships between sensory measurements and the posterior distribution are expressed as
conditional distributions with perceptual parameters; the relationships between the posterior distribution
and action decision are conditional distributions with action parameters. The novelty of our proposal is
to explicitly assume that perceptual parameters are constant, while action parameters vary, across tasks.
Merging data from multiple tasks can be accomplished by controlling for specific action parameters while
coupling perceptual parameters. In this manner perceptual parameters can be estimated through standard
methods like maximum-likelihood estimation or full Bayesian analysis. We apply our approach to a
novel spatial perception psychophysical study to illustrate its utility in testing complex Bayesian models.
Acknowledgments
This work was supported by NIH grant R01EY015261 and a NSF Graduate Student Fellowship.
References
[1] Knill DC, Pouget A (2004) The Bayesian brain: The role of uncertainty in neural coding and
computation for perception and action, Trends in Neuroscience, 27(12): 712-719.
[2] Green DM, Swets JM (1966) Signal detection theory and psychophysics. New York: John Wiley.

99

Thursday evening, Poster session I-92

Cosyne 2008

D etecting simultaneous spikes in multi-neuron
recordings using a generative model of electrode
data
J. Pillow 1 , C . Bako litsa 2 , E .J. C hic hilnisky 2 , and E.P. Simoncelli 3
1

Gatsby Computational Neuroscience Unit UCL, 2SNL-E The Salk Institute,
3
HHMI/NYU
The study of information processing mechanisms within the nervous system is greatly enhanced by
monitoring the activity of multiple components of a neural circuit in parallel. Developed over a decade
ago, multi-electrode array technology has enabled this task, allowing for simultaneous extra-cellular
recordings from large neuronal populations. However, detecting and classifying the action potentials of
neurons -a process referred to as spike sorting- presents a problem in such systems, especially in cases
where two or more neurons fire synchronously resulting in overlapping voltage waveforms. The problem
is further compounded by the presence of unknown background noise, both instrumental and
physiological, included in the spike trains. A number of approaches have been developed to address these
difficulties but no general solution has emerged (see [1] for review).
We report a new method for multi-neuron spike sorting that follows from a simple generative model of
the recorded data. We assume that the recorded data, y, is a linear combination of the net spike-induced
voltage deflections W*x, where W denotes the spike-induced voltage waveforms associated with each
cell across all electrodes, x an initial estimate of the spike times, and independent Gaussian noise, , such
that y=W*x+. By setting a Bernouli prior over spike trains, we obtain a posterior distribution whose
maximum (the maximum a posteriori, or MAP estimate) corresponds to the most likely spike train under
the model given the observed data. Note that estimation of W (or x) is a linear inverse problem when x (or
W, respectively) is known, and thus the overall estimation may proceed by alternating between solving
these two sub-problems.
Similar statistical models have been previously described ([1], [2]). The strength of our approach relies on
making full use of the knowledge provided by an initial (clustering-based) spike sorting procedure and
robust estimation of parameters governing the noise process. Starting from a given set of well-isolated
spikes, we estimate the waveform for each cell using the likelihood function. We use the mean absolute
deviation to determine the noise variance in the data, since this estimator is relatively insensitive to the
outliers in y that are due to undetected spikes. Additionally, a better model of the residuals is obtained by
whitening them in space and time, since the assumption of independence expressed in our likelihood
function is then closer to being correct.
We evaluate the performance of our sorting algorithm using real and simulated data, compare different
methods for maximizing the posterior, and examine how the choice of priors influences performance.
References
[1] Lewicki MS. (1998) A review of methods for spike sorting: the detection and classification of neural
action potentials. Network 9:R53-78.
[2] Sahani M. et al. (1998) On the separation of signals from neighboring cells in tetrode recordings.
Advances in Neural Information Processing Systems 10 MIT Press, Cambridge, MA.

100

Cosyne 2008

Thursday evening, Poster session I-93

Phase Response Curves in the Presence of Noise: A Study of
Purkinje Cell Oscillatory Dynamics
Elena Phoka1,2,3, Hermann Cuntz1,3, Arnd Roth1 and Michael Häusser1
1

Wolfson Institute for Biomedical Research, University College London
Department of Bioengineering, Imperial College London
3
These authors contributed equally to this work
2

The phase response curve (PRC) characterizes the effect of a short perturbation on the phase of an
oscillator [1,2]. Cerebellar Purkinje cells (PCs) are particularly suited for a description as oscillators due
to their regular spontaneous firing in the absence of synaptic input [3]. Conventionally, neuronal PRCs
are obtained by repeatedly injecting brief current pulses while neurons are firing action potentials (APs).
Phase and phase perturbation are measured in reference to the APs immediately preceding and
immediately following the current pulse. However, using this conventional method, we show in
electrophysiological experiments and in simulations that the noise present in PCs introduces a systematic
bias, leading to the miscalculation of the PRC. This bias is a result of a loss of causality caused by the
jitter of the reference APs around the current pulse. We show here how this bias can be removed by
obtaining PRC values using all APs of a spike train as a reference, one at a time. Averaging over
thousands of repetitions was necessary to obtain PRCs with reasonable signal-to-noise ratio.
Figure 1. PC PRCs depend on the intrinsic firing
rate. A) At high frequency rates (160 Hz) PCs
exhibit typical monophasic PRCs. B) At low firing
rates (30 Hz) PCs show behavior similar to nonleaky integrate-and-fire neurons.

Using this new approach, we show that the shape of the PRC changes fundamentally depending on the
firing rate of the cell. At higher firing rates, PCs exhibit typical monophasic PRCs (Figure 1A). On the
other hand, at low firing rates, PC PRCs are largely phase-independent, as would be the case in ideal nonleaky integrate-and-fire neurons (Figure 1B). This suggests that the sensitivity of PC responses to the
timing of synaptic inputs depends on the firing rate of the PC.
Acknowledgments
We thank J. Davie, A. Mathy and M. Barahona for helpful discussions. We are grateful for support from
the Wellcome Trust and Gatsby Foundation.
References
[1] Two modes of interspike interval shortening by brief transient depolarizations in cat neocortical
neurons. Reyes A.D., Fetz E.E., J.Neurophysiol. 69 (5): 1661-1672, May 1993.
[2] Chapter 10: Synchronization, in Izhikevich E.M. Dynamical Systems in Neuroscience: The Geometry
of Excitability and Bursting, Cambridge, Mass: MIT Press.
[3] Tonic synaptic inhibition modulates neuronal output pattern and spatiotemporal synaptic integration.
Häusser M. and Clark B.A., Neuron 69:665-678, September 1997.

101

Thursday evening, Poster session I-94

Cosyne 2008

Modelling self-organized criticality in developing cell cultures
Christian Tetzlaff1, Arjen Van Ooyen2, Florentin Wörgötter1, Markus Butz1
1

BCCN, University of Göttingen, Germany,
CNCR, VU University Amsterdam, Netherlands

2

Self-organized criticality (SOC) in neuronal networks has been first described by Beggs & Plenz (2003).
The characteristic property of neuronal networks being in a critical state is the appearance of avalanches
in network activity. Their intensity and duration are power law distributed with a slope of -1.5 and -2.0,
respectively, in a log-log plotting. This distribution is remarkably consistent for SOC across very different
physical domains (e.g. sand pile model, earthquakes etc.). Abbott & Rohrkemper (2006) used a selforganizing network model (Van Ooyen et al., 1995) to show that SOC can result from an activitydependent growth process of neuritic fields. In this model, neurons form bidirectional synapses activating
each other as soon as their circular neuritic fields overlap. However, this model is too abstract to be
compared to developing neurons in culture. Therefore, we extended the previous model by van Ooyen by
separately representing axonal and dendritic fields. Dendritic fields grow out when firing rates of the
neurons are too low and retract if firing rates turn too high like in the previous version. In addition, axonal
fields remain small as long as the firing rates are low and enlarge when firing rates increase (Butz et al.,
2006) because activity promotes axonal outgrowth. A further limitation of the Abbott’s approach is the
size of 100 neurons only which lead to non-neglectable finite-size effects. Instead, we used large scale
models with up to ten thousand neurons. As in networks of this size avalanches cannot be distinguished
by phases of inactivity, we had to develop an algorithm that assessed avalanches spatially comparable to
local field potentials measured by a grid of electrodes in a cell culture. We could show that there are
distinct phases of growth and retraction of axonal and dendritic fields until the model reaches a
homeostatic configuration. Interestingly, these phases produce different characteristic activity patterns as
the network develops from a subcritical over a supracritical into a critical state indicated by a changing
avalanche distribution. Phases during network development of growth and pruning are well known form
cell cultures. Assessing avalanches in cell cultures also revealed changing avalanche distributions
comparable to the model. Thus, we conclude that the changing activity patterns in culture are subject to a
developing morphology and, in turn, to a changing connectivity of the immature neurons. Further
modelling studies will provide testable hypotheses how pharmacological manipulations of neuronal
morphology should affect the appearance of avalanches in cell culture.
Acknowledgments
We thank Samora Okujeni, University of Freiburg, for providing us the spike train data from developing
cell cultures for statistical analysis.
References
[1] A simple growth model constructs critical avalanche networks. Abbott LF, Rohrkemper R (2006) (in
press).
[2] Neuronal Avalanches in Neocortical Circuits. Beggs & Plentz (2003) J Neurosci 23(35):11167–
11177.
[3] A theoretical network model to analyse neurogenesis and synaptogenesis in the dentate gyrus. Butz M,
Lehmann K, Dammasch IE, Teuchert-Noodt G (2006) Neural Netw 19:1490-1505.
[4] Implications of activity dependent neurite outgrowth for neuronal morphology and network
development.Van Ooyen A, van Pelt J, Corner MA (1995) J Theor Biol 172:63-82.

102

Cosyne 2008

Thursday evening, Poster session I-95

M-Currents modify bursting but not regular spiking in an in vivo
model of a midbrain dopaminergic neuron.
Maxime Bonjean1,2,3, Terrence J. Sejnowski1,2 , and Vincent Seutin3
1

Howard Hugues Medical Institute, Salk Institute, La Jolla, CA-92037
University of California San Diego, La Jolla, California 92093.
3
School of Medicine, University of Liege, Belgium
2

Background. Midbrain dopaminergic (DA) neurons sustain important physiological functions such as control of motricity, coding of error in prediction of reward (Schultz, 02), and modulation of emotion and
cognition. A dysfunctional dopaminergic signaling is implicated in Parkinson’s disease and in the pathophysiology of schizophrenia and drug abuse. In physiological conditions, DA neurons can switch between
tonic, irregular, and burst ﬁring (Grace & Bunney, 1984). Because burst ﬁring increases the amount of
dopamine release, a lot of effort has been devoted to unravel the mechanism(s) of this switch. It has been
recently demonstrated that DA neurons express KCNQ4 channels which can be opened by retigabine and
produce a functional M-type current (Koyama & Appel, 06; Hansen et al., 06). The inﬂuence of this channel
on the electrical activity of DA neurons is however unclear.
Materials and methods. A realistic multicompartmental model of a single DA neuron was developed.
These compartments were segregated into three major components: (i) soma, (ii) proximal dendrites, and
(iii) distal dendrites. All compartments have spiking capabilities and various additionnal ionic K + − and
Ca2+ −currents were added according the state-of-the-art known in the literature. The synaptic currents
followed a two-sate kinetic scheme (Destexhe, Mainen, and Sejnowski, 1994). The background of synaptic
activity was modeled through a Poisson stochastic process.
An additional simpliﬁed unicompartmental model was also designed on top of the previous model to investigate the nonlinear interaction of the parameters controlling the M-current.
Results. The introduction of an M-current in a model of DA cell can drastically modify its electrical excitability. The M-current has a differential effect depending on the ﬁring pattern of DA neurons. Due to
its slow kinetics of activation, the M-current acts when the neuron undergoes a sustained depolarization
(around -40mV). This is more likely to occur during burst ﬁring than tonic irregular ﬁring. This suggests
that KCNQ channels (underlying the M-current) may inﬂuence the excitability of DA neurons in vivo in a
“ﬁring-pattern dependent” manner, having little effect on single spikes, but profoundly disrupting the ability to ﬁre in bursts. Given the variety of intracellular pathways that control M-currents in central neurons,
modulation of this current may be a powerful technique by which various afferent neurotransmitters can ﬁne
tune the amount of DA transmission. If these results are conﬁrmed by experimental data, KCNQ4 channels
could be an important target to modulate the activity of midbrain dopaminergic systems to treat or cure
major pathologies resulting from a lack or excess of dopamine from these cells.
Acknowledgments. We thank David A. Brown, UCL, and Steven A. Prescott, Salk, for helpful discussions.
This work was supported by Fulbright, BAEF, and the Belgian National Fund for Scientiﬁc Research.
References. [1] Getting formal with dopamine and reward, Schultz, Neuron 36:241-263, 2002; [2] The control of ﬁring pattern in nigral dopamine neurons: burst ﬁring, Grace & Bunney, J. Neurosc., 4:2877-2890,
1984; [3] Characterization of M-current in ventral tegmental area dopaminergic neurons, Koyama & Appel, J. Neurophysiol., 96:535-543;, 2006; [4] The KCNQ channel opener retigabine inhibits the activity of
mesencephalic dopaminergic systems of the rat, Hansen, Seutin, et al., J. of Pharm. and Exp. Therapeutics, 318:1006-1019, 2006; [5] An efﬁcient method for computing synaptic conductances based on a kinetic
model of receptor binding, Destexhe & Sejnowski, Neural Computation, 6:14-18, 1994.

103

Thursday evening, Poster session I-96

Cosyne 2008

Disruption of visual target selection following inactivation of the
posterior pulvinar in monkeys
Melanie Wilke, Janita Turchi, Katy Smith, David A. Leopold
National Institutes of Mental Health, UCNI
The ability of primates to acquire visual information about the environment and to respond with appropriate motor
behavior is critical for survival. The pulvinar is a large thalamic structure that is heavily interconnected with the
visual cortex. Its inferior portion receives input from and projects to the ventral visual pathway, and contains at
least three retinotopic maps. Dorsally, the pulvinar is connected to several ‘dorsal stream’ regions, including the
posterior parietal lobe and the superior temporal sulcus. While the pulvinar is speculated to play a role in visual
attention and oculomotor behavior, large lesions have generally failed to produce major deficits in either basic visual
processing or saccade generation. Here we investigated the functional contribution of the posterior/dorsal pulvinar
to visually guided behavior by means of local reversible inactivation. Following injection of muscimol, monkeys
showed a disruption in their normal target selection behavior. This deficit was not obvious during a simple saccade
task, as they were able to respond normally to targets presented to both ipsilateral and contralateral sides. However,
under conditions where the animals could freely direct their gaze to two rewarded targets, one in each hemifield,
they almost never made a saccade to the target contralateral to the inactivation site. This was true even when the
contralateral site provided them with a much larger reward. We speculate that these deficits, which appear to be
closely related to visual neglect, reflect the pulvinar's role in integrating cortical information about potential visual
targets to guide motor behavior.

104

Cosyne 2008

Thursday evening, Poster session I-97

Attention Modulates the Phase of Alpha Band Activity Relative to
External Stimulus in a Visual Attention Task
Yanqing Chen1
1

The Neurosciences Institute, San Diego, CA 92121

Effects of attention on the amplitude and power of brain responses to sensory input have been extensively
studied. Previous research has demonstrated that attention enhances steady-state brain responses to
attended stimuli and suppresses unattended stimuli (e.g., Chen et al. PNAS, 100:3501-3506, 2003). Here
we study the effect of attention on the timing and phase of the intrinsic alpha band activity in a visual
attention task. 5 human participants were presented with an array of vertical bars flickering at 7.4Hz while
their whole head neuro-magnetic signals (MEG) were recorded. In the test condition they were instructed
to pay attention to the flickering bars and to detect width changes in the central bars with a key press as
soon as possible. In the control condition they passively looked at the flickering bars without making any
response. Steady state visual responses at the tagged frequency (7.4Hz) were significantly higher in
attended condition, while power at intrinsic alpha band decreased, similar to previous studies. More
interestingly, intrinsic brain activities in the alpha band (8-14Hz) were phase synchronized to the
flickering stimulus in both the attended and control conditions, indicating that steady-state visual
responses involve phase re-setting of the intrinsic alpha activity. In addition, phase distributions of the
alpha activity relative to the flickering stimulus in the visual areas were significantly different between
attended and control conditions. This result demonstrates that attention significantly modulates the phase
synchrony between alpha band activity and the external visual input.

Acknowledgments
This work was supported by the Neurosciences Research Foundation.
References
[1] The power of human brain magnetoencephalographic signals can be modulated up or down by
changes in an attentive visual task. Y. Chen, A.K. Seth, J.A. Gally, and G.M. Edelman, Proc. Natl. Acad.
Sci. 100:3501-3506, 2003.

105

Thursday evening, Poster session I-98

Cosyne 2008

Attention resolves the effects of a computational bottleneck:
modelling binding, precueing, and task-driven bias
Louise Whiteley and Maneesh Sahani
Gatsby Computational Neuroscience Unit, UCL
As William James famously said, “everyone knows what attention is”, and indeed we all have introspective
access to the effects (and frustrations) of attentional selection. But attempts to delineate exactly why the
brain needs to selectively ﬁlter incoming information, and what the mechanisms and effects of selection
are, have ﬂoundered in a sea of heterogenous effects. We have previously proposed a new probabilistic
computational framework that uniﬁes a number of attentional effects under a single normative description
of the resource that is limited, why it is limited, and how attention helps [1].
Our framework is grounded in the Helmholtzian notion that perception requires an inverse inference from
neural ﬁring to the features in the world that caused it. Multiple sources of noise and ill-posedness make
this inference poorly constrained, and the optimal approach is to compute posterior belief distributions over
features according to Bayes rule. There is much evidence for Bayesian optimality in tasks involving a
single object, but in cluttered natural scenes the resources required to represent the full posterior grow exponentially in the number of correlated features. We therefore suggest that a fundamental, computational
resource limitation is the ability to represent joint distributions over large numbers of features, and that the
brain makes approximations that neglect some of the correlations. Attention then consists of a ‘hypothesis’ that takes the mathematical form of an extra prior, but can be thought of as making a proposal about
which location, or feature value, is of interest. The brain approximates the product of the true posterior and
this extra attentional hypothesis, such that the approximation is more accurate in the proposed region. The
attentional hypothesis can be driven by top-down cues or bottom-up salience computations, but also dynamically evolves towards a better match between itself and the true posterior as measured by an approximated
partition function. This dynamic evolution towards ‘true’ proposals allows attention to reveal correlations
not explicitly represented in the approximation, as it settles on co-occuring feature values that have a high
correlation in the true posterior.
Here we illustrate this framework by implementing analogues of three top-down attentional paradigms in a
simple model, which consists of an array of feature maps connected to an output layer via a local weight
matrix. Noisy observations are drawn from this generative model, and we compute the posterior belief
both with the appropriate attentional hypothesis and without. Performance on pre-cueing, task-driven bias,
and binding tasks is simulated by mapping the posterior to a perceptual decision, and reveals the predicted
attentional beneﬁts. For pre-cueing, this approach has much in common with previous treatments of attention
as a prior over locations of interest [2]. However, it also encompasses paradigms that are semantically or
technically unsuited to such a treatment, and explicitly considers the case of multiple objects and thus the
binding problem. We also consider possible anatomical implications of the framework.
Acknowledgments
This work was supported by the Gatsby Charitable Foundation and the Wellcome Trust.
References
[1] A unifying probabilistic computational framework for attention. M. Sahani and L. Whiteley, Cosyne,
Abstract III-3, 2007.
[2] Statistical Models and Sensory Attention. P. Dayan and R. Zemel, ICANN 1017-1022, 1999.

106

Cosyne 2008

Thursday evening, Poster session I-99

Deriving the nature of the top-down input in visual area V4 for
spatial attention
Etienne Hugues1, Jorge V. José1,2
1

Department of Physics and 2Department of Physiology and Biophysics, SUNY at
Buffalo, NY, USA.
It was found that when paying attention the firing rate neural response is modulated as well as the
network oscillatory activity in V4. There have been several theoretical investigations on these
phenomena, but the underlying neural mechanism remains unclear. Actually, the available data was not
sufficiently constraining to obtain a unique model explanation. In a recent study in V4 [1], the activity of
two different classes of neurons, excitatory and inhibitory, has been reported for the first time. The
inhibitory neurons were found to have higher firing rates than the excitatory neurons, and the response
variability, measured by the Fano factor (FF), was found to be high for both neuron classes, whatever
their firing rate is. When attention was directed to their receptive fields (RF), the firing rate increase was
found to be connected with a decrease of the FF, related to a decrease of the spike train power spectrum at
low frequency [2]. Surprisingly, the attentional modulations were found to be more significant for
inhibitory neurons. The origin of the top-down attentional input to V4 neurons is still being debated.
However, the V4 network state brought by these results do allow us to investigate in more detail the
nature of this top-down input.
We have previously introduced a biophysical V4 network model that reproduced experimental data when
one or two stimuli were presented in the RF of a neuron and when attention was directed towards a
stimulus [3]. In that model, neurons received conductance-based inputs modelled by Poisson spike trains.
These include the visual input from area V2 and a background input from other brain areas. As suggested
by the experiments in [4], the top-down input was chosen to oscillate in the gamma frequency range when
attention was paid. In the present study, we have adapted the model parameters to reproduce
quantitatively the data presented in [1-2]. We found that the response variability of neurons can be high
even for elevated firing rates, not only when excitation almost balanced inhibition but when the level of
these input rates is comparable to what is found in vivo. Interestingly, we found that several forms of the
attentional input can induce the observed modulations, including a purely oscillatory signal. Based on our
analysis we predict different attentional modulations of the contrast response function that we compare to
those observed experimentally [5-6].
References
[1] Differential attention-dependent response modulation across cell classes in macaque visual area V4.
J.F. Mitchell, K.A. Sundberg and J.H. Reynolds, Neuron 55:131-141, July 2007.
[2] Attention-dependent response modulation differs between cell classes in macaque area V4. J.F.
Mitchell, K.A. Sundberg and J.H. Reynolds, Soc. Neurosci. Abs. 338.6, Nov. 2007.
[3] Spatial attention in V4: a biophysical model. E. Hugues and J.V. José, CNS Meeting, July 2007.
[4] Synchronous activity within and between areas V4 and FEF in attention. S.J. Gotts, G.G. Gregoriou,
H. Zhou and R. Desimone, Soc. Neurosci. Abs. 703.7, Oct. 2006.
[5] Attention increases sensitivity of V4 neurons. J.H. Reynolds, T. Pasternak and R. Desimone, Neuron
26:703-714, 2000.
[6] Effect of spatial attention on contrast response functions in macaque area V4. T. Williford and J.H.R.
Maunsell, J. Neurophysiol. 96:40-54, 2006.

107

Thursday evening, Poster session I-100

Cosyne 2008

FRONTO-PARIETAL COUPLING IS TASK AND RULE SPECIFIC
Salazar RF1, Bressler S2, Richter C2, & Gray CM1
1
2

Ctr Comp Biol, Montana State Univ., Bozeman, MT

Ctr. Complex Systems & Brain Sci., Florida Atlantic Univ., Boca Raton, FL

Prefrontal cortex plays a key role in controlling working memory processes according to the rules
underlying behavioral choices. It has been argued that such processes involve the temporal
coordination of activity between prefrontal cortex and the cortical regions with which it
communicates. In the present study, we describe task and rule specific changes in spectral coherence
of local field potentials (LFP) recorded from prefrontal and posterior parietal cortices. Working
memory processes were assessed with a delayed-match-to-sample task involving a non-instructed rule
switch. The rules consisted of matching either the location or the identity of a sample visual object.
We find strong inter-areal coherence of LFP signal pairs (n = 247) in the beta and gamma frequency
ranges. The median and 90th percentile of the distribution of significant coherence values (P<.001)
in the beta and gamma frequency ranges were 0.3 and 0.17, and 0.45 and 0.06, respectively. Ruledependent differences in coherence were observed in a limited percentage of site pairs; 23% and 24%
for the beta and the gamma band respectively. A fraction of site pairs showed stronger coupling
during the identity rule for the beta band (18% of all pairs) whereas the opposite occurred for the
gamma band (14% of all pairs). The results demonstrate robust synchronization of neural activity
between two widely separated areas of the neocortex known to be involved in top-down control of
working memory and visuospatial attention.

Figure
1.
Two
examples of the timefrequency coherence
spectrum computed on
pairs of LFPs recorded
simultaneously in the
prefrontal
and
posterior
parietal
cortices.
The two
examples
illustrate
prominent
synchronous signals in
the low gamma (A;
27-42 Hz) and beta (B;
15-27 Hz) frequency
ranges. The coherence

108

Cosyne 2008

Friday AM Talks-1

Neural Basis of Reach Preparation
Krishna V. Shenoy
Stanford University
Our seemingly effortless ability to reach out and swat a fly or grab a cup belies the sophisticated neural
computations at work in our nervous system. It has long been recognized that before moving we
somehow prepare neural activity such that, when called upon, the desired movement unfolds. But the
goals of such movement preparation and the underlying neural mechanisms remain poorly understood. To
address this problem, our recent electrophysiological and computational investigations have focused on
how premotor cortex in the rhesus monkey helps prepare movements, with a particular emphasis on
understanding how this is accomplished on a single-trial basis [1]. Results suggest that the brain is
attempting to optimize preparatory neural activity [2] and can delay movement until this activity is
sufficiently accurate [3]. Consistent with this view, even minor fluctuations (inaccuracies) in preparatory
activity result in altered movements [4]. To further investigate single-trial neural behavior we note that
spiking activity during motor preparation exhibits dynamics beyond that driven by external stimulation,
presumably reflecting the extensive recurrence of neural circuitry [5]. We have been investigating
methods for capturing the dynamics from (96 channel) simultaneous neural recordings, visualizing lowdimension state-space neural trajectories, and relating these preparatory trajectories to the resulting arm
movement kinematics. Characterizing these dynamics may reveal important features of neural
computation, as well as provide novel perspectives on how to interpret single-neuron preparatory activity
that appears to evade interpretation in traditional ‘representational’ terms [6-8].
Acknowledgments
Our research group, Maneesh Sahani, and various funding agencies including NIH (NINDS-CRCNS).
References
[1] Churchland MM, Yu BM, Sahani M, Shenoy KV (2007) Techniques for extracting single-trial activity
patterns from large-scale neural recordings. Curr Opin Neurobio 17:609-618.
[2] Churchland MM, Yu BM, Ryu SI, Santhanam G, Shenoy KV (2006) Neural variability in premotor
cortex provides a signature of motor preparation. J Neurosci 26:3697-3712.
[3] Churchland MM, Shenoy KV (2007) Delay of movement caused by disruption of cortical preparatory
activity. J Neurophysiol 97:348-359.
[4] Churchland MM, Afshar A, Shenoy KV (2006) A central source of movement variability. Neuron.
52:1085-1096.
[5] Yu BM, Afshar A, Santhanam G, Ryu SI, Shenoy KV, Sahani M (2006) Extracting dynamical
structure embedded in neural activity. NIPS. 1545-1552.
[6] Churchland MM, Santhanam G, Shenoy KV (2006) Preparatory activity in premotor and motor cortex
reflects the speed of the upcoming reach. J Neurophysiol. 96:3130-3146.
[7] Churchland MM, Shenoy KV (2007) Temporal complexity and heterogeneity of single-neuron
activity in premotor and motor cortex. J Neurophysiol. 97:4235-4257.
[8] Batista AP, Santhanam G, Yu BM, Ryu SI, Afshar A, Shenoy KV (2007) Reference frames for reach
planning in macaque dorsal premotor cortex. J Neurophysiol. 98:966-983.

109

Friday AM Talks-2

Cosyne 2008

Parietal Reach Region Cell Classes Have Complementary
Planning Responses
Elizabeth B. Torres and Richard A. Andersen
California Institute of Technology
Traditionally neural recordings to study arm reaches are performed during automatic behavior from overtrained subjects. These over learned reaches are already very precisely timed, with a symmetric smooth
bell-shaped speed profile. It is unknown how the brain learns to compute these temporal dynamics and
builds a new motor memory from naïve to automatic. It is also unknown where in the cortex these
computations take place although motor learning and adaptation have been primarily studied in motor
regions. This study targeted instead the reach region (PRR) of the posterior parietal cortex (PPC) and
asked how the computation of new temporal dynamics manifested in the pre-movement activity of these
cells. We tracked 111 PRR cells from 2 monkeys in a delayed-reach paradigm that interleaved an old
automatic straight reaching task with a new obstacle (OB)-avoidance task for which the animals had not
been trained. We found two complementary populations of cells that before, during and after target
presentation segregated according to the mean response differences (across all spatial locations) between
the automatic and the novel task. Type I cells decreased their firing rates whereas Type II cells increased.
Systematic anticipatory voluntary changes in arm posture gated their responses to the same retinal input
in the dark during the fixation epoch and continued in the memory period after the cue-presentation. An
analyses of their amplitude-normalized waveforms according to their peak-to-valley durations as in [1]
yielded a significant bimodal distribution whereby the Type I cells were broad-spiking (> 250 μs) and the
Type II cells were narrow-spiking ( 250 μs). As the temporal profiles of the conserved postural and hand
paths was being learned these cells continued to dramatically change their responses and eventually
converged to their original response patterns in the automatic straight reaches, once the OB-avoidance
task had also become automatic. In a control experiment where the initial posture was passively
repositioned at the beginning of the session to match the voluntary adjustment, and where the same
automatic straight-reaches were performed, the same cells that so dramatically changed during the
learning of new temporal dynamics did not show the same learning effects. Their response differences to
the passive change in initial posture were comparable to the voluntary postural change but the memory
planning pattern did not change across the block trials. These data suggest a very different role of PRR
neurons in automatic vs. novel tasks and posses the question of what other area(s) are driving the learning
of automatic dynamics for goal-oriented reaches.
Acknowledgments
We thank Chris Buneo for assistance with training and surgical procedures in the early stages of this
project. This work was supported by Sloan-Swartz Foundation, Della Martin Fellowship, James G.
Boswell Foundation, National Eye Institute (NEI).
References
[1] Differential attention-dependent response modulation across cell classes in macaque visual area V4.
J.F. Mitchell, K.A. Sundberg, J.H. Reynolds, Neuron 55:131-141, July 2007.

110

Cosyne 2008

Friday AM Talks-3

The Energetic Cost of Fast Spiking
A.R. Hasenstaub*1, S.L. Otte*1,2, E.M. Callaway1, and T.J. Sejnowski1
1
2

Crick-Jacobs Center for Theoretical and Computational Biology, Salk Institute.
Graduate Program in Neurosciences, University of California San Diego. *Equal contributors.

Brains are metabolically expensive: the human brain comprises only two percent of the body’s mass, but
consumes nearly twenty percent of the body’s energy [1]. Much of that energy consumption is believed
to be used to recover the ion fluxes associated with action potential generation [2,3]. The hypothesized
high cost of action potential generation has inspired a variety of studies on the efficient use of spikes in
neural coding and computation. Relatively little work, however, has been done to examine how neurons
might efficiently generate spikes in the first place. In particular, the relationships between the kinetics
and densities of the active conductances generating the action potential, and the sodium and potassium
fluxes during, and thus the metabolic cost of, the action potential, remain largely unknown.
In addition, the biophysics of spike generation are presumably subject to functional, as well as energetic,
constraints. For example, in neocortex, fast-spiking interneurons, compared to pyramidal neurons,
express faster potassium currents, and thus generate narrower action potentials; they also spontaneously
spike at higher rates in vivo, can fire faster trains of action potentials, and can follow faster sine-wave
stimuli [4], and their distinctive spike response properties are in part due to this difference in spike shape
[5]. Similar relationships between speed of spiking and width of action potential waveform are found in a
variety of neural structures, prompting us to ask: inasmuch as narrow spikes are required for the fastspiking phenotype, what are the energetic costs associated with narrow spiking? Are narrow action
potentials strictly required for fast spiking, or are there other biophysical strategies to obtain the fastspiking phenotype that cells might adopt? What are the energetic costs of these strategies?
First, using Hodgkin-Huxley style models, we predict substantial differences in efficacy and energy
efficiency between the various strategies for fast spiking. For example, we find that increasing Na+
current density increases the model’s maximum spike rate but dramatically increases the current fluxes
during spikes; speeding Na+ current activation permits faster spiking with little effect on current fluxes
during spikes; and speeding K+ current activation permits faster spiking, but also increases the cost of
singly generated spikes. We then validate a subset of these predictions in cortical fast-spiking
interneurons in vitro by blocking the cells’ Na+ conductances with TTX, replacing them with high-rate
dynamic clamp, and measuring the cost and speed of spiking as a function of the parameters governing
the model sodium conductance. Finally, we compare our predictions regarding the energetically
favorable strategies for fast spiking to the actual channel properties observed in faster and slower spiking
neurons in a variety of neural systems, and find our hypotheses to be well matched by experimental data.
Acknowledgements
This work was supported by HHMI, CCTB, and the NIH.
References
[1] L.C. Aiello and P. Wheeler, Current Anthropology 36:199-221, 1995.
[2] D. Attwell and S.B. Laughlin, J. Cerebral Blood Flow and Metabolism 21: 1133-1145, 2001.
[3] P. Lennie, Current Biology 13: 493-497, 2003.
[4] A. Hasenstaub et al, Neuron 47: 423-435, 2005.
[5] C.C. Lien and P. Jonas, J. Neurosci 23(6), 2003.

111

Friday AM Talks-4

Cosyne 2008

Timing based on stochastic neural and sensory processes
Misha B. Ahrens and Maneesh Sahani
Gatsby Computational Neuroscience Unit, UCL
Many perceptual processes and neural computations, such as speech recognition, or motor control and learning, depend on the ability to measure and mark the passage of time. However, the neural mechanisms that
make such temporal judgements possible are unknown. A number of different hypotheses have been advanced, all of which depend on the known evolution of a neural [1] or psychological state, possibly through
oscillations or the gradual decay of a memory trace [2,3]. We suggest a new model, which instead exploits
the fact that neural and sensory processes, even when their precise evolution is unpredictable, exhibit statistically structured changes [4,5]. We show that this structure can be exploited for timing, and that reliable
timing estimators can be derived from the statistics of the processes. This framework of decoding time from
stochastic processes allows for a much wider array of neural implementations of time estimation than has
been considered so far, and can simultaneously emulate several different behavioral ﬁndings, which so far
have only been understood in psychological terms.
We abstract neural and sensory processes to stationary Gaussian Processes (GPs), and show that, on the
basis of observations of a collection of such processes at two distinct points in time, the GP likelihood gives
rise to a probability distribution over elapsed time (Fig. 1). Under relatively loose assumptions, estimators
derived from this distribution follow the Weber law of timing.

probability

estimated duration

Formulating the inference of elapsed time within the above framework naturally allows stimulus statistics to
interact with the timing mechanism. Two compelling recent psychophysical observations [6,7] are emulated
by manifesting changes in the stimulus as changes in the temporal spectrum of a sense-driven subset of
the stochastic processes. This results in estimators that show biases towards over- or under-estimating
the elapsed time during stimulus presentation that are similar to those observed in humans. Preliminary
psychophysical results of our own (Fig. 2) further show that subjective time dilation or contraction, known
to depend on the temporal frequency of deterministic stimuli, can also be driven by stochastic stimuli.

time

2
1.5
1
0.5
0

0

0.5

1

1.5

2

real duration (s)

Figure 1: Likelihood curves from individual stochastic processes (gray) are weakly informative about elapsed time;
combining curves from multiple processes, at multiple
time-scales, results in a distribution (black) which peaks at
the right time.

Figure 2: Circles: Observers adjusted
the length of a slow stochastic process
to match the length of a fast process.
Diamonds: vice versa.

References
[1] Karmarkar and Buonomano, Neuron 53:427-438, 2007
[2] Gibbon, Psychol. Rev. 84: 279-325, 1977. [3] Miall, Neural Comp. 1:359-371, 1989
[4] Attias and Schreiner, NIPS 9, 27-33, 1996. [5] Osborne et al., J. Neurosci. 24: 3210-3222, 2004
[6] Eagleman et al., J. Neurosci. 10369-10371, 2005. [7] Kanai et al., J. Vision 6: 1421-1430, 2006

112

Cosyne 2008

Friday AM Talks-5

Encoding and processing of primary sensory variables by the rat
vibrissal/trigeminal system
Mitra Hartmann
Northwestern University
Rats are nocturnal, burrowing animals that use their vibrissae (whiskers) to tactually explore the
environment. Using only its whiskers, a rat can determine object size, shape, orientation, and texture. This
makes the rat vibrissal system an excellent model to explore the structure of movements that subserve
sensing. I will describe recent experiments in our laboratory that have aimed to understand neural
processing in the vibrissal system from the outside-in. I will walk through our laboratory's suggested
answers to the following questions:
1. What are the primary mechanical variables sufficient for three-dimensional feature extraction by the
whiskers? We suggest that these variables are angular position, angular velocity, and rate of change of
moment (torque) at the whisker base. I will show results from a hardware model to demonstrate that these
variables are sufficient for feature extraction.
2. How are these variables encoded by the electrical activity of neurons in the first stage of neural
processing? In a re-analysis of data from Jones et al. [1] we have found evidence that neurons of the
trigeminal ganglion use a state encoding scheme to represent pair-wise combinations of the mechanical
variables identified above.
3. How are the variables transformed in the second stage of neural processing, and why might they be
transformed in this way? We hypothesize that neurons with multi-whisker receptive fields in the
trigeminal nuclei help to compute the relationship between spatial and temporal gradients generated as the
animal moves its sensory surfaces through the environment. These gradients, expressed as the complete
derivative, are computed based on the animal's own velocity, and provide an inviolate mathematical
description of information flow over moving sensory surfaces. Computing the complete derivative at
multiple spatial and temporal scales would allow the animal to predict the stimulus that it will measure in
the next sensory instant, conferring tremendous survival advantage.
References
[1] Jones LM, Depireux DA, Simons DJ, Keller A (2004) Robust temporal coding in the trigeminal
system. Science 304:1986-1989.

113

Friday AM Talks-6

Cosyne 2008

Cortical activity influences the efficacy of geniculocortical
communication
Farran Briggs1, W. Martin Usrey1
1

Center for Neuroscience, University of California, Davis, Davis California 95618

Whether or not a cortical neuron responds to an incoming input depends on a number of factors. This
study examined the role of ongoing cortical activity on spike efficacy at the geniculocortical synapse. We
recorded single-unit responses from neurons in primary visual cortex (V1) of the macaque monkey and
used electrical stimulation in the lateral geniculate nucleus (LGN) to identify cortical neurons that
received direct LGN input. We then examined ongoing spontaneous activity preceding the arrival of
electrically-evoked geniculocortical inputs. Particular patterns of preceding cortical activity were
predictive of suprathreshold responses to geniculocortical input. Namely, cortical spiking activity
between 30-40 Hz and displaying rhythmic patterns (at roughly 30 msec intervals) in phase with
incoming input led to an increase in geniculocortical efficacy. Similarly, suprathreshold responses to
geniculocortical input were preceded by cortical activity with increased power in the gamma frequency
band. Based on these results, we suggest that increasing synaptic efficacy may be one of the mechanisms
by which gamma band activity enhances cortical communication in thalamocortical circuits.

Acknowledgments
This work was supported by NIH grants EY13588, EY15580, the McKnight Foundation, and the Esther
A. and Joseph Klingenstein Fund.
References
[1] Cortical activity influences geniculocortical spike efficacy in the macaque monkey. F. Briggs and W.
M. Usrey, Frontiers in Neuroscience in press, 2008.

114

Cosyne 2008

Friday AM Talks-7

	


				




					
	 !


	

	


 


!	
  "!#$  
 %%  
 	
 	
 &   %
' 
  
	 (%(
 % %
(   ) 
''  

 
 
' 	
 *%  %% 
 
 
* (
% ' 
  

 
  %	"
+,$'%'
 
 ' % 		) - 
 %
 	
 % 
  '   
' 
 
&
	'
*'
%

%%%).
%
&
 %%

	
+%,
	


*
/	


	% 
'	
	

	!# 	
)

0	

% 
	%
 

%)

%
	
%


 
  
 
 
   ' %   
 
% 
&%  	% 

) 1 
% 
	  !#    %     

 


	


+ 	
,
 

*
%%'%
% 




%) 

	


% 
%%+',

	
 	
     
	 ' 

 %%) 2
  '  
	
   % 
         ' % 

		'	

		%
%*%%
)
3 
 
	 !#    
  %
  ' 	
 % 	 

 
% 
 	
  	   %
  ' 
) 4 

 



5
	



%' )

6%    	
  !# 
 
      
 


%).  
  %



7

' 	
  ' 	
 
   8 
 
% 
     


%
	%"))	
$
	')7
 
  % 
 %		 '  
 
% % ' 
% 
!# 
%	
 
	
 " 
  
	
%5

$).
	
%	'
%349&

 + 
	,'
% 
'). 
  &

%

%"))
($9%%).

%
%%  
	 %( 

 %
 	
   
 	% '  %   
)4	
 
 
	 0


% 

(	% %'

%%5

		'%

%%).
%
 
%
 
%
'%
	%

%
%

 )*%

%	



	
9%
)-




%

%    
  	  % 
 	

 
% % ) .  

% %	
		%

:
%*


%' 
 %		
	' 
)

115

Friday AM Talks-8

Cosyne 2008

Hippocampal coding of point-free topology
Yuri A. Dabaghian, Anthony G. Cohn and Loren M. Frank
Keck Center for Integrative Neuroscience and Department of Physiology,
University of California San Francisco
Rodent hippocampal neurons are generally referred to as “place cells,” a name derived from the fact that
each active neuron tends to fire in a restricted region of the animal’s environment. This property has led
to the general statement that the rodent hippocampus codes for “space” but it is not clear exactly what is
meant by this claim. In particular, space can be thought of in terms of two types of representations:
topological (e.g. connectivity of locations) and geometric (e.g. distances and angles). Current theories
suggest that the hippocampus explicitly represents geometric elements of space derived from a path
integration process that takes into account distances and angles of self motion [1].
We hypothesize that this idea is not correct, and that hippocampus preferentially represents topological as
opposed to geometrical information, allowing the animal to understand the connectivity of its
environment even if the geometry changes. To test that hypothesis we carried out multielectrode
electrophysiological recordings of the cell populations in CA1 hippocampal region in a dynamic
environment. The environment consisted of a multi-component U-shaped linear track where each arm of
the U could change its geometric shape from a linear to a zig-zag configuration. The shape of each arm
was controlled through a stepper motor system, making it possible to sample the neural responses
associated with numerous different geometric configurations that shared the same internal topology.
Our preliminary analysis of the resulting patterns of the PF behavior on the dynamic U-track indicate that
single neurons tend to be active on the same section of the track despite the displacement of that section
from its initial location. In addition, the sequential order of multiple simultaneously recorded place fields
is preserved as the track expands and contracts. These initial findings are not compatible with the idea
that the hippocampus preferentially represents geometry and thus absolute spatial locations. Instead, they
suggest that the hippocampal map emphasizes topological rather than geometric information.
The apparent topological nature of spatial coding in hippocampus suggests a theoretical approach to
understanding hippocampal spatial computations. In particular, the paradigm and technical approaches
associated with point-free topology and specifically of the Region Connection Calculus [2] and Formal
Topology [3] may be adequate for describing the biological principles of space and memory coding in the
hippocampal network. Conceptually, these mathematical formalisms allow us to understand how
topological spatial maps, as mathematical objects, can arise from the discrete patterns of neuronal
activity.
Acknowledgments
NIH grants F32-NS054425-01, MH059733, MH077970, MH080283 and Sloan and Swartz Foundations.
References
[1] K. Gothard, W. Skaggs, K. Moore, B. McNaughton, J. of Neuroscience, 16(2), pp. 823-835 (1996).
[2] Yu. Dabaghian, A. Cohn and L. Frank, "Topological maps from signals" (ACM GIS 2007).
[3] G. Sambin, Theoretical Computer Science Topology in computer science Volume 305 (1-3) pp. 347 408 (2003).

116

Cosyne 2008

Friday PM Talks-1

Cerebellar Long Term Depression as a Supervised Learning Rule
with All or Nothing Character
Mitsuo Kawato
ATR Computational Neuroscience Laboratories
The cerebellar internal model theory postulates that the cerebellar cortex acquires many internal models
of controlled objects, dynamical processes in the external world, and even other one’s brain dependent on
long-term depression (LTD) of Purkinje cells. A specific version of this theory, the feedback-errorlearning model postulates that the climbing fiber inputs to Purkinje cells carry the feedback motor
command, which could be regarded as an approximation to the error signal for motor commands and can
supervise learning of inverse dynamics models. Many experimental supports were obtained from the
ventral paraflocculus of the cerebellum during monkey control of ocular following responses. For arm
movements under multiple force fields, firings of many Purkinje cells correlate with dynamics [1]. fMRI
studies mapped forward and inverse models of manipulated objects and tools in the cerebellar cortex.
Kinetic models of LTD [2,3] suggest a cascade of excitable and bistable dynamical processes, which may
resolve plasticity-stability dilemma at single spine level. That is, even a single pulse of climbing fiber
input combined with an early train of several parallel fiber pulses can induce Ca2+ induced Ca2+ release
via IP3 receptors on ER. The MAPK positive feedback loop leaky integrates resulting large Ca2+
elevation and if it crosses the threshold then the state moves to the depressed equilibrium. These models
explain diverse LTD experiments and clearly demonstrate that LTD is a supervised learning rule, and not
anti-Hebbian as erroneously characterized. The MAPK positive feedback loop model [2] was recently
supported by a Ca2+ photo-uncaging and imaging experiment [4] that supports LTD all-or-none character.
In the LTD study, the most important information within the system Ca2+ can be measured and
manipulated directly, thus the theory was quite rigorously proved. At the system level, the cerebellar
internal models are still debated and we need to develop a method to directly manipulate information.
References
[1] Yamamoto K, Kawato M, Kotoaska S, Kitazawa S: Encoding of movement dynamics by Purkinje cell
simple spike activity during fast arm movements under resistive and assistive force fields. Journal of
Neurophysiology. 97: 1588-1599, 2007.
[2] Kuroda S, Schweighofer N, Kawato M: Exploration of signal transduction pathways in cerebellar
long-term depression by kinetic simulation. Journal of Neuroscience, 21: 5693-5702, 2001.
[3] Doi T, Kuroda S, Michikawa T, Kawato M: Insoitol, 1, 4, 5-trisphosphate-dependent Ca2+ threshold
dynamics detect spike timing in cerebellar Purkinje Cells. Journal of Neuroscience, 25: 950-961, 2005.
[4] Tanaka K, Khiroug L, Santamaria F, Doi T, Ogasawara H, Ellis-Davies G, Kawato M, Augustine
GJ: Ca2+ requirements for cerebellar long-term synaptic depression: role for a postsynaptic leaky
integrator, Neuron, 54: 787-800, 2007.

117

Friday PM Talks-2

Cosyne 2008

Value representation via divisive normalization in parietal cortex
Kenway Louie, Lauren Grattan, and Paul W. Glimcher
Center for Neural Science, New York University, New York, NY
Value information is a critical component of the decision-making process. In the lateral intraparietal area
(LIP), visuomotor neurons are strongly modulated by reward variables such as expected gain, prior
probability, and reward income, suggesting that individual LIP neurons represent the subjective value of
specific saccades [1,2]. In this framework, decision activity across the LIP population initially encodes
the values of the available targets; comparison of these values results in action selection and output of
choice information to downstream oculomotor structures. Notably, such a decision framework would be
robust to many transformations of neural activity, permitting for example the scaling of value
information.
We explore here the computational representation of multiple option values during decision-making. In an
initial experiment, we explicitly examine the effect of the value of targets outside the response field (RF)
on single neuron activity. Our results show that even though such stimuli elicit no activity when presented
alone, when presented in a choice situation with a target in the RF they strongly modulate LIP activity.
Specifically, the spiking rate of LIP neurons decreases significantly as the value of targets outside the RF
increase. The activity of these same neurons increase when target value in the RF increases, as previously
reported for LIP. Examined together, these results show that LIP neurons encode a relative measure of
saccade value, normalized across available options.
What is the form of this normalization? Visual cortical processes such as gain control in V1 and responses
to multiple stimuli in MT have been explained by a form of divisive normalization [3,4]. More general
models of divisive normalization include tunable normalization weights, allowing activity to be optimized
for example to minimize statistical dependencies between responses [5]. To fully characterize value
normalization in this context, we have developed a multiple-option version of the previous choice
experiment. In each trial of this task, the monkey fixates a central cue and views three peripheral targets.
Target onsets are asynchronous to allow examination of the dynamics of normalization within a trial, and
reward values are varied to characterize how target number and value interact. The existence of valuebased divisive normalization in parietal cortex suggests that this may represent a general mechanism of
cortical operation, and provides insight into how decision processes handle multiple options and values.
Acknowledgments
This work was supported by the NEI and the NINDS.
References
[1] Platt, M.L. and Glimcher, P.W. Neural correlates of decision variables in parietal cortex. Nature 400,
233-238 (1999).
[2] Sugrue L.P., Corrado, G.S., and newsome, W.T. Matching behavior and the representation of value in
the parietal cortex. Science 304, 1782-1787 (2004).
[3] Heeger, D.J. Normalization of cell responses in cat striate cortex. Vis Neurosci 9, 181-197 (1992).
[4] Britten, K.H. and Heuer, H.W. Spatial summation in the receptive fields of MT neurons. J Neurosci
19, 5074-5084 (1999).
[5] Schwartz, O. and Simoncelli, E.P. Natural signal statistics and sensory gain control. Nat Neurosci 4,
819-825 (2001).

118

Cosyne 2008

Friday PM Talks-3

Associative Learning Signals in the Monkey Medial Temporal
Lobe
Wendy A. Suzuki
Center for Neural Science, New York University
A critical function of the medial temporal lobe is the ability to successfully acquire new
declarative information about facts and events that includes new associations between initially
unrelated items (associative learning). A major goal of my lab is to understand the brain basis of
new associative learning. I will first summarize the studies we have done characterizing the
patterns of neural activity seen as monkeys learn new associations on-line. These studies have
shown that neurons throughout the medial temporal lobe signal new learning with changes in
their stimulus-selective response properties. More recent studies have revealed that these changes
in stimulus-selective responses reflect the animal’s behavioral learning strategy. A surprising
new finding shows that hippocampal neurons also provide a powerful signal of trail outcome,
differentiating between trials that are correct or wrong. I will discuss the possible of role of these
signals in a feedback process by which information about behavioral outcome can be used to
strengthen correct performance and modify error performance.

119

Friday evening, Poster session II-1

Cosyne 2008

Reorganization of neural circuitry during growth of cat visual
cortex
Wolfgang Keil1,3, Siegrid Löwel2, Fred Wolf1 and Matthias Kaschube3
1
MPI for Dynamics and Self-Organization, and Bernstein Center for
Computational Neuroscience, Göttingen, 2Friedrich-Schiller-University, Jena,
3
Lewis-Sigler Institute, and Physics Department, Princeton University
The contribution of plasticity to normal development remains unsolved. Cortical growth may involve
substantial modiﬁcations of neural circuitry, thus enabling to study cortical plasticity under natural rearing
conditions. Here, we present experimental evidence for reorganization of ocular dominance (OD) column
layouts during postnatal cortical growth. Furthermore, we show that the observed mode of reorganization
is predicted by a large class of models for activity dependent development of neural circuitry.
We test three scenarios of how the layout of OD columns might change during cortical growth: i)
Balloon-like isotropic expansion of cortex and OD domains, ii) anisotropic expansion of cortex by which
OD domains would elongate in the direction of stretching, and iii) isotropic or anisotropic expansion
accompanied by bending of OD domains resulting in a more isotropic layout of OD domains. In contrast
to i and ii, scenario iii requires remodeling of OD in parts of the neuronal population.
In experiment, we observed a considerable reorganization of OD columns during the late phase of
postnatal cortical growth consistent with scenario iii, but inconsistent with i and ii. Using wavelets, we
analyzed the two-dimensional spatial organization of OD columns in ﬂat mount sections of 2-DG labeled
OD columns in cat visual cortex. We found that the size of area 17 increased by 50% between postnatal
week 4 and 10. However, during this period the mean spacing between adjacent OD columns showed
only a weak and transient increase of about 10%. Consequently, the total number of OD domains in area
17 increased by more than ~30%. Moreover, the bandedness, a parameter quantifying the degree of
anisotropy of layouts, decreased considerably over this time course. The absence of a persistent increase
of column spacing excludes the balloon hypothesis (scenario i) while the decrease of the bandedness
excludes anisotropic stretching as a possibility (scenario ii). However, isotropic expansion following
reorganization (scenario iii) appears consistent with these data.
Scenario iii is indeed predicted by most models for OD column development that have been proposed so
far. It represents the generic behavior upon isotropic expansion under the presence of intracortical
Mexican-hat like linear interactions of constant range. The basic mechanism is that the system tries to
maintain its intrinsic spatial scale and the simplest way to achieve this is by bending its domains. This
behavior is known under the name zigzag instability. We analyzed the effect of cortical expansion on
columnar layouts in the elastic net model and compared it to the degree of reorganization observed
experimentally. Under various expansion conditions, the elastic net model indeed showed stereotypical
behavior of a zigzag instability with bending and reorganization of OD domains. Consistent with
experiment, the spacing of OD columns increased only transiently and the bandedness measure decreased
over the same period. The degree and duration of reorganization strongly depended on the amount of
expansion. The predicted period of reorganization fairly matched the period observed in visual cortex.
In conclusion, the columnar reorganization described here appears to be induced by cortical expansion
during growth. This phenomenon is predicted by most models for the activity dependent development of
OD columns and suggests that cortical plasticity remodels columnar organization in normal development.

120

Cosyne 2008

Friday evening, Poster session II-2

Fragment-based Learning of Visual Object Categories
Evgeniy Bart1, Jay Hegdé2 , and Daniel Kersten2
1

Caltech, 2University of Minnesota

Previous research has shown that visual object categorization can be based on local, informative fragments of
an object, rather than on the whole object. However, whether and how humans learn informative fragments
has remained unclear.
We trained subjects to distinguish two synthetic, but naturalistic, visual classes, using complete objects (not
fragments). Subsequently, we tested whether subjects were still able to distinguish the classes when only a
small fragment of a given object was visible, rather than the entire object. When an informative fragment was
visible, subjects performed almost as well as with complete objects. In contrast, when visually comparable
but uninformative fragments were viewed, the performance was at chance level. These results indicate that
the visual system preferentially learns informative object fragments during category learning.
Using synthetic classes allowed us to tightly control subjects’ exposure to training examples, and in particular to ensure that training objects were never seen occluded or in clutter. Therefore, informative fragments
were learned not because it was necessary (e. g. to cope with occlusion), but naturally as part of category
learning. Additional analysis also indicates that no learning took place during testing (data not shown).
Therefore, we conclude that informative fragments were acquired during initial exposure to the categories.
Supported by NIH grant EY017835 and by IMA.

0

1

2

3

4

0

1

2

3

4

5

6

7

8

9

5

6

7

8

9

(a) Informative fragments

(b) Uninformative fragments

Figure 1: Panel (a): informative fragments. Panel (b): uninformative fragments selected from interest points
to be visually comparable to informative fragments. In each panel, top: 10 fragments. Bottom left: a typical
training object (‘digital embryo’) of one of the categories and locations of the fragments on this object.
Bottom right: categorization performance (higher values correspond to better performance).

121

Friday evening, Poster session II-3

Cosyne 2008

Retinal Position and Object Category Effects in the Human Lateral
Occipital Cortex
Rory Sayres1 and Kalanit Grill-Spector1
1

Department of Psychology and Neuroscience Institute, Stanford University

Object-selective regions of human cortex, including the lateral occipital complex (LOC), are
known to be sensitive to the retinotopic position of object stimuli and to object category.
However, there has been little quantitative measure of the extent, organization or relative
magnitude of these effects. We sought to relate measures of object selectivity and retinotopy
with a series of fMRI experiments. We imaged six subjects in a 3T MRI scanner using standard
traveling wave checkerboard retinotopic mapping experiments, as well as block-design
experiments in which objects from different categories were presented at six distinct retinotopic
positions. We then examined responses in region LO, a subset of the LOC positioned posterior to
hMT+ on the lateral surface.
We found substantial retinotopic modulation by checkerboard traveling wave stimuli in LO. LO
exhibited a modest overlap with the visual field maps LO-1 and LO-2 (<20% for most subjects,
with a greater degree of overlap with LO-2), and retinotopic modulation (polar angle and
eccentricity) in LO extended well beyond the boundaries of LO-1 and LO-2. We also observed a
pronounced lower visual field bias in LO: more LO voxels represented the lower contralateral
visual field during the retinotopic mapping experiment and the mean LO response was higher to
object stimuli presented below fixation than above fixation. Finally, LO responses were higher
for contralateral than ipsilateral stimuli
We also examined how object category and retinal position affect the distributed response
patterns across LO. We found a stronger effect of position than category on the distributed
response: the correlation between response patterns to objects from different categories presented
at the same position was higher than the correlation between response patterns to objects from
the same category at different positions. Overall, these results demonstrate that retinal position
modulates LO responses more than object category, and position effects can be explained by
retinotopic organization in LO.

Acknowledgments
We thank Serge Dumoulin for helpful discussions. This work was supported by NEI grants 5 R21
EY016199-02, 5 F31 EY015937 and Whitehall Foundation grant 2005-05-111-RES.

122

Cosyne 2008

Friday evening, Poster session II-4

	

	

	



	






		




	
		




 !"#$%&




%  '
    	(  
      	
  	  	  	'  	  )	
  	      '  	      '  		  	  	&  *
'
				)		'		(		

	''	+,$-''.& 
			(	
	
/	(		
0	1	
					
	'+  23.& 4	$		(		
	)		'
					1
					(	$	(	$
		2,3&
*	
'$		$'	)5
'		
			$	$(		&6
''
$'
	)1&4		
'		(		
	
	
)		
'							273&
4(		
			)(		
	
			

')		
		
81
)				)		&
%)1
		/	
	
'
 '	 	
 		&	' 	 	 		  	 	
					)
'&*		'		'	
$(		
					)
'
'		'1
			&9''				
	'			'		)	
'&9	
  	  (		
      	  '      	  	'  	    	  

	
  		
      	  
'  '  2:3&  4  )  	  		  	
		'	1	
'	
					
)'	
						&4)		
	'				
'''2;3&
!
	

4)1)	
		6#-	)1	#	9
&
"




23*&<1=&&>,,+"">.&
2,3 &?&?=&
&!:@,A:!$,A-,+,AAA.&
2739&(&<&B
&&		&":,7!A7+,AA;.&
2:3&<		&<&B
&&		&+,AA>.	
C	D@A>&,AA28
&3+,AA>.&
2;3&<		&<&B	

	

123

Friday evening, Poster session II-5

Cosyne 2008

An Ontogenetic Approach to Invariant Recognition
Urs M. Bergmann1 and Christoph von der Malsburg1
1

Frankfurt Institute for Advanced Studies, Germany

A central problem for the sensory parts of the brain, and the visual cortex in particular, is to represent the
input invariantly under many transformations (e.g. translation, scale, lighting...). This representation enables
the brain to recognize the constant entities of the physical world (i.e. the objects), although their retinal
representation varies signiﬁcantly. We assume that invariant recognition is based on ‘retransforming’ the
visual input to a normalized form [1]. Implementing efﬁcient control for these transformations enables the
system to perform at physiological observed recognition times [2]. It would be advantageous for an animal
to have the necessary invariance transformations implemented at birth, making e.g. postnatal learning easier
and faster.
We follow and extend the approach of [3] to multimap formation: the organization of several maps with
different transformation parameters. A key idea is to use an adiabatic elimination process to derive the
formulas for the dynamics. This has two major advantages: First, the only dynamic variables to deal with
are the weights themselves. Second, although we derived the equations from an explicit model, an abstract
formulation captures a wider range of possible underlying mechanisms.
The main formula of the system can be written as Wτmρ = Fτmρ Wτmρ − Wτmρ Bτmρ (F W ), where τ and ρ
are the indices in the input and output ﬁeld, respectively, and m denotes the different units for which the
receptive ﬁelds are to be organized. The second term is a neighborhood cooperation term, where the ‘ﬁtness’
F m = W m ∗C of a synapse is a convolved version of the the weights W m . This term ensures the emergence
of a topographic map and is implemented
decaying excitatory
weights with distance. The
 by monotonically

m +  Xm + 
m /(2N + M ), serves two distinct
X
X
last competitive term, Bτmρ (X) =
τ  τ ρ
ρ τ ρ
m τ ρ
purposes. First, the ﬁrst two terms ensure a one-to-one mapping in each of the maps. And second, the last
term makes the different mappings compete at the same locations τ and ρ, so that the ﬁnal mappings will be
mutually orthogonal in connection space [4], thus implementing different translations. This is implemented
via competition of the axons for some growth-regulating transmitter.
We tested the mechanism on unstructured all-to-all connectivity between two layers and found the translations to emerge. A two-layered system would need an unrealistic number of connections. However, a
realistic minimal multilayer system [4], for which an ontogenetic mechanism has been proposed [5], has
been shown to be a viable basis for the proposed mechanism, exemplifying the ﬂexibility of the process.
Acknowledgments
Supported by EU project FP6-2005-015803 “Daisy”, the Hertie Foundation and the Volkswagen Foundation.
References
[1] J. Zhu and C. von der Malsburg, Neural Networks 17:1311-1326, 2004.
[2] S. Thorpe, D. Fize and C. Marlot, Nature 381:520–522, 1996.
[3] A. F. Häussler and C. von der Malsburg, J. Theor. Neurobiol. 2, 1983.
[4] B. A. Olshausen, C. H. Anderson and D. C. Van Essen, J. Neurosci. 13:4700–4719, 1993.
[5] P. Wolfrum and C. von der Malsburg, Artiﬁcial Neural Networks – ICANN 2007, 2007.

124

Cosyne 2008

Friday evening, Poster session II-6

Is the rodent a valuable model system for studying invariant visual
object recognition?
Davide Zoccolan1, David D Cox1,2, Nadja Oertelt1,2, Basma Radwan1, Sabrina
Tsang1 and James J DiCarlo1
1

McGovern Institute for Brain Research, MIT;

2

The Rowland Institute at Harvard

Despite the many advantages rodents offer in terms of experimental accessibility, they have never been extensively used as models to investigate the neuronal processing of visual objects. A crucial step in establishing whether rodents are suitable models for the study of object vision, is to assess
if they are capable of invariant object recognition – i.e., recognition of visual objects across the range
of transformations that they typically undergo during natural vision (e.g., changes in position and size).
In this study, we tested rats (Long-Evans) capability to discriminate between two geometrical shapes (square
and triangle) presented at different sizes, positions and orientations. In each trial, a single shape was presented on a computer monitor and the animal had to report its identity. The experiment consisted of three phases.
In phase I, each animal learned to discriminate between the two shapes presented at ﬁxed size (40° of visual
angle), position (center of the monitor), and orientation. Naïve rats typically achieved > 70% correct performance in 2-3 weeks of training. In phase II, the animals were trained to perform the task while either the size,
the horizontal position, or the orientation of the shapes was separately varied. Rats readily acquired this task
(> 70% correct performance) across sizes ranging from 40° to 10°, positions spanning ±12°, and orientations
spanning ±40°. During phase III of the experiment, we asked if rats could generalize to novel combinations
of size, position, and orientation (a total of 100 transformations of each target shape were tested). Performance was typically > 70% correct for nearly all of these previously unseen transformations (performance
of one rat over a subset of the tested conditions is shown in the ﬁgure) and even for a fraction of transformations for which feedback was withheld.
Position (o)

These results show that rats can readily learn to: 1) discriminate between
simple visual shapes; 2) disregard
variations in their position, size and
orientation; and 3) generalize to novel
views. This suggests that the rat visual
system contains the fundamental mechanisms that support object recognition.
Therefore, given the broad ranges of
experimental approaches available
in rats, they may be a powerful new
model system to study the neuronal
basis of invariant object recognition.

100
70

64

77

67

59

5

75

76

75

68

74

0

78

68

83

75

66

-5

73

81

83

73

67

-10

58

73

75

72

64

-30

-15

0

15

30

75

50

% Correct

10

25

Orientation (o)

Acknowledgments
We thank E. Flister, P. Meier, D.
Pritchett, P. Reinagel, J. Ritt, and G. Westmeyer for helpful discussions about training rodents in sensory
discrimination tasks. DZ was supported by a Charles A. King Trust Postdoctoral Research Fellowship.

125

Friday evening, Poster session II-7

Cosyne 2008

Physiological differences between neurons in layer 2 and layer 3 of
primary visual cortex (V1) of alert macaque monkeys
D. Max Snodderly and Moshe Gur 1,2
1

University of Texas, Austin,

2

Technion, Haifa, Israel

The physiological literature does not distinguish between the superficial layers 2 and 3 of primary visual
cortex even though these two layers differ in their cytoarchitecture and anatomical connections. To
distinguish layer 2 from layer 3, we have analyzed the response characteristics of neurons recorded during
microelectrode penetrations perpendicular to the cortical surface. Extracellular responses of single neurons to
sweeping bars were recorded while macaque monkeys performed a fixation task. Data were analyzed from
penetrations where cells could be localized to specific depths in the cortex. Although the most superficial
cells (depth: 145-371 μm; presumably layer 2) responded preferentially to particular stimulus orientations,
they were less selective than cells encountered immediately beneath them (depth: 386-696 μm; presumably
layer 3). Layer 2 cells had smaller spikes, higher levels of ongoing activity, larger receptive field activating
regions, and less finely tuned selectivity for stimulus orientation and length than layer 3 cells. Direction
selectivity was found only in layer 3.
These data suggest that layer 3 is involved in generating and transmitting precise information about image
features, while the lesser selectivity of layer 2 cells may reflect more global processing or top-down
modulation of V1 by feedback from higher cortical areas. As far as we know (always subject to correction)
these are the first physiological measurements to clearly separate functional properties of layer 2 from layer 3
in any cortical area.
Acknowledgments
Supported in part by National Eye Institute grant EY12243 to DMS and by USA-Israel Binational Science
Foundation grant 2003252 to MG and DMS

126

Cosyne 2008

Friday evening, Poster session II-8

Quantifying noise reduction in sensory-motor processing
Leslie C. Osborne 1 and Stephen G. Lisberger 2
1
Sloan-Swartz Center for Theoretical Neurobiology, Department of Physiology,
2
Howard Hughes Medical Institute, UCSF, San Francisco CA 94143-0444, USA.
In tasks where precision is at a premium, the brain must reduce the impact of internal noise sources in
sensory estimation by integrating incoming stimulus information across neural populations and across
time. To study how the brain reduces noise, we added noise at the sensory input and measured its impact
on behavior. In sensorimotor systems like pursuit, where eye movements are a reliable read-out of the
brain’s estimate of the motion of a visual target, behavior can reveal the underlying computation of target
direction and speed from incoming visual motion signals. To probe how the brain integrates visual motion
signals, we performed pursuit experiments in which we rewarded monkeys for tracking targets that
moved stochastically. We had two goals for these experiments.
Our first goal was to determine how adding noise designed to increase variation in sensory estimates of
target direction or speed would affect motor performance. In previous work, we put forward the
hypothesis that variation in pursuit initiation is dominated by variation in the underlying sensory
estimation of the target’s trajectory. We demonstrated that behavioral variation had the appropriate form
and scale predicted from variations in visual estimates of target direction and speed and also that the time
course mutual information between the eye movement and the stimulus is highly correlated with the time
course of stimulus information signaled by motion-sensitive visual cortical neurons. In those
experiments, the components of the target, a pattern of bright “dots” against a dark background, all moved
coherently at the same speed and in the same direction. In the present experiments, we selected the
direction or speed of each dot in the pattern randomly at each time step from a distribution of potential
trajectories. This created a target in which each dot did a random walk in time about the same central
trajectory, and at any point in time there was a distribution of dot directions or speeds within the target.
We found that variation in pursuit direction and speed scaled linearly with the variance of stimulus pattern
speed or direction even when the motor task was held constant. This result supports the hypothesis that
sensory variation is the dominant source of behavioral variation in pursuit and further suggests that the
visual estimate of target motion is sensitive to spatial variation in motion inputs.
Our second goal was to measure the temporal filter applied to incoming visual motion signals to drive
pursuit. To isolate the temporal processing, we used targets in which each dot in the target performed an
identical random walk about a central trajectory such that motion was spatially uniform but temporally
stochastic. We computed the optimal linear filter between target and eye movement. The linear model
did a surprisingly good job at predicting eye velocity, accounting for up to 70% of eye velocity variance.
Filters had a width of about 25ms (full width at half maximum), consistent with the effective temporal
sampling estimated from the first experiment. Stimulus manipulations that increased the frequency
bandwidth of the stimulus also tended to narrow temporal filters slightly. We also found that filter shape
did not change between the initial open-loop (sensory-driven) portion of the eye movement and the later
closed-loop (feedback available) portion of the pursuit response.
Our results demonstrate that pursuit movements are well-predicted from simple models of spatial and
temporal integration of incoming visual motion signals. Through an analysis of variation, we have shown
that behavior can be closely tied to the brain’s sensory estimates of target trajectory. Our measurement of
the temporal filter between target and eye velocity provides a strong constraint for models of the brain’s
decoding of visual motion signals at the system level.

127

Friday evening, Poster session II-9

Cosyne 2008

Receptive Field Structure in the Cat's Perigeniculate Nucleus.
Vishal Vaingankar1, Cristina Soto Sanchez1, Xin Wang1, Friedrich T.
Sommer2, and Judith A. Hirsch1
1

University of Southern California, 2University of California, Berkeley

Before reaching the cortex, axons of relay cells in the lateral geniculate nucleus of the thalamus send
collaterals to the perigeniculate nucleus. This structure, a network of inhibitory cells that is part of the
larger thalamic reticular formation, sends dense axonal projections back to the lateral geniculate to form
the first feedback loop in the visual pathway. Early studies that compared the lateral and perigeniculate
nuclei showed that the receptive fields of cells in the two structures were different from each other. Relay
cells have receptive fields that are monocular and built of two concentrically arranged subregions that
have opposite preference for stimulus polarity, On or Off. Cells in the perigeniculate have been described
as having receptive fields that are usually binocular, large and oddly shaped and have spatially
overlapping On and Off responses [1]. Even though the spatial and temporal frequency tuning of cells in
the perigeniculate had been analyzed quantitatively and found to be diverse, there had been no
commensurate analysis of receptive field structure. As well, there are virtually no physiological studies
that address the mechanisms by which these receptive fields in the perigeniculate are built.
Thus, we were motivated to reexamine visual responses of cells in the perigeniculate nucleus. Our
experimental approach was to make simultaneous extracellular recordings from several cells in response
to visual stimulation (Gaussian noise) and compare receptive fields of cells recorded in the lateral and
perigeniculate nuclei. The receptive fields of cells in the lateral geniculate nucleus were straightforward
to reconstruct. The receptive field of any given relay cell could be obtained by computing the mean
stimulus that elicited a spike, the spike triggered average. This method is not adequate for analyzing
records from the perigeniculate because On and Off subregions usually overlap and cancel each other's
effect in the spike triggered average. Therefore, we used the method of spike triggered covariance [2],
which identifies the axes of maximum variance in the set of stimuli that evoke spikes (the spike-triggered
stimulus ensemble). This technique extracts a set of subunits made of spatiotemporal filters that
characterize neural stimulus preferences and corresponding nonlinear functions that map the output of
each filter to the neural response. With this method, we were able to recover up to four subunits for
single cells (significance assessed with the nested bootstrapping method [3]). The nonlinearities could be
approximated by a squared function, which indicates that all the subunits were excitatory regardless of
stimulus polarity. Further, the filters changed polarity over time, much like the spike triggered averages
made from the responses of relay cells. Thus our results are consistent with the idea that receptive fields
in the perigeniculate nucleus are mainly built by convergent input from On and Off center relay cells.
Acknowledgments: The work was supported by NIH grant EY09593 and the Redwood Center for
Theoretical Neuroscience. We thank Qingbo Wang for contributing software.
References:
[1]Binocular corresponding receptive fields of single units in the cat dorsal lateral geniculate nucleus. KJ
Sanderson, I Darian-Smith, PO Bishop, Vision Res. 1969:1297-303, 1969.
[2]Isolation of relevant features from random stimuli in cortical complex cells. J Touryan, B Lau, Y Dan,
J Neurosci 22: 10811-10818, 2002
[3]Spike-triggered neural characterization. O Schwartz, JP Pillow, NC Rust, EP Simoncelli, Journal of
Vision 6: 484-507, 2006

128

Cosyne 2008

Friday evening, Poster session II-10

A unifying account of peri-saccadic receptive ﬁeld
dynamics
Marc Zirnsak, Fred H Hamker
Westfälische Wilhelms-Universität Münster, Germany
At the time of an impending saccade receptive ﬁelds (RFs) dynamically alter their spatial proﬁle.
This phenomenon has been observed in several monkey extrastriate areas like V3A, LIP, FEF and
also in the SC. Commonly these effects are interpreted in terms of ”remapping”, i.e. even prior to
saccade onset some cells become responsive to stimuli presented in their future post-saccadic RF.
However, recordings in V4 revealed a different pattern: The overall effect of RF dynamics shows a
shrinkage and shift towards the saccade target (ST).
Here we aim to provide a unifying explanation for the seemingly contradictory observations based
on a recently developed computational model of peri-saccadic perception [1]. It is assumed that
the planning of a saccade is accompanied by a spatially selective feedback signal centered at the
internal representation of the ST. This signal targets multiple visual areas and increases the neurons
gain. On the level of single cells, neurons encoding the ST are activated stronger. However, the
population response as a whole is distorted, inducing brief perceptual errors termed ”compression
of visual space”. Further, we ﬁnd combinations of shrinkage, shift and also expansion of RFs.
Cells with RF centers similar to those tested in remapping studies are consistent with the reported
dynamics, i.e. show a pre-saccadic increase of responsivity primarily along the saccade vector
(Figure 1a). However, other cells do not (Figure 1b). Instead, these cells change their activity
proﬁle towards the ST as reported in V4. Thus, we can account for the different observations by
considering the exact RF position, suggesting that remapping is a special case limited to a certain
region of visual space.
“remapping” cell

b)

“non-remapping” cell

60
40

RF-center: (31,30)

20
0
-20

RF-center: (2,10)

-40
-60

-60 -40 -20

0 20 40 60

-60 -40 -20

Visual space (deg)

a)

Figure 1: Examples of receptive ﬁelds (RFs) showing ”remapping”
(a) and ”non-remapping” (b). The blue proﬁle indicates the presaccadic RF and the red proﬁle the peri-saccadic RF just before saccade onset. The actual saccade is indicated by the arrow, where the
end point denotes the saccade target.

0 20 40 60

Visual space (deg)

Acknowledgments
This work has been supported by BMBF 01GW0653.
References
[1] The peri-saccadic perception of objects and space. FH Hamker, M Zirnsak, D Calow and M
Lappe, PLoS Comput Biol in press.

129

Friday evening, Poster session II-11

Cosyne 2008

Saccade-related LFP oscillations set the stage for processing
visually evoked spikes
Junji Ito1 , Pedro Maldonado2 , and Sonja Gruen1
1

2

RIKEN Brain Science Institute,

University of Chile

When inspecting visual scenes, primates perform on average four saccadic eye movements per second,
which implies that identiﬁcation of image components is accomplished in less than 200ms. Thus, individual
neurons may contribute only with a small number of spikes for these complex computations, suggesting that
information is encoded not only in the ﬁring rate but also in the timing of spikes. To test this hypothesis we
analyzed the neuronal activities in V1 in relation to eye movements and ﬁxations performed while monkeys
freely viewed natural scenes [1]. LFP oscillations in the visual cortex have recently been suggested to act as a
mechanism for converting ﬁring rate code into temporal code [2]. It was also suggested that this mechanism
could be used to realize the temporal coding scheme of latency coding, where the latency of the ﬁrst spikes
encode particular stimulus features [3]. We hypothesize that such a mechanism would require the precise
timing of early spikes, in particular the very ﬁrst spikes elicited in a neuron after a stimulus input, i.e. after
the onset of a visual ﬁxation.
Thus, we analyzed the relation of single spikes, particularly the ﬁrst spikes elicited by visual input, to LFP
oscillations related to eye movements. By examining the event-related averages of LFP signals and ﬁring
rate, we found that there are LFP oscillations in the beta band related to saccade onset (cmp to [4]), while
ﬁring rate are time-locked to ﬁxation onset, thus reﬂecting responses to the visual input. To directly test
our hypothesis, we explored the relationship of the timing of individual spikes to the phase of the LFP
oscillation by calculating the degree of phase locking (phase locking value, PLV) of the spikes to the evoked
beta-oscillation. Relating all spikes elicited during ﬁxation to the oscillation evoked by saccades did not
reveal any signiﬁcant phase locking. However, relating only the ﬁrst spikes to the oscillation, we obtained
signiﬁcant phase locking with a maximum at about 70 ms after ﬁxation onset. A possible interpretation of
this result is that these ﬁrst spikes are part of a ﬁrst wave of visually evoked activity and that the saccadeevoked LFP oscillations serve as a reference signal to accurately time these spikes for further processing.

Firing rate (Hz)

3

20

2
10

0
-1

5

0.2
0.15
0.1
0.05

-2
-3
-0.1 -0.05

First spike PLV
Surr. med.
Surr. 95%

0.25
15

1

0.3

PLV

4

Firing rate (all spikes)
Firing rate (first spikes)
Avg. LFP

Average LFP (μV)

5

0
0 0.05 0.1 0.15 0.2 0.25 0.3
Time from fixation onset (s)

0
0

0.05

References
[1] Maldonado et al. (under review)
[2] Fries et al., Trends Neurosci. 30:309-16 (2007).
[3] VanRullen and Thorpe, Vis. Res. 42:2593-615 (2002).
[4] Bartlett et al., Exp. Brain Res. 25:486-509 (1976).

130

0.1
0.15
0.2
0.25
Time from fixation onset (s)

0.3

Figure: (Left) Average ﬁring rate for
all spikes and ﬁrst
spikes, and average
LFP. (Right) Phase
locking value for
ﬁrst spikes. Dark
area indicates 95
percentile of the
PLV distribution
for surrogate data.

Cosyne 2008

Friday evening, Poster session II-12

Monocular Proximity Derivation in the Teleost Midbrain
Brett J Graham and David PM Northmore1
1
University of Delaware
A recent study in two teleost fishes reported that activity in the midbrain nucleus isthmi (NI) ramps up
linearly during object approach at a rate proportional to object speed, suggesting that NI activity signals
object proximity[1]. NI is reciprocally connected with the optic tectum (OT) which in teleosts receives
retinal input almost exclusively from the contralateral eye and plays a critical role in orienting, approach
and avoidance behaviors[3]. The medullary Mauthner cells in fish receive direct input from tectum and a
single Mauthner spike is sufficient to produce an avoidance movement[2]. Because object approach elicits
a ramp up in depolarization of Mauthner neurons[2], NI-OT interactions may well play a role in shaping
the input to these neurons
A model was constructed to examine the NI proximity response. The model consisted of a retina, OT and
NI, each modeled using integrate-and-fire neurons. Visual stimuli consisted of video files of computer
generated looming spheres of various sizes and speeds. Frames from these video files were convolved
with a Laplacian of Gaussian kernel and high-pass-filtered to provide input to the simulated retina. These
transient retinal units made excitatory synapses with tectal units including simulated inhibitory
interneurons (IN) and excitatory periventricular neurons (PV), the latter projecting to NI. OT was also
connected laterally, IN-to-PV and PV-to-IN, and received a feedback projection from NI. NI neurons were
simulated using bursting model neurons that responded when the tectal signal exceeded a particular
threshold.
During object approach, the population activity of PV units signaled the rate of expansion, while the
population activity of NI increased linearly at a rate directly proportional to the speed of the looming
sphere. This response was observed across the range of sphere radii and looming speeds tested.
The results of this modeling study suggest that the feed-forward tectoisthmic projection carries a
representation of the rate of image expansion that NI processes to derive object proximity. Furthermore,
individual NI cells may become active at a particular rate of image expansion creating a population
activity that rises linearly at a rate proportional to looming speed. This signal is then returned to tectum
where it could be used to initiate a variety of behaviors, such as Mauthner cell mediated escape.
References
[1] Responses of the nucleus isthmi in teleost fishes to looming objects and other moving stimuli. S.P.
Gallagher and D.P.M. Northmore, Visual Neurosci. 23(2): 209–219, March 2006
[2] Neural representation of object approach in a decision-making motor circuit. T. Preuss, P.E. OseiBonsu, S.A. Weiss, C. Wang, and D.S. Faber, J Neurosci. 26(13): 3454-3464, March 2006
[3] Springer A.D., Easter S.S., and Agranoff B.W. The role of the optic tectum in various visually
mediated behaviors in goldfish. Brain Research 128: 393-404, June 1977

131

Friday evening, Poster session II-13

Cosyne 2008

Putative Fast-Spiking and Pyramidal Neurons Show No Difference
in Orientation Selectivity in Macaque Visual Area V4
Emily B. Anderson1,2, Jude F. Mitchell1, Kristy A. Sundberg1, and John H.
Reynolds1
1

Salk Institute for Biological Studies,

2

University of California, San Diego

Neurophysiological studies in primary visual cortex have found evidence that fast spiking inhibitory
interneurons tend to be more broadly tuned for orientation than are pyramidal cells. We find that we can
distinguish between two classes of neurons in macaque area V4, which we believe correspond largely to
fast spiking interneurons and pyramidal neurons. These two classes differ markedly in their attentional
modulation [1]. Here, we compared the orientation selectivity of these two classes of neurons in macaque
area V4. As reported earlier, we find that neurons with brief action potentials have a significantly higher
firing rates than broad spiking neurons, consistent with the properties of fast spiking interneurons and
pyramidal neurons. After controlling for artifactual differences in tuning induced by differences in firing
rate, we find that there is no significant difference in orientation tuning across the two classes of
cells. This may be a difference between V4 and V1, where orientation tuning first emerges. Orientation
tuning in V4 may simply be inherited from its inputs and be reflected in the response properties of both
cell classes. Alternatively, there may be a significant difference in tuning width among layer 4 neurons in
V4, where this difference is most pronounced in V1. Distinguishing between these possibilities will
require careful measurement of the laminar position of each V4 neuron.

Acknowledgments
We thank C. Williams and J. Reyes for help with animals and technical support. This work was supported
by a grant from the National Eye Institute (EY016161, J.F.M. and J.H.R.), a National Institutes of Health
Training Fellowship (J.F.M.), and two National Science Foundation Graduate Research Fellowships
(E.B.A. and K.A.S.).
References
[1] Differential Attention-Dependent Response Modulation across Cell Classes in Macaque Visual Area
V4. J.F. Mitchell, K.A. Sundberg, and J.H. Reynolds, Neuron 55(1):131-41, Jul 5 2007.

132

Cosyne 2008

Friday evening, Poster session II-14

Predicting response variability in the primary visual cortex
Timothy J Blanche, Kilian Koepsell, Nicholas Swindale, Bruno A Olshausen
The Poisson-like spiking of visual cortex neurons driven by drifting gratings or noise stimuli has led to
the general conclusion that cortical neurons are noisy and temporally imprecise. In recent work, however,
we found that naturalistic stimuli evoke spike trains that are remarkably reliable (fano factors as low as
0.1), precise (temporal jitter on the order of a few milliseconds), and sparse (lifetime sparsity ~0.85) [1].
What is it about natural stimuli that produce such highly reliable, sparse responses? Conversely, why are
the responses to ‘traditional’ stimuli so variable?
One clue comes from looking at the larger scale response variability as measured by the local field potential (LFP), and the coupling between the LFP and single-unit spike timing. High-density silicon electrode
arrays (polytrodes) were used to make simultaneous recordings of 100+ neurons spanning all cortical layers in anesthetized cat primary visual cortex [2]. Brief 5s white noise and natural scene movie segments [3] were presented 25-75 times (50Hz frame rate, 200Hz refresh). The complex analytic signal was
used to obtain an instantaneous measure of LFP phase in frequency bands from 1-150Hz. LFP inter-trial
phase coherence (ITC) was also calculated. LFP modulation of spike timing was quantified by fitting a
von Mises distribution (circular Gaussian) to the histograms of spike phase relative to the LFP oscillation.
The LFP exhibited stimulus-locked phase alignment across trials at several frequencies. ITC was dramatic
(~1.0) and sustained in response to natural stimuli, whereas to repeated white noise the ITC was weak and
transient. The activity of individual neurons showed varying degrees of phase locking to the LFP with
predominant peaks in all the classically defined EEG bands. Neurons often had multiple peaks.
Spike event reliability and temporal precision were correlated with the broadband trial-averaged LFP amplitude, however ITC was an even better predictor of spike response variability. For individual neurons,
the spike count reliability of stimulus-evoked events (as quantified by the fano factor) was correlated with
ITC in several distinct frequency bands. Periods of high ITC in the beta & gamma bands were predictive
of not only spike count reliability but also coincided with spike events that showed the highest temporal
precision (onset spike time jitter with  ~ 3ms).
We have demonstrated that neurons in the primary visual cortex are capable of high spike timing precision comparable to that reported in the lateral geniculate and retina, but this is apparent only when the
larger scale activity is also coherent. When ongoing stimulus-independent cortical dynamics dominate, as
is the case with white noise stimuli, response variability is high. Understanding the complex interplay between the stimulus and neuronal and network level dynamics is necessary not only for estimating the limits of spike timing reliability and precision, but is also a pre-requisite for revealing the true nature of spatiotemporal activity patterns in neural populations that may underlie cortical processing.
[1] Blanche TJ & Koepsell K. (2007) Spike timing precision and the influence of cortical dynamics. COSYNE, Salt Lake City, Utah.
[2] Blanche TJ, Spacek MA, Hetke JF, Swindale, NV. (2005) Polytrodes: high-density silicon electrode
arrays for large-scale multiunit recording. J NPhys. 93(5):2987-3000.
[3] Kayser C, Salazar RF, Konig P. (2003) Responses to natural scenes in cat V1. J NPhys. 90(3):1910-20.

133

Friday evening, Poster session II-15

Cosyne 2008

The organization of classical and non-classical receptive ﬁelds
of V1 neurons of the California ground squirrel (Spermophilus
beechey)
Hsin-Hao Yu1 , Virginia R. de Sa1 , and Martin I. Sereno1,2
1

Cognitive Science, UC San Diego,

2

Psychology, Birkbeck College, London.

The squirrel is a highly visual rodent with a V1 lacking orientation columns[1] and its long-range horizontal ﬁbers do not form patchy terminals[2]. Since these anatomical features inﬂuence the organization of
receptive ﬁelds (RF), the classical RF (CRF) and non-classic RF (nCRF) of this species were studied in
detail with extracellualr single-unit recording. In addition to drifting gratings, dynamic (reverse-correlation)
stimuli were used to examine the timecourse of their interaction.
The size of V1 integration ﬁeld was measured by four independent measures: the minimal response ﬁeld
(MRF), the grating summation ﬁeld (GSF), the annular minimal response ﬁeld (AMRF), and reverse correlation kernels. While the MRF was small (mean=4.7◦ in diameter), the spatial extent to which bigger
gratings elicit higher spike response (GSF) was very large (15.3◦ ) for the majority (88.6%) of the neurons
recorded. Reverse correlation stimuli [3] produced subﬁeld maps that were circular with average diameter
7.4◦ , and therefore failed to explain the entire extent of the GSF. The facilitatory effect from beyond the
CRF (as deﬁned by the reverse correlation kernel) was shown to originated mainly from the co-axial “end”
position. Only a small percentage (11.4%) of neurons exhibited surround suppression.
Even when the CRF was excluded from stimulation, annular gratings typically elicited spike responses
that were orientation tuned – possibly a mechanism for detecting illusory contours. Reverse correlation
excluding the CRF demonstrated subﬁeld organization similar to that of the CRF. The nCRF is therefore the
insensitive part of a large and elongated summation ﬁeld which is silent when stimulated by discrete stimuli
but becomes above-threshold when stimulated by more extended patterns.
Orientation selectivity of the nCRF was tested by drifting gratings in center/surround conﬁguration. Most
neurons were facilitated by iso-orientation surround gratings and suppressed by other orientations. A small
population neurons were suppressed by all orientation. This pattern diverged signiﬁcantly from the behavior
of cats and primates. The time course of the nCRF in the orientation domain was also examined by reverse
correlation with rapidly changing (50hz) surround orientation[4]. The results indicate two independent
mechanisms: facilitation was fast and orientation tuned, while suppression was delayed for 20 msec.,with
respect to facilitation, and was not tuned in orientation.
Acknowledgments
This work was supported by NSF Career Grant 0133996 and NSF BCS 0224321.
References
[1] Van Hooser et al. (2005) Journal of Neuroscience (25) 19–28.
[2] Van Hooser et al. (2006) Journal of Neuroscience (26) 7680–7692.
[3] Rust el al. (2005) Neuron (46) 945–956.
[4] Ringach et al. (1997) Nature (387) 281–284.

134

Cosyne 2008

Friday evening, Poster session II-16

Principles of auditory grouping in the presence of multiple sounds.
Ilana B. Witten1, Phyllis F. Knudsen1, and Eric I. Knudsen1
1

Stanford University School of Medicine, Stanford, CA, 94305

The auditory system uses many cues to group and segregate sounds, including timing, spectral, and
localization information [1]. When these cues conflict, timing cues are particularly powerful in grouping
auditory input, and will tend to override localization information. For example, in humans, when two
sounds are presented simultaneously from different horizontal positions, the timing cues cause the sounds
to be grouped, and only the location of the lower frequency sound is perceived [2]. We investigated the
effect of timing cues on discrepant localization cues in the barn owl, using behavioral and
neurophysiological experiments.
We trained two barn owls in a simple task to assess their localization of two simultaneous sounds
originating from different locations. During an initial training period, the owl was rewarded for first
fixating a zeroing light and then generating a head orienting movement to a subsequent sound, which was
either a low (3-5kHz) or a high (7-9kHz) frequency sound. During the test period, either one sound was
presented alone, or else both sounds were presented at the same time from different locations. The owl
was rewarded for any short latency head movement following the onset of the sound. We found that, in
agreement with the results from humans, the barn owls preferentially oriented to the low frequency sound
when the sounds were separated in azimuth. In contrast, when the sounds were separated in elevation, the
barn owls oriented exclusively to the high frequency sound.
To identify the neural basis of this localization bias, we recorded neural activity in response to the same
sounds in the barn owl’s optic tectum (OT; homolog of superior colliculus), which contains a map of
auditory space and is involved in generating orienting head movements to sounds. We found that
although both the low and high frequency sounds generated spatially restricted responses in the OT on
their own, when the two sounds were presented together with a spatial separation in azimuth, the center of
mass of the responses was shifted towards the location of the low frequency sound (n=45). The
dominance of the low frequency representation developed dynamically, stabilizing about 25 ms following
sound onset. The direction and magnitude of the owls’ head movements in the behavioral experiments
correlated with the center of mass of the neural activity. Additionally, we found an agreement between the
behavioral performance and neural activity when the sounds were separated in elevation as well: in this
case, the responses corresponding to the high frequency sound dominated in the space map.
We hypothesize that the localization bias is related to the resolution and spatial ambiguity of the
localization cues. The bias towards the low frequency sound for sounds separated in azimuth may be
related to the greater spatial ambiguity inherent to the high frequency sound in azimuth, while the bias
towards the high frequency sound for sounds separated in elevation may be related to the lack of spatial
resolution provided by the low frequency sound in elevation.
References
[1] A.S. Bregman. Auditory scene analysis: the perceptual organization of sound. Cambridge, Mass:
Bradford books, MIT press. 1990.
[2] Binaural interference and auditory grouping. V. Best, F. Gallun, S. Carlile, and B.G. ShinnCunningham, JASA. 121(2):1070-1076. Feb 2007.

135

Friday evening, Poster session II-17

Cosyne 2008

Information about present and past stimulus features in human
tactile afferents
Hannes P Saal1 , Sethu Vijayakumar1 , and Roland S Johansson2
1

University of Edinburgh, UK,

2

Umeå University, Sweden

The tactile system of the human ﬁngertips provides information which is crucial for dexterous object manipulation and it does so quickly and reliably. Decoding of relevant sensory information from the neural
responses of tactile afferents involves identifying features like curvature of objects in contact and direction
of ﬁngertip forces. A complicating factor is that not only the current but also previous ﬁngertip stimuli may
inﬂuence the afferent signals because of viscoelastic properties of the human skin. We collected microneurography data of multiple single-afferent spike trains (43–72 FA-I and 49–73 SA-I afferents) in response
to repeated and controlled stimulation of the human ﬁngertip with spherical objects of three different curvatures, each applied in ﬁve different force directions. Using methods from information theory [1] and
machine learning, we examined the capacity of different decoding schemes for fast and reliable discrimination of tactile features.
An information-theoretic analysis of spike trains from single afferents revealed that information about the
stimulus is considerably higher when spike timing is taken into account as compared to when only spike
counts are used. Moreover, the ﬁrst spikes of each afferent contain a considerable fraction of the overall
information that is transmitted and more than half of the information transmitted by spike counts alone.
This provides a rigorous justiﬁcation for the success of decoding schemes for rapid classiﬁcation that use
ﬁrst spike latencies, as reported earlier [2].
Previous ﬁngertip stimulation systematically inﬂuenced the latencies of the afferents’ ﬁrst spikes. This effect impairs stimulus discrimination due to the higher amount of jitter in the ﬁrst spike latencies. We found
that ﬁrst spikes in a population of tactile afferents contain enough information to classify the nature of the
preceding stimulus with high accuracy. Furthermore, knowledge about the previous stimulus enhanced classiﬁcation of the current stimulus, which could be discriminated faster and more reliably; i.e., classiﬁcation
improved after reducing the jitter in the ﬁrst spike latencies by compensating for estimated latency shifts
attributed to the past stimulus. This suggests that knowledge (e.g. memory) about past tactile events can in
principle be used to disambiguate the contributions of past and current stimuli to the neural signal, especially
in situations when stimuli are expected to change quickly, for example in object manipulation under natural
conditions.

Acknowledgments
The data used in this study was collected at the Dexterous Manipulation Laboratory at Umeå University,
Sweden. This work was supported by the EPSRC and MRC.
References
[1] Metric-space analysis of spike trains: theory, algorithms and application. JD Victor and KP Purpura,
Network: Computation in Neural Systems 8:127–164, 1997.
[2] First spikes in ensembles of human tactile afferents code complex spatial ﬁngertip events. RS Johansson
and I Birznieks, Nature Neuroscience 7(2):170–177, February 2004.

136

Cosyne 2008

Friday evening, Poster session II-18

Nonlinear signal amplification at genetically-identified central
synapses
Hokto Kazama and Rachel I. Wilson
Harvard Medical School, Department of Neurobiology
In order to understand how diverse computations arise from neural assemblies, it will be important to
integrate synaptic and circuit-level approaches. Ideally, we would like to examine both the in vivo tuning
of specific neurons and the properties of synapses interconnecting them. Here we describe the properties
of identified central synapses in an invertebrate brain circuit, and show how these distinctive properties
can shape the in vivo computations performed by this circuit. The model circuit we use is the Drosophila
antennal lobe, a brain region analogous to the vertebrate olfactory bulb. Olfactory receptor neurons
(ORNs) provide input to the antennal lobe and postsynaptic projection neurons (PNs) carry the output of
this circuit [1, 2]. One virtue of this circuit is that specific types of ORNs and PNs as well as the synapses
connecting them can be genetically labeled and identified for functional characterization.
In vivo, this antennal lobe circuit performs several fundamental computations on olfactory signals [3].
First, the circuit increases the signal-to-noise ratio of odor-evoked spike trains. Individual PN spike trains
are more reliable than individual ORN responses. Second, this circuit performs a nonlinear transformation
on odor-evoked ORN signals. Weak ORN responses are powerfully amplified in PNs, but strong ORN
inputs are not amplified to the same degree. Third, the antennal lobe preferentially transmits information
about odor onset. Whereas ORNs show maintained responses to odors, PNs only respond robustly to odor
onset. Here we show that the unusual properties of ORN-PN synapses can at least partially explain all
these features. The synapse between ORNs and PNs is very strong, reflecting a large number of vesicular
release sites and a high probability of release. This is likely one reason why weak ORN odor responses
are amplified in PNs and why PN odor responses are reliable. Moreover, as expected from a high
probability of release, ORN-PN synapses depress profoundly to strong ORN stimulation. This helps
explain why PN odor responses are transient, and why weak ORN odor responses are amplified more
powerfully than strong responses. Furthermore, because of synaptic depression, synaptic charge in PNs is
more broadly tuned than the original distribution of ORN stimulus frequencies. Thus, we propose that
synaptic depression is also likely to be a major reason why PNs are more broadly tuned to odors than the
presynaptic ORNs.

References
[1] Hallem, E.A. and J.R. Carlson, The odor coding system of Drosophila. Trends in Genetics, 2004.
20(9): p. 453-9.
[2] Wilson, R.I. and Z.F. Mainen, Early events in olfactory processing. Annual Review of Neuroscience,
2006. 29: p. 163-201.
[3] Bhandawat, V., et al., Sensory processing in the Drosophila antennal lobe increases reliability and
separability of ensemble odor representations. Nat Neurosci, 2007. 10(11): p. 1474-82.

137

Friday evening, Poster session II-19

Cosyne 2008

Different oscillations in the superior temporal sulcus integrate faces
and voices differently
Chandramouli Chandrasekaran, Asif A. Ghazanfar
Princeton University
During vocal communication (including speech), combining auditory and visual information leads to
better detection, discrimination and learning. The neural mechanisms of such audiovisual integration have
received considerable attention. For instance, hemodynamic studies [1] have shown that the superior
temporal sulcus/gyrus is the site of enhanced responses to audiovisual stimuli relative to unisensory
stimuli. In contrast, EEG and MEG studies with human subjects have consistently shown that evoked
responses to audiovisual stimuli from putative sources in the superior temporal gyrus are attenuated
relative to unimodal stimuli [2,3]. Our study uses a macaque monkey model system [4,5] to investigate
the neural bases for these discrepant results.
We hypothesized that different components of the oscillatory hierarchy in the neocortex might integrate
audiovisual information differently depending on the temporal relationship between the auditory and
visual components of the vocal signal. To test this, we recorded local field potentials (LFPs) and single
units from the upper bank of the superior temporal sulcus (STS) of monkeys while presenting them with
face+voice, voice alone and face alone stimuli from their vocal repertoire. The monkey subjects were
simply required to maintain their fixations on the screen for the duration of a given stimulus. Importantly,
our stimuli possessed a natural variation in the timing of the visual component relative to the auditory
component: for each call, there is a delay between the onset of the mouth movement and the onset of the
voiced component (“time-to-voice”) that could range from 66ms to 331ms. We used wavelet-based
spectral analyses to examine LFP responses to bimodal versus unimodal signals and as a function of the
time-to-voice.
We found that in low frequency bands (alpha and theta), audiovisual responses are suppressed relative to
unimodal auditory responses when the time-to-voice is greater than 200ms, but enhanced when the timeto-voice is less than 100 ms. Surprisingly, audiovisual responses in the gamma band were consistently
enhanced regardless of the time-to-voice variable. The finding that the gamma band activity and the low
frequency activity possess different temporal properties supports differential roles for these signals in
integrating multisensory signals. Furthermore, these results explain the discrepant findings in
hemodynamic versus EEG/MEG studies of audiovisual integration. The enhanced BOLD response seen
in hemodynamic studies of audiovisual integration [1] is largely driven by local gamma band activity [6].
In support of this, our data show that, regardless of time-to-voice, gamma band activity is always be
enhanced. With regard to the EEG/MEG studies of audiovisual integration [2,3], the experimental
paradigms incorporated a very large and artificial time-to-voice component (>250ms) to eliminate the
onset of the evoked response to the face in influencing their analyses. Our data show that such large
delays between the visual and auditory components will almost invariably lead to suppressed responses in
the low frequency bands of the LFP signal.
1. Calvert GA. 2001. Cereb Cortex. 11: 1110-1123.
2. van Wassenhove V et al. 2005. PNAS 102: 1181-1186.
3. Besle J et al. 2004. Eur J Neurosci. 20: 2225-2234.
4. Ghazanfar AA & Logothetis NK. 2003. Nature 423: 937-938
5. Ghazanfar AA et al. 2005. J.Neurosci. 25: 5004-5012

6. Nir et al. 2007. Curr Biol. 17: 1275-1285.

138

Cosyne 2008

Friday evening, Poster session II-20

Eye position has an additive effect on neurons in monkey auditory
cortex
Joost X. Maier, Kristin K. Porter and Jennifer M. Groh
Center for Cognitive Neuroscience, Department of Psychology & Neuroscience
and Department of Neurobiology, Duke University, Durham, NC 27708
Recent studies have reported effects of eye position on the activity of neurons in both core and belt
auditory cortex. Eye position signals are necessary for computing coordinate transformations, where
sound location needs to be converted from a head-centered to an eye-centered reference frame. Some
coordinate transformation models involve eye position signals that act as a multiplicative gain, while
others rely on additive operations. Here, we quantified the contribution of additive versus multiplicative
effects of eye-position on spiking activity of neurons in core and caudal belt auditory cortex of the awake
macaque monkey.
We recorded spiking activity from 164 neurons (114 in core, 50 in caudal belt). On each trial, subjects
were presented with noise bursts originating from one of 9 equally spaced loudspeakers spanning 24
degrees ipsi- and contralaterally in the horizontal dimension. Eye position was independently varied by
having subjects fixating LEDs at the same set of locations as the loudspeakers.
We used a two-way ANOVA with sound location and eye position as the two factors to get a first
impression of the contribution of additive versus multiplicative effects, with main effects indicating
additive operations, and interaction effects suggesting multiplicative ones. We found a significant main
effect of sound location on the activity of 36.8% (n = 42) of auditory responsive neurons. Eye position
had a significant effect on 26.2% (n = 43) of all neurons. Significant interactions between sound location
and eye position were observed in only 7% (n = 8) of auditory responsive neurons. This pattern is
consistent with a primarily additive pattern of eye position sensitivity.
The presence of eye position effects in the absence of stimulus-evoked activity (measured during
baseline: n = 35, 21.3%) is also consistent with an additive effect. The relationship between eye position
sensitivity during the baseline and stimulus periods affords another comparison between additivity and
multiplicative interactions. If eye position has a multiplicative effect on the stimulus-evoked response, we
expect the change in activity evoked by the stimulus to be modulated proportionally to the magnitude of
the effect of eye position in the absence of a stimulus. Alternatively, an additive model predicts that there
should be no correlation between the magnitude of the effect of eye position and the stimulus-evoked
activity. We found that the distribution of correlation coefficients was centered around zero (median =
0.14; z = -1.39, p = 0.16), suggesting an additive effect of eye position on the activity of auditory cortical
neurons.
Our results demonstrate a strong contribution of additive, relative to multiplicative effects of eye position
on spiking activity of neurons in auditory cortex. This is more consistent with a vector subtraction model
for coordinate transformations than with a multiplicative gain modulation model. An additional advantage
of an additive operation is that eye position information is preserved and can be relayed to other areas.
Acknowledgments
We thank Abigail Underhill for technical assistance. This work was supported by NSF Grant #0415634.

139

Friday evening, Poster session II-21

Cosyne 2008

Probabilistic auditory scene analysis from natural statistics.
Richard E. Turner and Maneesh Sahani
Gatsby Computational Neuroscience Unit, UCL.
How can the auditory system make sense of the maelstrom of voices and sounds at the proverbial cocktail
party? One set of clues to the answer comes from studies of auditory perception and scene analysis in more
constrained settings. In particular, Gestalt psychology proposes a set of ‘laws’ which qualitatively describe
how auditory features are bound to auditory objects. Three of these form the basis for the model we present
here. First, the principle of good continuation identiﬁes smoothly varying features with a single source and
abrupt changes as a signature of separate sources. For instance, a sequence of alternating high and low
tones separated by small gaps separates into high and low auditory streams, but if the tones are linked by a
smoothly frequency-modulated sinusoid, the tones fuse into a single auditory object. Second, the principle
of closure suggests perceptual completion of fragmentary features. If the frequency modulated links of the
last example are interrupted the grouping is destroyed, but if these gaps are then ﬁlled by noise sufﬁcient
to have masked the linking signal had it been present, the single object is restored once more. Third, the
principle of common fate states that different frequency components group together if they undergo similar
changes. For example, if the components of a harmonics stack are comodulated in frequency or amplitude
they are bound together, but if modulation is independent for each sinusoid, they are heard separately.
Here we derive a simple computational model for auditory scene analysis based on these Gestalt laws.
Where previous such efforts have concentrated on tuning parameters in largely deterministic models to
ﬁt human behaviour, we instead assume that these percpetual laws reﬂect statistical regularities in natural
sounds and learn the parameters of a probabilistic model from acoustic recordings. The model encodes the
general Gestalt principles as follows. The laws of good continuation and closure suggest components of auditory objects are often well described by combinations of sinusoidal signals (sin(θt )) which are modulated
smoothly in frequency (θ̇t = θt − θt−1 ) and amplitude (at ). Moreover, the law of common fate suggests
auditory objects are often formed from a group of these components which share modulatory structure. This
suggests the following probabilitic model for sound waveforms (yt ),
p(akt ) ∼ Slow; p(θ̇1:D,kt ) ∼ Slow and correlated; yt =


k

akt



wdk sin (θdkt ) + σy t

(1)

d

where t is unit Gaussian noise. Auditory scene analysis corresponds to inference in this model or a mixture
of such models.
When ﬁt to acoustic recordings, this model is found to provide an excellent description of natural sounds. For
example, in a task analagous to the closure experiment, it can be used to accurately reconstruct 50ms sections
obliterated from speech waveforms. Currently, we are exploring further connections with psychophysics,
both relating results from the model to psychophysical measurements, and generating synthetic data for use
in experiments.
Acknowledgments
We thank P. Berkes for stimulating discussions. Supported by the Gatsby Charitable Foundation.
References
[1] Auditory Scene Analysis. A. S. Bregman, MIT Press 1990.

140

Cosyne 2008

Friday evening, Poster session II-22

Functional model for nociceptive allodynia and hyperalgesia
P. Aguiar
IBMC – Universidade do Porto, Portugal
Our understanding on how cutaneous nociceptive information is integrated and processed in the dorsal
horn of the spinal cord is still very limited, despite the obvious importance of such knowledge. This is
largely due to the vast complexity of this system: local circuits in the dorsal horn are far from being
simple relay stations transmitting nociceptive information from the periphery nociceptors to specialized
brain structures; instead they actively filter and process signals and, in addition, are subject to several
types of plasticity. Quantitative models of nociceptive information integration in the spinal cord are
nonexistent.
In this work we have made a first step in modeling the dynamics associated with two specific conditions
which may arise from skin injury: hyperalgesia and allodynia. Hyperalgesia is characterized by and
increased response to a stimulus that is normally painful whereas allodynia embody the situation where a
stimulus that does not normally provoke pain becomes painful. Our goal was to model the principal
functional components of the system, namely afferent C-fibers and A-fibers, propriospinal nociceptive
neurons and ascending nociceptive neurons, and assess which dynamics may give rise to the two
conditions. The model was created and simulated using the simulation environments SpiNet and
NEURON. The analysis was performed by assessing the changes (area, threshold and sensitivity)
produced in the receptive fields of propriospinal neurons, due to plasticity mechanisms (sensitization). In
the model, special attention was given to the architectural properties of the circuitry, namely, the
topographic projections and presence of extensive collaterals in the C-fibres, and the local connectivity
within the dorsal horn laminae. We have found in our model that these collaterals are critical for the
production of secondary hyperalgesia, and the extent of the affected area (lowered threshold and
increased sensitivity to pain) is directly related to the collaterals divergence parameters. Also according to
the model, we have found that allodynia may be explained by propriospinal neurons which receive both
nociceptive and non-noxious tactile inputs and are subject to strong plastic changes. This type of neurons
is abundant in the dorsal horn laminae and often gives rise to ascending nociceptive pathways. The
sensory “blending” produced by these neurons, which are subject to plasticity, can well explain why,
under some pathological conditions, non-noxious stimuli produces pain. Understanding how these
conditions occur may provide important information on how to reverse these pathological situations.

Acknowledgments
This work was supported by grant PTDC/SAU-NEU/68929/2006 from Fundação para a Ciência e
Tecnologia.
References
[1] Belmonte C and Cervero F. Neurobiology of nociceptors, Oxford University Press, 1996
[2] Hu N, Zhang H, Hu X, Li M, Zang T, Zhou L and Liu X. Protein synthesis inhibition blocks the latephase LTP of C-fiber evoked field potentials in rat spinal dorsal horn. J Neurophysiol 89:2354-2359,
2003
[3] Sandkühler J. Learning and memory in pain pathways. Pain 88: 113-118, 2000
[4] Ziegler E, Magerl W, Meyer R and Treede R. Secondary hyperalgesia to punctate mechanical stimuli.
Brain 122: 2245-2257, 1999

141

Friday evening, Poster session II-23

Cosyne 2008

Learning Data Representation with a Population of Spiking
Neurons as Encoder
Michael Gutmann1 , Aapo Hyvärinen2 , and Kazuyuki Aihara13
1

The University of Tokyo,

2

3

University of Helsinki,

ERATO, JST

The neural representation of sensory input remains an incompletely understood topic in neuroscience. On
the theoretical side, neural representation has been modeled by data representation. In, for example, [1], an
extension of ICA was used to ﬁnd a linear encoding transform to represent movies. In [2], an efﬁcient representation of sound was learned where matching pursuit [see reference within [2]] was used for encoding.
The modeling of neural representation by means of data representation
could be improved by using established, and more detailed neuron models for the encoding. This would facilitate theoretical investigations into
neural representation on a cellular level.
We derived learning rules for data representation where a population
of spiking neurons acts as encoder. Each neuron was modeled by the
spike response model [3]. Figure 1 explains our approach. In case of
a population of neurons, for each neuron m, an encoding ﬁlter wm and
decoding ﬁlter hm is learned. Figure 2 left, shows an example where
the input (blue) is after learning accurately represented by a population
of spiking neurons (red). We used initially ﬁve neurons for the representation. Learning silenced two neurons, and the other neurons differentiated to code for different aspects of the input (Figure 2 right). As an
extension, we have also derived learning rules for synaptic connections
between the neurons, and included punishment of energy consumption.

In

x(t)

x∗w

x̂(t)



u

f

δ(t − tf )

∗η(t)
Encoder

Data representation
Decoder

h(t − tf )

f : t − T p < tf < t + T d

Figure 1: Only a single neuron is shown to highlight
the encoding process. The convolution of input x
with causal w deﬁnes the input current I. Voltage
u is the sum of I and the recovery current η, and
an optional noise current In . If u passes threshold
θ at t = tf , the spike timing tf is recorded for the
reconstruction, the voltage is reset to −θ, and a new
recovery current is triggered. A delay is allowed in
the decoding. Encoding ﬁlter w and decoding ﬁlter
h are unknown and learned from the input so that
the mean squared reconstruction error is minimized.

00
11
0110
00
11
10
10101111111111111
10100000000000000
0000000000000
1111111111111
1010
10
11111111111111
00000000000000
11
00
1010
10101111111111111
0000000000000
0000000000000
1111111111111
Neuron 2

1

1

Input
Reconstruction
2.5

0.8

0.6

0.4

0.2

0
0

2

0

50

100

0

50

100

150

200

250

300

150

200

250

300

1

Neuron 5

1

1.5

1

0.8

0.6

0.4

0.2

0
0

0.5

Neuron 1

1

1

0

−0.5

0

50

100

150

200

250

300

Neuron 1

Figure 2: Left: Input and reconstruction. The
reconstruction is obtained
 by
decoding all fthe
h (t − tm ).
spike trains: x̂(t) =
m
f m
The input was artiﬁcially created by summing
two randomly shifted bump functions of different width. Right: Spike timings (blue) and partial approximation (red) for each neuron. The
partial approximation
 shows forfﬁxed neuron m
h (t − tm ) to the reconits contribution
f m
struction. Neuron 2 codes for the local mean,
neuron 5 for an intermediate resolution, and
neuron 1 for ﬁne details.

Neuron 2

3

0.8

0.6

0.4

0.2

0
0

−0.2

0

0

50

50

100

100

150

150

200

200

250

250

300

300

Acknowledgments
This research is partially supported by Grant-in-Aid for Scientiﬁc Research on Priority Areas - System study
on higher-order brain functions - from MEXT (17022012). MG is supported by MEXT grant 040680.
References
[1] Hyvärinen, Hurri and Väyrynen, J.Opt. Soc. Am. A, OSA 20:1237-1252, 2003.
[2] Smith and Lewicki, Nature 439:978-982, 2006.
[3] Gerstner and Kistler, Spiking Neuron Models Cambridge University Press 2002.

142

Cosyne 2008

Friday evening, Poster session II-24

Good timing is everything: Millisecond delays between action
potentials carry stimulus information as reliably as firing rates
Martha Nari Havenith1, Shan Yu1, Nan-Hui Chen3, Julia Biederlack1, Wolf
Singer1,2, Danko Nikoli1,2
1
Max Planck Institute for Brain Research 2 Frankfurt Institute for Advanced
Studies, ³ Kunming Institute of Zoology
It was previously shown that groups of synchronized neurons produce fast temporal sequences of action
potentials, which occupy an overall time window of ~10 ms, and can thus unfold within a single cycle of
a gamma oscillation. The temporal order in which units tend to fire within these sequences changes
dynamically depending on stimulus properties [1, 2].
Here, we investigated whether such preferred firing sequences repeated sufficiently accurately to
constitute a reliable neuronal code. To this end, we recorded single- and multi-unit activity from up to 32
recording sites simultaneously in area 17 of anaesthetized cats.
We first determined the preferred delays with which
pairs of units fired action potentials by measuring the
phase offset of the central peak in their cross-correlation
histogram. The preferred delays between different pairs
of units were always additive, i.e. the delays  between
any three units A, B, and C obeyed the relation AB + BC
= AC. This allowed us to convert the pair-wise delays
into a simple temporal sequence describing the order in
which the units tended to fire (see figure).
Using effect sizes, we analyzed the accuracy with which such preferred firing sequences reflected
stimulus properties, and found that the accuracy increased with the strength of oscillatory activity. In the
presence of strong gamma oscillations, the time at which a unit fired in the firing sequence conveyed
stimulus-related information with approximately equal precision as did the firing rates of the same unit. In
addition, different stimulus properties were reflected by firing rates and firing times with different degrees
of precision: While the grating’s orientation was conveyed more accurately by firing rates than by firing
times, the opposite held true for centre-surround stimuli involving spatial phase offsets between isooriented gratings.
Our findings suggest that, despite operating at a time scale of only a few milliseconds, the firing sequence
created by a group of synchronized neurons provides a precise neuronal code. Firing sequences thus have
the strong potential to complement firing rates in the cortical coding of stimulus information.
References
[1] How precise is neuronal synchronization? P. König et al., Neural Computation 7(3):469-485, May
1995.
[2] Spatiotemporal structure in large neuronal networks detected from cross-correlation. G. Schneider et
al., Neural Computation 18(10):2387-2413, October 2006.

143

Friday evening, Poster session II-25

Cosyne 2008

Learning a Novel Visuomotor Task and Performing FourDimensional Movements in a Closed Loop Brain-Machine Interface Using an Adaptive Dynamic Kernel Based Algorithm
Lavi Shpigelman1,2,3,∗ , Hagai Lalazar1,2,∗ , Maayan Shahar2 , Eilon Vaadia1,2
* equal contribution
1 Interdisciplinary
3 School

Center for Neural Computation, 2 Department of Physiology, Hadassah Medical School,
of Computer Science and Engineering, Hebrew University of Jerusalem, Israel

In this work we describe a visuomotor control experiment in which a monkey learned to use a Brain Machine
Interface (BMI) to control the movement of an object on a screen in a 3D and 4D setting. Part of the task
(consecutive target-to-target reaching in 3D) was ﬁrst learned and performed using hand movements. This
task was then relearned in BMI mode. At this stage, control over cursor rotation (around a single axis) was
introduced and learned by the subject from scratch entirely in the BMI setting over the course of a few days.
Once the monkey was able to perform cursor rotation to match given target rotations with good precision, the
two tasks were combined into one 4D BMI task (reaching and rotating to match consecutive targets). This
task proved substantially more difﬁcult for the monkey than the sum of it’s parts but the subject was able to
perform it with high success rates after a few more days. Hand movements (during hand control sessions)
were recorded using an optical tracking device. In BMI mode the monkey had both hands restrained and
appeared to keep them relaxed as gauged with high resolution video and optical sensors. The BMI was
driven by single and multi-unit activity recorded from a 96 electrodes array (Cyberkinetics Inc.), implanted
in primary motor cortex.
An adaptive version of a learning algorithm called KARMA (Kernel Auto-Regressive Moving Average)
was used to learn and infer intended movements from the recorded neuronal patterns. This BMI algorithm
is a kernel method that combines both previous predictions and current neural observations in a non-linear
fashion to make new predictions. It has several attributes which were important in this work.
1. Movement prediction can be done in real-time (every 50 milliseconds). To do this with a kernel based
method, the number of support points was kept small.
2. The algorithm learns extremely fast. Even though the model was learned from scratch every day, the
algorithm managed to achieve the ﬁrst successful trial in about 5 attempts. This means learning a
usable model from 30 seconds of data.
3. The algorithm adapts continuously in the background. After movements are performed using the
previously learned model, the neuronal activity from these sequences is then paired with desired
movements that are based both on the cursor movements (produced by the previous model) and the
target locations. As new sequences enter the algorithm’s learning process, previous examples are
thrown out in a probabilistic fashion, inducing a better ﬁt to recent observations as compared to older
ones.
4. The algorithm creates an implicit model of the movement dynamics that are smooth and seem natural.
This is due in part to the way in which desired trajectories are created and also to the fact that the
previous predictions are provided as part of the input for the next prediction.
Acknowledgments
Supported in part by the BSF, ISF and J&J grants and by special contributions from the Rosetrees Trust and
the Ida Baruch fund. EV is the Jack H. Skirball Chair of Brain Research.

144

Cosyne 2008

Friday evening, Poster session II-26

Spatiotemporal encoding by a CA1 pyramidal neuron model
Eleftheria Kyriaki Pissadaki1,2 and Panayiota Poirazi1
1

Institute of Molecular Biology and Biotechnology, FORTH, Heraklion, Crete,
Greece.
2
Department of Biology, University of Crete, Heraklion, Crete, Greece.
While the firing pattern of hippocampal CA1 pyramidal neurons changes from regular spiking to bursting
during various behavioural tasks, the role of such activity changes in information processing and memory
storage remains unknown. Using a refined version of a detailed biophysical model of a CA1 pyramidal
neuron [1], we investigate the firing properties of these cells under layer-specific synaptic stimulation,
which varies both in the timing as well as the spatial arrangement of activated synapses. A stimulation
protocol that resembles the in vivo activity of the two principal afferent pathways [2], namely the Schaffer
Collaterals and the Temporoammonic (TA) pathways is induces a repertoire of firing patterns, ranging
from facilitation to repression of baseline responses, depending on the spatio-temporal characteristics of
the incoming signals. Specifically, we find (a) that if the two pathways are stimulated within less than
100ms (TA goes first), the activity of the model neuron changes from regular spiking to strong bursting
irrespectively of the arrangement of stimulated contacts and the amplitude of this facilitation declines
exponentially for longer delays. (b) When synapses are activated diffusely throughout the two receiving
layers of the model neuron (SC and SLM, respectively) and the temporal offset ranges from 190-340ms
this facilitation turns into repression, whereby somatic firing is significantly reduced. (c) Activity
repression becomes stronger when synapses in the SLM -but not the SC- layer are activated in clusters
and (d) disappears completely when activated synapses are clustered in both or just the SC layer. Overall,
this variation in somatic responses is modulated primarily by TA (cortical) signals which selectively gate
or enhance SC (intra-hippocampal) inputs according to their spatio-temporal characteristics. Interestingly,
partial information about such spatio-temporal characteristics of incoming stimuli is captured by the
average burst Inter-Spike-Interval, which is a steady state parameter of the neuron's response amendable
to a rate code. More importantly however, the temporal characteristics of model responses such as the
timing of the first spike and the succession of spikes within bursts appear to carry discriminatory
information that may be used to classify TA and SC inputs as temporally close or distant and spatially
clustered or diffused with very high accuracy. Taken together, these findings suggest that a single CA1
pyramidal neuron may be capable of detecting and propagating to its targets several spatio-temporal
characteristics of incoming signals via the use of a readable temporal code.
Acknowledgments
We thank K. Sidiropoulou for helpful discussions. This work was supported by the EMBO Young
Investigator award and the GSRT grant PENED 01E311.
References
1.
Poirazi, P., Brannon, T. and Mel, B.W., Arithmetic of Subthreshold Synaptic Summation in a
Model CA1 Pyramidal Cell. Neuron, 2003. 37: p. 977-987.
2.
Dvorak-Carbone, H.a.S., E.M., Patterned Activity in Stratum Lacunosum Moleculare Inhibits
CA1 Pyramidal Neuron Firing. J. Neurophysiol., 1999. 82: p. 3213-3222.

145

Friday evening, Poster session II-27

Cosyne 2008

Learning for Transient Inference
Reza Moazzezi and Peter Dayan
Gatsby Computational Neuroscience Unit, UCL
A new way of conceptualizing the inferential capacities of non-linear recurrent networks has recently
been investigated. This is based on the change in a statistic of the population activity in such a network
over the time since a stimulus is presented. This notion was first proven in the context of a
psychophysical task (the bisection task; figure 1; ε >  < 0 ), using recurrent weights whose values
were determined by hand, and evaluating the change in the centre of mass (in visual space) of the
population. Amongst its other beneficial computational characteristics, evaluating changes offers an
intrinsic solution to the key problem of positional variability (y) in the task.
One central observation was that various different recurrent weights support near-optimal behaviour,
suggesting that a learning algorithm could work well. Here, we considered the simplest case, in which the
centre of mass of the input to the network is compared to the centre of mass of the output of the network
after only one discrete iteration. We applied the backpropagation-through-time algorithm to learn weight
matrices to solve the task, imposing explicitly obvious constraints such as translation invariance.
We found that there is a huge range of recurrent weights that performs near-optimally; different initial
conditions lead to different weights. Figure 2 shows some examples, and figure 3 shows their
performance at the bisection task. This breadth demonstrates the benefits of an appropriate computational
representation of the task. We are presently analyzing the statistical properties of the simpler network’s
far-from-equilibrium inferential mechanisms.
0.05

-0.05
-0.1

0.005

1

0

Fraction Correct

Synaptic weight (W0j)

Synaptic weight (W0j)

0.01
0

-0.005
-0.01
-0.015

-0.15
-10

-5

0

5

10

-5

Distance between units

0

0.9
0.8

Distance between units

Recurrent net

0.7
0.6
0.5
0

5

Ideal observer

0.01 0.02 0.03 0.04 0.05

ε

Poisson noisy activities

0
-0.05
-0.1
-0.15
-10

-5

0

5

0.05
1

Fraction Correct

0.05

Synaptic weight (W0j)

Synaptic weight (W0j)

0.1

0

-0.05

10

-5

Distance between units

Figure 1

0
Distance between units

Figure 2

Acknowledgments
This work was supported by Gatsby Charitable Foundation.

146

5

Ideal observer

0.9
0.8

Recurrent net

0.7
0.6
0.5
0

0.01 0.02 0.03 0.04 0.05

ε

Figure 3

Cosyne 2008

Friday evening, Poster session II-28

Characterizing neural dependencies with Poisson copula models
P. Berkes, J. W. Pillow, and F. Wood
Gatsby Computational Neuroscience Unit, UCL
The activities of individual neurons in cortex and many other areas of the brain are often well described by
Poisson distributions. Unfortunately, there is no simple joint Poisson distribution that can incorporate statistical dependencies (i.e., correlations) between neurons. For this reason, neural population coding models often either assume that the individual neurons are independent, or transform the joint activity mathematically
and model it using a multivariate distribution that naturally encodes dependency, such as the multivariate
Gaussian. However, these solutions are sometimes poorly suited to describing neural population responses,
failing to match either the marginal distributions of individual neurons or the detailed form of their dependencies. Here we develop a joint model for neural population responses using copulas, which allow Poisson
marginal distributions to be combined into a joint distribution that captures dependencies between multiple
neurons.
Copulas are mathematical objects that specify a joint distribution’s dependency structure separately from
its marginal structure [1]. More formally, copulas are joint probability distributions deﬁned on the unit
cube [0, 1]N . Copula models are constructed by projecting the original variables through their cumulative
density functions onto the unit cube. This is a central result for copula models and is formalized in Sklar’s
theorem [2], which states that every multivariate probability density p(x1 , . . . , xN ) canbe written as the
product of a copula density and marginal densities: p(x1 , . . . , xN ) = c(u1 , . . . , uN ) N
i=1 p(xi ), where
ui = cdfi (xi ) and c(u1 , . . . , uN ) is a copula density. Copulas also provide a principled way to quantify
non-linear dependencies that go beyond correlation coefﬁcients (which are only appropriate for elliptical
distributions), in a manner that is independent of rescaling of individual variables [1], and are applicable to
the problem of estimating the mutual information between stimulus and response, as discussed in [3].
Here we present preliminary results on constructing a multivariate joint distribution for neural activity by
choosing the marginals to be Poisson distributed, selecting an appropriate parametric family of copulas, and
ﬁtting the model parameters (of both the marginals and the copula) using Maximum Likelihood estimation.
Different copula families are able to capture dependencies of different kinds (e.g., dependencies limited
to the lower or upper tails of the distribution, or negative dependencies). The selection of an appropriate
parametric family for the copula distribution can be addressed by cross-validation, and is the focus of current
research.
Acknowledgments
This work was supported by the Gatsby Charitable Foundation and the Royal Society USA/Canada Research
Fellowship.
References
[1] R.B. Nelson (1999) An introduction to copulas. Lecture notes in statistics 139, Springer Verlag, New
York.
[2] A. Sklar (1959) Fonctions de répartition à n dimensions et leurs marges. Publications de l’Institut de
Statistique de L’Université de Paris 8, 229–231.
[3] R.L. Jenison and R.A. Reale (2004) The shape of neural dependence. Neural Comp., 16(4), 665–672.

147

Friday evening, Poster session II-29

Cosyne 2008

A Neural Implementation of Predictive Coding
Robert C. Wilson1, Sandhitsu R. Das1, Leif H. Finkel1
1

University of Pennsylvania

The world is an ever-changing place. To make sense of it, the brain must be able to process a constant
stream of noisy input data in real time and in an (apparently) optimal manner in order to be able to direct
behavior. Formally, to achieve optimality, the brain ought to be performing some sort of Bayesian
filtering; yet how such computations can be performed in neural networks is still an open question [1, 2].
In a previous abstract to this conference [3] we extended the work of Deneve et al. [4] to allow optimal
decoding of a single moving stimulus with divisive normalization neural networks; and in this work we
extend it still further to handle the case of more complex moving stimuli.
For concreteness, we use the example of gait recognition and tracking, in particular the case where we
wish to track several different joint angles simultaneously. The approach we take is to build a causal
model for the generation of joint angle data, where each joint angle configuration is a function of both the
type of gait and the current phase through the gait cycle.
Such a hierarchical scheme maps readily onto a neural network. In particular we encode each joint angle
as a bump of activity in a low level network (one per joint angle), and the phase angle in a high level
network, with one high level network for each gait. Feedforward connections from low to high level
networks implement an inverse model of the system, connecting effect to possible cause; while feedback
connections propagate predictions of the causal model back to the input layer. Recurrent connections in
both layers perform two functions. The first is to make a ‘within layer’ prediction; while the second is to
try to explain away inputs by providing inhibition to the places where input (either from below or above)
is expected to appear at the next time step. The resulting network can be shown to be approximating
predictive coding [5].
The figure illustrates some of the properties of the network. On the left is a noisy input signal
corresponding to one of the time varying joint angles. Next, the low level network decodes this input
with the help of top-down signaling to produce a much cleaner signal. All of the low level networks input
into the top-level network. Since each joint angle is associated with two high-level phase angles (i.e. on
the forward and reverse parts of the motion), this input is not a single bump, but many. However, the
high level network, with the help of inhibitory recurrent connections, is able to decode this input to reveal
the single phase angle that can be fed back to inform the decoding in the low level networks.

References
[1] S. Deneve NIPS 2005 [2] R. P. N. Rao NIPS 2005 [3] R. C. Wilson and L. H. Finkel. COSYNE 2006
[4] S. Deneve et al. Nature Neuroscience 2(8):740-745, 1999 [5] R. P. N. Rao and D. H. Ballard. Nature
Neuroscience: 2(1), 79-87, 1999.

148

Cosyne 2008

Friday evening, Poster session II-30

Rapid synaptic adaptation increases cortical population coding
accuracy
J.M. Cortes1 , D. Marinazzo2 , P. Series1 , T.J. Sejnowski3 and M.C.W. van
Rossum1
1

Institute for Adaptive and Neural Computation, University of Edinburgh, UK
Dipartamento di Fisica, Universita di Bari, ITALIA
3
Howard Hughes Medical Institute and Computational Neurobiology Lab, The
Salk Institute, USA
2

The cellular and synaptic mechanisms underlying visual adaptation have extensively been debated during the
last years (for reviews see [1,2]). However, the effect of neural adaptation on information coding is difﬁcult
to study experimentally as it requires an enormous amount of data. Furthermore, as adaptation typically
reduces the ﬁring rate (lowering information) but also reduces correlations (increasing information), its net
effect is not known.
Here we present a mechanistic model for rapid visual adaptation to examine the effect of adaptation on
population coding. Compared to the longer time scale for spike-frequency adaptation, short-term synaptic
plasticity is likely to be responsible for the rapid adaptation occurring on the time scale of the stimulus
presentation (typically hundreds of milliseconds)[3]. A standard recurrent model of V1 [4] that included
short-term synaptic depression of the excitatory cortical synapses [5] reproduced post-stimulus changes at
the single-neuron and population levels. After adaptation, individual neurons reduced their activity, thus saving metabolic expenses on coding a repetitive stimulus, while the population code of the neurons reproduced
perceptual changes after adaptation, e.g. tilt after-effects.
Based on an analysis using Fisher information, stimulus discriminability increased for stimuli close to the
adapting stimulus. These results suggest that visual adaptation is functionally advantageous from an information coding perspective and validate the ”efﬁcient neural coding hypothesis.”
This research was support by EPSRC (project COLAMN Ref. EP/CO 10841/1) MEC-Fulbright (project
EX2005-0804) HPC-EUROPA (Ref. RII3-CT-2003-506079) and HHMI.

References
[1] A. Kohn. Visual adaptation: physiology, mechanisms, and functional beneﬁts. J Neurophysiol
97:3155-3164, 2007
[2] O. Schwartz, A. Hsu and P. Dayan. Space and time in visual context. Nat Rev Neurosci 8: 522-535
(2007)
[3] J.R. Muller, A.B. Metha, J. Krauskopf, and P. Lennie. Rapid adaptation in visual cortex to the structure
of images. Science 285:1405-1408, 1999
[4] A.F. Teich and N. Qian. Learning and Adaptation in a Recurrent Model of V1 Orientation Selectivity. J
Neurophysiol 89: 2086-2100, 2003
[5] M.V. Tsodyks and H. Markram. The Neural Code between Neocortical Pyramidal Neurons Depends on
Neurotransmitter Release Probability. Proc Natl Acad Sci USA 94: 719-723, 1997

149

Friday evening, Poster session II-31

Cosyne 2008

A Bayesian approach to extracting spatio-temporal spike pattern
with an application to spike data from HVC in Bengalese finch
Kentaro Katahira1, 2, Jun Nishikawa2, Kazuo Okanoya2, and Masato Okada1, 2
1
University of Tokyo, 2RIKEN Brain Science Institute
Brain activity is highly non-stationary and spike statistics vary from time to time according to stimuli and
internal states. In many previous studies that explore spike statistics (e.g. spike correlation, entropy),
stationarity is assumed for obtaining enough samples from limited trials (e.g. [1]). Hidden Markov
Models (HMM) have been used to segment spike trains into quasi-stationary interval, in which statistics
can be obtained [2, 3]. In this context, EM algorithm that achieves maximum likelihood estimation is used
to estimate the parameters of HMM. Therefore, the previous method has problems of overlearning, and
the number of hidden states must be determined by trail-and-error.
To overcome those problems, we apply Variational Bayes (VB) method to train the HMM. Our method
can determine the number of states automatically and avoid overlearning. Even when the model that has
redundant hidden states is used, VB can use only necessary hidden state by the automatic pruning ability.
Because VB is a gradient algorithm, it suffers from a serious local optimal problem in practice. To
alleviate the local optimal problem, we introduce deterministic annealing method to VB (DAVB) [3].
Multivariate Poisson distribution is used for the output probabilistic distribution of HMM.
We perform an experiment on a set of multielectrode spike data recorded from nucleus HVC of Bengalese
finch while it is hearing the bird's own song (BOS) and reversed song (REV), as an example of highly
non-stationary spike train. See [5] for details of the recordings. Regardless of the number of hidden state,
proposed method using DAVB showed better recognition rate of the auditory stimuli (BOS, REV, and
Silent) than conventional EM algorithm and VB without annealing. This result suggests that our method
can easily capture the underlying state of neurons.

(b)

(a)
Stimulus

Spike train
State Index

1

2

3

4

1

3

2 5

1

3 4

1

2

Figure1: (a) An example spike train when listening to the bird’s own song (BOS) and estimated hidden state by
proposed method. Top panel shows sound spectrogram of BOS. Bottom panel shows corresponding spike train.
Background colors show estimated hidden states. (b) Average recognition rate of auditory stimuli.

References
[1] E.Schneidman, M.J.Berry, R. Segev and W. Bialek, Nature, vol. 440, pp. 1007-1012, 2007
[2] G. Radons, J.D. Becker, B. Dülfer, and J Krüger, Biol. Cybern, vol.71, pp.359-73, 1994
[3] M. Abeles, H. Bergman, I. Gat, I. Meilijson, E. Seidemann, N. Thishby, and E. Vaadia,
Proc. Nat. Acad. Sci. USA, vol.92, pp.8616-8620, 1995
[4] K. Katahira, K. Watanabe, M. Okada, J. Phys. Conf. Ser., in press.
[5] J. Nishikawa, K. Okanoya, Cosyne 2008.

150

Cosyne 2008

Friday evening, Poster session II-32

Interactions between thalamocortical neurons in driving cortical
cells in the rat vibrissa system
Qi Wang and Garrett B. Stanley
School of Engineering and Applied Sciences, Harvard University
Understanding the sensory neural code involves not only characterizing the neural representation of the
sensory input at various stages of processing in the pathway, but also how these representations are
transformed between the various brain regions. The thalamocortical circuit in the rat vibrissa system has
been utilized as a model system by a number of laboratories to address this question. In the rat vibrissa
pathway, cells in the layer IV of the primary somatosensory cortex receive convergent input from
multiple cells within the ventral posterior medial (VPM) nucleus of the thalamus [1], and it is widely
believed that interactions between these thalamic cells contribute to the cortical representation. Although
the time course of homosynaptic and heterosynaptic interactions in the cat visual pathway has been
previously studied [2], how the spikes from different VPM cells interact in vivo to drive cortical cells is
not as well understood in the vibrissa pathway, nor is the full functional consequence of this phenomenon
in the context of coding sensory stimuli.
We investigated how neural activity of multiple presynaptic neurons interacted to affect the firing of
postsynaptic neurons in the rat vibrissa system. By simultaneously recording single unit activity of
multiple cells in the VPM nucleus and barrel cortex of anesthetized Sprague-Dawley rats in response to
weak whisker stimulation, we examined interactions between presynaptic spikes of VPM cells which
project to the same barrel cell target. For homosynaptic pairs, we asked how multiple closely-timed spikes
in a thalamic event interacted in driving a cortical event. For heterosynaptic triplets, we were interested in
how spikes from a presynaptic neuron interacted with closely-timed thalamic events from another
presynaptic neuron in driving a common cortical target. In our preliminary analyses, we found that an
event from a single thalamic neuron with 2 spikes had higher efficacy in driving a cortical response than
that with 1 spike, and a thalamic event with 3 spikes had higher efficacy than that with 2 spikes. However,
the efficacy appeared to saturate with increasing numbers of spikes within a thalamic event. Furthermore,
preliminary data also suggested that a thalamic event with one or two spikes was statistically more
effective in driving the cortical cell when coincident with a spike from another VPM cell also projecting
to the same cortical cell target.

Acknowledgments
This work was supported by NIH grant R01 NS048258.
References
[1] Feedforward mechanisms of excitatory and inhibitory cortical receptive fields. RM. Bruno and DJ.
Simons, J Neurosci., 22(24):10966-75, 2002.
[2] Synaptic interactions between thalamic inputs to simple cells in cat visual cortex. WM. Usrey, JM.
Alonso, and RC. Reid, J Neurosci., 20(14):5461-7, 2000.

151

Friday evening, Poster session II-33

Cosyne 2008

Feedback Inhibition in the Mushroom Body and Gain Control
Maria Papadopoulou1, Glenn Turner2, and Gilles Laurent1
1

California Institute of Technology, 2Cold Spring Harbor Laboratory

The giant GABAergic neuron (GGN) is a single, paired, non-spiking neuron that arborizes extensively in
the mushroom body (MB) [1], where it overlaps with the dendrites and the axons of Kenyon cells (KCs).
KCs are the intrinsic neurons of the MB and are thought to be required for learning and memory [2]. We
are interested in understanding the function of GGN in olfactory processing: in particular, its pattern of
arborization makes it an attractive candidate for controlling or modulating KC responses to odors, with
potential implications for learning and recall. Physiological recordings of KCs in locust show that these
neurons respond sparsely to odors, by contrast with their excitatory input from the antennal lobe
(projection neurons or PNs) [3]. Inhibition appears to be critical to control KC response threshold,
probability and duration during odor stimulation [3]. We have shown that there exists a feedback loop
whereby KCs provide excitatory input to GGN and this cell -because of its GABAergic outputcontributes to the inhibitory control of KC excitability. As such, this neuron could act to control the gain
of PN-to-KC information transfer and normalize KC-population output, making it independent of input
strength. Using electrophysiological techniques, we are studying the properties and modes of action of
GGN in locust.

Acknowledgments
This work is supported by NIDCD.
References
[1] Leitch and Laurent (1996) J Comp Neurol. 4 :487-514
[2] Heisenberg M. (2000) Nat Rev Neurosci. 4: 266-275
[3] Perez-Orive J., Mazor O., Turner G.C., Cassenaer S., Wilson R & Laurent G. (2002) Science 297:
359-365

152

Cosyne 2008

Friday evening, Poster session II-34

Simultaneous multi-neuronal recording from anesthetized Bengalese
finch HVC with high-density silicon electrodes
Jun Nishikawa, and Kazuo Okanoya
Laboratory for Biolinguistics, RIKEN Brain Science Institute, Japan.
Birdsong is a complex vocalization composed of various song elements organized according to sequential
rules. Songbirds have a specialized set of discrete brain nuclei for song production and learning. Past
electrophysiological studies revealed the inter-nuclei interaction and their hierarchical organization for
song production. Although the multi-neuronal interactions within the single nucleus are important to
decipher the information coding of the song, simultaneous recording from a local circuit has not been
reported. It has been difficult to record multi-neuronal activities from a tiny brain area of about several
hundreds of microns.
In this study, we tried to record the multi-neuronal activities of anesthetized Bengalese finch HVC with
16-channel high-density silicon electrodes. We simultaneously recorded up to 26 well-isolated single
units from a tiny brain area HVC of the same individual. HVC contains at least three classes of neurons:
neurons that project to the RA, neurons that project to area X, and interneurons. We identified some of the
recorded neurons into each class by antidromic activation from RA and from area X. Neural responses to
bird’s own song and their correlated dynamics between neuronal ensembles were analyzed. Functional
connectivity within HVC was extracted as a mono-synaptic interaction or a weak interaction based on
their crosscorrelograms. As a result, we obtained a functional network induced by the song stimulation.
Ongoing studies try to analyze the context dependent changes of the functional network.
These techniques will give us excellent tools to understand local circuit mechanism to recognize
sequential sensory stimuli and also to generate complex motor sequences.

153

Friday evening, Poster session II-35

Cosyne 2008

The role of the prefrontal cortex in context-dependent reversal
learning.
Jeffrey C. Erlich and Carlos D. Brody
Princeton Neuroscience Institute and Dept. of Molecular Biology
Princeton University
One hallmark of behavioral flexibility is the ability to use context information to inhibit inappropriate
behaviors. When the ‘inappropriate’ behavior is innate or habitual, inhibition can be very difficult. This
is especially true for those suffering from psychiatric and neurological disorders such as schizophrenia
and Parkinson’s disease. Neuroscientists have consistently found that the prefrontal cortex, in concert
with premotor and parietal regions, are involved in the inhibition of an inappropriate prepotent response
and the volitional generation of appropriate behavior. However, the mechanisms by which these cortical
regions give rise to the contextual control of behavior are unknown. To investigate the interactions
between these brain regions during context-dependent behavioral control we developed the ‘Pro/Anti’
sound localization task for rats: in the ‘Pro’ context rats orient towards a sound and in the ‘Anti’ context
they orient away from a sound for reward. The task has a form similar to a task previously used with
monkeys, the Pro-saccade/Anti-saccade task.
Preliminary data indicate that performing the Anti sound orienting task is more difficult than the Pro
orienting task, suggesting that the Anti task requires the inhibition of an innate orienting response. Rats
take 1-2 days to learn the Pro task and 1-2 weeks to learn the Anti task. They take much longer to learn to
use contextual information to switch between the Pro and Anti rules. Using electrophysiological,
pharmacological, and computational techniques we are beginning to explore the neural mechanism
underlying this task.

Acknowledgments
We thank the PDP group at Princeton University for helpful discussions.
.

154

Cosyne 2008

Friday evening, Poster session II-36

Neural decoding of goal-directed movements using a linear statespace model with hidden states
Wei Wu1 , Jayant E. Kulkarni2 , Nicholas G. Hatsopoulos3 , and Liam
Paninski2
1

Florida State University,

2

Columbia University,

3

University of Chicago

Decoding algorithms, which translate neural activity into command signals for external devices such as
robotic limbs and computer cursors, are a key component of neural prosthetic devices. Classically, a linear
state-space model, with bayesian inferencing methods, has been used as it enables real-time implementation. This model is based on a linear Gaussian approximation between neural ﬁring rates and hand motion,
and an autoregressive representation for the hand kinematics over time. In this work we suggest two key
modiﬁcations to the classical model in order to improve decoding accuracy.
Firstly, the classical model does not account for kinematic or kinetic terms, such as joint angles at shoulder
and elbow, muscular activation, or other behavioral state such as the subject’s attentional level. All these
features, however, are closely related to the brain-controlled, muscle-executed hand movement. We address
this limitation by adding a hidden state to the Kalman ﬁlter model to represent these features. We assume
that the observed neural ﬁring rate is linearly related to the hidden state, and furthermore, we allow the
dynamic of the hand state to impact the dynamics of the hidden state, and vice versa. The parameters in the
model can be identiﬁed by the conventional Expectation-Maximization algorithm.
In our second modiﬁcation to the classical approach we condition the estimated trajectory on available endpoint information. In most clinical settings, the patient interacts with an external device by choosing one of
a few targets on the screen. Since we know the location of these targets in advance we can use this information in conjunction with the estimate obtained from the Kalman ﬁlter to obtain a more accurate estimate
of intended hand-position. This updated estimate is obtained by using forward-backwards computations
familiar from the theory of the Kalman ﬁlter.
We tested our method using data recorded from a Macaque monkey executing a visuo-motor task with one
of its arms. The data consists of hand-position and 125 simultaneously recorded single-units. We found
that the decoding accuracy is an increasing function of the dimension d of the hidden state. For example,
when d = 1, the improvement in the mean-squared error (cm2 ) is 5%, whereas for d = 3, the improvement
reaches about 16%. Furthermore conditioning on the target can increase the accuracy by up to 45%.
Table 1: Decoding error (cm2 )
Without target information
With target position information

Classical Model
8.2
4.6

1-d hidden state
7.8
4.3

2-d hidden state
7.1
3.8

3-d hidden state
6.9
3.7

Acknowledgments
WW is supported by an FSU Planning Grant, JEK by a Swartz Foundation Fellowship, LP by an NSF
CAREER award and a Sloan Research Fellowship, and NGH by an NIH-NINDS grant R01 NS45853-01.

155

Friday evening, Poster session II-37

Cosyne 2008

Olfactory Information Processing in Behaving Mice
Roman Shusterman and Dima Rinberg
Janelia Farm Research Campus, Howard Hughes Medical Institute
Mitral/tufted cells in the olfactory bulb are the first recipients of the olfactory information from the odor
receptors. Odor responses of these cells are significantly different in awake and anesthetized animals. The
spontaneous rate of mitral cells is much higher and odor responses are much sparser in awake state [1].
Also, the activity of these cells are modulated by behavior [1,2].
The main features of the olfactory code on the mitral cell level still remain unknown. In order to
understand the code, we combine electrophysiological measurements with behavioral experiments. We
independently manipulate stimuli parameters, such as odor identity, concentration, duration, and
behavioral parameters, such as reward - stimuli association. That allows us to extract features of the
mitral cell responses that carry information about odors and invariant for different behaviors and those
which are due to behavioral modulations. Simultaneous measurements of mitral cell firing rates,
breathing/sniffing pattern, and local field potential oscillations provide clues about olfactory code in
behaving mice.
In behavioral experiments we demonstrated that accuracy of odor discrimination task increased with the
increase of the duration of odor exposure even beyond one sniff [3]. The electrophysiological
measurements in behaving mice showed that firing rate odor response saturated at approximately the
same time when behavioral accuracy reached its maximum. That points out that sensory integration
happens at mitral cell level or earlier. Such observation set restrictions on the possible models of olfactory
information processing.

References
[1] Sparse odor coding in behaving mice. D. Rinberg, A. Koulakov, and A. Gelperin J. Neuroscience
26(34): 8857-8865, 2006.
[2] Odor- and context-dependent modulation of mitral cell activity in behaving rats. L.M. Kay, and G.
Laurent, Nature Neuroscience 2(11): 1003-1009, 1999.
[3] Speed accuracy tradeoff in olfaction. D. Rinberg, A. Koulakov, and A. Gelperin. Neuron 51(3): 351358, 2006.

156

Cosyne 2008

Friday evening, Poster session II-38

Auto-structure of spike-trains matters for testing on synchronous activity: Take it serious, do not ignore or change it!
Gordon Pipa1,2 , Carl van Vreeswijk3 , and Sonja Grün4
1

Max-Planck Institute for Brain Research and Frankfurt Institute for Advanced
Studies, Germany, 2 Massachusetts Institute of Technology, and Massachusetts
3
General Hospital, USA
René Descartes Univeristy, Laboratoire de Neuro4
Computational Neurophysique et Physiologie, CNRS, UMR 8119, France
science Group, RIKEN Brain Science Institute, Japan
Coordinated neuronal activity across many neurons, i.e. synchronous or spatio-temporal pattern, had been
discussed to be a major component of neuronal activity. However, the discussion if coordinated activity
really exists remained heated and controversial. A major uncertainty was that many analysis approaches
either ignored the auto-structure of the spiking activity, assumed a very simpliﬁed model, i.e. poissonian
ﬁring, or changed the auto-structure by spike jittering. We studied wether a statistical inference that tests
wether coordinated activity is occurring beyond chance can be falsiﬁed by ignoring or changing the real
auto-structure of recorded data. To this end we investigated the distribution of coincident spikes in mutually
independent spike-trains modeled as renewal processes. We considered Gamma-processes with different
shape parameters γ as well as renewal processes in which the ISI distribution is log-normal. For Gammaprocesses of integer order γ we calculated the mean number of coincident spikes, NC , as well as the Fano
factor of the coincidences, F FC , analytically. We determined how these measures depend on the bin width
and also investigated how they depend on the ﬁring rate, and on rate difference between the neurons. We
used Monte-Carlo simulations to estimate the whole distribution for these parameters and also for other
values of γ.Moreover, we considered the effect of dithering for both of these processes and shaw that while
dithering does not change the average number of coincidences, it does change the shape of the coincidence
distribution.
Our major ﬁndings are: 1) The width of the coincidence count distribution, described by F Fc , depends
very critically and in a non-trivial way on the detailed properties of the inter-spike interval distribution. 2)
The dependencies of F Fc on the coefﬁcient of variation of the ISI distribution CV are complex and mostly
non-monotonic. Moreover, F Fc depends on the very detailed properties of the individual point processes,
and cannot be predicted by CV alone. Hence, given a recorded data set, the estimated value of CV of the ISI
distribution is not sufﬁcient to predict the Fano factor F Fc of the coincidence count distribution. 3) Spike
jittering, even if it is as small as a fraction of the expected ISI, can falsify the inference on coordinated ﬁring.
In most of the tested cases and especially for complex synchronous and spatio temporal pattern across many
neurons, spike jittering increased the likelihood of false positive ﬁnding very strongly. Last, we discuss
a procedure (1) that considers the complete auto-structure of each individual spike-train for testing wether
synchrony ﬁring occurs at chance and therefore overcomes the danger of an increased level of false positives.
Acknowledgments
Supported by the Hertie Foundation, the EU (GABA project FP6-2005-NEST-Path-043309), the German Ministry for
Education and Research (BMBF grants 01GQ01413), and the Stifterverband für die Deutsche Wissenschaft.
References
[1] NeuroXidence: Reliable and efﬁcient Analysis of an Excess or Deﬁciency of Joint-Spike Events. G. Pipa and D.W.
Wheeler and W. Singer and D. Nikolic, J. Comp. Neuroscience in press

157

Friday evening, Poster session II-39

Cosyne 2008

Invariant feature recognition in neurons using dendritic processing
Mark Flynn1, Ethan Brown2, and Garrett Kenyon1
1

Los Alamos National Laboratory,

2

University of Oklahoma

The fundamental building blocks of the mammalian visual system are the cortical edge detectors--simple
cells in V1 that respond selectively to short line segments of a particular orientation. Over forty years ago
Hubel and Wiesel postulated that edge detectors receive input from optimally aligned sets of LGN relay
neurons. However, the aspect ratio of the LGN input to cortical edge detectors has been determined to be
very low, between 1:1 to 2:1, which is not sufficient to produce the observed sharpness of their measured
orientation tuning curves. Intracortical interactions have been postulated to improve orientation tuning but
blocking such interactions does not broaden measured tuning curves. Based on recent experiments in
several brain regions, we hypothesized that nonlinear dendritic subunits could enable simple cells in V1 to
achieve good orientation selectivity despite the very low aspect ratio of their retinotopic input. We
compared the response of model simple cells, both with and without dendritic spikes, to LGN inputs
activated by short oriented line segments. Lateral interaction between V1 simple cells was not modeled,
to mimic the conditions present in cortical inactivation experiments. Without dendritic spiking, responses
to different orientations were nearly independent of bar orientation, due to the low aspect ratio of the
LGN input (1.5). However, by breaking the receptive field into smaller nonlinear dendritic subunits, each
with twice the aspect ratio of the soma, the orientation tuning curve of the model V1 neuron became
nearly identical to that measured experimentally after intracortical feedback was inactivated. Furthermore,
the spiking dendrite model responded in a position-invariant fashion to thin bars placed anywhere in its
receptive field. This has interesting implications for neuronal processing. Model V1 neurons with spiking
dendrites responded selectively to orientation while remaining invariant to small orthogonal translations,
suggesting a general mechanism whereby neurons in higher visual areas could acquire selective responses
to very specific features (faces, landmarks, objects), regardless of scale, translation or rotation..

158

Cosyne 2008

Friday evening, Poster session II-40

Predicting the timing of spikes evoked in mechanoreceptive
afferents by dynamic stimuli
S.S. Kim1, A.P. Sripati2 and S.J. Bensmaia*1
1

Johns Hopkins University 2Carnegie Mellon University

What does the hand tell the brain? The glabrous skin of the hand is innervated by three kinds of
mechanoreceptive afferents – SA1, RA and PC –, each of which responds to different aspects of a tactile
stimulus. We previously developed a continuum mechanical model that describes the responses of these
afferents to statically indented patterns of arbitrary spatial complexity.1 In that study, we showed that
different mechanoreceptors are sensitive to different aspects of the local static skin deformation.
However, afferents are also exquisitely sensitive to the temporal structure of the stimulus. Repeated
presentations of a given dynamic stimulus (i.e., a mechanical vibration) evoke virtually identical spiking
patterns in individual afferents. Mechanotransduction has been studied in Pacinian corpuscles by
recording the receptor and action potentials evoked by direct stimulation of the receptor. Little is known,
however, about transduction in Merkel and Meissner corpuscles, which, unlike their Pacinian
counterparts, cannot be readily excised from the surrounding tissue and thus have not been studied in
isolation.
In the present study, we seek to understand how dynamic stimuli are transduced by the three populations
of mechanoreceptive fibers. What aspect of the stimulus does each type of fiber respond to? Specifically,
we tested the possibility that SA1, RA and PC afferents are primarily sensitive to the indented position,
velocity, and acceleration of the stimulus as a function of time, respectively. To address these issues, we
constructed a quantitative model describing the transduction of dynamic stimuli. To that end, we recorded
the responses evoked in SA1, RA and PC afferent fibers of anaesthetized macaque monkeys by a wide
variety of vibratory stimuli, ranging from simple sinusoids to band-pass noise sequences. The model is a
leaky and noisy integrate-and-fire model, the input of which is passed through a non-linear then a linear
stimulus pre-filter. We use the approach developed by Paninski and colleagues2,3,4 to derive the
parameters of the model. For each afferent, we use position, velocity, acceleration and jerk, either
individually or in combination, as input to the model to assess the degree to which the timing of the
spikes can be predicted from these quantities. We also examine the properties of the pre-filters and assess
the extent to which the spectral sensitivity characteristic of the afferents is reflected in these pre-filters.
Acknowledgments
This work was supported by NIH grants NS18787, NS34086 and DC00023.
References
[1] Sripati, A.P., S.J. Bensmaia & Johnson, K.O. A continuum mechanical model of mechanoreceptive
afferent responses to indented spatial patterns. J Neurophysiol 95, 3852-3864 (2006).
[2] Paninski, L., J.W. Pillow, et al. (2004). "Maximum likelihood estimation of a stochastic integrate-andfire neural encoding model." Neural Comput 16: 2533-61.
[3] Pillow, J.W., L. Paninski, et al. (2005). "Prediction and decoding of retinal ganglion cell responses
with a probabilistic spiking model." J Neurosci 25: 11003-13.
[4] Paninski, L. (2006). "The most likely voltage path and large deviations approximations for integrateand-fire neurons." J Comput Neurosci 21: 71-87.

159

Friday evening, Poster session II-41

Cosyne 2008

On the collective computational abilities of inhibitory neurons
Andreas Knoblauch1 , Friedrich T. Sommer2, Marc-Oliver Gewaltig1 ,
Rüdiger Kupper1 , Ursula Körner1 , Edgar Körner1
1

Honda Research Institute Europe, Offenbach, Germany 2 Redwood Center for
Theoretical Neuroscience, University of California, Berkeley, USA
In many parts of the brain such as neocortex, the critical part of information processing is traditionally
attributed to excitatory networks, whereas inhibitory neurons are often seen as merely having control functions. For example, theories of associative memory based on attractor networks usually assume that the
memories are stored in the excitatory synaptic network, while inhibitory neurons merely provide an appropriate activation threshold or control synchronization [1,3,4].
In this work we challenge this view by analyzing the computational abilities of inhibitory neuron populations
[2]. Speciﬁcally, we investigate how many sparse binary activation patterns and how much information (in
bits per synapse) can be stored in a recurrent or feed-forward inhibitory network, and how the information
can be retrieved in an optimal way. Our theory shows that storing a large number of activation patterns
requires only a few inhibitory synapses. For realistic population sizes, these networks can already store
several bits per synapse (whereas the storage capacity of the classical Willshaw and Hopﬁeld models for
inﬁnitely large excitatory networks is limited by 0.72 and 0.14 bits per synapse, respectively).
Moreover, we argue that inhibitory networks can easily implement strategies of optimal threshold control
as previously suggested for associative memory in excitatory networks [1]. These strategies signiﬁcantly
enhance the storage capacity by adjusting the ﬁring threshold of a neuron based on its presynaptic input
activity. However, no plausible hypothesis has been proposed how biological excitatory neurons could
sense presynaptic input activity. Here we show that networks of inhibitory biological neurons have natural
access to the presynaptic input. Our results indicate that inhibitory neurons with strong inﬂuence on the
postsynaptic target are ideally suited for optimal threshold control, for example chandelier cells or basket
cells targeting axonal inital segment and cell soma [4].
We have veriﬁed our theoretical results by additional simulation experiments of cortex-like network models
involving both strongly inhibiting chandelier and basket cells, and less strong gradually inhibiting dendritetargeting cells. Our theory closely predicts the performance of the simulations, even for realistic neuron
models and small as well as large networks. Finally, we discuss possible brain structures where our theory
may be of relevance, such as particular networks in the cerebral cortex, basal ganglia, and cerebellum.
References
[1] Improving recall from an associative memory. B. Graham and D. Willshaw, Biological Cybernetics,
72:337–346, 1995.
[2] On the computational beneﬁts of inhibitory neural associative networks. A. Knoblauch, Technical Report
at the Honda Research Institute Europe, HRI-EU 07-05, 2007.
[3] Pattern separation and synchronization in spiking associative memories and visual areas. A. Knoblauch
and G. Palm., Neural Networks, 14:763–780, 2001.
[4] Interneurons of the neocortical inhibitory system. H. Markram, M. Toledo-Rodriguez, Y. Wang,
A. Gupta, G. Silberberg, and C. Wu, Nature Reviews Neuroscience, 5:793–807, 2004.

160

Cosyne 2008

Friday evening, Poster session II-42

Putative excitatory and inhibitory neurons synchronize at different
phases of the gamma cycle in visual area V4 of awake monkeys.
T. Womelsdorf1, M. Vinck1, C. Bosman1, R. Oostenveld1, R. Desimone2, and P.
Fries1
1

F.C. Donders Centre for Cognitive Neuroimaging, Radboud University
Nijmegen, Nijmegen, The Netherlands. 2McGovern Institute for Brain Research at
MIT, Cambridge, MA.

Activated neuronal groups typically engage in rhythmic gamma-band synchronization. These periods of
gamma-band oscillations likely emerge from the interplay of excitatory drive and rhythmic inhibition
imposed by interneuron networks [1]. Interneurons generate synchronized inhibition of local excitatory
neurons, and thereby synchronize the excitatory neurons’ discharges. As a consequence, there should be a
particular phase relation of discharge probability between excitatory and inhibitory neurons within the
gamma cycle: Inhibitory neurons should fire a few milliseconds after excitatory neurons [2,3]. Here, we
tested this hypothesis by classifying putative inhibitory and excitatory neurons from extra-cellular
recordings in visual area V4 of the macaque, and by determining their preferred oscillatory gamma-band
phases during constant visual stimulation.
Similar to previous studies [e.g. 4] our sample of well-isolated neurons could be subdivided based on
narrow and broad spike waveforms, reflecting putative inhibitory interneurons (narrow) and putative
excitatory pyramidal neurons (broad). We validated this distinction by calculating the cross-correlation
function between the isolated single neurons and simultaneously recorded multi-unit activity. Putative
interneurons showed higher firing rates and stronger gamma-band synchronization than putative
pyramidal cells. This difference remained after equating the number of spikes across neuron types, or
restricting analysis to periods with maximum gamma-band power in the local field potential. Importantly,
the phase of synchronization in the gamma-band differed significantly by an average 70 degrees between
cell classes, consistent with interneurons firing on average 3.2 ms after pyramidal cells.
These results from awake monkey visual cortex are fully consistent with previous findings in
hippocampal slices, awake rat hippocampus and anesthetized ferret prefrontal cortex. Models of gamma
based mostly on rat hippocampal slices might thus readily apply to awake monkey visual cortex.
Acknowledgments
This research was supported by The Netherlands Organization for Scientific Research, grant 452-03-344
(P.F.) and grant 016-071-079 (T.W.), the NIMH-IRP (R.D.) and NIH grant R01-EY017292 (RD).
References
[1] Synaptic mechanisms of synchronized gamma oscillations in inhibitory interneuron networks. M.
Bartos, I. Vida, and P. Jonas. Nat Rev Neurosci 8:45-56, 2007.
[2] The gamma cycle. P. Fries, D. Nikolic, and W. Singer. Trends Neurosci 30:309-316. 2007.
[3] Inhibitory postsynaptic potentials carry synchronized frequency information in active cortical
networks. A. Hasenstaub, Y. Shu, B. Haider, U. Kraushaar, A. Duque, and D.A. McCormick.
Neuron 47:423-435. 2005.
[4] Differential attention-dependent response modulation across cell classes in macaque visual area
V4. J.F. Mitchell, K.A. Sundberg, J.H. Reynolds. Neuron 55:131-141. 2007.

161

Friday evening, Poster session II-43

Cosyne 2008

Network Analysis of EEG Coherence in Autism Spectrum Disorder
Sungho Hong1, Michael Murias2, Adrienne L. Fairhall2 and Geraldine
Dawson2
1

Okinawa Institute of Science and Technology, 2University of Washington

Autism Spectrum Disorder (ASD) is a severe lifelong developmental disorder involving impaired social
interaction, impaired communication, and restricted and repetitive behaviors. The biological basis for
ASD is currently under intense investigation. An emerging common finding is that of diminished
functional connectivity within the brains of individuals with ASD. Specifically, patterns of reduced
correlation in the time-courses of brain activity recorded from distinct neural regions have been
repeatedly demonstrated using hemodynamic and electrical recording techniques. These measures of
functional connectivity show robust differences between individuals with ASD and typical controls, and
may assist in diagnosis or in the evaluation of neural systems that are suspected to be deficient in
individuals. In this work, we have applied methods from complex network theory [1,2] to identify key
network structures and to quantify differences in functional connectivity.
We analyzed functional connectivity networks obtained by high-density electroencephalographic (EEG)
recordings in the eyes-closed resting state from high functioning adults with ASD (N = 18) and an age
and IQ matched control group (N = 18). This data has previously revealed significant differences between
the two groups in EEG power and pairwise coherence in the alpha and theta bands [3]. Here, we
computed global and local network measures from that data to identify network characteristics correlated
with ASD.
The global network clustering coefficient and average path length measures showed significant
differences for the functionally connected cortical networks derived from the ASD and control groups.
While the networks generated from both groups could be classified as small-world networks [4], the ASD
group’s networks had a significantly longer average path length, implying that communications within the
cortex are consistently less efficient with ASD.
We investigated the basis of the relative efficiency of the control over the ASD group by calculating local
network measures, the node and edge betweenness [5]. These capture the influence of individual nodes or
edges to the entire network by measuring the total number of the shortest paths passing through them. We
found that the key determinant of the relative efficiency is a small number of long distance connections
with moderately high coherence between frontal and occipital regions. Such long-distance connections in
the ASD case have significantly lower coherence and consequently less influence in the network. This
implies that ASD could be related to deficiency or weakening of electrophysiological structures (such as
white matter) that support long-range communications, particularly between occipital and frontal cortex.

References
[1] Newman, M. E. J. (2003) SIAM Review 45:167–256.
[2] Reijneveld, J. C. et al. (2007) Clin. Neurophysiol. 118(11):2317-31.
[3] Murias, M. et al. (2007) Biological Psychiatry 62:3, 270-3.
[4] Watts, D. and Strogatz, S. H. (1998) Nature 393:440-2.
[5] Newman, M. E. J. (2001) Phys. Rev. E 64:016132.

162

Cosyne 2008

Friday evening, Poster session II-44

Temporal relationships between independent EEG frequency
modulations
Julie Onton1 and Scott Makeig1
1

University of California San Diego

Mean power spectra of scalp EEG signals exhibit distinct peaks emerging from the general decrease in
power with increasing frequency, suggesting the existence of characteristic oscillatory modes in cortical
field potentials. The interactions between peaks in different frequency bands, within and between cortical
EEG sources, are not well understood. The present analysis was designed to separate second-to-second
changes in the power spectra of EEG source domains into distinct modes.
EEG data from an emotional imagery task were first decomposed by independent component analysis
(ICA) to isolate independent EEG activities from the signal mixtures recorded at each scalp electrode.
Each independent component (IC) was determined to be artifact or plausibly brain-derived by examining
its scalp projection, power spectrum, and task-related activation time course. IC activations were then
decomposed in one-second sliding time windows (with half-second overlap) over the entire session by
fast Fourier transform. The mean log power spectrum over all time windows was removed for each IC,
leaving spectral fluctuations from the mean in each 1-sec time window.
The log spectral differences for all brain ICs were concatenated into a large matrix of size (windows by
ICs*frequencies). This matrix was reduced by principal component analysis (PCA), and then decomposed
by infomax ICA into maximally independent spectral modulators (IMs). Here, ICA separated the log
spectral fluctuations into the multiplicative effects of frequency templates with maximally distinct profiles
across frequencies and ICs. The resulting IM templates were clustered across subjects according to
modulatory effects with narrow or wide band changes in the canonical frequency ranges: delta, theta,
alpha (with harmonics), beta, and non-oscillatory broadband gamma (roughly 35-128 Hz or higher). Each
IM template was associated with a weight in each time window giving the strength of its effect on the
mean log power spectrum of the affected IC process. Since ICA, as applied here, did not constrain the IM
time courses to be independent, relationships between IM weight series within-subject were assessed by
correlation. A weak mean negative correlation was found across subjects between all lower frequency
modulations (delta, theta, alpha, and beta) and the broadband gamma modulations, suggesting that within
single cortical source domains, when low-frequency power was below average, broadband highfrequency power was high. While the mean correlations were significant according to permutation
statistics, the correlation was relatively low over the entire time course of the experiment (near -0.1).
These results suggest that opposite regulation of high and low frequency power may be one characteristic
mode of cortical processing, though the relationship is by no means constant over time.

Acknowledgments
This work was supported by the Swartz Foundation (Old Field, NY) and the National Science Foundation

163

Friday evening, Poster session II-45

Cosyne 2008

The Interaction of Spontaneous and Evoked Activity in Model
Networks
Kanaka Rajan1 , L.F. Abbott1 and Haim Sompolinsky2
1
2

Department of Neuroscience, Columbia University, New York, NY
Racah Institute of Physics and ICNC, Hebrew University, Jerusalem, Israel

Most activity in the brain is internally generated. Even sensory neural circuits, such as the primary visual
cortex, generate complex patterns of activity in the absence of input [1]. Complex spontaneous activity
would appear to be an unwanted complication in interpreting evoked activity that represents the sensory
environment. What mechanisms give rise to spontaneous activity in neuronal circuits and what are the
functional implications of this activity?
We consider a ﬁring-rate network with connection weights chosen randomly from distributions that represent
excitatory and inhibitory synaptic strengths. Previous work has shown that, in large networks of this kind,
spontaneous activity is characterized either by a ﬁxed point of the rate dynamics, corresponding to steadystate tonic activity, or by a chaotic attractor representing large temporal ﬂuctuations in the neuronal activity
[2,3].
Our research addresses the effects of chaotic dynamics on the encoding of stimuli. In particular, we study
chaotic networks driven by periodic input. The analysis is done both through network simulations and by
using mean-ﬁeld analytic methods to compute the average correlation function of network neurons in the
limit of large network size. This work reveals a phase transition between two basic dynamic behaviors: a
periodic state in which the network is locked in phase and frequency to the oscillatory external drive, and a
chaotic state where neurons behave as noisy oscillators with only partial locking to the external drive. We
construct phase diagrams (see below) showing how the behavior of the system depends on the strength and
frequency of the external input and the strength of the network connectivity. A surprising result is the nonmonotonic dependence of the chaotic dynamics on the frequency of the external stimulus. For a broad range
of stimulus amplitudes, the network activity is chaotic for small and large input frequencies, but periodic at
intermediate frequencies. This prediction can be tested experimentally.
Although the networks we consider are random, they have a distinct selectivity to the input that drives them
that can be analyzed in terms of the eigenvectors of the underlying connectivity matrix. This selectivity, as
well as network ampliﬁcation, is maximized when the undriven network is strongly chaotic and the driven
network is on the edge of the phase transition.
Acknowledgments
Supported by NIH Pioneer Award 5-DP1-OD114-02.

RATE 
CORRELATION 

)NPUT AMPLITUDE 

RATE

PERIODIC 

CORRELATION

References
[1] Dynamics of ongoing activity. A. Arieli et al., Science 273:
1868-71,1996
[2]Chaos in random neural networks. H. Sompolinsky,
A. Crisanti & H. Sommers, Phys Rev Lett 61:259-62, 1988
[3] Chaos in neuronal networks with balanced ex. & inh. activity.
C. van Vreeswijk & H. Sompolinsky, Science 24:1724-6,1996

0HASE TRANSITION AS A FUNCTION OF INPUT AMPLITUDE AND FREQUENCY 

CHAOTIC

0

1

2

3

4

5

)NPUT FREQUENCY (Z

164

6

7

8

Cosyne 2008

Friday evening, Poster session II-46

!"#$%&'()*+#,%-'./(%0#.%1"2,*)!3/%&'!2,/4%)'%
%#$3%
%-5#4/%

%

6#*%78%9*,,/(:;%7/<<(/=%>8%?@/"#$$A;%#$3%9#(+/,%3/$B*@4:%
%

% !"%#$%&'()(C )%D&'C%'% *%*%%E!F%'*%(+%G !C*(!%
%
, F!C%'%%!*)%F )" *%"%(&'%-(*%%(+%+&!*(! )% !C%%!% %('* )%-%*' )%-(.%'%) .%.*%)( )%
*F*%!%&/ !%('*%H%IJ0%.%%/(F%%*(%*%%C!+ !%%(+%*%+!"!C1%E!C% %23%K %"%/%*("%(!%
%)%*'(('*(C' -%'%('"!C%!%&/ !0%.%%.%'%% K)%%*(%"%(&-)%%*%-(.%'%) .%K% F('%+'(/%*%%
) % % !"% % '*/0% '%F% )!C% *% -'%%!%% *% )(.% +'%L&%!%1% G%% %H /!%"% *%% '%) *(!-% (+%
 !C%% !% *%% /-)*&"%% (+% *% -(.%'% ) .% *(% *%% - %% (+% !*'!% )(.% +'%L&%!% '*/1% ,%'%% .%%
"%/(!*' *%%* *%%- %%*(%CM% /-)*&"%%(&-)!C%I#J%%!(*0%!%+ *0%)/*%"%*(%*%%CM%' !C%1%N*%% %
-'(-%'*% (+% *%% /-)*&"%%(+% *% -(.%'% ) .% -%!(/%!(!0% F '%% .*%  !C%%!% )( )% ('* )% *F*%
OK%*.%%!%+!C%'%/(F%/%!*% !"%'%*P0% !"%)(Q%*(%*%%*'(&C%(+%*%%- %%(+% !"% %.%))% %1%
%
G%%(.%(.% %/-)%0%/ ))M )%0%/("%)%(+%! -*%('C !R *(!%/ %-'(F"%%!*&*(!%+('%*%%) 'C%%
 )%% - %M /-)*&"%% (''%) *(!% .%% '%-('*1% % N!% *% /("%)S% P% T%% *%/-(' ))%  )%% +'%%% O-(.%'% ) .P%
-(*%!* )%  !C%% '%+)%*% !'(!(&% &// *(!% (+% % ) 'C%% !&/K%'% (+% ('*(M('* )% !-&*% K%*.%%!%
-' /" )% !%&'(!% !% *%% "* )% "%!"'*% 'K('1% #P% D!'(!(&0% &KM('* )% O* ) /% !"% (*%'P0%
-'(4%*(!%*(%*%%-'(H/ )%"%!"'*% 'K('%(+%-' /" )%!%&'(!% '%%'%+)%*%"%!%*%%% !"%'*/1%%
T%%!*%' *(!%(+%*%%%*.(%-'(%%%%*%!%'%-(!K)%%+('%*%%- %M /-)*&"%%(&-)!C%.%%(K%'F%1%
%

%
UC&'%S%%3P%(.%'%) .%O&--%'%'C*P%!%*%%('* )%-%*'&/% !%K%%%H*' *%"%+'(/% )- % !"%K%* %-% Q%
"&'!C% %/(F%/%!*%* Q0% !"%*V%+*%"&'!C% *F*% !%K%%F& )R%"% )(!%%O)(.%'%'C*P1%%%
WP%%T%+*!C%-(.%'%) .%%(&-)%"%/('%%*'(!C)%*(% !"%%- %%"&'!C%! *F*%* !% *F*1%%%
2P%3%/-)%%/("%)%%-(*&) *%"%.%'%%* ) /%!-&*%&!*%('*(M('* )%!*%' *(!1%%
%
1+5$'.,/36"/$)4%
T%.('Q%. %&--('*%"%K%$DU%W2DMXYZ#[Z[1%T !Q%*(%- *%!*% !"%* ++% *%, 'K('F%.%,(-* )1%
%
\/</(/$+/4%
IJ%]2(7%(K%'F *(!%(+%-(.%'%) .% )!C%!%*%%&/ !%('*%H1%89%:))%'0%;W%D('%!%!0%97%<4%/ !!0%
:%"%!$40%=% !"#$%%$&'%^()(/K(*%+XX_%O`,-.,/(!Pa%D0()/*,-%12,!C(%!%1(*/),-%D&*3,)(%4(/(!/,-%
.&*!C%5(/(*%5(F(/(!/6%%78%5--(*%(/6%,-6'%% !"#$%&'(%)_ObPS)Z)ZM)Zc)(%U(K*&**+%)XX_6%O,+-(./(P%
I)J% ,C/% C*00*% -(1(*% % -/*(M2()Q(3% .(% ./(.*% ()22*.(4% 4% /&0*4% 4(()(*.(H6% 5*4(2.+(% 66T6% (.% *2%
7&'!&!%c8cS%8Y)YM[(%)XXY6%

165

Friday evening, Poster session II-47

Cosyne 2008

Learning Odor Features
Stijn Cassenaer1 and Gilles Laurent1
1

California Institute of Technology

The insect olfactory system has been used as a model for studying a number of neural coding issues,
employing a range of experimental methods, including genetic manipulations, imaging and
electrophysiology. Learning and memory have been studied extensively in this system as well,
predominantly from a genetic and behavioral perspective. In the current work, we present
electrophysiological and modeling data that address associative learning in the insect olfactory system.
Broad activation of the antennal lobe (AL) by olfactory stimuli gives rise to oscillatory population activity
and diverging trajectories of projection neuron (PN) activation [1-4]. Different points along these
trajectories can be thought of as representing different features of the odor stimuli (e.g. class vs. identity),
and cells that decode PN activity, Kenyon cells (KCs), respond sparsely at specific time-points along the
PN trajectories [5]. Previous work suggests that individual oscillation cycles are meaningful units for the
encoding and decoding of olfactory information by PNs and KCs [3-7] and this appears to be the case also
for extrinsic neurons in the mushroom body beta-lobe (bLNs), which decode the KCs' sparse responses.
Spike-timing-dependent plasticity at KC-bLN synapses facilitates the synchronous flow of olfactory
information, and thereby maintains the segregation between oscillation cycles [8].
We describe evidence for an additional component that ensures this segregation, namely feed-forward
inhibition onto bLNs. We identify the source of this inhibition to be neighboring bLNs of the same class,
and implement this in a network model to evaluate the consequences of the interaction between STDP and
the resultant competition among bLNs due to the phase-locked inhibition. We consider our modeling
results within the context of the circuit in which the KC-bLN network is embedded, and propose a
mechanism for learning an arbitrary subset of odor features extracted and formatted as a function of
oscillation cycle by the antennal lobe. We also describe experiments that address some of the model's
results.

Acknowledgments
This work was supported by an NIH training grant and grants from the NIDCD.
References
[1] Laurent G, Davidowitz H (1994) Science 265:1872–5.
[2] Laurent G, Wehr M, Davidowitz H (1996) J. Neurosci 16:3837–47.
[3] Perez-Orive J, Mazor O, Turner GC, Cassenaer S, Wilson RI, Laurent G (2002) Science 297:359–65.
[4] Stopfer M, Jayaraman V, Laurent G (2003) Neuron 39:991–1004.
[5] Broome BM, Jayaraman V, Laurent G (2006) Neuron 51:467–482.
[6] Wehr M, Laurent G (1996) Nature 384:162–6.
[7] Mazor O, Laurent G (2005) Neuron 48:661–73.
[8] Cassenaer S, Laurent G (2007) Nature 448:709–713.

166

Cosyne 2008

Friday evening, Poster session II-48

Microsaccades Dynamically Modulate Gamma-band
Synchronization in Macaque Visual Cortex
Conrado Bosman1, Thilo Womelsdorf1, Robert Desimone2, 3, Pascal Fries1, 4
1

F.C. Donders Centre for Cognitive Neuroimaging, Radboud University Nijmegen, The Netherlands.
Laboratory of Neuropsychology, National Institute of Mental Health, National Institutes of Health,
Bethesda, MD. 3 McGovern Institute for Brain Research at the Massachusetts Institute of Technology,
Cambridge, MA. 4 Department of Biophysics, Radboud University Nijmegen, The Netherlands.
2

Microsaccades (MSs) are able to counteract retinal fading and improve visual perception[1]. They are
triggered rhythmically when retinal image slip is low, suggesting active underlying control processes[2].
Oscillatory synchronization within visual cortex has been described as a signature of perceptual
processes, but it is unknown whether it is modulated by MSs. The present study evaluates the rhythmic
activity in awake monkey visual areas V1 and V4 around MSs.
Two monkeys were trained to keep their gaze within a small (±0.8 degree) fixation window for several
seconds during a change-detection task. MS occurrences were evaluated during that period. We recorded
spiking activity and local field potentials (LFPs) simultaneously from up to eight electrodes positioned
either in area V1 or area V4. Receptive fields were stimulated with a moving grating. We quantified
oscillatory synchronization by obtaining time- and frequency-resolved LFP power and spike-LFP
coherence around the time of MSs.
We found peri-MS modulations in firing rates and in rhythmic synchronization, both following, but also
preceding the MS. Post-MS modulations in gamma-band are present in both V1 and V4 and their timing
is consistent with a bottom-up response to the MS. By contrast, pre-MS modulations are restricted to V4,
consistent with a top-down generation, and they consist of an early gamma-band synchronization
enhancement followed by a firing rate reduction. Furthermore, MSs were partly phase-locked to the 3 Hz
component of the LFP and this MS rhythmicity was found back in the peri-MS perturbations of rhythmic
activity in V1.
These results further advance the notion that MSs are actively generated and they might provide first
insights into the neuronal mechanisms behind this active control.
Acknowledgments
We thank J.H. Reynolds, A.E. Rorie and A.F. Rossi for help during the monkey experiments. Supported
by: Mideplan, Chile (C.B.); the European Science Foundation (EURYI Program), the Volkswagen
Foundation (P.F.); NIH-R01EY017292, NIMH-IRP (R.D.).
References
1. Microsaccades counteract visual fading during fixation. Martinez-Conde S, Macknik SL, Troncoso
XG, Dyar TA. Neuron 49 (2):297-305, Jan 19 2006.
2. Microsaccades are triggered by low retinal image slip. Engbert R, Mergenthaler K. Proceedings of the
National Academy of Sciences of the United States of America 103 (18):7192-7197, MAY 2 2006.

167

Friday evening, Poster session II-49

Cosyne 2008

Shaping of tuning properties in motor cortex during long-term
visuomotor learning
Hagai Lalazar1,2, Maayan Shahar2, and Eilon Vaadia1,2
1

Interdisciplinary Center for Neural Computation,
Department of Physiology, Hadassah Medical School,
The Hebrew University of Jerusalem

2

Many studies have examined the relation between movement parameters and the neural responses in the
motor cortex of behaving primates. Encoding of several movement parameters has been shown, such as
speed and direction, muscle activation, joint angles, and the sensory cues for movement. In such studies
the primates are overtrained—trained for many months or years until they reach expert and constant
performance.
In the current study we recorded from the arm region of the primary motor cortex of a monkey for three
months as she learned a three-dimensional point-to-point reaching task. During this period her behavioral
performance improved from a novice level to highly proficient. We recorded the full arm kinematics
(using optical sensors) and 96 neural signals from a chronically implanted electrode array (Cyberkinetics,
Inc.).
Our task involved a wide variety of movements in order to span a large portion of the movement space
and thus allow us to sample the tuning properties in a broad manner. We examined the changes in the
encoding of these motor parameters in relation to the parameters of the improving movements. In some
cases we could track the encoding of a single neuron for several days, as it remained convincingly well
isolated and identifiable.

Acknowledgments
Supported in part by the BSF, ISF and J&J grants and by special contributions from the Rosetrees Trust
and the Ida Baruch fund. EV is the Jack H. Skirball Chair of Brain Research.

168

Cosyne 2008

Friday evening, Poster session II-50

Mental Perspective Transformations:
ERP Analyses and Single Trial EEG Classification
Lars Schwabe, Nathan Evans, Bigna Leggenhager, and Olaf Blanke
1

Cognitive Neuroscience Lab, Brain Mind Institute, EPFL, Lausanne, Switzerland

We are interested in the neuronal basis of embodiment and how the brain generates and maintains the
first-person perspective. Investigations of dysfunctional first-person perspectives in pathological
conditions like, for example, the multisensory illusion of an out-of-body experience and related illusions
[1] have been proven as valuable tools to investigate the normal first-person perspective. In addition,
embodiment has also been investigated in healthy subjects using neuroimaging by adapting paradigms
from mental imagery [2,3]. Here, we extend these studies towards a more comprehensive description of
mental imagery involving perspective transformations and their neurophysiological correlates.
We designed 3D visual stimuli of life sized humanoid figures rotated around the vertical yaw axis (45°
steps) as viewed from a canonical perspective behind and from two distanced non-canonical perspectives:
lowered and elevated (comparable to the perspective reported during spontaneous out-of-body
experiences). Stimuli were presented for 200 ms on a large back projection screen (2.5 x 3m). In the own
body transformation task (OBT) subjects (N=11) indicated as to whether the left or the right arm of the
figure was extended. In the lateralization control task (LAT), they indicated if the extended arm was on
the left or right side of the screen. Continuous EEG was acquired from 256 electrodes.
In the OBT, we find the well-known dependence of the reaction times (RTs) on the rotation angle
(500..700 ms for 0°..180° rotation), whereas no such dependence was observed in the LCT.
Interestingly, the RTs for the elevated perspective in the OBT were on average 30 ms shorter than the
RTs for the canonical floor perspective, which were 30 ms shorter than the RTs for the lowered
perspective. In an event-related potential (EP) analysis, we used k-means clustering of the spatial patterns
of the group-averaged EPs to segment it into stable map topographies yielding sequences of functional
microstates of the brain. This approach uncovered, only for the OBT, a functional microstate at around
350-400 ms after stimulus onset with its duration correlating to the RTs. Applying it to the responselocked EPs, we found another microstate just prior to the motor response, but only in the OBT and again
correlating in duration to the RTs. Finally, we analyzed the stimulus-locked EEG on a single-trial basis,
where we applied the linear Fisher discriminant to the changes in the power of selected frequency bands.
First results indicate that the strength of the 10 Hz oscillations is particular informative in the sense of
yielding higher classification performances compared to higher frequencies for discriminating between
OBT and LAT.
Our results provide a comprehensive description of mental perspective taking, providing another
yardstick for future modeling studies of perspective taking. They emphasize the critical role of brain
activity at ~350-400 ms after stimulus onset for the cognitive manipulation of the first-person perspective,
which may turn out (in addition to motor imagery) to qualify as another candidate paradigm for EEGbased brain-computer interfaces.
[1] Blanke et al., Nature, 2002. [2] Blanke et al., J Neurosci, 2005. [3] Arzy et al., J Neurosci, 2006

169

Friday evening, Poster session II-51

Cosyne 2008

Neuronal shaping in a Co-Adaptive Brain-Machine Interface
Babak Mahmoudi1, Jack DiGiovanna 1, Jose C. Principe2, Justin C. Sanchez3
1

Department of Biomedical Engineering, 2Department of Electrical and Computer
Engineering, 3Department of Pediatrics, Neurology University of Florida

A new framework for studying causation between biological networks and computational models has
been developed using a motor Brain-Machine Interface (BMI) system based on Reinforcement Learning
(RL). The closed-loop RLBMI shown in Fig. 1 is a framework to test theories of goal-based learning and
decision making through experience using a robotic arm in 3-D space during a reaching task. Here, the
interaction between an agent and user’s brain occurs through the generation of a sequence brain states that
are mapped by the agent to a series of actions of a robotic arm. Using states, actions, and rewards, the
agent and user must learn to co-adapt with each other to maximize the earned reward.
In this paradigm, we have trained 3 rats in a two-target choice task. To obtain
a water reward, the rats were required to use a robotic arm to press one of
two levers (left and right positions), cued by an LED light. The rats were
bilaterally implanted with two electrode arrays in the forelimb region of
primary motor cortex (16 electrodes in each M1). During brain-control of the
robotic arm, single unit activity of cortical cells was chronically recorded and
their firing rates (100ms bins) were used as environmental state in RLBMI.
In this architecture, the agent is the robot controller and at each time step,
Figure. 1 RLBMI architecture
the agent maneuvers by evaluating its environmental state and selecting
one of the 26 actions in the 3-D space. Initially, both the rat and agent are naïve to the solution of the task.
We have used a two layer neural network (MLP) for Value Function Estimation (VFE). In the VFE
structure, trained with temporal-difference learning, the action corresponding to the output node with
maximum value was selected for robot control. To test the neural response to the co-adaptation, the
shaping of complex behaviors was enabled with an adjustable threshold which sets the necessary target
proximity to earn reward. This threshold was iteratively adjusted from close to the robot starting position
to far away where the probability of randomly intersecting the target was 25% - 9%.
Using this architecture, we demonstrate that the three animals were able to maintain a reward
performance 460%, 517%, and 515% over chance despite the increasing task difficulty. Here we study
two adaptive elements of the architecture the agent model and the user’s neuromodulation. First, in the
agent’s VFE network we observed the weight tracks to be smoothly varying (no discontinuities) over
multiple days indicating that past experience was being maintained by the agent for each difficulty level.
Excited by the neuronal firings, the input layer of the MLP showed decreasing variability as task
performance increased. Second, for the user’s neuromodulation, 60% of the neurons had a decrease, 30%
had an increase and 10% had no significant change in the mean firing rate as a function of the difficulty
level when compared to the first session. Interestingly, of the neurons with a decrease in firing rate, 73%
had an increase in their coefficient of variation (CV) of firing and the rest had no significant change in
their CV. 86% of the neurons which had increase in their mean firing had no significant change in their
CV. The remaining 10% of the neurons that had no significant change in their mean firing rate also did
not show a significant change in their CV. All metrics were tested for significance using ANOVA at
95%. Based on the results, co-adaptation does not primarily occur as a general up-regulation in neuronal
firing but as an increase in temporally specific neuromodulation of the ensemble related to subgoals of the
complete reaching task.
Acknowledgements: This work was supported by NSF Grant #CNS-0540304.

170

Cosyne 2008

Friday evening, Poster session II-52

The Computational Impact of Adult Neurogenesis in the Dentate
Gyrus on Memory Formation
James B. Aimone1, Janet Wiles2, and Fred H. Gage1
1
The Salk Institute for Biological Studies; La Jolla, CA
2
University of Queensland; Brisbane, Australia
The dentate gyrus region of the hippocampus (DG) is one of two brain regions that incorporates newly
born neurons throughout adulthood. Neurogenesis is limited to the principal cell type of the DG, the
glutamatergic granule cells, and after about two months of maturation, adult born neurons appear to
become anatomically and physiologically similar to those that were born embryonically and postnatally.
Although the integration of a considerable number of new neurons into the hippocampus suggests that
neurogenesis may be a key form of plasticity in the creation of new memories, the function of these new
neurons has proven difficult to establish.
By virtue of its large size and sparse firing rates, the DG region is believed to be involved in the
separation of cortical inputs during memory formation. This pattern separation is critical in the formation
of new memory attractors in the downstream hippocampal layers (CA3 and CA1). The requirement of
neurogenesis in such a process remains unclear, but we have hypothesized that the immature neurons may
have a direct influence on this pattern separation function, possibly even contributing temporal
information to new memories [1].
We designed a computational model to investigate whether adding these new neurons to the dentate gyrus
could affect memory formation in a manner consistent with what our hypothesis predicted. The model we
generated focuses on the DG region itself, including several feedback neuron populations (basket cells,
mossy cells, hilar interneurons) in addition to the neurogenic granule cell layer. The input layer of the
model is the entorhinal cortex, which we simulate in part by using the "grid cell" behavior that has been
recently described in that region. We implement neurogenesis in the model in a biologically realistic
manner, with new neurons being incorporated into the network gradually over several months in the
simulation. This includes passing through the multiple stages of development observed in the biological
system, including an early GABA excitatory phase and a transient increased LTP phase.
Here we will show how immature neurons affect pattern separation at short and long time scales. Our
results demonstrate that pattern separation by the DG is reduced in specific conditions because of
neurogenesis. We refer to this effect as pattern integration (as opposed to the existing pattern separation
and pattern completion concepts used in theories of memory formation), and will discuss the potential
ramifications of this in learning. Furthermore, we will extend the modeling results to show how dentate
gyrus function can be improved by the long-term survival of these new neurons and their experiencedependent integration into the network.
Acknowledgments
We thank the Kavli Institute for Brain and Mind, the NSF Temporal Dynamics of Learning Center, and
the James S. McDonnell Foundation for financial support.
References
[1] “Potential Role for Adult Neurogenesis in the Encoding of Time in New Memories.” J.B. Aimone, J.
Wiles and F.H. Gage; Nature Neuroscience, 2006.

171

Friday evening, Poster session II-53

Cosyne 2008

Internally generated assembly sequences in the hippocampus and
episodic memory
Eva Pastalkova1, Vladimir Itskov2, Gyorgy Buzsaki1
1

Rurgers University, Newark, NJ

2

Columbia University, New York, NY

Our ability to recall a spatio-temporal sequence of events from our personal past experience is called
episodic memory. It is believed that ‘mental travel in time’ is uniquely human. Clinical evidence shows
that the hippocampus is critical to coding and retrieving of one’s past episodes. The hypothetical neuronal
source of an episode recall is a network wherein the neuronal assemblies representing different events
within the same spatial and temporal framework subsequently activate each other. Interestingly,
experiments with rodents have revealed that cell assemblies in the hippocampus respond differentially to
spatial cues and thus subsequently visited places are represented by the temporal relationship among the
neurons. However, it is believed that spatial cues continuously control the hippocampal activity. Given
the assumption that neuronal algorithms serving a useful function in a simple brain can be used for more
complex functions in larger brains, one can hypothesize that a rodent’s hippcampal network is capable of
the perpetuation of cell assemblies even without any external control. I have demonstrated that
hippocampal cell assemblies evolve internally when environmental cues are fixed while a rat is running in
a wheel (is ‘frozen’ in space) and that the specificity of the activated assemblies carries information about
a subsequent choice of an animal in a memory task. I have also demonstrated that hippocampal
assemblies are activated in a steady, non-progressing manner during a wheel running with minimal
memory demand. These results indicate that the internal dynamics of a rodent’s hippocampal network is
capable of generating context-dependent, sequentially progressing neuronal assemblies.

Acknowledgments
This work was supported by The Patterson Trust Postdoctoral Fellowship Program in Brain Circuitry,
NIH NS34994, NS43157, Mh54671.

172

Cosyne 2008

Friday evening, Poster session II-54

!""#$%&"%'()*"'%+(!,(%('%)-*(*,.!)#,&*,/(0"%,0(&1'/!"'*()!""'*0
23#&%0(45(6%.!70#,8(9%:!%,(;'##0/*)&%,8(%,7(<%//3*=(>5(?!'0#,
!"#$%&'()*+!+,+%'-#&'.%/&)!)0'/)1'2%3#&45'2/**/"6,*%++*'()*+!+,+%'#-'7%"6)#8#045'9/3:&!10%5'2;<
=!&!)0'#-'6!>>#"/3>/8'>8/"%'"%88*'3/4'*,:*%&?%'*>/+!/8'8%/&)!)0<'@)*%3:8%'&%"#&1!)0*'!)'&/+*'6/?%'*6#$)'
+6/+ ' +6%*% ' "%88* ' &%A%B>&%** ' :%6/?!#&/8 ' -!&!)0 ' *%C,%)"%* ' D+%&3%1 ' E&%>8/4EF ' 1,&!)0 ' *8#$A$/?% ' *8%%> ' /)1'
1,&!)0'$/G%-,8)%**<'76%*%'>6%)#3%)/'/&%'64>#+6%*!H%1'+#'>8/4'/'&#8%'!)'3%3#&4'"#)*#8!1/+!#)<
I%'6/?%'3/1%'%)*%3:8%'&%"#&1!)0*'#-'>8/"%'"%88*'!)'/&%/'9;J'#)'1/4'K'#&'L'#-'%B>#*,&%'+#'/'JM3'8!)%/&'
+&/"G'D=!0<'JN'&/+'?!*!:8%'/+'&!06+F<'2/)4'>8/"%'"%88*'%B6!:!+'3,8+!>8%'>%/G*'#)'+6!*'+&/"G5'3/G!)0'&%>8/4'
1%+%"+!#)':4'"%88'-!&!)0'#&1%&'1!--!",8+<'I%'+6%&%-#&%'%3>8#4'/'*!3>8%'>&#:/:!8!*+!"'1%"#1!)0'/80#&!+63'+#'
*%&!/884'%*+!3/+%'>#*!+!#)'-&#3'*6#&+'$!)1#$*'#-'+6%'*>!G!)0'&%"#&1'D=!0<'KF<'O%>8/4'!*'+6%)'1%-!)%1'/*'
E?!&+,/8'+&/P%"+#&!%*E'!)'+6!*'&%"#)*+&,"+%1'*!0)/8<'Q,&!)0'>%&!#1*'#-'!33#:!8!+45':#+6'-#&$/&1'/)1'&%?%&*%'
&%>8/4 ' /&% ' >&%?/8%)+5 ' $!+6 ' *!0)!-!"/)+ ' &%>8/4 ' %>!*#1%* ' #"",&&!)0 ' /+ ' / ' &/+% ' #- ' RK<ST3!),+%< ' O%>8/4%1'
+&/P%"+#&!%*'8/*+',>'+#'UMM3*'/)1'*>/)',>'+#'V3'#-'+6%'+&/"G5'/)1'#"",&'/+'/'E?!&+,/8E'?%8#"!+4'#-'RJM3T*<'
O%>8/4'!*'"#&&%8/+%1'$!+6'!)"&%/*%1'>#$%&'!)'+6%'&!>>8%A:/)1'DJSMAKSMWHF'!)'+6%'8#"/8'-!%81'>#+%)+!/85'$!+6'
>&%?!#,*84',)&%>#&+%1':,&*+*'#-'*6/&>A$/?%T&!>>8%'DXIOF'"#3>8%B%*'-&%C,%)+84'"#A#"",&&!)0'$!+6'&%>8/4'
D=!0<'KF<
76!*'>&#8#)0%1'&%>8/4'!*')#+'>&%1!"+%1':4'%B!*+!)0'3#1%8*' JAL'$6!"6'%B>8/!)'&%>8/4'/*'/)'E,)3/*G!)0E'#-'
>&#0&%**!?%84 ' $%/G%&5 ' 8#"/8 ' >8/"%A&%8/+%1 ' !)>,+*< ' I% ' >&#>#*% ' !)*+%/1 ' / ' 3#1%8 ' !) ' $6!"6 ' *6#&+'
6!>>#"/3>/884A0%)%&/+%1 ' *%C,%)"%* ' D>/!&%1 ' $!+6 ' XIO*F ' /&% ' "6/!)%1 ' +#0%+6%& ' ?!/ ' +6% ' %)+#&6!)/8A
6!>>#"/3>/8'8##>5'/88#$!)0'-#&'+6%'%B>&%**!#)'#-'/&:!+&/&!84'8#)0'6!>>#"/3>/8'*%C,%)"%*<

@*A*)*,$*0B
J<'O%?%&*%'&%>8/4'#-':%6/?!#,&/8'*%C,%)"%*'!)'6!>>#"/3>/8'>8/"%'"%88*'1,&!)0'+6%'/$/G%'*+/+%<'=#*+%&5'Q<'
Y<'Z'I!8*#)5'2<';<' !"#$%'[[MDUMV[F\]VMA]VL5'2/&'KMM]<
K<'=#&$/&1'/)1'&%?%&*%'6!>>#"/3>/8'>8/"%A"%88'*%C,%)"%*'1,&!)0'&!>>8%*<'Q!:/5'^<'Z'_,H*/G!5'`<' !"&'
%#$()*+&5'X%>'KMMU<
L<' 8/"%A*%8%"+!?%'-!&!)0'"#)+&!:,+%*'+#'+6%'&%?%&*%A#&1%&'&%/"+!?/+!#)'#-'9;J'>4&/3!1/8'"%88*'1,&!)0'*6/&>'
$/?%*'!)'#>%)A-!%81'%B>8#&/+!#)<'9*!"*?/&!5'Y<'%"'!,<'-#$&'.&' %#$()*+&'K]DLF\UM[AUJ]5';,0'KMMU<

173

Friday evening, Poster session II-55

Cosyne 2008

The role of sparseness for the Hebbian learning of receptive
ﬁelds from natural scenes
Jan Wiltschut, Fred H Hamker
Westfälische Wilhelms-Universität Münster, Germany
The concept of ‘sparse coding’ refers to a neural representation where only a few cells (out of a large
population) are effectively used to represent a visual scene. There is strong theoretical evidence that
sparseness is essential for learning receptive ﬁelds (RF) which resemble those neurons observed in area V1
[3]. However, a high degree of sparseness is not necessary to learn Gabor-like RFs [1]. Presently there
exists no study that systematically varied the degree of sparseness.
Here, we vary the degree of sparseness and report the correlation of sparseness with RF properties.
We use a two layered dynamic model with lateral inhibition in the second layer. We simultaneously learn
the lateral weights using the anti-Hebbian and the feedforward/feedback weights according to the Hebbian
principle. The degree of sparseness depends on two parameters, the anti-hebbian weight normalization
constraint and the degree of the non-linearity of the lateral interactions.
The weights converge to localized, oriented and bandpass ﬁlters similar as the ones found in monkey V1. We observe a linear correlation between sparseness and frequency (Figure 1a). Particularly, more
sparse representations result into RFs with properties that are similar to characteristics found by monkey V1
cell recordings [4] (Figure 1b).
b)
a)
Figure 1: a) The mean frequency over all ﬁtted ﬁlters against the
mean sparseness of the cell populations. We used a sparseness measure based on the relationship between the L1 and the L2 norm of the
cell population vectors [2]. b) The vertical against the horizontal RF
f(s) = 1.5 s - 0.31
size multiplied with the frequency. The black dots show the results of
our network and the red dots the results of [4].
	
 f
Mean sparseness
1.5

1.1

1

0.9

	Y
 f

Mean frequency

1.3

0.7

0.5

•

0.5

0.6

0.7

0.8

0.9

1

0

0.5

1

X

Acknowledgments
This work has been supported by the German Science Foundation (DFG HA2630/4).

References
[1] FH Hamker and J Wiltschut. Hebbian learning in a model with dynamic rate coded neurons: An alternative to the generative model approach for learning receptive ﬁelds from natural scenes. Network,
Computation in Neural Systems, 18(3):249–266, 2007.
[2] PO Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of Machine Learning
Research, 5:1457–1469, 2004.
[3] BA Olshausen and DJ Field. Sparse coding with an overcomplete basis set: a strategy employed by v1?
Vision Res, 37(23):3311–3325, 1997.
[4] DL Ringach. Spatial structure and symmetry of simple-cell receptive ﬁelds in macaque primary visual
cortex. J Neurophysiol, 88(1):455–463, 2002.

174

Cosyne 2008

Friday evening, Poster session II-56

Balanced changes in Excitation and Inhibition modify the gain,
while preserving the threshold, of neuronal I/O functions
Tiago P. Carvalho1,2 and Dean V. Buonomano1
1

Gulbenkian Ph.D. Program in Biomedicine, 2Departments of Neurobiology and
Psychology, University of California, Los Angeles
The neural response to virtually all sensory stimuli consists of overlapping excitatory and inhibitory
currents – a consequence of the ubiquitous presence of feed-forward inhibitory disynaptic circuits in the
nervous system. Additionally, it is well established that both the excitatory and inhibitory synapses onto a
neuron are plastic. However, the computational significance of independently, or jointly, changing the
strength of excitation and inhibition onto that neuron is not clear. For example, from a computational
perspective what is the functional difference between potentiating excitatory inputs and depressing
inhibitory ones? Furthermore, a number of plasticity protocols can potentiate both EPSPs and IPSPs onto
the postsynaptic neuron; what is the computational benefit of this apparently self-defeating form of
plasticity? Or, what is the computational consequence of the potentiation of excitatory synapses, in
parallel with depression of inhibition?
Given that the contribution of a neuron to information processing is given by whether or not it fires an
action potential, it is useful to define the behavior of neurons in terms of their input/output (I/O) functions
- which represent the probability of an action potential being generated in response to different levels of
input. Theoretical and experimental studies generally characterize the I/O function with a sigmoid defined
by two variables: the threshold (defined as level of input that yields a half-maximal response) and the gain
(defined as the slope of the linear portion of curve). It is likely that the threshold and gain of a neuron are
directly related to its computational role, as both of these features can be used to quantify the ability of
neurons to encode sensory information. Similarly, the same features of sigmoid functions are used to
quantify psychophysical thresholds in the context of signal detection.
Here, we used computer simulations to show that excitatory plasticity shifts the threshold of the neuronal
IO function, while balanced changes in excitation and inhibition can modify the gain, independently of
the threshold. We show that the combination of different excitatory and inhibitory synaptic weights
allows neurons to control independently both features of IO functions. Whole-cell recordings in acute rat
hippocampal slices supported experimentally these observations. As predicted, decreasing inhibition
pharmacologically resulted in a leftward shift of the IO function, together with an increase in the gain.
Plasticity protocols leading to an increase in the excitatory drive alone resulted in a leftward shift of the
IO function, but without an accompanying change in the gain. Thus, we suggest two general modes of
plasticity: threshold plasticity produced by changes in excitatory strength, and gain plasticity produced by
parallel changes in excitation and inhibition.

Acknowledgments
Supported by NIMH (M.H. 60163) and NSF (0543651), and T.P.C. was supported by the Portuguese
Science and Technology Foundation (SFRH/BD/15211/2004).

175

Friday evening, Poster session II-57

Cosyne 2008

On the equivalence between differential Hebbian and temporal
difference learning
Christoph Kolodziejski1 , Bernd Porr2, and Florentin Wörgötter1
1

2
BCCN, Universität Göttingen, Germany,
Department of Electronics & Electrical Engineering, University of Glasgow, Scotland, UK

Network learning subdivides into supervised (SL), reinforcement (RL) and correlation based (Hebbian, CL)
according to the existence and role of an error signal for controlling the learning. In SL explicit error signals
exist, in RL networks learn from unspeciﬁc reinforcement signals (rewards), whereas in CL learning takes
place as a self-organization process relying on multiplicative signal correlations only, hence without an error
signal. Convergence proofs exist for some generic cases of these learning mechanisms [1]. The equivalence
of the different rules, however, remains a pressing question as it would allow extending conclusions on convergence across different mechanisms. In particular, it has been suspected that RL ought to be deducible
from the more general CL by introducing a third factor into the correlation. If true, this conjunction would
have substantial inﬂuence on our understanding of network learning as CL and RL could be interchanged
under these conditions. A similar impact can be expected for biophysics, as there is currently no clear evidence for any biophysical molecular mechanism directly at a single neuron that could emulate the required
subtractive comparison between reward and expected reward. Biophysically, RL can currently only be understood as a network effect. On the other hand, evidence for Hebbian mechanisms at single neurons with
and without the modulation by a third factor are abundant (see [4] for a review).
Here we show that the most inﬂuential form of Reinforcement Learning, temporal difference learning, and
differential Hebbian learning modulated by a third factor are indeed equivalent under certain conditions [3].
This proof relies only on commonly applicable, fairly general assumptions, thus rendering a generic result
not constraining the design of larger networks. Therefore, all network structures using such circuitry can
rely on optimal temporal difference values.
With such a circuit we can build different kinds of networks. One of these has strong similarities to the direct
pathway of the basal ganglia. Our third factor can therefore be linked to the signaling of acetylcholineric
interneurons. These neurons have very precise and reliable ﬁring characteristics similar to that proposed by
our model [2].
Acknowledgments
F.W. acknowledges the support of the European Commission, IP-Project PACO-PLUS.
References
[1] Theoretical neuroscience. P. Dayan and L. F. Abbott. Cambridge, MA; MIT Press. 2001.
[2] The basal ganglia and chunking of action repertoires. A. Graybiel. Neurobiol Learn Mem., 70(1-2):119–
36, 1998.
[3] On the equivalence between hebbian and reinforcement learning.
F. Wörgötter. Neural Network (submitted), 2008.

C. Kolodziejski, B. Porr, and

[4] LTP and LTD: An embarrassment of riches. R. C. Malenka and M. F. Bear. Neuron, 44:5–21, 2004.

176

Cosyne 2008

Friday evening, Poster session II-58

ICA with Spiking Neurons?
Cristina Savin and Jochen Triesch
Frankfurt Institute for Advanced Studies, Frankfurt, Germany
The brain undergoes a wide variety of plastic changes. Experimental and theoretical work has focused
mainly on the activity-dependent regulation of synaptic strength. However, persistent changes in neuronal
excitability after alterations in neuronal ﬁring have also been reported for various biological systems [1].
This process is known as intrinsic plasticity (IP).
While it is widely accepted that synaptic mechanisms such as LTP and LTD are involved in information
processing and learning, the function of IP remains poorly understood. It is thought to play a homeostatic
role, keeping the activity of neurons in a certain dynamic regime. More interestingly, it may also be involved
in efﬁcient neuronal coding. Neurons could use IP to optimize the transmission of information, under the
constraint of a limited energy budget [2]. Furthermore, it was shown that combining IP with Hebbian
learning makes it possible to discover heavy-tailed directions in the input [3], supporting ICA.

Synaptic
strengths

Rate (Hz)

150
We propose an IP implementation for
100
integrate-and-ﬁre (IF) neurons. Speciﬁcally,
50
we have devised a rule for adjusting neuron
4
0
0
0.5
1
1.5
2
2.5
3
3.5
4 x10
excitability by modifying the resistance and
Time(s)
capacitance of the IF model. These parameters
are changed such that the ﬁrst and second
moment of the distribution of the output
ﬁring rate match the optimal (exponential)
distribution, for a given mean activity. In this way, the neuron exploits the statistical regularities in the
input, ﬁring strongly only for rare events, as shown in the ﬁgure. In particular, we have considered the
Földiák bars problem [4], a classic nonlinear independent component analysis task. When combined with
various forms of Hebbian synaptic learning, IP allows the neuron to learn one independent component in
the input (one bar).

Our model suggests a biologically plausible mechanism how spiking neurons may perform ICA-like learning. Future work will address various ways through which the output of different neurons can be decorrelated
to allow efﬁcient coding at the population level.
Acknowledgments
This work is supported by the Hertie foundation and EC MEXT-project PLICON.
References
[1] The other side of the engram: experience-driven changes in neuronal intrinsic excitability. W. Zhang and
D. Linden, Nature Reviews Neuroscience 4:885-900, 2003.
[2] How voltage-dependent conductances can adapt to maximize the information encoded by neuronal ﬁring
rate. M. Stemmler and C. Koch, Nature Neuroscience 2(6):521-527, 1999.
[3] Synergies between intrinsic and synaptic plasticity. J. Triesch, Neural Computation 19:885-909, 2007.
[4] Forming sparse representations by local anti-Hebbian learning. P. F öldiák, Biological Cybernetics
64:165-170, 1990.

177

Friday evening, Poster session II-59

Cosyne 2008

Modularization through the prism of shaping
Kai A Krueger and Peter Dayan
Gatsby Computational Neuroscience Unit, UCL
The only possible solution to the problem of acquiring temporally and structurally rich and complex cognitive capabilities is modularization, or divide and conquer. We, and indeed other species, are able to learn
simple elements and recombine them in multifarious ways in order to address sophisticated challenges.
However, despite some important suggestions[3], most computational models have focused on uniform,
rather than modularized learning, that is, taking on the full complexity of tasks starting from a naive state.
Not only does this make learning of a single problem more difﬁcult, thus encouraging the development
of architectural artiﬁce, but it also fails to generate sub-solutions that can be used to solve future problems. Evidence for the nature and importance of modularity comes from the behavioral procedures used for
training subjects to solve complex tasks. Subjects are shaped, ie are led step-by-step to acquire elemental
sub-components before being presented with full tasks.
Previously, we used a computational model to elucidate shaping’s substantial beneﬁcial effects on learning.
We demonstrated this in a hierarchical, conditional one-back memory-based cognitive task called 12-AX [4],
which we continue to employ here. However, in that study, we solved by hand one of the critical problems
in making shaping work, namely the resource allocation mechanism that creates new network resources for
each stage of shaping. This allowed our network to represent all the sub components of the task without
interference. Here, we explore mechanisms that replace this homunculus with more principled algorithmic
methods.
The basic approach has been clear since one of the earliest inventions in the probabilistic era of neural networks, namely mixture models[2] . These involve multiple modules, and allocate learning to each module
for a particular input according to the responsibility that it is estimated to have for any error associated with
that input. Different sorts of modules, and different ways of estimating responsibilities, lead to different architectures. We extend our earlier model for cognitive shaping into a mixture model, adapting the MOSAIC
framework of Haruno et al. [1] for the responsibility estimation. MOSAIC uses a form of learned forward
model for this; the sequential nature of tasks such as 12-AX, which are not Markovian in the input, impose
extra demands on the forward model. We investigate solutions to this problem, and compare the ability of
such an extended network to beneﬁt from shaping, to that with the original, homuncular, resource allocation.
Acknowledgments
Support from the Gatsby Charitable Foundation.

References
[1] M. Haruno, D. M. Wolpert, and M. Kawato. MOSAIC Model for Sensorimotor Learning and Control. Neural
Comp., 13(10):2201–2220, 2001.
[2] R. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural Computation,
3(1):79–87, 1991.
[3] R. A. Jacobs. Computational studies of the development of functionally specialized neural modules. Trends in
Cognitive Sciences, 3(1):31–38, 1999.
[4] R. C. O’Reilly and M. J. Frank. Making Working Memory Work: A Computational Model of Learning in the
Prefrontal Cortex and Basal Ganglia. Neural Computation, 18(2):283–328, 2005.

178

Cosyne 2008

Friday evening, Poster session II-60

What efﬁcient code for adaptive spiking representations?
A model for the formation of simple cell receptive ﬁelds.
Laurent U. Perrinet
INCM-CNRS / University of Provence, 13402 Marseille Cedex 20, France
Taking advantage of the constraints of spiking representations, we derive an unsupervised learning algorithm solving efﬁciently pattern matching in a model of the input to the primary visual cortex. In fact, spikes
carry temporal event-based information in bundles of parallel ﬁbers and may be considered as distributed
all-or-none binary events. This property may be used to formulate the efﬁciency of a representation problem
as ﬁnding the L0 -norm sparsest representation, a “hard” NP-complete problem. This framework improves
previous results based on an Adaptive Matching Pursuit scheme by explicitly implementing an homeostatic
constraint in the choice function by a spiking gain control mechanism in the neural population. For comparison purposes, we applied this scheme to the learning of small images taken from natural images as in
S PARSE N ET [1] and compare the results and efﬁciency of both algorithms. This study provides a generic
application of Sparse Spike Coding to learning independent components for a model of the input to the
primary Visual Cortex and shows an advantage of using spiking representations compared to a rate-based
one [1] and which may explain its ubiquity in the central nervous system.
0.07
cg X2
ssc

Residual Energy (L2 norm)

0.06
0.05
0.04
0.03
0.02
0.01
0

0

0.2

0.4
0.6
0.8
Sparseness (L0−norm)

1

Figure 1: Results of the proposed SHL scheme compared to S PARSE N ET. Starting with random ﬁlters, we compare here the results of the learning schemes with 324 ﬁlters at convergence (20000 steps)
using (Left) the classical conjugate gradient function method as is used in S PARSE N ET [1] with (Middle) the Sparse Spike Coding method. Filters of the same size as the imagelets (16 × 16) are presented
in a matrix (separated with a black border). Results replicate the original results of S PARSE N ET and
are similar for both dictionary which consist of gabor-like ﬁlters similar to the receptive ﬁelds of simple cells in the primary visual cortex. But which set of ﬁlters represents the world most efﬁciently?
This is quantiﬁed by (Right) plotting the mean ﬁnal residual error (L2 norm) as a function of the L0 norm sparseness of the coefﬁcients. Get the code to reproduce ﬁgures and supplementary material on
http://incm.cnrs-mrs.fr/LaurentPerrinet/SparseHebbianLearning

References
[1] Learning a sparse code for natural images produces a multiscale family of localized receptive ﬁelds.
Bruno A. Olshausen and David J. Field Nature, 381:6̃07–9, 1996.

179

Friday evening, Poster session II-61

Cosyne 2008

Experience-dependent Plasticity of Cortical Organotypic Networks
in Response to Days of Patterned Stimulation
Hope A. Johnson1 and Dean V. Buonomano1,2
Departments of Neurobiology1 and Psychology2, University of California, Los
Angeles
Synaptic plasticity is traditionally studied at the level of single synapses on the time-scale of minutes and
hours. However, many forms of learning ultimately rely on changes in network function which occur over
days. Indeed days of altered sensory input are often necessary to induce experience-dependent
plasticity1,2,3. In order to study these long-term forms of plasticity in a reduced system, and study the
learning rules involved, we developed a method to administer patterned stimuli to cortical organotypic
networks over the course of days. Here we examined how chronic patterned stimulation affects synaptic
strength and network dynamics. We also explore whether an in vitro network can reorganize its response
to reflect the patterns of sensory stimuli to which they are exposed. Specifically, we asked whether the
network has the capacity to learn a stimulus pattern or time intervals.
First, we examined the predictions of cooperation/competition models of cortical plasticity, where
coactive inputs are cooperatively potentiated while uncorrelated inputs compete in the ability to drive a
neuron4,5. Two implanted electrodes were activated in-phase or out-of-phase. Subsequent whole cellrecordings did not reveal any bias in the form of pathway selectivity, indicating that the predictions of
BCM were not observed in this preparation. However, we find that sequential stimulation of the two
input pathways with a short delay interval (50 or 100 ms) produces synaptic and network plasticity as
measured by differences in the initial EPSP slope and the integrated EPSP area. The response is modified
so that the pathway activated last becomes dominant, which is opposite of what would be predicted by
STDP rules. Additionally, the most robust change is in the polysynaptic response, suggesting that we are
observing network reorganization in response to patterned stimulation. Mechanistically, the observed
change could be explained by decreased strength of excitatory to inhibitory synapses in response to the
second stimulation pathway; however, many mechanisms are likely operating in concert, including local
changes in recurrent excitatory connections. Regarding the functional implications of the observed
plasticity, the dominance of the response to the second input suggests that the sequence may be treated as
a unitary spatial-temporal stimulus, and that the most salient feature of the stimulus pattern is the end.
In order to understand whether the networks were adapting to the patterned stimuli, we analyzed the
timing of larger amplitude polysynaptic events after chronic stimulation. We found that exposure to the
in-phase or sequential stimulation patterns produces different temporal profiles of activity, thus
suggesting that networks are shaped by the specific temporal structure of the stimuli to which they are
exposed.
1. Sawtell NB, Frenkel MY, Philbot BD, Nakazawa K, Tonegawa S, & Bear MF. (2003) Neuron 38,
977-85.
2. Wang X, Merzenich MM, Sameshima K, & Jenkins WM. (1995) Nature 378, 71-5.
3. Buonomano DV & Merzenich MM (1998) Ann. Rev. Neurosci. 21, 149-86.
4. Bienenstock EL, Cooper LN & Munro PW. (1982) J. Neurosci. 2, 32-48.
5. Miller KD. (1996) Neuron 17, 371-7.

180

Cosyne 2008

Friday evening, Poster session II-62

A Bayesian framework for explaining the dynamics of sensory
pattern adaptation
Beau Cronin1, Mriganka Sur1 and Konrad Kording2
1

Massachusetts Institute of Technology,

2

University of Chicago

Sensory adaptation effects such as the waterfall illusion and the tilt aftereffect are perceptually repulsive
in nature (after watching a waterfall, the rest of the world seems to be moving upward). Recent studies
have shown that the responses of individual neurons in primary visual cortex undergo corresponding
repulsive adaptation in such situations and that this adaptation exhibits surprisingly rich temporal
dynamics[1, 2].
Several experiments indicate that the excitability of neurons fluctuates. Such changes in presynaptic
neurons would bias the activity of a postsynaptic neuron. Here we derive how a postsynaptic neuron
could correct for changes in presynaptic neurons by constantly estimating their excitability. We associate
with each synaptic input a time-varying gain whose current value is estimated from its previous value, in
combination with prior information about the distribution of the neural drives (which are assumed to be
sparse).
We show that this Bayesian model,
when embedded in a simple network
model of orientation tuning, is able to
reproduce the repulsive tuning shifts
observed experimentally (see figure), as
well as several of the subtle aspects of
the dynamics of these changes.
Previous models have shown that adapting neurons transmit more information (e.g. [3]); however, one
may argue that the repulsive biases in resulting perception are more harmful than the gain in information.
Other models have proposed a role of adaptation in homeostasis (e.g. [4]), and indeed our model can be
understood as a statistically optimal way of keeping computations stable with a changing brain.
Acknowledgments
This work was supported by NIH grant EY07023.
References
[1] Adaptation-induced plasticity of orientation tuning in adult visual cortex. V. Dragoi, J. Sharma and
M. Sur, J Neurophysiol 28:287-298, 2000.
[2] Adaptation in single units in visual cortex: The tuning of aftereffects in the spatial domain. A. Saul
and M. Cynader, Vis Neurosci 2:593-607, 1989
[3] Visual adaptation as optimal information transmission. M. Wainwright, Vis Res 39:3960-3974, 1999
[4] A functional angle on some after-effects in cortical vision. C. Clifford, P. Wenderoth and B. Spehar,
Proc Biol Sci. B. 267(1454):1705-1710, 2000

181

Friday evening, Poster session II-63

Cosyne 2008

The contribution of spontaneous activity to plasticity can explain
pre-critical and critical period plasticity of ocular dominance.
Taro Toyoizumi1,2 and Kenneth D. Miller1
1
2

Department of Neuroscience, Columbia University
RIKEN, Brain Science Institute

In mice, maturation of cortical inhibitory circuitry initiates the critical period (CP) for ocular dominance
(OD) plasticity in response to monocular deprivation (MD). A certain threshold of inhibition is required for
the initiation of the CP [1]. However, the mechanism by which maturation of inhibition initiates the CP is not
known. This maturation of inhibition is not simply a switch that turns on OD plasticity. An imaging study of
mice revealed that the retinotopic precision of cortical receptive ﬁelds is constantly being reﬁned by activity
dependent plasticity during the pre-CP [2]. MD of the stronger, contralateral eye retards the reﬁnements of
both eyes’ retinotopy without changing the relative strength of the two eyes’ responses [2]. That is, before
the CP, MD does not cause an OD shift, but it does affect activity-dependent reﬁnement of retinotopy.
We study how the maturation of inhibition initiates the CP of OD plasticity, and how there can be activitydependent plasticity without OD plasticity before the maturation of inhibition. More speciﬁcally, we study
the hypothesis that the relative contribution of spontaneous activity and visually evoked activity on synaptic
plasticity changes at the onset of the critical period due to maturation of inhibition and a nonlinear inputoutput function. Before the critical period, frequent but weak spontaneous input can drive neurons and thus
modulate synapses, but maturation of cortical inhibition prevents spontaneous input from driving neurons.
To investigate the effect of a reduced amount of spontaneous activity, activity dependent synaptic plasticity
is modeled as a phenomenological learning rule for synaptic weights that has the Hebbian property as well
as a homeostatic property. While Hebbian learning can extract a correlation structure in input, homeostatic
plasticity regulates the overall activity level and stabilizes the learning.
During the pre-CP the receptive ﬁelds of neurons are retinotopically reﬁned and strengthened through
activity-dependent plasticity, partially because of weaker but frequent drive from spontaneous activity in
the two eyes and partially because of stronger but occasional drive from visual activity in the eyes. The
contribution of frequent spontaneous activity allows partial retinotopic reﬁnement and strengthening under
MD of the contralateral eye, but the reﬁnement is weaker due to the abnormal visual activity of the deprived
eye. The reason that MD does not change the balance between open and closed eyes is that the responses
to input from each eye are roughly balanced because of the frequent spontaneous input. The maturation
of inhibition at the onset of the CP reduces spontaneous activity, so that cortex becomes mainly driven by
visually-evoked activity from the eyes. Application of MD will then cause an OD shift to the open eye
because, under MD, there is a large imbalance in the input from two eyes. Hence, if maturation of inhibition
suppresses the effect of weak spontaneous input, this can explain (1) plasticity during the pre-CP: there is
activity-dependent reﬁnement of receptive ﬁelds, and MD weakens reﬁnement but does not cause an OD
shift; and (2) the initiation of the CP – the period in which MD causes an OD shift – by the maturation of
inhibition.
Acknowledgments
This work was supported by the JSPS and a Grant-in-Aid No. 1806772, and NIH grant R01 EY11001.
References
[1] T. K. Hensch, Nat Rev Neurosci 6, 877–888 (2005).
[2] S. L. Smith and J. T. Trachtenberg, Nat Neurosci 10, 370–375 (2007).

182

Cosyne 2008

Friday evening, Poster session II-64

Decision making: Neural prediction errors show risk sensitivity
Yael Niv1 , Peter Dayan2 , and John P. O’Doherty3
Department, Princeton University, 2 Gatsby Computational Neursocience Unit, UCL
of Humanities & Social Sciences and Computation & Neural Systems Program, Caltech

1 Psychology
3 Division

Decision making is inﬂuenced not only by the expected reward value of options, but also by their variance
or risks. Traditional reinforcement learning models of action selection, however, use temporal difference
(TD) methods to learn the mean value of an option, ignoring risk. These models have been strongly linked
to learning via prediction errors conveyed by dopaminergic neurons. Although variance clearly inﬂuences
choice behavior, risk sensitivity does not have to be manifest at the level of TD learning: the dopaminergic
system may be learning expected values, while risk is monitored in other systems, with both contributing
to the ﬁnal decision. Here, we used fMRI in a human decision-making task, in order to determine whether
correlates of risk sensitivity can be seen in neural prediction error signals themselves. Our results reveal that
choice behavior is better accounted for by incorporating risk-sensitivity into TD learning, and, furthermore,
that the BOLD correlates of prediction error learning in the brain indeed reﬂect subjective risk-sensitivity.
Humans performed a choice task in which three options were rewarded with 0, 20 and 40 cents, respectively,
and one option was rewarded with 0 or 40 cents with equal probabilities. Some trials involved choosing
between the ‘sure’ 20 cents option and the ‘risky’ 0/40 cents option. Preference for the ‘sure’ option was
used to quantify each subject’s degree of risk-aversion. Although the traditional TD model can explain risk
sensitivity as a result of choice-induced sampling biases, model comparison showed that a recently proposed
risk-sensitive TD model predicted subjects’ behavioral choices signiﬁcantly better.


!










	


















b




a

	







 






"#
"#













c



"#!

A critical difference between the models is that the traditional TD model predicts that the ‘risky’ option will,
on average, have the same value as the ‘sure’ option, while risk-sensitive TD penalizes variance, making the
value of the ‘risky’ option lower than that of the ‘sure’ option for risk-averse subjects, and vice versa for
risk-seeking subjects. To differentiate between the models on the level of neural TD learning, we therefore
assessed the correlation between behavioral risk-preference and the valuation of the ‘sure’ and ‘risky’ options derived from the BOLD signal in anatomically deﬁned regions of interest in the left and right nucleus
accumbens (NAC) of each subject. First we conﬁrmed, for the ﬁrst time using a model-free non-exploratory
analysis, that both right and left NAC BOLD signals correspond to a prediction error signal (Figure 1a,b),
and, at least in our task, do not encode for other quantities of interest (such as reward magnitude). We then
used the BOLD prediction error signal at trial onsets to derive the subjective mean value of each option. A
regression of the behavioral risk preferences and the subject-by-subject difference between the values of the
‘safe’ option and the ‘risky’ option showed a signiﬁcant positive correlation in both the left and the right
NAC (Figure 1c). This suggests that risk-sensitivity can be seen at the level of TD learning in the brain.




%&'(
)&'(




 



$%"



Acknowledgments. We are indebted to Jeffrey Edlund for acquiring the imaging data, and to Laura deSouza
for marking the anatomical ROIs. Supported by a Human Frontiers Science Program fellowship to YN.

183

Friday evening, Poster session II-65

Cosyne 2008

Why does sensitivity to reward devaluation disappear over learning?
A single system Bayesian account
Trent Toulouse and Suzanna Becker
Department of Psychology, Neuroscience and Behaviour, McMaster University
Evidence suggests that a change in the firing rate of dopamine cells (DA) is a major neurobiological
correlate of learning. The Temporal-Difference (TD) model views the DA signal as conveying the error
between expected and actual rewards. Learning occurs by updating a cached value using the TD error
signal. Another approach has been to learn an explicit world model including states, transitions and
rewards. Action choice is made by looking ahead to future rewards rather than relying on cached values
one step ahead. Model-based approaches often view the DA signal as either a reward-prediction error or
as a signal of salience and surprise. These two approaches make different predictions when the perceived
value of a reward is changed after training, for example, by exposing a satiated animal to the same food
pellet it was trained on while hungry. Model-based approaches predict an immediate change in action
choice policy but cached value systems require that actions be experienced with the devalued reward
before updating policy. Both results have been found, with moderately trained rats being sensitive to
devaluation while extensively trained rats are insensitive to devaluation [1].
Recent attempts to explain the devaluation data propose a transition from a model-based system to a
cached value system over the course of learning [1]. Our proposed model instead uses a single system,
combining a model-based approach with an episodic memory system for caching recently experienced
anomalous events. Bayesian learning is used to estimate specific probabilities of reward values, including
a confidence interval for the accuracy of the predicted reward. If the perceived value of a reward falls in
this confidence interval any new experience with the reward will update global expectations of that
reward. Early in training the confidence interval is wide so almost any exposure to the reward in any
context will update the globally estimated value of that reward. Early sensitivity to devaluation occurs
because the perception of the reward while satiated is still within the confidence interval of the expected
reward. Change in the perceived value of the reward outside the confidence interval signals that the
information is anomalous and should not be used to update the global reward value. Instead, a specific
episodic memory is created with the context of when the reward value changed and any learning that
takes place will only be applied within that context. Thus, over training sessions the confidence interval
narrows leading to devaluation insensitivity.
In our model DA signals how far an experienced reward deviates from expectation. Rather than signaling
TD error, the change in firing rates is calculated based on the predicted probability of the unexpected
reward. If vi is the log probability of the reward and c is a gain factor then the change in firing rate is fi =
c · vi up to some maximum firing rate. This approach also explains recent DA cell recording data that is
not explainable as a TD error, for example under some conditions a 100 fold increase in reward value
produces the same increase in firing rates as only a 10 fold increase in reward value [2].

References
[1] Daw, N., Niv, Y., & Dayan, P. (2005). Uncertainty-based competition between pre-frontal and dorsolateral striatal
systems for behavioral control. Nature Neuroscience, 8(12), 17041711.
[2] Tobler, P., Fiorillo, C., & Schultz, W. (2005). Adaptive Coding of Reward Value by Dopamine Neurons.

184

Cosyne 2008

Friday evening, Poster session II-66

Where in the brain are sounds timed? Studying time perception
using an auditory approach
S. S. Pai1 and C. D. Brody2
1
Watson School of Biological Sciences, Cold Spring Harbor Laboratory
2
Princeton Neuroscience Institute and Dept. of Molecular Biology, Princeton
University
To study vision, one starts from the eyes. To study audition, one would start from the ears. But where in
the brain would one start looking for the sense of passing time? To guide a neuroanatomical search for
timing centers in the brain, we are using a sensory approach: we are focusing on the timing of the duration
of an auditory stimulus. We therefore search for timing centers throughout auditory pathways in the
central nervous system. We have trained two groups of rats in separate two- alternative forced-choice
tasks that serve as controls for each other. The first group is trained to categorize pure tones based on their
duration (range: 200 to 500 ms, category boundary fixed at 316 ms, typical Weber ratio ~0.3). The second
group categorizes pure tones based on their frequency (range: 8KHz to 16 KHz, boundary at 11.3 KHz,
typical Weber ratio ~0.25 octaves). Both tasks involve the same stimuli, motor acts, and rewards. To best
assay performance, rats in both groups were trained to psychophysical threshold, and a psychometric
curve was obtained for each rat. We then removed neurons from candidate brain regions via stereotactic
injections of an excitotoxin. If lesioning a brain region impairs frequency but not duration discrimination,
we can rule out that brain region as unnecessary for duration discrimination. If lesioning a brain region
impairs duration but not frequency discrimination, then that region is specifically necessary for timing.
The initial candidate areas we have selected for lesions are the extended auditory cortex (A1, AuD, AuV-collectively, ACx), the auditory thalamus (MGv, MGd, MGm), and the dorsomedial prefrontal cortex
(dmPFC). Our pilot studies suggest that, after recovery, ACx lesions do not impair the ability to time
sound, but do impair the ability to discriminate frequency (Fig. 1). In contrast, dmPFC lesions initially
impair the ability to discriminate duration, but do not impair frequency discrimination (Fig. 2- very
preliminary data). This suggests that in the 100s of milliseconds range, dmPFC may play a particularly
important role in duration perception, but ACx does not.We are currently confirming these results and
conducting lesion studies in the auditory thalamus. Once lesion studies confirm the specific involvement
of a candidate area in timing, we will use electrophysiology and pharmacological manipulation to
determine the role of the candidate area in timing sound (e.g., storing the memory of a duration vs. being
the “timer” for the current stimulus, etc.). Thus, by using a sensory approach we will begin to uncover the
biology behind a measure used in daily activities by invertebrates and vertebrates alike.

(a,b). Lesions of auditory cortex impair frequency but not duration discrimination. (c) Lesions of
dmPFC impair duration but not frequency discrimination. Graph shows average squared error
difference between pre-lesion and postlesion psychometric curves.

185

Friday evening, Poster session II-67

Cosyne 2008

Strategy for matching behavior leads to the optimal behavior
Yutaka Sakai1 and Tomoki Fukai2
1

Tamagawa University, Japan,

2

RIKEN BSI, Japan

About half century has passed since Herrnstein (1961) found an empirical law, so-called ”matching law”, in
pigeons’ choice behavior. The matching law states that the frequency of choosing an option is proportional
to the amount of the past reward obtained by the choice,
Na
Ia
=
,

a N a
a Ia



where Na is the number of times of choosing option a in a sufﬁciently long time, and Ia is the cumulative
income obtained by choosing option a. A number of experiments have revealed that humans and other
animals exhibit ”matching law” in a wide range of decision-making tasks. Interestingly, such a behaviour
is observed even when it does not maximize obtainable reward. Recently, Loewenstein and Seung (2006)
proposed a general class of synaptic learning rules to achieve the matching behavior. However, the most
essential question remains to be clariﬁed, that is, why animals exhibit matching behavior rather than maximizing behavior.
In this paper, we propose ”matching strategy” to explain why animals show matching behavior so robustly
based on a general computational framework of decision behavior. We assume that a set of decision synapses
exist, and that the choice probabilities are determined by the synaptic weight w = (w1 , w2 , · · · ),
Prob(chosing option a) = pa (w).
We prove, without assuming any speciﬁc learning rule, that the matching law appears whenever the brain’s
decision system tries to maximize return under the naive assumption that past choice behaviors do not affect
the expected values of future returns. In addition, we show that the matching strategy achieves maximization
when the decision is based on information sufﬁcient to determine the possible changes in the expected return.
Therefore, only if the subject can select the appropriate information source, matching strategy achieves the
optimal behavior. In other words, the matching strategy is a different approach to the optimal behavior
when the subject explores the appropriate information source. Our results provide a novel insight into the
relationship between maximizing and matching, a long standing issue in the studies of decision making,
from viewpoints of neural computation.
Acknowledgments
This work was partially supported by the Grant in Aid for Priority Researches, no.17022036 and
no.17021038.
References
[1] R.J. Herrnstein, The Matching Law, Harvard Univ. Press, Cambridge, MA, 1997.
[2] Y. Loewenstein & H.S. Seung, Proc. Natl. Acad. Sci. USA 103: 15224-15229, 2006.
[3] Y. Sakai, H. Okamoto & T. Fukai, Neural Networks 19: 1091–1105, 2006.
[4] Y. Sakai & T. Fukai, Neural Computation 20(1), 2008.

186

Cosyne 2008

Friday evening, Poster session II-68

Biologically Plausible Model Learning for Neural Bayesian
Inference
Lars E. Holzman, Hava T. Siegelmann, and Oscar Loureiro
Department of Computer Science, U. Mass. Amherst.
There is increasing neurophysical evidence that the brain performs Bayesian inference during
decision making. During Bayesian inference, evidence from diverse sources is integrated
optimally to enable decision making considering unseen properties of the world. To perform
Bayesian inference it is necessary to know, to a high degree of precision, probability distributions
describing variables about the world, such as size or distance, as well as how the variables affect
each other. To maintain a good internal model of the world it is necessary to have a continuously
updated model that combines new evidence with previous models. Without a model that is
continuously updated Bayesian inference would be inaccurate and far from optimal. Little work,
however, has been done to explain how the probability distributions describing the different
variables and their relationships are updated over time in a neural setting. In this paper we
demonstrate how to continuously learn probabilities using a biologically plausible generalized
Hebbian learning update. This update will cause the strength of synapses in the neural circuit to
optimally learn the probabilities that describe the variables and their relations. The synaptic
learning is equivalent to a statistically optimal learning scheme derived from Bayesian updating.
A simple neural Bayesian inference scheme is described which, using the same neural circuit as
learning, allows the quick estimation of the posterior probabilities of the variables. Finally,
learning and inference are demonstrated on the cue integration task where the reliability of
individual sensory cues is varied, and it is shown that the system has the ability to learn the
relevant distributions and apply them, yielding Bayesian optimal performance on the task.

187

Friday evening, Poster session II-69

Cosyne 2008

Testing the bottom-up explanation of decision-related activity in
visual neurons.
Hendrikje Nienborg1 and Bruce G. Cumming1
1

Laboratory of Sensorimotor Research, National Eye Institute, NIH

Previous studies have shown that during perceptual tasks, the activity of individual sensory neurons
correlates with a subject’s perceptual decision. The origin of this correlation is unknown. Current
quantitative models explain this as the causal effect of noise in sensory neurons on perceptual decisions
(bottom-up). But it could also result from different brain-states associated with choice (top-down). The
two schemes have markedly different implications for the role played by sensory neurons in forming
decisions. Here, we tested the current bottom-up view.
Two macaques performed a disparity discrimination task while we simultaneously recorded single unit
activity from disparity selective neurons in their V2. We used white-noise analysis to quantify
simultaneously a) the choice-related activity, b) the neuronal tuning-functions (“disparity subspace
maps”) associated with each choice and c) how the animals used the disparities in noisy stimuli (their
“psychophysical kernel”). Two characteristics of our results suggest a top-down component contributing
to the choice-related activity. First, we find that the amplitude of the monkeys’ psychophysical kernels
decreased over the course of each trial. Trial duration was always fixed at 2seconds. The bottom-up
framework predicts that the time-course of the choice related signal parallels the time-course of the
amplitude of the psychophysical kernel. Contrary to this prediction, our data show that the choice-related
activity increases with time, and stays at asymptotic values after about 500ms until the end of the trial.
Second, our data show a choice-dependent gain in the neuronal tuning functions which we cannot
reconcile with bottom-up models.
The monkeys’ psychophysical behavior can be fit in the bottom-up framework, when assuming a twostage pooling model similar to [1, 2]. The first stage consists of two pools of neurons, each supporting one
of the alternative decisions. In the second stage, a decision variable integrates the difference of the signals
from each pool over time. A decision is cast once this decision variable hits an upper or lower bound. In
order to also explain our physiological data, we expand this bottom-up model to incorporate two topdown components. One is a change in neuronal response gain which is associated with a response bias
and changes from trial to trial. This may be similar to feature selective attention. The second is decisionrelated feed-back which starts after the decision-bound has been reached and which increases the neuronal
gain of the neurons of the pool supporting the respective decision.

References

1.
2.

Kiani, R. The Read Out of Visual Cortex Embraces a Termination Rule. Abstract. Society
for Neuroscience Meeting. 2007. San Diego.
Mazurek, M.E., et al., A role for neural integrators in perceptual decision making. Cereb
Cortex, 2003. 13(11): p. 1257-69.

188

Cosyne 2008

Friday evening, Poster session II-70

Risk-taking as exploration in a fast-changing world
Erick Chastain1 , Pradeep Shenoy1 , and Rajesh P. N. Rao1
1

University of Washington

In order to survive in an uncertain world, animals have to continually learn about the outcomes of various
actions, and make decisions based on this knowledge in order to maximize reward. This leads to the classic
exploration/exploitation dilemma where the information gained by choosing previously untried actions is
traded off against the reward gained by taking a known action. Closely related to this tradeoff is the consistent preference or aversion shown by certain animals towards actions of uncertain outcome in a variety of
experiments. For example, in a two-choice experiment where a monkey is repeatedly offered either a variable reward or a constant reward that add up on average to the same quantity, the monkey may repeatedly
show a preference for the variable outcome. Here we describe a reinforcement-learning-based explanation
for risk-preference in animals. Our main hypothesis is that animals expect the world to be nonstationary,
and are therefore continually trying to learn about and keep track of the outcomes of actions. Reinforcement
learning theory is a formal framework that balances explore/exploit actions in an optimal manner, and has
been used to explain animal behavior in a variety of contexts. By modifying this optimal recipe to accomodate the nonstationary-world hypothesis, our model can explain risk-preference shown by monkeys in
recent experiments, including the changes in risk-preference with increased variability of rewards, and with
increase in waiting times between actions. Previous accounts of risky behavior have suggested that individuals subjectively revalue the magnitudes of rewards. We discuss the speciﬁc differences in predictions made
by this account of risk-preference and our learning-based model of the same data. In our proposed framework, risky behavior can be understood as an animal’s attempt to address model uncertainty in a (presumed)
changing world. This perspective suggests a re-evaluation of current models of animal behavior since these
models assume a stationary world.
0.75

Probability of risky choice

0.7
0.65
0.6
0.55
0.5
0.45
0.4
0

20

40
60
ITI (seconds)

80

100

Figure 1: The ﬁgure shows experimental data (red circles) and model predictions (blue circles) of the probability of risky action as the intertrial interval is varied. Experimental data is from Hayden & Platt 2007.
In the experiment, a monkey repeatedly made a choice between a risky (high variance) outcome and a safe
outcome, and the intertrial intervals were varied throughout the experiment. The data indicates that the
monkey was less likely to choose a risky option after longer intervals. The ﬁt is statistically signiﬁcant with
p < .001. Also shown (red and blue lines) are polynomial ﬁts to the two sets of points.
Acknowledgments
We thank Benjamin Hayden and Michael Platt for experimental data and helpful discussions.

189

Friday evening, Poster session II-71

Cosyne 2008

A Model of Self-Consistent Perception
Alan A. Stocker and Eero P. Simoncelli
Center for Neural Science, New York University, and Howard Hughes Medical
Institute

decision A

decision B

bias

vstim

vperceived

perceived direction [deg]

Human perception is context-dependent. In addition to sensory context, two recent psychophysical studies
have shown that context can also include previous perceptual decisions [1, 2]. In both studies, subjects were
asked to estimate a stimulus parameter (orientation [1], or direction of motion [2]) after being forced to make
a categorical decision (orientation to the left or right of vertical [1], direction of motion to left or right of a
visual reference [2]). On each individual trial, the subjects’ estimates were consistent with their preceding
decision (i.e., a decision of ”left of the reference” was followed by an estimated direction to the left of
the reference). The distribution of estimates were bimodal, indicating repulsion away from the decision
boundary (middle panel - data from [2] for a single subject).

model

data
40
20
0
-20
-40

-20

-10

0

10

20

-20 -10

0

10

20

stimulus direction [deg]

We present an observer model that can account for this perceptual behavior. Speciﬁcally, we adopt the
general hypothesis that the brain attempts to perform optimal estimation of stimulus parameters based on
noisy sensory evidence and prior expectations. However, we augment this hypothesis by assuming that the
brain performs the secondary estimation task in the belief that its previous decision regarding the data was
correct. Noisy sensory evidence may initially support both decisions, although with different probability
(left panel: a portion of the likelihood falls under each decision region). However, after being forced to
make a decision (e.g. decision B), the observer discards all potential estimates that are not in agreement
with the choice (thus all values in the gray shaded area), performing inference conditioned only on the
decision made. This leads to the observed repulsive bias away from the decision boundary (right panel).
It is worth noting that the behavior of the model (and the human subjects) is suboptimal in terms of estimation performance. An optimal (Bayesian) observer model [3] would compute estimates from the sensory
evidence under each possible decision value, and then average these estimates, weighting each according
to the probability that the corresponding decision is correct. Thus, our model implies that humans sacriﬁce
performance in order to maintain self-consistency.
References
[1] S. Baldassi, N. Megna, D.C. Burr. Visual clutter causes high-magnitude errors. PLoS, 4(3):e56, 2006.
[2] M. Jazayeri and J.A. Movshon. A new perceptual illusion reveals mechanisms of sensory decoding.
Nature, 446:912ff, April 2007.
[3] D.C. Knill and W. Richards, eds. Perception as Bayesian Inference. Cambridge University Press, 1996.

190

Cosyne 2008

Friday evening, Poster session II-72

On Natural Statistics in Multisensory Integration
Terry Elliott, Xutao Kuang, Nigel R. Shadbolt and Klaus-Peter Zauner
School of Electronics and Computer Science, University of Southampton, UK
Multisensory neurons in the deep layers of the superior colliculus (DSC) integrate different sensory modalities, responding more strongly to spatially congruent inputs from distinct modalities than to input from
a single modality (cross-modal enhancement, or CME) [1]. Multiple stimuli presented within a unimodal
receptive ﬁeld, however, evoke a weaker response than a single stimulus presented in the same receptive
ﬁeld (modality-speciﬁc suppression, or MSS) [2]. Although CME and MSS have been widely studied experimentally, the mechanisms underlying these responses remain unclear. As sensory neurons, multimodal
DSC neurons may employ the same adaptive processes as unimodal neurons earlier in sensory pathways by
adapting to the statistical properties of their inputs (cf. [3]) through processes such as gain control and ﬁring
threshold adjustment. Here we report, from a theoretical perspective, on the role that input statistics may
play in multisensory neurons.
We have analysed a representative model of multisensory integration that focuses particularly on the role of
input statistics [4]. This model generates CME and MSS for multivariate Gaussian inputs in a parameter
regime in which all modalities have identical variance σ02 in their ﬁring rates when ﬁring spontaneously.
If the covariance between spontaneously ﬁring inputs is close to σ02 , then the model exhibits MSS in the
presence of evoked, stimulus-driven ﬁring. The model thus exploits a parameter regime in which the spontaneous activity covariance matrix is close to singular. As a result, the model’s behaviour is exquisitely
sensitive to the choices of statistical parameters and the intensities of cross-modal inputs.
It is known that a thresholded, saturating response function is entirely sufﬁcient to generate CME and the
associated phenomenon of inverse effectiveness. We propose that DSC neurons adapt their response functions according to their input statistics through gain control and threshold adjustment. We approach this by
ﬁrst deriving an adaptation rule for unimodal sensory neurons, and then extending the rule to multimodal
neurons. The resulting model has the virtues of simplicity and ease of neuronal implementation, and it may
shed light on the underlying information processing principles of sensory neurons. Moreover, our model
produces robust, testable predictions about the impact of changes in input statistics on CME in DSC neurons. The model could therefore be invalidated, in contrast to some other models that fail to make robust
predictions.
References
[1] Visual, auditory, and somatosensory convergence on cells in superior colliculus results in multisensory
integration. M. A. Meredith and B. E. Stein, J. Neurophysiol. 56:640-662, 1986.
[2] Mechanisms of within- and cross-modality suppression in the superior colliculus. D. C. Kadunce, J. W.
Vaughan, M. T. Wallace, G. Benedek, and B. E. Stein, J. Neurophysiol. 78:2834–2847, 1997.
[3] Natural image statistics and neural representation. E. P. Simoncelli and B. A. Olshausen, Annu. Rev.
Neurosci. 24:1193–1216, 2001.
[4] Modeling cross-modal enhancement and modality-speciﬁc suppression in multisensory neurons. P. E.
Patton and T. J. Anastasio, Neural Comput. 15:783–810, 2003.

191

Friday evening, Poster session II-73

Cosyne 2008

State-space methods for inferring synaptic inputs and weights
Liam Paninski and Daniel Ferreira
Columbia University
Advances in intracellular recording and imaging methods have opened the door to increasingly detailed
investigations of synaptic computations in vitro and in vivo. In this work we introduce several new methods
based on “state-space” statistical methods for efﬁciently extracting meaningful biophysical information from
postsynaptic voltage data.
First, we discuss methods for optimally inferring the synaptic inputs to an electrotonically compact neuron,
given intracellular voltage-clamp or current-clamp recordings from the postsynaptic cell. These methods
are based on sequential Monte Carlo techniques (“particle ﬁltering”). We demonstrate on model data that
these methods can accurately recover the time course of excitatory and inhibitory synaptic inputs on a
single trial; no averaging over multiple trials is necessary. Once these synaptic input time courses are
recovered, it becomes possible to ﬁt (via tractable convex optimization techniques) simple models describing
the relationship between the sensory stimulus and the observed synaptic input. We develop an expectationmaximization (EM) algorithm that consists of alternating iterations between these synaptic recovery and
model estimation steps. These proposed methods have direct and immediate applications to understanding
the balance between excitation and inhibition in sensory processing in vivo.
Second, we describe efﬁcient methods for mapping the location and strength of a synaptic input on a dendritic tree, given the presynaptic spike times and (possibly noisy and incomplete) observations of the voltage on the postsynaptic dendrite (available, e.g., via voltage-sensitive imaging techniques). The proposed
techniques in this case have the potential to signiﬁcantly complement recently introduced methods for computational neuroanatomy (c.f. Nikolenko et al, Nature Methods 2007; Briggman and Denk, Curr. Opin.
Neurobio. 2007).

Acknowledgments
LP is supported by an NSF CAREER award and a Sloan Fellowship. DF is supported by the Portuguese
Foundation for Science and Technology.

192

Cosyne 2008

Friday evening, Poster session II-74

Observing the observer
J. Daunizeau1, K. E. Stephan1, H. E. M. den Ouden1, M. Pessiglione2, and K. J.
Friston1
1
2

Functional Imaging Laboratory, University College of London, London, UK
INSERM U610, Université Pierre et Marie, Paris, France

This paper concerns the nature of evoked brain responses and the principles underlying their generation.
We start with the premise that the sensory brain has evolved to represent or infer the causes of changes in
its sensory inputs [1]. The problem of inference is well formulated in statistical terms. The statistical
fundaments of inference may therefore afford important constraints on neuronal implementation. Our
focus is on plausible phenomenological models that can be inverted using behavioural/neurophysiological
data; namely models that are useful operationally in an experimental setting and that embody a principled
account of learning.
From a Bayesian perspective, the brain might be considered as an observer of its own sensory signals [2].
In other words, the brain dynamically inverts some forward model of its sensory inputs for representing
the unobserved (hidden) causes that might have generated them. Additionally, we assume that human
observers make decisions on the basis of their (posterior) states of belief regarding these hidden causes.
We then rely on Bayesian decision-theoretic arguments for modelling action prescription as a goalreaching problem in an uncertain environment. This boils down to defining the so-called utility/loss
function for which observed decisions are optimal.
We argue that, as we observe the observer, we can interpret its behavioural/neurophysiological activity
by inferring the structure of both its prior beliefs and its utility/loss function from
psychophysical/physiological measures. As a consequence, two key concepts are required: (i) the
definition of the class of priors and utility/loss function the brain might be using and (ii) the relationship
between the brain’s posterior states of belief and the experimental psychophysical/physiological
observations. Note that there is a crucial distinction between the brain’s (primary) generative model,
which is generating sensory signals and the experimenter’s (secondary) generative model, which is
describing the subject’s responses (e.g., reaction times, neuroimaging time series).
We then propose a principled and generic Bayesian hierarchical modelling framework for inferring the
brain’s states of belief from behavioural/neurophysiological measurements during learning/decision
making experiments. We illustrate the benefits of this methodology in the final section using reaction
times in a simple cue-outcome associative learning task [3].
Acknowledgments
This work was supported by a Marie-Curie Intra-European fellowship program.
References
[1] K. J. Friston (2006): a Free Energy Principle for the brain, J. of Physiol., 100: 70-87.
[2] K. J. Friston (2005): a theory of cortical responses, Phil. Trans. Roy. Soc., 360: 815-836.
[3] H. Den Ouden (2007): cross-modal conditioning to dynamic auditory-visual contingencies, Cosyne
2008 (submitted: companion paper).

193

Friday evening, Poster session II-75

Cosyne 2008

The Computational Structure of Spike Trains
Robert Haslinger1,2 , Kristina Lisa Klinkner3 , and Cosma Rohilla Shalizi 3,4
1

Martinos Center for Biomedical Imaging, Massachusetts General Hospital,
2
Charlestown, MA,
Department of Brain and Cognitive Sciences, Mas3
sachusetts Institute of Technology, Cambridge MA
Department of Statistics,
4
Carnegie Mellon University, Pittsburgh, PA
Santa Fe Institute, Santa Fe, NM
Neurons are computational devices, but identifying the computations they carry out has proved difﬁcult, not
least because synaptic inputs are generally unknown. Most research on computation by neurons has either
asked how neuron-like devices might be assembled to perform given functions, or have used information
theory to determine, via calculation of the entropy rate, a neuron’s channel capacity. However, entropy
quantiﬁes randomness, and tells us little about how much structure is present in a spike train or the amount
and type of computation which must have, at a minimum, taken place to generate this structure. Here
we present a practical, theoretically-grounded method for inferring the minimal computational structure
necessary and sufﬁcient to generate a neuron’s spike train.
The term “computational structure” is used in the information theoretic sense: the most compact effective
description of a spike generation process which can statistically reproduce the observed spike train. Our
approach generates minimal, optimally predictive hidden Markov models or stochastic automata (Predictive State Representations, PSRs) of a spike train’s generative process. The PSRs are generated using the
Causal State Splitting Reconstruction (CSSR) algorithm, a non-parametric method which makes no a priori
assumptions as to the exact form the structure takes. The structure of the spike train is inferred solely from
the data, and the PSR deﬁnes a program which describes the structure, allowing us to quantify, in bits, the
structure’s complexity. We demonstrate our approach using both simulated single neuron spike trains and
those recorded experimentally in anesthetized rat barrel cortex during vibrissa stimulation.
Beyond its nonparametric nature, the primary advantage of using a PSR is that it contains within it not only
the time and history-dependent spike probability, but (in the limit of inﬁnite data) all the information about
the computational structure of the spike generating process, which is present in the spikes themselves. This
allows for a theoretically rigorous description of the spike train’s algorithmic information content, e.g. the
amount of information in bits needed to describe the spike sequence exactly. We show that this can be
split into three parts describing (1) the time-invariant structure (complexity) of the spike generating process,
(2) the randomness (internal entropy rate) of the spike generating process and (3) a residual pure noise
term not described by the spike generating process. We give precise deﬁnitions of these quantities, both
their ensemble averages and their functional dependence on time. The time-dependent versions allow us
to determine when the neuron is traversing states which require a complex description and also put a hard,
time dependent, numerical lower bound on the amount of information supplied to the neuron by external
processes. We discuss how these measures should be interpreted and what they can add to our understanding
of neuronal computation.
Acknowledgments
The Authors thank M. Andermann and C.I. Moore for the use of their barrel cortex data. R.H. was supported
by NIH grant K25 NS052422.

194

Cosyne 2008

Friday evening, Poster session II-76

Activity waves in a network model of Purkinje cells mediated by
asymmetric axonal connectivity
Hermann Cuntz, Alanna Watt, P. Jesper Sjöström and Michael Häusser
Wolfson Institute for Biomedical Research, University College London
Cerebellar Purkinje cell (PC) interconnectivity has previously been proposed to drive network
synchronization. Based on our recent electrophysiological characterization of PC-PC connectivity [1], we
have investigated the effect of this novel connection in a network model. We analyzed the anatomical
reconstructions of PC axon collaterals in the vermis of juvenile mice (P4-14) to define connectivity in the
model. The resulting density profile of PC collaterals (Fig. 1A, N = 28) suggested that PC-PC
connectivity was strongly asymmetric, directed along the axon tract towards the deep cerebellar nuclei
within one parasagittal plane. The path distance along the axon was independent of the soma-to-soma
distance, but a peak of collateral density occurred between 50 and 70 μm from the PC body. This implies
that PC-PC connections function in a feed-forward, chain-like manner.

Figure 1. A, Axon collateral density plot of 28 superimposed cells. The cells were oriented such that their
axons leave to the right. B, Rastergrams of spiking in a network model consisting of 50 Purkinje cells;
top; Erev = -80 mV; bottom, Erev = -40 mV. PC-PC synapses were activated at t = 0 ms.
We built a network model consisting of point neurons possessing the known set of PC active
conductances [2]. The network was arranged in a feed-forward manner based on the directed asymmetry
observed in the anatomical data: each neuron was connected with the following five neurons in the chain.
This resulted in robust spontaneous propagation of waves of activity. Under adult conditions, when these
GABAergic synapses are hyperpolarizing, waves were led by the last cell in the network and propagated
at ~2 mm/sec (Fig. 1B, top). In the juvenile scenario, when GABAergic synapses are depolarizing, waves
propagated in the opposite direction at a slightly higher speed (Fig. 1B, bottom). Model predictions were
validated in experiments, which revealed a linear relationship between firing phase and PC spatial
separation. To conclude, we find waves of activity that propagate spontaneously along parasagittal planes
in the cerebellum.
References
[1] Watt A, Sjöström J, Ramakrishnan L, Häusser M “Monosynaptic inhibitory connections between pairs
of Purkinje cells in mouse cerebellar cortex” SFN Abstract, 2004.
[2] Khaliq ZM, Gouwens NW, Raman IM (2003) “The contribution of resurgent sodium current to highfrequency firing in Purkinje neurons: an experimental and modeling study.” J Neurosci 23(12):4899-912.

195

Friday evening, Poster session II-77

Cosyne 2008

Sparse Coding Predicts Noiseless Sensory Representations and
Noisy Neurons
Hiroki Asari½ ¾, Claire Biot½ ¾, and Anthony M. Zador¾
½

Watson School of Biological Sciences,

¾

Cold Spring Harbor Laboratory

Neuronal noise is often assumed to be a major factor limiting information processing in the brain. However,
the degree to which trial-to-trial variability in single neuron activity represents “noise” from the point of
view of the organism remains controversial, in part because the partition of variability into “signal” and
“noise” is model-dependent. We have therefore compared how two models of cortical encoding  partition variability. We ﬁnd that, whereas in conventional (dense) models of population encoding, trial-to-trial
variability in neuronal ﬁring implies that the population response is noisy, in sparse overcomplete models
trial-to-trial single neuron variability can, surprisingly, coexist with a nearly perfect—noiseless—population
representation.
These two models can be distinguished experimentally by examining the pairwise noise correlations of
stimulus-evoked responses. Speciﬁcally, (i) noise correlation should be stimulus-dependent under a sparseness prior, while it should not be stimulus-dependent under a dense prior; and (ii) the distribution of the noise
correlations should be more kurtotic—less Gaussian—as the neuronal representation becomes sparser. With
modern techniques for recording simultaneously from large populations of neurons, such characteristics
could be assessed experimentally.
Although our model framework should be viewed as a ﬁrst approximation of more complex neuronal sensory
processing, this study provides a novel perspective on the implications of the neuronal variability in cortical
computation.

Reference
 Sparse Representations for the Cocktail Party Problem. H. Asari, B. A. Pearlmutter, and A. M. Zador,
Journal of Neuroscience 26(28):7477–7490, July 2006.

196

Cosyne 2008

Friday evening, Poster session II-78

Ripple Detection using Frequency Modulation
David P. Nguyen1,2,3, Riccardo Barbieri2,3, Emery N. Brown2,3, Matthew A. Wilson1,2
1. Picower Institute for Learning and Memory, Riken-MIT Neuroscience Research Center,
Cambridge, MA; 2. Department of Brain and Cognitive Sciences, Massachusetts Institute of
Technology, Cambridge, MA; 3. Neuroscience Statistics Research Laboratory, Department of
Anesthesia and Critical Care, Massachusetts General Hospital, Boston, MA
In the rat hippocampus, high frequency oscillations in the range of 100-200Hz, termed “ripples”, can be
found in slow wave sleep and awake immobility. Recent evidence suggests that ripples are an
extracellular signature for a process called “replay”, whereby encodings of recent experience are rapidly
transferred between the hippocampus and cortex. Although the functional properties of ripples have
received considerable attention, the problem of ripple event detection has been understated. Like the
multi-unit spike classification problem, errors in ripple detection may lead to erroneous results.
Currently, ripple detection is based on the assumption that ripple amplitude will exceed the amplitude of
the background noise. With this definition there is no robust criterion to reject high-frequency, highamplitude noise (type I error) and there is no chance of detecting ripple events with amplitudes below the
noise threshold (type II error). Using a Kalman filter for tracking sinusoids in noise, we found that ripple
events contain a stereotypic frequency modulation profile. Although this characteristic can be seen by
eye, spectral methods such as FFT and multi-taper analysis do not have the temporal and spectral
resolution to resolve such abrupt modulations in frequency. In our analyses, we have used this frequency
information to successfully identify small amplitude ripples that are usually missed, count the number of
ripples in a burst of overlapping ripples, and reject noise events. Hence, our adaptive filter provides
useful frequency information that can be used to lower the rate of type I and type II errors in the problem
of ripple detection.

197

Friday evening, Poster session II-79

Cosyne 2008

Visual Response in LGN Neurons Beyond the Monosynaptic
Retino-Geniculate Transmission
Baktash Babadi1 , Alexander Casti2 , Youping Xiao2 , Liam Paninski1,3
1

Center for Theoretical Neuroscience, Columbia University, New York, NY
3
Mount Sinai School of Medicine, New York, NY
Department of Statistics,
Columbia University, New York, NY
2

Relay neurons in Lateral Geniculate Nucleus (LGN) receive strong feed-forward excitation predominantly
from a single retinal ganglion cell (RGC), from which the LGN neuron inherits the primary features of its
receptive ﬁeld. LGN neurons also receive synaptic connections from other sources including interneurons,
thalamic reticular neurons, the visual cortex and the brainstem. To what extent these other sources inﬂuence
the response properties of the LGN neurons is an important question. To address this, we ﬁt a Generalized
Linear Model (GLM) to the spike responses of cat LGN neurons elicited by spatially homogenous spots with
different sizes, whose luminance was rapidly modulated in a pseudo-random fashion. Our extra-cellular
recordings captured both the LGN spikes and the incoming RGC input (S potentials), allowing us to provide
our LGN model with the exact times of each retinal input [1]. The instantaneous ﬁring rate of the GLM has

 X(t)

 l(t) ), where X(t),

the general form f (b + D.
+ j Hj nt−j + K.
nt−j and l(t) represent the RGC
 H and K
 are the linear
spikes, past LGN spikes and the luminance of the visual stimulus respectively. D,
 represents
temporal ﬁlters acting on the inputs and the parameter b deﬁnes the background ﬁring rate. D
 potentially transmits the stimulus information beyond
the retino-geniculate (RG) transmission, whereas K
that transmitted to the LGN by the retina (such as cortical feedback and intrageniculate inhibition). After
convolving the inputs with the ﬁlters, the spike train is produced by an inhomogeneous point process whose
rate is an increasing function (f (.)) of the convolved inputs. The ﬁlter parameters are optimized so that the
likelihood of reproducing the observed LGN spikes is maximized [2].
 ﬁlter is large and resembles an exponentially decayThe results show that for all spots sizes, the D
 is nearly zero; i.e. the RG
ing function. For small spot sizes (no larger than the receptive ﬁeld center), K
transmission is sufﬁcient to account for the LGN responses. However, for larger spots the waveform of the
K ﬁlter has a peak. This result is particularly interesting since the analyzed LGN neurons were OFF cells;
 is of opposite sign than the primary visual receptive ﬁeld of these cells. For
i.e., this secondary ﬁlter K
 ﬁlter vanishes again. Cross-validating the model
even larger spots (4 times the receptive ﬁeld center), the K
 ﬁlter accounts for up to 12% of the variance of the LGN spiking activity. We conclude
reveals that the K
that apart from the RG monosynaptic transmission, the LGN neurons receive information about the visual
stimulus from other sources, the anatomical nature of which is yet to be determined [cf. 3].
Acknowledgments
AC: K25 MH67225; YX: NEI EY016371; LP: Sloan research fellowship & NSF CAREER award
References
[1] Casti A. et al (2007) A simple model of retina-LGN transmission. J Comput Neurosci. 2007 Sep 1.
[2] Paninski L., Pillow J., Lewi J. (2007) Statistical models for neural encoding, decoding, and optimal
stimulus design. Prog Brain Res. ;165:493-507.
[3] Wang X. et al (2007) Feedforward excitation and inhibition evoke dual modes of ﬁring in the cats visual
thalamus during naturalistic viewing. Neuron 55(3):465-78

198

Cosyne 2008

Friday evening, Poster session II-80



 °  !"#! ° $
%%  !
!
6!

"#$ "%&'() *+",+-./.0. "1(*"2*3/+"4 - 3*&$5"%,#
"#$ "4(783+9",+-./.0. "3.":3*)3*9

;*(<* -- " /+ " 0+9 *-.3+9/+< " .$ " =*3/+ " > &$3+/->- " 0+9 *8?/+< " )/-/(+ " * @0/* - " .$ " &(+-.*0&./(+ " (1"
&(>A0.3./(+38">(9 8-".$3."+(."(+8?" >083. ".$ "=*3/+B-"3+3.(>?"3+9"A$?-/(8(<?5"=0."08./>3. 8?">3.&$"/.-"
A *1(*>3+& "(+")/-038".3-C-!"",+"* & +."? 3*-5"83*< "93.3=3- -"(1"(-. +-/=8?"D+3.0*38D"/>3< -"$3) "= &(> "
A(A083*"/+".$ "-.09?"(1")/-/(+"3+9"$3) "= +"0- 9".("-$(7"3AA3* +.8?"/>A* --/) "A*(<* --"/+"=0/89/+<"
-0&$">(9 8-!"":(7 ) *5"7$/8 ".$ - ". -."- .-"$3) "<*(7+"/+&* 3-/+<8?"83*< 5"/."* >3/+-"9/11/&08.".("C+(7"
7$ .$ *".$ ?"A*(A *8?"&3A.0* ".$ " -- +& "(1".$ "A*(=8 >"9(>3/+".$3.".$ ?"3* "/+. +9 9".("* A* - +.!"
: * 5"7 "&$388 +< ".$/-"3--0>A./(+"1(*"3"*3+< "(1"-.3+93*9". -."- .-"/+".$ "9(>3/+-"(1"(=E &."3+9"13& "
* &(<+/./(+!
FA &/1/&388?5 " 7 " -$(7 " .$3. " 3 " -/>A8 "G H8/C " >(9 8 " I " 3 " + 0*(-&/ +./-.B- " D+088D " >(9 85 " 7$/&$ " -$(089"
A *1(*>"A((*8?"3."* 38H7(*89")/-038"(=E &."* &(<+/./(+".3-C-"I">3.&$ -"(*"(0.A *1(*>-" J/-./+<"-.3. H(1H
.$ H3*."* &(<+/./(+"-?-. >-"K=/(8(</&388?H/+-A/* 95"3+9"(.$ *7/- L"(+"3")3*/ .?"(1"-.3+93*9"(=E &."3+9"13& "
* &(<+/./(+". -.-!" "M-"3"&(0+. *A(/+.5"7 "9 -/<+ 9"3"- */ -"(1"D-/>A8 *D"-?+.$ ./&"* &(<+/./(+". -.-".$3."
= .. *"-A3+".$ "* 38"7(*89")3*/3./(+"/+"(=E &."A(- 5"A(-/./(+5"3+9"-&38 5"3+9"7 "-$(7".$3.".$ - ". -.-"
&(** &.8?" JA(- ".$ "/+39 @03&?"(1".$ "G H8/C ">(9 8!""#3C +".(< .$ *5".$ - "* -08.-"9 >(+-.*3. ".$3.". -.-"
=3- 9"(+"0+&(+.*(88 9"D+3.0*38D"/>3< -"&3+"= "- */(0-8?">/-8 39/+<5"A(. +./388?"<0/9/+<"A*(<* --"/+".$ "
7*(+<"9/* &./(+!
'(/+<"1(*73*95"7 " J3>/+ "&83-- -"(1"/>3< ". -."- .-".$3.">/<$."= ">(* ")3803=8 "1(*"<0/9/+<"(=E &."
* &(<+/./(+" 11(*.-!""F/>/83*8?5"7 " JA8(* "7$3."> 3-0* -"&(089"* A* - +."13/*"=3- 8/+ -"1(*". -.-"(1"(=E &."
3+9"13& "* &(<+/./(+5"3+9"-$(7"$(7".$ - "&3+"= " J. +9 9".(".$ "* 83. 9"A*(=8 >"(1"(=E &.".*3&C/+<5"
7$ * "-.3+93*9/N 9". -.-"3* " ) +"8 --"&(>>(+!"""M.".$ "& +. *"(1".$ - "/--0 -"/-".$ "+ 9".("&8 3*8?"9 1/+ "
7$3.".$ "A*(=8 >"/-5"7$?"/."/-"9/11/&08.5"3+9"7$3."* -08.-"7(089"&(+-./.0. "-0&& --!""#$ "A3.$"1(*73*9"7/88"
+(."= " 3-?5"=0."7 "3*<0 ".$3."/."/-"./> "1(*".$ "1/ 89".("</) ".$/-"A*(=8 >">0&$">(* "& +.*38"3.. +./(+!

199

Friday evening, Poster session II-81

Cosyne 2008

Bayesian Theory of Visual Search
Wei Ji Ma1, Vidhya Navalpakkam2, and Jeffrey M. Beck1
1
University of Rochester, 2California Institute of Technology
Human performance in searching for a tilted target among vertical distractors is well described by signal
detection theory (SDT) models which combine the outputs of noisy detectors using a maximum or
summation operation [1]. These theories assume that each detector is a neuron best tuned to the target,
and that a detector’s response to the target or distractor follows a Gaussian distribution. However, recent
work has questioned the validity of SDT measures in search tasks with more complex stimulus
distributions, for instance ones in which distractors flank the target in orientation space [2]. Instead, a
saliency-based signal-to-noise ratio was proposed to measure task difficulty. A drawback of this theory is
that it cannot predict receiver operating characteristics (ROCs).

Percentage correct loss

We reconcile and generalize both approaches by developing a fully Bayesian model of visual search. We
study the detection of a target among distractors, where target and distractor features are drawn from
arbitrary distributions. At each location in the display, the feature (such as orientation) is coded in a
population of neurons with Poisson-like variability [3]. Decisions are based on the log odds of target
presence given the responses of all neurons at each location (unlike SDT, where decisions are based on
the neuron at each location that is best tuned to the target). These global log odds are obtained through a
nonlinear operation on the location-specific log odds, which incorporate the feature distributions. Search
difficulty is quantified by a number of metrics including mutual information, Kullback-Leibler
divergence, area under the ROC, and proportion correct. This model 1) reproduces behavioral effects and
ROCs in simple visual search, such as the increase of search difficulty with set size, target-distractor
target
similarity, and distractor heterogeneity; 2) explains behavior in
complex search conditions, such as when distractors flank the target;
One side
4
3) can in special cases be approximated by the maximum or sum rule.
Flanking
3
In simple search, the stimulus-conditioned distributions of the local log
odds are approximately equivariant Gaussian distributions, indicating
2
that a decision variable from Bayes-optimal computation can be linked
to the SDT models. Moreover, for any distribution of target and
1
distractor features, the Bayesian model predicts which neurons are
0
most informative. For instance (see figure), when distractors are
-80 -60 -40 -20 0 20 40 60 80
similar to the target (either flanking the target at ±10° or on one side at
distractors
+10°), this model predicts that neurons tuned to orientations slightly
Lesioned preferred orientation (°)
away from target or distractor are most crucial to search performance.
It is important to note that Bayes-optimal performance requires full knowledge of the generative model,
including location and feature distributions of target and distractors. In many cases, this information is not
available or known with certainty, leading to extra behavioral variability and suboptimal performance.
We suggest that the effect of attention is to provide prior knowledge that improves the quality of the
approximation to the generative model. This notion is compatible with human experiments.
References
[1] Verghese, P. (2001). Visual search and attention: a signal detection theory approach. Neuron 31, 523-35.
[2] Navalpakkam, V. and Itti, L. (2007). Search goal tunes visual features optimally. Neuron 53, 605-17.
[3] Ma, W.J., Beck, J.M., Latham, P.E., and Pouget, A. (2006). Bayesian inference with probabilistic
population codes. Nature Neuroscience 9 (2), 1432-8.

200

Cosyne 2008

Friday evening, Poster session II-82

Capturing spike train pairwise correlations across cells and time
Kolia Sadeghi1 , Michael Berry2
1,2

Princeton University

We introduce a model of joint spike train statistics which can capture pairwise correlations across cells and
across many time bins. The number of spikes in a given time bin is not limited to being binary, it is an
integer. Fitting model parameters is as fast as diagonalizing the data’s covariance matrix, thus making it
useable for large numbers of cells and large numbers of time bins. In particular, ﬁtting a time-shift invariant
covariance matrix will yield a time-shift invariant model. The model is deﬁned in Fourier space by its
characteristic functional, which can be quickly inverted using a fast fourier transform to yield probabilities
of events. As a property of this representation, marginal distributions are in the same family of models, and
are straightforward to calculate. We ﬁt the model to several multi-cell recordings to show goodness of ﬁt
and usage.

Acknowledgments
We thank J. Macke for helpful discussions.

201

Friday evening, Poster session II-83

Cosyne 2008

Towards automatic and fast segmentation in SBFSEM image stacks
Daniel R. Berger1
1

Max Planck Institute for Biological Cybernetics, Tübingen, Germany

Recent methodological advances (SBFSEM, see [1]) have made it possible for the first time to scan large
volumes of nervous tissue rapidly and automatically at electron microscopic level. If individual neurons
could be segmented in these image stacks, neuronal circuits with all synapses could be reconstructed. Due
to the huge amount of image data it is not practical to do the segmentation by hand, and methods for fast
and automated segmentation are needed.
Contrary to previous work [2], we approach the segmentation problem as one of finding local edge probabilities in the 3-dimensional image block. First, 3-dimensional Fourier transforms are computed on local
blocks of 4x4x4 voxels. Next, an unsupervised clustering algorithm (K-Means) is used to group the
Fourier blocks into typical image features. This reduces the computational cost, as further computations
can be performed on the smaller set of image features rather than on the original data. It also helps to
reduce image noise. Then, we fit generated ground-truth data, which contain edges of known position and
orientation, to the image features. For each voxel we can then compute an edge probability from the
blocks overlapping it. A flood-filling algorithm can then rapidly segment the image into its components.
This method could be easily extended to incorporate further constraints, such as edge continuity. The
method is quite fast, since much of the computation can be done once for a certain type of images and
then stored and applied to new images; for example the clustering into features and association of edge
probability patterns to these features. This probabilistic framework offers great potential for automated,
efficient and robust segmentation of neurons in electron-microscopic image stacks.

Figure 1: A: Original SBFSEM slice image (see supplementary material in [1]). B: Fitted local edges in
3D (direction is color-coded). C: Resulting edge probability map. D: Segmentation attempt, using floodfilling with edge threshold and connectivity probability propagation.
Acknowledgments
Thanks to J. Butler for helpful discussions. This work was supported by the Max Planck Society.
References
[1] Serial block-face scanning electron microscopy to reconstruct three-dimensional tissue nanostructure.
W. Denk and H. Horstmann, PLoS Biology, 2(11):1900-1909 (e329), November 2004.
[2] Axon tracking in serial block-face scanning electron microscopy. E. Jurrus et al., Workshop on
microscopic image analysis with applications in biology, Copenhagen, Denmark, October 2006.

202

Cosyne 2008

Friday evening, Poster session II-84

Measures of Neural Discrimination
P. Gill1, J. Zhan2, M. Merolle1, M. Gastpar2 and F. Theunissen1,3
1

Neurosciences, 2EECS, 3Psychology. University of California, Berkeley.

In sensory systems, neural discrimination measures the ability of single or ensemble of neurons to classify
stimulus features. Measuring neural discrimination is useful because it provides a model free assessment
of the neural coding occurring in a given processing stage. Neural discrimination measures can thus be
used to investigate the nature of code (eg. rate vs temporal code), to validate models (eg. linear vs. nonlinear) and to assess the relevant stimulus space (eg. natural vs synthetic; phoneme vs. syllable). Neural
discrimination for ensembles of neurons can also be compared to neural discrimination for single neurons
thus measuring the degree of redundancy or synergy in ensemble codes. Finally, neural discrimination
can be assessed in different processing stages to investigate changes in the neural representation.
In this study, we compared four alternative measures of neural discrimination each with successively
fewer assumptions. We first calculated the within-d' statistic: the difference in firing rates normalized by
their standard deviation for different pairs of stimuli. This measure assumes that the information is
represented in the total number of spikes fired and that these are normally distributed. Second, we used
an ideal observer (IO) approach and calculated the percentage of successful stimulus identifications [1].
The IO method we used calculated the similarity between template spike trains and observed spike trains
and, in this manner, took spike timing information into account. However, just as for the d' measure, it
assumes that the scale we used to define the elements of the stimulus ensemble was the relevant one.
Finally, we used two alternative methods to estimate the Mutual Information (MI): one based on an
inhomogenous gamma model [2] and one based on the instantaneous firing rate only.
We tested the validity of these four measures of neural discrimination by estimating the information in
single neurons in the avian auditory forebrain while processing song. We found that the gamma
information and the IO measure showed a very high degree of concordance. The rate information could
also provide a good approximation but only when it was estimated for a particular time window of ~ 40
ms. The d' measure fell short in that it correlated poorly with all the other measures. Our study suggests
that IO measures and rate information could also be used to estimate ensemble information as long as the
relevant time scale is assessed independently.
Acknowledgments
This work was supported by NIH grant DC007293 to FT and MG.
References

1.
2.

Wang, L., et al., Cortical discrimination of complex natural stimuli: can single neurons
match behavior? J Neurosci., 2007. 27(3): p. 582-9.
Hsu, A., et al., Modulation power and phase spectrum of natural sounds enhance neural
encoding performed by single auditory neurons. J Neurosci, 2004. 24(41): p. 9201-11.

203

Friday evening, Poster session II-85

Cosyne 2008

Spike-Time Dependent Plasticity Could Explain Temporal Tuning
of Auditory Cortical Cells
Sohrab Saeb1, and Aydin Farajidavar2
1

Amirkabir University of Technology,

2

University of Texas at Arlington

Molecular mechanisms underlying temporal tuning (plasticity) of cells in the primary auditory cortex
(A1) remains controversial. Experimental studies indicate that the frequency response of cortical cells
changes noticeably under behavioral learning. In an experimental study [1], rats were trained in a “sound
maze” where navigation using only auditory cues led to a target location paired with food reward. In this
task, the repetition rate of noise pulses increased as the distance between the rat and target location
decreased. After training, neurons in A1 showed greater responses to high-rate noise pulses.
Some researchers believe that this plasticity may originate from a modification in NMDA receptors [2-3].
Based on this theory, the variation of NMDA receptor conductances modifies the ration of excitation and
inhibition in cortical circuitry, and provides a way to tune the frequency response of cells. However,
according to a recent theoretical study [4] which utilizes the dynamic synapse model [5], the synaptic
plasticity is able to explain cortical temporal tuning without any need to inhibition or modulatory
feedback. In this approach, by changing the contribution of synaptic facilitation to the short-term
dynamics of synapses (U1), similar results to the results obtained in [1] can be achieved.
We hypothesize that this effect is also reproducible by changing the absolute synaptic efficacy parameter
(A) of the dynamic synapse model. It means that due to the variation of A, the cut-off frequency of the
model frequency response is modified, which is similar to the effect of temporal tuning in the cortex. On
the other side, the long-term variation of the absolute synaptic efficacy can be related to spike-time
dependent plasticity (STDP) [6]. This fact suggests that STDP may cause modification of absolute
synaptic efficacy that in turn leads to temporal tuning. We hypothesize that, by using specific parameter
values for the STDP model, the value of the absolute synaptic efficacy can increase autonomously when
the model is exposed to the auditory learning noise pulses. Therefore, based on the interplay of STDP and
short-term synaptic plasticity, the temporal tuning of cortical cells can be explained in a feedforward,
synaptic plasticity based manner.
References
[1] Temporal plasticity in the primary auditory cortex induced by operant perceptual learning. S. Bao, E.
F. Chang, J. Woods, and M. M. Merzenich, Nature Neuroscience 7(9):974-981, 2004.
[2] Plasticity of temporal information processing in the primary auditory cortex. M. P. Kilgard, and M. M.
Merzenich, Nature Neuroscience 1(8):727-731, 1998.
[3] Thalamocortical NMDA conductances and intracortical inhibition can explain cortical temporal
tuning. A. E. Krukowski, and K. D. Miller, Nature Neuroscience 4(4):424-430, 2001.
[4] Modeling the primary auditory cortex using dynamic synapses: Can synaptic plasticity explain the
temporal tuning? S. Saeb, S. Gharibzadeh, F. Towhidkhah, and A. Farajidavar, Journal of Theoretical
Biology 248(1):1-9, 2007.
[5] Neural networks with dynamic synapses. M. Tsodyks, K. Pawelzik, and H. Markram, Neural
Computation 10(4):821-835, 1998.
[6] Synaptic modification by correlated activity: Hebb's postulate revisited. G. Bi, and M. Poo, Annual
Review of Neuroscience 24:139-166, 2001.

204

Cosyne 2008

Friday evening, Poster session II-86

Identifying core circuit principles for solving complex tasks
Charles D. Kopec, Jeffrey C. Erlich and Carlos D. Brody
Princeton Neuroscience Institute and Dept. of Molecular Biology, Princeton
University
Model neural circuits can help explain experimental data, but any designed circuit is limited by the
imagination of the investigator. An example is a model, based on data from awake monkeys in the
laboratory of Ranulfo Romo, that was recently designed to solve a two alternative forced choice (2AFC)
frequency comparison task (called here “the Romo Task”) (Machens et al. 2005). The purpose of the
current project is to find novel alternative circuits that can solve tasks involving short-term memory and
decision making (The Romo Task). We have initially tested our methods by investigating models capable
of a simpler single-stimulus categorization (in a task we call “the Pro Localization Task”).
Our simulated circuits consist of a small number of excitatory and inhibitory neurons that each operate
using a simple integrate and fire spiking model. Both tasks have a two-alternative forced-choice structure:
on any given trial, the circuit must categorize the set of sensory stimuli provided during the trial as
belonging to category “A” or category “B.” The categories are pre-defined by the experimenter, and the
circuit’s task is to learn these categories. Each circuit’s performance is scored based on the neuron within
the circuit that best differentiates the two groups of stimuli. With minimal constrains we begin with
randomly connected circuits and refine them using a genetic algorithm (GA). Free from any initial design,
we hope to discover interesting and potentially non-intuitive connectivity patterns that give rise to
functional behavior. By running the GA multiple times each beginning with randomly connected circuits
we are able to uncover multiple independent solutions. Analysis of these may yield core principles by
which such a task could be solved by a network of spiking neurons. Since we know the exact connectivity
and synaptic properties in the simulated circuits we can dissect them to uncover what gives rise to the
behaviors required in each task (e.g., short-term memory and decision-making in the Romo task).
On each iteration of the GA, the worst circuits are discarded while the best performers are mated to
produce the next generation. Synapses are inherited as individual units.
Initial efforts to test the feasibility of using a GA to identify functional neuronal circuits involved a Pro
Localization task. On each trial of this task, a stimulus that can vary continuously from full “Left” to full
“Right” is provided. The network must produce a categorized output, either “Left” or “Right.” Multiple
independent solutions were found and then aligned to generate a consensus circuit. Consistent with
previous models of such binary decision tasks, this consensus contains both self excitatory and mutually
inhibitory elements arranged in such a way that the circuit solves the task using a direct competition
winner take all strategy.
Now that we have confirmed that a GA can produce functional circuits that can be analyzed by aligning
independent solutions producing a consensus circuit, we will apply this technique to more complex tasks
such as those described above.

205

Friday evening, Poster session II-87

Cosyne 2008

Buildup Activity in Monkey LIP Neurons Reflects Accumulation of
a Motoric Rather Than a Sensory Representation: Reanalysis of
Roitman and Shadlen (2002).
Joonkoo Park and Jun Zhang
Department of Psychology, University of Michigan
Recent single neuron studies have identified various roles for the neurons in the lateral intraparietal (LIP)
area, a subdivision of the inferior parietal lobule. Evidence shows support for both sensory-related
representations (e.g. saliency and information accumulation) and motor-related representations (e.g.
potential saccadic direction). A previous study using random-dot motion-discrimination paradigm [1]
supports the view of the information accumulation model with a threshold-crossing mechanism in the
activity of the LIP neurons during a simple choice reaction time task. This analysis was, however, unable
to distinguish the sensorimotor role of the neurons during the stimulus-response association task.
The mathematical techniques of Locus Analysis [2] and an analysis based on the Signal Detection Theory
(SDT) [3] were applied to this dataset in order to quantitatively characterize the sensorimotor role of the
neuronal activity. The Locus Analysis provides an index of the contribution of neuronal activity toward
the sensory, motor, or decisional aspect of the task. The SDT-based analysis provides discriminability
values for the stimulus identification and for the response preparation aspects of the task which together
quantitatively specify the locus of the neuronal activity along the sensorimotor continuum. The two
techniques separately provide the temporal dynamics of the activity of the neurons in the sensorimotor
continuum.
The results from both analyses indicate that the activation of the LIP neuronal population is not taskspecific in the beginning of a trial. The neuronal population activity then starts to encode task-specific
representation indicative of motoric coding as opposed to sensory coding. Data are presented both at the
populational level and at single unit level. These results demonstrate that the buildup activity in the LIP
neurons mostly encode intended saccadic movement and little, if any, stimulus direction information.

Acknowledgments
This work was supported by AFOSR grant FA9550-06-1-0298. Single cell recording data were kindly
made available by Mike Shadlen.

References
[1] Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction
time task. Roitman JD and Shadlen MN, Journal of Neuroscience 22:9475-9489, 2002.
[2] Dynamics of single neuron activity in monkey primary motor cortex related to sensorimotor
transformation. Zhang J, Riehle A, Requin J, and Kornblum S, Journal of Neuroscience 17:2227-2246,
1997.
[3] Analyzing neuronal processing locus in stimulus-response association tasks. Zhang J, Riehle A, and
Requin J, Journal of Mathematical Psychology 41:219-236, 1997.

206

Cosyne 2008

Friday evening, Poster session II-88

Ising Models for >40 Ganglion Cells in a Dense Patch of Salamander
Retina
Dario Amodei, Greg Schwartz, and Michael Berry
Princeton University
Recent work (e.g. [2, 3]) has shown that the collective spike patterns of groups of up to 40 ganglion cells
in the retina can be represented to high accuracy by a fully-connected Ising model constructed from the
pairwise correlations of the cells’ spikes. Extending this work, we investigate the accuracy of fullyconnected Ising models for groups of >40 densely clustered retinal ganglion cells in salamander. We
construct Ising models for each of three types of stimuli: natural scenes, random checkerboards, and
geometric shapes. To make these measurements we have fabricated a multielectrode array with 60
channels in a square pattern and a 30 ¢m interelectrode spacing (figure at bottom). The array has features
down to 2 ¢m and is fabricated using a combination of photolithography, electron beam lithography, liftoff, and dry etching. The fabrication process is a modified version of that of [1]. We also report progress
on the fabrication of a 240 channel array (also with 30 ¢m spacing), which will be used to construct even
larger Ising models and to test the hypothesis [3] that dense patches of ~200 correlated cells can be
approximated by an Ising model poised near a critical point. We explore the energy landscape of our Ising
model and compare it with the energy landscape of Ising models from simulated networks. To test the
idea that the energy basins of large Ising models represent feature classifications by the retina, we
compare basin identity with shape identity for Ising models constructed from the geometric shapes dataset
and determine the mutual information between the two. We also test the relative strength of noise
correlations versus stimulus correlations by shuffling spike trains on each cell independently over
repeated presentations of the same stimulus.

Completed 60 channel array (scale bar = 30 m)
References
[1] D. Gunning, C. Adams, W. Cunningham, K. Mathieson, V. O'Shea, K. M. Smith, E. J. Chichilnisky,
A. M. Litke, and M. Rahman, Nuclear Instruments & Methods in Physics Research A 546, 148 (2005).
[2] E. Schneidman, M.J. Berry, R. Segev, and W. Bialek, Nature 440, 1007 (2006)
[3] G. Tkacik, E. Schneidman, M.J. Berry, and W. Bialek, arXiv:q-bio/0611072v1 (2006)

207

Friday evening, Poster session II-89

Cosyne 2008

A sparse, subspace model of higher-level sound structure
Jimmy Wang1 , Bruno Olshausen1 & Vivienne L’Ecuyer Ming1,2
1
2

Redwood Center for Theoretical Neuroscience, UC Berkeley
Mind, Brain & Computation/MBC, Stanford University

An understanding of auditory processing requires an understanding of the statistics of natural sounds. It
has been shown in previous work, for example, that the response properties of neurons in the cochlea are
well matched to the statistics of the raw sound waveform so as to provide a sparse code of natural sound[1].
Learning higher-level structure and evaluating its relevance to processing along the auditory pathway will
require analyzing more abstract properties of sound rather than the raw sound waveform. Here we present a
model for learning a representation of sound in terms of multidimensional subspaces that are adapted to the
statistics of natural sound. The signal is represented as a linear combination of subspaces
y(t) =

N 
M


m
sm
n An (t) + η(t)

(1)

n=1 m=1
nth subspace,

th component of the
where Am
sm
n is the m
n is its corresponding coefﬁcient and η(t) is additive Gaussian noise. A sparse prior is imposed on the norm of the subspace coefﬁcients that encourages
independence between different subspaces, but other than the norm there are no constraints on the values of
the coefﬁcients within a subspace. The learning is done via a variational EM method that iterates between
maximizing the log-probability of the coefﬁcients and the dimensions of the subspaces. Speciﬁcally, we
alternate between
 M

N


S t+1 = argmin ||Y − At S||22 s.t.
C
(sm )2 < λ
(2)
S

A

t+1

= argmin ||Y −
A

n=1

AS t+1 ||22

m=1

s.t. ||An || < 1, ∀n = 1, ..., N

(3)

where C(.) is a sparse penalty function. To optimize the coefﬁcients given the subspaces (equation 2),
we employ the subspace thresholding circuit, a computationally efﬁcient and neurally plausible gradient
descent based method[2]. It implements a dynamical system inducing local competition to achieve a sparse
decomposition of the signal. Optimizing the subspaces given the coefﬁcients (equation 3) is a quadratic
programming problem which we solve using its Lagrangian dual[3]. Solving the dual problem is much
faster as the number of dual variables to be optimized equals the number of basis functions, N , while the
number of primal variables equals the total number of basis elements, N ×M . Using these methods we adapt
the model to both waveform and spectrogram representations of spoken English. The resulting subspaces
learn a variety of acoustic invariances, including phase- and bandwidth-invariance in waveform subspaces,
formant- and limited ”pitch”-invariance in spectrogram subspaces.
References
[1] Efﬁcient auditory coding. E. Smith & M. Lewicki, Nature 439(7079), 2006.
[2] Neurally plausible sparse coding via thresholding and local competition. C. Rozell, D. Johnson, R.
Baraniuk & B. Olshausen, Neural Computation in press.
[3] Efﬁcient sparse coding algorithms. H. Lee, A. Battle, R. Rajat & A. Ng, Advances in Neural Information
Processing 19, 2007.

208

Cosyne 2008

Friday evening, Poster session II-90

Dynamical Phase Transitions and Scaling Laws in the Response
of a Rhythmically Perturbed Neuron
Jan R. Engelbrecht and Renato Mirollo
Boston College
In order to explore how a local rhythm inﬂuences the timing of a neuron’s spikes, we study the dynamics of
an integrate-and-ﬁre (IF) model neuron with an oscillatory stimulus. The frustration due to the competition
between the neuron’s natural ﬁring period and that of the oscillatory rhythm leads to a rich structure of
asymptotic phase locking patterns and ordering dynamics. The phase transitions between these states can
be classiﬁed as either tangent or discontinuous bifurcations, each with its own characteristic scaling laws.
The discontinuous bifurcations exhibit a new kind of phase transition that may be viewed as intermediate
between continuous and ﬁrst order, while tangent bifurcations behave like continuous transitions with a
diverging coherence scale.
Even though the IF model drastically oversimpliﬁes the dynamics of real neurons, we invoke notions of
phase transitions in statistical mechanics to argue that the scaling and pattern formation we ﬁnd is universal
and will manifest itself in more realistic neuron models as well as in real functioning neurons. Our results
can then shed light on how rapidly a neuron can adjust its spike timing in the presence of a local rhythm and
we also emphasize that complex patterns in spike timing that go beyond simple entrainment, can emerge.

Acknowledgments
We thank Chuck Stevens, Mike Hasselmo, Vitaly Klyachko and Motoharu Yoshida for very useful interactions. This work was supported in part by, ICAM, the Institute for Complex Adaptive Matter.

209

Friday evening, Poster session II-91

Cosyne 2008

Characterizing reach strategies in ambiguous tasks
Paul Schrater1, Peter Battaglia1
1

University of Minnesota

How does the brain develop a control strategy for tasks like reaching to a line or grasping a cylinder that
do not have unique solutions? One possibility is that particular points on the line or cylinder are selected
as targets by minimizing generic motor costs (e.g. the proximity of the target the initial hand
configuration or the control cost of acquisition). These selected targets could then be used to drive a
feedback controller that minimizes distance to the target. However, it is also possible that the brain
maintains a representation of solution ambiguity during control, to allow for control strategies that adapt
goals to provide robustness against noise, sensory error, and changes in the environment. These two
strategies behave differently under a perturbing force field mid-reach: the first corrects the perturbations,
while the second adapts by contacting the new closest viable point. The goal of this study was to test
these possibilities psychophysically, and to develop computational methods for characterizing control
strategies to ambiguous targets. Participants used a PhanTom to virtually touch graphically rendered
lines that appear at various orientations on a virtual surface. During some trials, a force (oriented in
various directions with respect to the line) perturbs the movement. If participants maintain solution
ambiguity, they should “go with the flow”, taking advantage of the perturbing force to the extent that it
doesn’t affect task success. Conversely, if subjects use target selection, then we should see attempted
correction of the perturbing forces in all conditions. Qualitatively, we found that reaches "go with the
flow", which suggests that the brain visually encodes and uses the set of viable contact points to construct
control policies. Moreover, when the line lengths were made shorter, the amount of corrective force
increases with decreases in line length, suggesting that policies are explicitly based on cost functions that
encode task success rather than target acquisition. To quantify action policies and estimate the cost
functions effectively used by participants, we use non-parametric Bayesian regression to learn state-action
visit distributions conditioned on initial state and the reward for each task. Using the assumption that
these state-action visit distributions are optimal for some cost function defined on state-action pairs, we
show how reach behavior can be interpreted in terms of equivalent cost functions.
Acknowledgments
This work was supported by NIH grant R01 EY015261 and an NSF Graduate Fellowship.

210

Cosyne 2008

Friday evening, Poster session II-92

Learning Hierarchical Representations of Natural Scenes with a
Cooperative Energy Hierarchy
Jascha Sohl-Dickstein1 and Bruno Olshausen1
1
Redwood Center for Theoretical Neuroscience, UC Berkeley
A common assumption in neuroscience is that the nervous system is adapted to the statistics of natural
stimuli [1]. By learning the statistical structure of natural stimuli, we can thus hope to gain insight into how
those stimuli are represented and processed in the brain. For example, it has been shown that learning a
sparse code for images predicts the shapes of simple cell receptive ﬁelds in V1 [2].
We propose here a new model, with several appealing features, for the unsupervised learning of statistical
structure in stimuli. It is hierarchical, with higher levels describing input with increasing degrees of abstraction. It is factorial, meaning that nodes will be encouraged to discover independent and invariant aspects of
the input. It uses only local information for learning, and connectivity is similarly local, making it scalable
and its error function more locally relevant. Its units are heterogeneous and highly ﬂexible. Finally, as in
[3], probabilistic contributions are combined from everywhere within the hierarchy, freeing higher levels
from taking responsibility for structure which has already been adequately described elsewhere.
In this cooperative energy hierarchy, an energy based probabilistic description of the input (q (x0 ; θ) =

e−E(x0 ;θ)
i σi (φi x0 ; θ)). Reﬂecting the belief
Z(θ) ) is built from a sum of sigmoid-like experts (E (x0 ; θ) =
that the nonlinearities which best describe the data’s energy landscape are a reasonable jumping off point
for further description of that energy landscape, a change of variables is then performed on both the data
and associated energy
	
	 landscape into the output space of the nonlinearities (x1,i = σi (x0 ), E (x1 ; θ) =
	 1	
E (x0 ; θ) + log 	 ∂x
∂x0 	). A new layer x2 is then learned above x1 in a similar fashion, except that it is
initialized with the already learned energy function and is thus only sensitive to deviations from it. This



process repeats. The ﬁnal form of the energy function (E (x0 ; θ) = i x1,i + i x2,i + i x3,i + · · ·) is
quite simple, but each additional term represents a further step into the space of nonlinear transformations
of the input, carefully designed to address only those aspects which have not yet been adequately described.
(left) Receptive ﬁelds learned by
the cooperative energy hierarchy’s
ﬁrst layer when trained on 12x12
natural image patches. (right top)
Receptive ﬁelds for the 5 ﬁrst
layer units contributing most positively to the output of 10 randomly
chosen second layer units, intensity weighted by their contribution,
(right bottom) The 5 most negatively contributing ﬁrst layer receptive ﬁelds
References
[1] Possible principles underlying the transformation of sensory messages. H.B. Barlow, Sensory Communication,
page 217. MIT Press, Cambridge, MA, 1961.
[2] Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. B.A. Olshausen
and D.J. Field, Nature 381: 607-609, June 1996.
[3] Unsupervised Discovery of Non-Linear Structure using Contrastive Backpropagation. G.E. Hinton et al. Cognitive
Science, 30:4

211

Friday evening, Poster session II-93

Cosyne 2008

Sub-millisecond ﬁring-rate response in cerebellar Purkinje cells
Srdjan Ostojic1, Nicolas Brunel2 , and Vincent Hakim1
1

Laboratoire de Physique Statistique, Ecole Normale Supérieure, France,
Laboratory of Neurophysics and Physiology, Université Paris René Descartes,
France
2

One of the fundamental issues in neural coding is how fast the ﬁring rate of a neural population can track
the temporal variations of its input. The relevant time-scale is usually thought to be given by the membrane
constant, which is of the order of 10-20 milliseconds, so that faster variations in the input are ﬁltered out.
Here we show that the relevant time-scale greatly depends on the morphology of considered neurons. In the
particular case of cerebellar Purkinje cells, this time scale appears to be as small as 0.1 millisecond, so that
these cells are able to track their inputs with exquisite temporal precision.
The Purkinje cells play a key functional role as they provide the only output from the cerebellar cortex.
Morphologically, their hallmark is a spectacularly large and ramiﬁed dendritic tree. In spite of their size,
these cells appear to be electrotonically very compact, so that their morphology seems to inﬂuence only
marginally their static properties. Nevertheless, it is known that the large dendritic capacitance induces several time-scales for sub-threshold voltage transients. To clarify the effect of these time-scales on the ﬁringrate dynamics at a level of a population, we compare a single-compartment model with a two-compartment
model of a Purkinje cell based on electro-physiological data. The latter model is characterized by a clear separation of time-scales for the soma and the dendrites, the time constant being around 0.1 ms in the somatic
compartment and 5 ms in the dendritic compartment.
To characterize the ﬁring rate dynamics, we quantify the modulation of the ﬁring rate in response to stochastic inputs and sinusoidally varying currents or conductances of different frequencies and small amplitude.
As expected, the modulation of the output is signiﬁcant only for inputs varying at frequencies slower than a
cut-off frequency. While for a single-compartment model, the cut-off frequency is of the order of the ﬁring
frequency, and therefore rather low, for the two-compartment model we ﬁnd that this cut-off frequency is
determined only by the inverse of the somatic time constant, and thus extremely large. The value of the
dendritic time-constant on the other hand shapes the response to frequencies below cut-off, and in particular gives rise to a resonance around 200 Hz, which can be related to fast oscillations recently observed in
vivo. To check the robustness of our results, we have implemented the spike-generation at different levels
of realism, starting from simple integrate-and-ﬁre mechanisms which can be studied analytically, up to a
detailed conductance based model. A comparison with a full multi-compartment model further shows that
our two-compartment model faithfully captures the ﬁring-rate dynamics of Purkinje cells.
While we have studied in detail the case of the Purkinje cell, our two-compartment model provides a simple
framework to explore more generally the effect of the neuronal morphology on population ﬁring-rate dynamics. Our results suggest that electro-physiological data should be re-examined to better understand the
temporal coding properties of different types of neurons.
Acknowledgments
We thank Boris Barbour for helpful discussions. This work was supported by Agence Nationale de la
Recherche.

212

Cosyne 2008

Friday evening, Poster session II-94

Independence of the granule cell’s dendritic branches may lead
to multiple place ﬁelds in the dentate gyrus
Balázs Ujfalussy1 , Tamás Kiss1 , and Péter Érdi1,2
1

Department of Biophysics, KFKI Research Institute for Particle and Nuclear
Physics of the Hungarian Academy of Sciences, Budapest, Hungary,
2
Center for Complex System Studies, Kalamazoo College, Michigan, USA
According to classical theoretical studies, the dentate gyrus performs an orthogonalization on it’s input,
producing sparse and efﬁcient code for associative memory in the CA3 of the hippocampus [4, 6]. The
current view of the underlying mechanism is competitive learning, i.e., there is a strong competition between
granule cells, and the synaptic strength between the active cells and the active inputs (perforant path axons)
are modiﬁed according to Hebb’s rule. As the result of this learning, each neuron will code a speciﬁc feature
of its the input.
Discovery of grid cells in the entorhinal cortex [2] which serves as the main input system for the dentate
gyrus, allowed to test this hypothesis under more realistic conditions. Indeed, modelling studies [1, 5]
showed, that competitive learning on periodic grid cell input leads to the formation of single place ﬁelds like
in the CA regions of the rodent hippocampus. However, further experimental studies revealed that majority
of the granule cells in the dentate gyrus ﬁred at several distinct places [3]. It was also shown that these
place ﬁelds were independent, i.e., their distribution was irregular and they changed incoherently during the
transformation of the environment.
We assume, that the morphology of the granule cell could dissolve this inconsistence: these neurons have
several, long dendritic branches, that join together in the proximity of the soma. We test whether the cell
could respond to several distinct features (locations), participating in different memories, being activated
through different dendritic branches. We set up a simple model for the somato-dendritic interactions of
granule cells, and we derive that it is statistically very unlikely that single dendritic branches could activate
the soma under physiological conditions (e.g., sparseness, number of branches) with statistically independent inputs and equal synaptic weights. We show this width linear, sigmoid and quadratic integration of the
synaptic inputs within a dendritic branch. We propose, that a local learning rule in the dendritic branches
could drive the system toward the independence of the branches.
Acknowledgments This work was supported by ICEA grant IST-027819 IP.
References
[1] M C Fuhs and D S Touretzky. Journal of Neuroscience, 26:4266–4276, 2006.
[2] T Hafting, M Fyhn, S Molden, M-B Moser, and E I Moser. Nature, 436:801–806, 2005.
[3] J K Leutgeb, S Leutgeb, M-B Moser, and E I Moser. Science, 315(5814):961–6, 2007.
[4] A Marr. Philosophical transactions of the Royal Society of London. Series B, Biological sciences,
262:23–81, 1971.
[5] E T Rolls, S M Stringer, and T Elliot. Network, 17:447–465, 2006.
[6] A Treves and E T Rolls. Hippocampus, 4:374–391, 1994.

213

Friday evening, Poster session II-95

Cosyne 2008

Spatio-temporal spike dynamics: Localization of single cell currents based on extracellular potentials patterns
Zoltán Somogyvári1 , István Ulbert2 , and Péter Érdi1,3
1

Department of Biophysics,
KFKI Research Institute for Particle and Nuclear Physics
of the Hungarian Academy of Sciences, Budapest, Hungary,
2
Research Institute for Psychology of the Hungarian Academy of Sciences,
Budapest, Hungary
3
Center for Complex System Studies, Kalamazoo College, Michigan, USA
Traditional current source density calculation method (CSD) method allows calculation of transmembrane
current source distribution on neurons from the extracellular potential patterns, thus provides important information for neurophysiology. The traditional CSD method is based on strong physical foundations, but in
most cases, it is applied onto one dimensional data, so derivatives according to the orthogonal dimensions
are neglected, in the lack of information. This one dimensional method uses an implicit assumption, that
current source density changes can be neglected in these two dimensions on the spatial scale of the electrode.
In other words it assumes laminar source distribution, with inﬁnitely large, homogeneous laminar sources.
Considering the laminar organization of the cortex, this can be a good approximation in case of large population activities such as epilepsy or evoked potentials, but certainly not valid in case of single cells. Thus
traditional one dimensional CSD method gives incorrect results for spatial potential patterns, originated
from a single neuron. To solve this problem, a new spike CSD (sCSD) method was designed, which ﬁts
more to the properties of individual cellular sources, thus it is able reconstruct the cellular transmembrane
currents, based on extracellular potential measurements. This new method is based on the inverse solution
of the Poisson-equation.
The new method was tested on simulated data and its performance was compared to the traditional CSD
method. It was shown, that the sCSD method was able to reconstruct the original source distribution with
much higher accuracy than the traditional method and precisely determinate the cell-electrode distance as
well.
Our new method was applied to in vivo measured action potentials. These spikes were measured in cat
primary auditory cortex with a 16 channel chronically implanted linear probe. Using our new method, many
ﬁne details of the spatio-temporal dynamics of spikes were uncovered. Dendritic back propagation was
proven to be much more frequent than it was known before, it was observable in every cell. The speed
of back propagation was typically different in the apical and basal directions. In contrast to the literature,
forward propagation preceding the spikes was also observable. In perspective, this new method raises the
possibility of identifying synaptic inputs, which causes a cell ﬁre.

Acknowledgments

This work was supported by ICEA grant IST-027819 IP.

214

Cosyne 2008

Friday evening, Poster session II-96

The role of interneuron diversity in the cortical microcircuit for
attention
Calin I Buia1,2 and Paul H Tiesinga1
1

Computational Neurophysics Laboratory, Physics and Astronomy, University of
North Carolina at Chapel Hill
2
Children's Hospital Boston, Harvard Medical School
Receptive fields of neurons in cortical area V4 are large enough to fit multiple stimuli, making V4 the
ideal place to study the effects of selective attention at the single neuron level. Experiments have revealed
evidence for stimulus competition and have characterized the effect thereon of spatial and feature-based
attention.
We developed a biophysical model with spiking neurons and conductance-based synapses. In order to
account for the comprehensive set of experimental results, it was necessary to include in the model, in
addition to regular spiking excitatory (E) cells, two types of interneurons: feedforward interneurons (FFI)
and top-down interneurons (TDI). Feature-based attention was mediated by a projection of the TDI to the
FFI, stimulus competition was mediated by a cross-columnar excitatory connection to the FFI, whereas
spatial attention was mediated by an increase in activity of the feedforward inputs from cortical area V2.
The model predicts that spatial attention increases the FFI firing rate, whereas feature-based attention
decreases the FFI firing rate and increases the TDI firing rate. During strong stimulus competition, the E
cells were synchronous in the beta frequency range (15-35 Hz), but with feature-based attention they
became synchronous in the gamma frequency range (35-80 Hz). We propose that the FFI correspond to
fast-spiking, parvalbumin-positive basket cells and that the TDI correspond to cells with a double-bouquet
morphology which are immunoreactive to calbindin or calretinin.
Taken together, the model results provide an experimentally testable hypothesis for the behavior of two
interneuron types under attentional modulation.

Acknowledgments
This work was supported by the Human Frontier Science Program.

215

Friday evening, Poster session II-97

Cosyne 2008

Human visual attention networks revealed by fMRI coherency
analysis
T.Z. Lauritzen1,2,3, M. D’Esposito2,3, D.J. Heeger5, and M.A. Silver2,3,4
1

Redwood Center for Theoretical Neuroscience, 2Helen Wills Neuroscience
Institute, 3Henry H. Wheeler, Jr. Brain Imaging Ctr., 4School of Optometry,
University of California, Berkeley,
5
Dept. of Psychology and Center for Neural Science, New York University.
Purpose: To test the hypothesis that posterior parietal cortex provides top-down, topographic, spatial
attention signals to early visual cortex.
Background: Visual spatial attention enhances perception at attended locations. Functional magnetic
resonance imaging (fMRI) studies have shown that allocation of attention increases fMRI signals in
portions of early visual cortex that retinotopically represent the attended location, even in the absence of a
visual stimulus. When attention is maintained during a delay period, these visual cortical signals are
sustained for the duration of the delay period [2]. IPS1 and IPS2 are topographically-organized areas in
human posterior parietal cortex that may transmit top-down spatial attention signals to early visual cortex
[1].
Methods: We employed fMRI and computed coherency among visual cortical areas and IPS1 and IPS2
during periods of sustained visual spatial attention in the absence of visual stimulation. Subjects
performed a visual detection task in which a variable-duration delay period preceded target presentation.
The target was presented on half of the trials in an annulus surrounding the fixation point. During the
delay period, subjects continuously maintained attention at the visual field locations corresponding to the
annulus, in anticipation of target presentation [2]. Coherency yields a measure of magnitude (strength of
coupling between two time series) and phase delay (temporal differences in activity). We calculated
coherency magnitudes and phase delays during periods of sustained visual spatial attention for every pair
of the following cortical areas: V1, V2, V3, V3A, V3B, V7, IPS1, and IPS2. Coherency analysis was also
performed for the same areas during periods of visual fixation. The magnitudes and phase delays during
fixation were subtracted from the corresponding measures during sustained attention.
Results: High attention-specific coherency magnitudes were measured in two clusters between IPS1/IPS2
and visual cortical areas as well as among early visual areas V1, V2, V3, and V3A. Substantially lower
coherency magnitudes were observed among mid-level and early visual cortical areas. Coherency phase
delays showed that activity in IPS1 and IPS2 led activity in visual cortical areas.
Conclusion: Both the coherency magnitude and phase delay results are consistent with transmission of
top-down spatial attention signals from IPS1 and IPS2 to early visual cortex.
Acknowledgments
This work was supported by the Lundbeck Foundation and NIH ROI MH63901.
References
[1] Topographic maps of visual spatial attention in human parietal cortex. M.A. Silver, D.B. Ress and
D.J. Heeger, J. Neurophysiol. 94:1358-1371, 2005.
[2] Neural correlates of sustained spatial attention in human early visual cortex. M.A. Silver, D.B. Ress
and D.J. Heeger, J. Neurophysiol. 97:229-237, 2007.

216

Cosyne 2008

Friday evening, Poster session II-98

Pre-stimulus neural activity predicts success in ignoring
Kristina Visscher1 and Robert Sekuler1
1

Brandeis University

It is well known that attention alters neural processing as shown by behavior and neural measures [1].
However, neither the time-course of such effects nor their relationship to ongoing background activity is fully
understood. We set out to test the idea that background neural activity just prior to stimulus presentation both
reﬂects the subject’s attentional set and inﬂuences subsequent stimulus-driven neural activity and behavior.
We made coordinated behavioral and EEG measurements in a recognition memory task with sequentially
presented arrays of oriented bars. In one condition, subjects tried to remember just one array, while ignoring
a later array. In another condition, subjects tried to remember both arrays. Subjects noted whether the
remembered array(s) matched a probe array(as in [2]).
Using this behavioral paradigm, the success or failure of ignoring can be assessed for individual subjects
by examining the difference in recognition with memory load. On average, when subjects were required
to remember two arrays, the second array interfered with the memory for the ﬁrst, resulting in poorer performance: lower proportion correct and slower reaction times. When subjects were instructed to ignore the
second stimulus, they were able to overcome this interference, bringing reaction time and proportion correct
to values statistically indistinguishable from the condition in which only one stimulus was presented. These
behavioral measures of interference are an objective index of how effectively each subject ﬁltered out the
task-irrelevant array.
Increased EEG power in the α band reﬂected decreased visual attention to the ignored array. In addition,
event-related potentials showed greater amplitude in the attended as opposed to the ignored condition. The
convergence with previous results supports the idea that we had successfully manipulated the attentional
system.
With this powerful tool for examining EEG correlates of dynamic within-trial and between-trial changes in
attention, we examined how pre-stimulus EEG oscillatory activity modulated memory performance. High
α-band power just prior to the to-be-ignored stimulus predicts whether that stimulus will be successfully
recognized. This result is consistent with the idea that the brain uses dynamic modulations in α-band
activity in order to suppress potentially interfering or task-irrelevant information
Acknowledgments
This work was supported by NIH grants MH068404 and T32 NS07292. Thanks to John Langton
and Tim Hickey for use of the exploratory data analysis tool NDVis, which can be downloaded at
http://neuron.cs.brandeis.edu
References
[1] Hillyard, S.A. and Anllo-Vento, L. (1998). Event-related brain potentials in the study of visual selective
attention. PNAS, 95(3):7817.
[2] Vogel, E.K. and Machizawa, M.G. (2004). Neural activity predicts individual differences in visual working memory capacity. Nature, 428(6984):74851.

217

Friday evening, Poster session II-99

Cosyne 2008

Foreground and background at the cocktail party: A neural and
behavioral study of top-down and bottom-up auditory attention
Mounya Elhilali, Juanjuan Xiang, Shihab A. Shamma, and
Jonathan Z. Simon
University of Maryland, College Park
The mechanism by which a complex auditory scene is parsed
into coherent objects depends on poorly-understood interactions
between task-driven and stimulus-driven attentional processes.
We illuminate these interactions in a simultaneous behavioralneurophysiological study, in which we manipulate subjects’
attention to different features of an auditory scene (with a regular
target embedded in an irregular background). Our experimental
results reveal that target focused attention correlates with a
sustained increase in the neural target representation, beyond
auditory attention’s well-known transient effects. This
enhancement, in both power and coherence, occurs not between
states of attending and non-attending, but rather between two
separate attentional states, each focusing on different acoustic
features of the stimulus. The enhancement originates in auditory
cortex and covaries with both the behavioral state appropriate to
the task and the bottom-up saliency of the target. Furthermore,
the target’s perceptual detectability improves over time,
correlating strongly with the target representation’s neural
buildup. These results have substantial implications for models
of foreground/ background organization and mechanisms
mediating auditory object formation.
Figure Caption: (a) Power enhancement across frequencies: The
neural response at the target rate (only) shows a significant
increase from zero (bootstrap across subjects, p < 10-4). (b) Phase
enhancement across frequencies: Phase coherence analysis at the
target rate (only) shows a significant increase from zero
(bootstrap across subjects, p < 0.01) (c) Phase coherence
increases (gray lines) and reductions (black lines) for a representative subject. (d) Hemispheric effects: left hemisphere
enhancement for target task, (bootstrap across subjects, p =
0.001); and right hemisphere enhancement for masker task
(bootstrap across subjects, (bootstrap across subjects, p = 0.04).
Acknowledgments: We thank Jonathan Fritz and David Poeppel
for comments and discussion. We are grateful to Jeff Walker for
excellent technical support. Support has been provided by NIH
grants 1R01DC007657 and (via the CRCNS NSF/NIH joint
mechanism) 1R01AG027573.

218

Cosyne 2008

Friday evening, Poster session II-100

SUN: A Bayesian Framework for Saliency Using Natural
Statistics
Matthew H. Tong, Tim K. Marks, Lingyun Zhang, Honghao Shan
and Garrison W. Cottrell
University of California, San Diego
We propose a deﬁnition of saliency by considering what the visual system is trying to optimize when directing attention. The resulting model is a Bayesian framework from which bottom-up saliency emerges
naturally as the self-information of visual features, and overall saliency (incorporating top-down information with bottom-up saliency) emerges as the pointwise mutual information between the features and the
target when searching for a target. An implementation of our framework demonstrates that our model’s
bottom-up saliency maps perform as well as or better than existing algorithms in predicting people’s ﬁxations in free viewing. Since humans operate in a dynamic world, we go on to extend our model to video
using spatiotemporal ﬁlters and show that the resulting model performs as well as other published results.
Unlike existing saliency measures, which depend on the statistics of the particular image being viewed, our
measure of saliency is derived from natural image/video statistics, obtained in advance from a collection
of natural images/videos. For this reason, we call our model SUN (Saliency Using Natural statistics). A
measure of saliency based on natural image statistics, rather than based on a single test image, provides a
straightforward explanation for many search asymmetries observed in humans; the statistics of a single test
image lead to predictions that are not consistent with these asymmetries. In our model, saliency is computed
locally, which is consistent with the neuroanatomy of the early visual system and results in an efﬁcient
algorithm with few free parameters.

Acknowledgments
This work is supported by NIMH grant MH57075 to GWC, and NSF Science of Learning Center grant to
the Temporal Dynamics of Learning Center #SBE-0542013 (GWC, PI).
References
Lingyun Zhang, Matthew H. Tong and Garrison W. Cottrell (2007) Information Attracts Attention: a Probabilistic Account of the Cross-Race Adavantage in Visual Search. Proceedings of the Twenty-eighth Annual
Cognitive Science Society Conference.

219

Saturday AM Talks-1

Cosyne 2008

Traveling waves in cerebellar cortex mediated by asymmetric
synaptic connections between Purkinje cells
Michael Häusser1, Alanna Watt1, Hermann Cuntz1, Masahiro Mori1,
Zoltan Nusser2 and Jesper Sjöström1.
1

University College London; 2Institute of Experimental Medicine, Hungary.

Understanding how anatomical connectivity and neuronal biophysics interact to govern patterns of
activity in the brain is essential if we are to understand neural computation. Purkinje cells exhibit
spontaneous pacemaker activity and form the sole output of the cerebellar cortex. Although Purkinje cell
axon collaterals were first postulated by Cajal to synapse onto other Purkinje cells, the function of this
connection in the cerebellar cortex is unknown. We have used multiple patch-clamp recordings targeted
with two-photon microscopy to characterize functional monosynaptic connections between GFPexpressing Purkinje cells in the mouse cerebellar cortex. Purkinje cell axon collaterals project
asymmetrically in the sagittal plane, directed away from the lobule apex, and form synapses on other
Purkinje cell somata. Based on our anatomical and physiological characterization of this recurrent
connection, we construct a network model that robustly generates waves of activity traveling along chains
of connected Purkinje cells. We provide direct experimental evidence for traveling waves of activity in
Purkinje cells in sagittal slices from juvenile mice. Since similar waves of activity are essential for proper
circuit formation in several brain regions, we suggest that these waves may play an important role in
wiring the cerebellar cortex and in governing the timing of activity in the mature circuit.

Acknowledgments
This work was supported by grants from the Wellcome Trust and the Gatsby Foundation.

220

Cosyne 2008

Saturday AM Talks-2

Analysis of decision making in an insect’s gap-crossing behavior
using Markov models
A. Aldo Faisal1,2, Jeremy E. Niven 2,3 and Steve M. Rogers 2
1

University of Cambridge, Dept. of Engineering, Computational & Biological Learning Group
University of Cambridge, Dept. of Zoology
3
Smithsonian Tropical Research Institute, Panama
2

Success or failure of particular behavioral tasks often depends on the selection of an appropriate
action based upon the interpretation and accumulation of sensory information. However, it
remains often unclear how sensory information is used to decide upon the appropriate action. We
studied grasshoppers, which had to cross gaps of varying widths (>200 trials in 4 animals and ~5
gap widths; Niven et al., in preparation). These insects choose to either reach or jump across
gaps or refuse to cross at all. Insects displayed a stream of dozens of behaviors (e.g. parallax
movements, probing the gap with antennae, grooming) executed before the final action (reach,
jump, refusal) occurred. These pre-action behaviors are interwoven with each other and are
highly variable from trial to trial. This has previously made it difficult to understand what
behavior (and thus what sensory information) contributed to the decision making process, which
is symptomatic for behavioral studies of decision making in freely behaving animals.
We take a novel, computational approach to overcome this problem and analyze the behavior
leading to the decision. We identified the initiation and termination of each behavior, such as
beginning to groom a feet or finishing to probe the gap with antennae, as a discrete observable
state (“emission states”). Thus, we converted the observed behavioral stream (video data) into a
symbolic sequence of behavioral states (we distinguished 25 states at the beginning).
In a first step, we calculated the empirical transition probabilities between the behavioral states
and reconstructed a Markov chain transition matrix. Unconditional state transitions (transition
probability ~1) in the matrix suggested that we initially distinguished to many states, and so
fused these state pairs together. Thw Markov matrix allowed us to visualize the behavioral
sequence in terms of a directed, weighted graph of observed behaviors and allowed us to
compare the overall behavior (irrespective of final action), to those leading to one of the three
actions by using only the sequences with the respective outcome to reconstruct the transition
probability matrix.
Finally, using decision trees we predicted the final action of a behavioral sequence. Prediction
success rate overall trials was high (86%) after observing only the first 14 behavioral states in the
sequence (sequences were as long as over 100 state transitions) – thus the decision was taken
early on and much of the subsequent sensory behavior must have been only used to fine tune the
action. We propose that this behavioral sequence approach is directly amenable to analysis using
traditional bioinformatics techniques; for example Hidden Markov Models to estimate how
sensory behavior affects internal states representing the build up towards an action decision.
Acknowledgments: We thank S.B. Laughlin for discussions.

221

Saturday AM Talks-3

Cosyne 2008

Caudate activity in a decision-making reaction time task
Long Ding1 and Joshua I. Gold1
1

University of Pennsylvania

Perceptual decision-making is a deliberative process that integrates sensory information with other factors
like bias and reward expectation to generate a categorical choice. It is not known how and where in the
brain these disparate kinds of information are integrated, but one intriguing candidate is the caudate
nucleus in the basal ganglia. Caudate neurons are intimately related to behavioral planning and execution
and also have been shown recently to reflect reward expectation.
To study how the caudate is involved in the decision process, we have trained one monkey on a reactiontime version of a random-dot motion direction discrimination task. This task has been used previously to
study decision-making behavior and its neural correlates in the lateral intraparietal area (LIP). LIP activity
measured during task performance represents an accumulation over time of noisy motion information that
reaches a threshold level of activation just prior to response initiation. These LIP responses and the
monkeys’ behavioral choices and reaction times are consistent with a class of decision models related to
mathematical descriptions of diffusion or random-walk processes that drift noisily to a fixed threshold.
Here we use this well-established computational framework to examine how bias, reward expectation and
sensory evidence are represented in the activity of individual caudate neurons and to relate the caudate
activity to the speed and accuracy of the decision process.
We recorded single-unit activity of putative projection neurons in the caudate while the monkey
performed the motion discrimination task. In our preliminary dataset, we have observed activity that
reflects the strength of the motion evidence in a substantial portion of neurons. Unlike those observed in
LIP, these neural responses did not reach a common threshold just prior to saccade onset and often
persisted into the reward period, consistent with an internal online estimate of reward probability or
choice value. We also observed bursts of firing at saccade onset that were independent of motion strength
but selective to a particular saccade direction, suggesting that these responses might reflect detection of
the threshold associated with a particular choice. Interestingly, the bursts in some neurons were preceded
by elevated firing that began after trial onset and was independent of the strength of the motion stimulus.
This elevated firing ended ~300 ms after onset of the motion stimulus only in trials in which monkey
made a saccade to the null direction, regardless of the actual direction of motion. Such tonic activity
might thus represent the monkey’s initial bias. These results suggest that the caudate nucleus might
provide signals related to reward expectation, threshold crossing and bias that contribute to the overall
decision-making process.
Acknowledgments
This work was supported by NIH 5K99EY018042 to L.D. and the McKnight Foundation, Sloan
Foundation, Burroughs-Wellcome Fund and MH062196 to J.I.G.

222

Cosyne 2008

Saturday AM Talks-4

Cortical topography of intracortical inhibition explains speed of
decision making
Hubert R. Dinse1 and Claudia Wilimzig2
1

Institute for Neuroinformatics, Neural Plasticity Lab, Ruhr-University Bochum,
Germany, 2Div. of Biology, California Institute of Technology, Pasadena CA, USA
The functional consequences of the topographical arrangement of cortical maps of the body surface are
still debated. To address these questions we used a tactile multiple-choice reaction time (RT) task which
requires selecting a given finger out of all ten of both hands [1] In this task the distribution of RTs
displays a striking inverted U-shape, where the RTs for the middle fingers of each hand are significantly
slower. This effect is restricted to the condition of selection among all fingers as all fingers show identical
RTs when tested in a dual choice task. To investigate a role of cortical topographies in the speed of the
selection decision, we applied tactile coactivation [2-7], a stimulation protocol based on Hebbian learning
to induce changes of the cortical hand representation through cortical plasticity. Changes in topography
influenced RTs as after coactivating the right middle finger, RTs on this finger were significantly
shortened resulting in the inverted U-shape distribution to almost disappear. This effect was restricted to
the multiple choice task as no changes of RTs were observed in a dual-choice task. Accordingly, early
stages of somatosensory cortical areas are involved in this task and their topographies play an important
role in determining the speed of selection processes.
We used neural fields as an approach to population activation [8,9] with different subgroups of neurons
coding for different fingers and interacting through Mexican-hat type interaction with recurrent excitation
and lateral inhibition to account for our experimental results. Simulations show the inverted U-shape
distribution of RTs as an emergent consequence of the lateral inhibition within cortical representational
maps indicating a crucial role of interaction in the process of decision making. By weakening the strength
of inhibition [10] we can model the influence of coactivation on the level of changes of cortical
topography and its influence on RTs. Our model thus attributes the emergence of the inverted U-shape
distribution and its modification through learning to lateral inhibition within cortical topography. While
the role of cortical topographies for localization is straightforward, we provide evidence that interaction
processes within cortical maps are also crucial for selection and thus functions related to decision making.
References
[1] Alegria J, Bertelson P (1970) Acta Pychologica 33: 36-44
[2] Godde B, Stauffenberg B, Spengler F, Dinse HR (2000) J Neurosci 20: 1597-1604
[3] Pleger B, Dinse HR, Ragert P, Schwenkreis P, Malin JP, Tegenthoff M (2001) Proc Natl Acad Sci
USA 98: 12255-12260
[4] Dinse HR, Ragert P, Pleger B, Schwenkreis P, Tegenthoff M (2003) Science 301: 91-94
[5] Pleger B, Foerster AF, Ragert P, Dinse HR, Schwenkreis P, et al. (2003) Neuron 40: 643-653
[6] Dinse HR, Kalisch T, Ragert P, Pleger B, Schwenkreis P et al. (2005) Transaction Appl Perc 2: 7188
[7] Seitz A, Dinse HR (2007) Curr Op Neurobiol 17: 1-6
[8] Amari S (1977) Biol Cybern 27: 77-87
[9] Amari S (1980) Bull Math Biol 42: 339-64
[10] Höffken O, Veit M, Knossalla F, Lissek S, Bliem B, Ragert P, Dinse HR, Tegenthoff M (2007) J
Physiol 584: 463–471

223

Saturday AM Talks-5

Cosyne 2008

Adaptive Gain Control in the Auditory System
David McAlpine
University College London
The principle of adaptive coding in sensory neurons is widely accepted, although outstanding questions
remain. One concerns the extent to which adaptive coding constitute an emergent property of a particular
level of processing, or is inherited from neurons upstream - quantifying the relative contributions of these
components for any one level of processing can be difficult as relatively few stimulus parameters exist
that do not influence processing at the very earliest stages of a sensory system. Another concerns the
extent to which adaptive coding constitutes a means for improving behavioural performance per se. Here,
I report adaptive coding of inferior colliculus (IC) neurons in anaesthetized guinea pigs to interaural time
differences (ITDs) - small differences in the timing of a sound at the two ears used to determine azimuthal
location of a sound. ITDs are first processed in the medial superior olive (MSO) of the brainstem, making
them particularly suited for determining the site at which central adaptive mechanisms might arise contributions below the first stage of binaural integration can be excluded. Stimuli consisted of
interaurally-delayed white noise of 5s duration (noise seed and ITD sequence randomized across 75
repeats). ITDs were restricted to the maximum physiological range of the guinea pig (±330μs), and
randomly chosen every 50ms from a defined distribution, with a high-probability region (HPR), from
which ITDs were selected with probability = 0.8. Responses were obtained for 5 HPRs centred at 0, ±132
and ±264ȝs. Both single neurons and the population demonstrated some adjustment in coding accuracy to
ipsilateral-leading sounds, suggesting improvement of coding accuracy (as assessed by Fisher
Information, FI) to stimulus statistics of naturally-encountered ITDs. Nevertheless, this adjustment was
broad (essentially hemispheric) and markedly less than that recently demonstrated for sound intensity in
the same brain region. Although peak FI coincided with the centre of the HPR when the HPR was centred
at zero, this was not the case for the more lateral HPRs, and ITDs were not necessarily most accurately
processed when presented as part of a HPR. Several reasons can be posited to account for the relatively
small effects observed. First, adaptive gain control may require more than a single (MSO to IC) synaptic
stage to emerge. Second, responses to changing HPRs were often multiplicative; regardless of the
statistical distribution, rate-vs-ITD functions often rose from a common location on the ITD axis rather
than showing lateral shifts to follow the HPR. This multiplicative/divisive effect is similar to that
observed when GABA-ergic inhibition is locally blocked/applied to ITD-sensitive IC neurons, and does
not necessarily provide for an increase in FI, since response variance normally increases with rate.
Related to this, response variability often decreased markedly (Fano factors <<1.0) with increasing rate,
despite IC neurons being assumed to show Poisson spiking statistics. Combined, these factors appear to
favour high FI for ITDs relatively close to the midline (i.e. sound sources located frontally) regardless of
the statistical distribution of ITDs. In this regard, it should be noted that a sound heard to one side or the
other often elicits a rapid orienting response, with the interaural cues resetting to zero as the head swiftly
moves to face the presumed location of the source. Thus, the relative failure of peak FI to follow the
stimulus distribution of ITDs may be the result of mechanisms specialized to maximize spatial
information in frontal space when a sound is heard in any location, rather than a failure to adapt to ITD
per se. The relative importance of frontal space was confirmed by psychophysical investigation; ITD
discrimination thresholds improved, and false alarm rates decreased, when sounds with ITDs at or close
to zero were preceded by adapting sounds (1-2s duration) with ITDs of 0 or ±400ȝs. In contrast,
discrimination thresholds for sounds with ITDs close to ±400ȝs were largely unaffected, and could be
worse, when preceded by adapting sounds, regardless of the ITD of the adaptor.

224

Cosyne 2008

Saturday AM Talks-6

Mechanisms for Complex Feature Selectivity in the Songbird
Auditory Forebrain
C. Daniel Meliza1 , Zhiyi Chi2 , and Daniel Margoliash1
1

University of Chicago,

2

University of Connecticut

Neurons in many secondary sensory areas exhibit high selectivity for speciﬁc stimuli. These representations
are thought to underlie the ability of animals to quickly recognize complex objects under a wide range of
conditions, but it is not well understood how this selectivity is built up from the simpler response properties
of neurons at earlier stages of processing. The songs of European starlings (Sturnus vulgaris) incorporate large repertoires of motifs (each a complex vocalization circa 1 s in duration) that are unique to each
individual bird, and contribute to individual recognition by song [1–3]. We explored the receptive ﬁeld
properties of cells in the caudomedial mesopallium (CMM), which respond selectively to learned songs and
may contribute to vocal recognition behavior [4]. In a series of awake-restrained and urethane anesthetized
recordings from 5 starlings, we recorded the responses of 70 well-isolated single units in CMM to novel
and familiar motifs. Out of 59 neurons that gave signiﬁcant responses to at least one motif, 36 (61%) were
signiﬁcantly selective, responding robustly to only a small number of motifs. Selective neurons had signiﬁcantly lower spontaneous ﬁring rates, and exhibited phasic ﬁring patterns, with one or more precisely
timed peaks of excitation. Because these response properties are poorly predicted by linear spectrotemporal receptive ﬁeld (STRF) models [4], we examined instead if CMM neurons were responding to multiple
auditory features of the stimuli. Motifs were decomposed into spectrotemporal features that corresponded
to stereotyped vocal gestures, which are employed by many individuals. Both selective and non-selective
neurons responded robustly to these features when they were presented in isolation, but whereas nonselective neurons tended to give excitatory responses to a broad range of features, selective neurons responded
to exhibited excitatory responses to a few features and suppressive responses to many other features. The
responses of CMM neurons to motifs could be predicted from the sum of the excitatory and suppressive
contributions of the constituent features with a high degree of accuracy (median correlation coefﬁcient =
0.50; inter-quartile range 0.24–0.68; 86 motifs, 40 neurons). These data suggest that CMM neurons achieve
speciﬁcity for particular stimuli by sampling from disjoint regions of an underlying feature space, and by
making use of the temporal interplay between excitation and suppression to further narrow tuning.
Acknowledgments
This work was supported by NIH grants DC007206 and F32DC008752 (CDM).
References
[1] M. Eens. Understanding the complex song of the European starling: an integrative approach. Advances
in the Study of Behavior, 26:355–434, 1997.
[2] T. Q. Gentner and S. H. Hulse. Perceptual mechanisms for individual vocal recognition in European
starlings, Sturnus vulgaris. Anim Behav, 56(3):579–594, Sep 1998.
[3] T. Q. Gentner and S. H. Hulse. Perceptual classiﬁcation based on the component structure of song in
European starlings. J Acoust Soc Am, 107(6):3369–3381, Jun 2000.
[4] T. Q. Gentner and D. Margoliash. Neuronal populations and single cells representing learned auditory
objects. Nature, 424(6949):669–674, Aug 2003.

225

Saturday AM Talks-7

Cosyne 2008

Learning Transformational Invariants from
Time-Varying Natural Images
Charles F. Cadieu and Bruno A. Olshausen
Redwood Center for Theoretical Neuroscience, University of California, Berkeley
How does the brain represent and learn the structure contained in dynamic visual scenes? Previous work
using unsupervised learning in the time domain has shown that sparse coding can uncover direction-selective
components that are tuned to speciﬁc spatial and temporal frequency bands. However, these models do
not capture more complex motion selectivity such as speed tuning or pattern motion, which are response
characteristics found in higher visual area MT.
We have developed a hierarchical, probabilistic generative model that learns the higher-order temporal structure in natural movies and produces representations that are similar to the known properties of visual area
MT. The model consists of three layers: the ﬁrst layer represents the pixel values of dynamic visual stimuli
(natural movies); the second layer decomposes visual information into two sets of variables, one that is
sparse and temporally stable and another that is quickly changing; the top layer learns the sparse, higherorder structure of the quickly changing variables. After learning on time-varying natural images, the second
layer represents the structural visual content (local orientation) separately from the dynamical visual content (motion). This allows the top layer to learn the sparse causes of the dynamical visual content. After
learning, the top layer units code transformational invariants: they are selective for the speed and direction
of a moving pattern, but are invariant to the appearance and spatial frequency content in the moving pattern.
The diversity of units in the middle and top layer provides a set of testable predictions for representations
that might be found in V1 and MT. The top layer represents a variety of transformational invariants and
contains a population that is selective for pattern motion, a characteristic of visual area MT. Interestingly,
because the model is selective to complex motions beyond translation, it predicts a set of MT responses that
extends beyond the component-pattern classiﬁcation. These results suggest that the response properties of
neurons in both primary and extra-striate cortex may be accounted for in terms of an efﬁcient coding strategy
adapted to time-varying natural images.

Acknowledgments
This work was supported by NGA grant MCA 015894-UCB and NSF grant IIS-06-25223.

226

Cosyne 2008

Saturday AM Talks-8

Natural experience drives online learning of tolerant object
representations in visual cortex
Nuo Li and James J DiCarlo
McGovern Institute for Brain Research, MIT
Object recognition is computationally challenging because each object produces a myriad of retinal
images. Yet the visual system somehow solves it effortlessly. Neuronal responses at the top of the
primate ventral visual stream (inferior temporal cortex; IT) have a key response property that likely
underlies this ability -- they are selective among visual objects, yet tolerant to changes in object position,
size, pose, lighting, etc. How this tolerant selectivity is constructed remains a fundamental mystery. One
possibility is that the visual system builds that tolerance via the spatiotemporal statistics of natural visual
experience. Because objects are typically present for relatively long time intervals, while object motion
or viewer motion (e.g. eye movements) cause rapid changes in each object’s retinal image, the ventral
visual stream could construct tolerance by associating neuronal representations that occur closely in time.
If this hypothesis is correct, then we might create “incorrect”
tolerance by targeted manipulation of these spatiotemporal statistics.
Specifically, if we engineered an altered visual world in which some
objects consistently changed identity across retinal position then,
following sufficient exposure to this world, the visual system might
incorrectly associate the representations of those objects at those
positions. The main prediction is that individual IT neurons would
lose their normal position-tolerance (i.e. object preference maintained
across retinal position), and would instead tend to prefer one object at
one position, and another object at the other position (see figure).
We monitored single IT neurons’ position-tolerance in two monkeys while they visually explored our
altered visual world. We used real-time eye tracking to present visual objects at controlled retinal
positions during free viewing: as the animal saccaded toward a specific object (A), it was consistently
replaced by another object (B). This manipulation caused the image of object A at a peripheral retinal
position ("swapped") to be consistently temporally associated with the image of object B on the fovea.
Remarkably, while each animal explored this altered world, its IT neurons gradually began to reverse
their object preferences at the swapped position, exactly as predicted. This effect continued to get larger
for as long as we could hold neurons (~1 hour), it was specific for object position (counterbalanced across
neurons) and object identity, and it cannot be explained by adaptation.
We have previously found that similar manipulations of experience produce changes in the positiontolerance of human object perception [1]. Taken together, our results suggest that the ventral visual
stream acquires and maintains a tolerant object representation via the spatiotemporal statistics of natural
visual experience, without external supervision. The relatively fast time-scale of this unsupervised
learning opens the door to rapid advances in characterizing the crucial spatiotemporal image statistics,
understanding other types of tolerance (e.g. size, pose), and ultimately connecting a central cognitive
ability -- tolerant object recognition -- to cellular and molecular plasticity mechanisms.
References
[1] ‘Breaking’ position-invariant object recognition. Cox DD, Meier P, Oertelt N, and DiCarlo JJ, Nature
Neuroscience 8:1145-1147, 2005.

227

Saturday PM Talks-1

Cosyne 2008

Charting the Human Posterior Parietal Cortex
Sabine Kastner
Princeton University
Much is known about the structural and functional organization of the posterior parietal cortex (PPC) in
non-human primates. The systematic testing of neurons in the intraparietal sulcus (IPS) for the preferred
type of stimulus in association with behavior has led to the definition of a multitude of subregions mainly
associated with the sensory control of actions [1]. In contrast, less is known about the structural and
functional organization of human PPC, because it has proven difficult to define distinct regions in the
human IPS. Using a memory-guided saccade paradigm, we identified six topographically organized areas
in human PPC, which are each characterized by a representation of the contralateral hemifield. IPS1 and
IPS2 are located in the posterior segment of the IPS, while IPS3 and IPS4 are located in its anterior/lateral
segment. We identified a fifth area anterior to IPS4, which extends into the intersection between the IPS
and the postcentral sulcus and is termed IPS5. A sixth area, SPL1, was found to branch off the IPS and
extends into the superior parietal lobule. The identification of a multitude of different areas in human
PPC in individual subjects is an exciting development, since it permits a region-of-interest approach in
studying their response properties.
In several series of studies, we investigated the role of topographically defined human PPC areas in
spatial attention, the encoding of fast and slow eye movements and, by using adaptation paradigms, their
object- and motion-selectivity. Our results revealed functional differences among these PPC areas. The
posterior IPS (IPS1-IPS2) exhibited object-selective responses induced by multiple types of object stimuli
independent of the size and viewpoint of these objects, whereas the other areas along the IPS did not
respond object-selective [2]. Strikingly, these object responses were indistinguishable from those
typically found in ventral stream areas and suggest a second, parallel object pathway in the human. All
topographically organized areas in PPC responded motion-selective with IPS1-IPS3 showing preference
for radial as compared to planar and circular optic flow stimuli. Spatially specific attention signals,
however, were similarly represented in all topographically organized areas. Finally, a gradient
representation of eye movements was found with decreasing responses for saccadic eye movements and
increasing responses for smooth pursuit eye movements from posterior/medial to anterior/lateral regions.
Together, our findings reveal striking similarities in the organization of human and monkey PPC, but
indicate also some important differences.
Acknowledgments
This work was supported by grants from NIH (RO1 MH64043, RO1 EY017699, P50 MH–62196) to S.K.
and a grant from the German Academic Exchange Service to C. S. Konen.
References
[1] Space and attention in parietal cortex. C.L. Colby and M.E. Goldberg. Annual Review of
Neuroscience 22: 319-349, 1999.
[2] Two hierarchically organized neural systems for object information in human visual cortex. C.S.
Konen and S. Kastner. Nature Neuroscience, in press, 2008.

228

Cosyne 2008

Saturday PM Talks-2

A computational model relating changes in the BOLD (Blood
Oxygen Level Dependent) signal to neural activity in cortex
W.G. Gibson1 , L. Farnell1 , and M.R. Bennett2
1
2

School of Mathematics and Statistics, University of Sydney, NSW, Australia,
The Brain and Mind Institute, University of Sydney, NSW, Australia

Brain imaging methods, and in particular fMRI (functional Magnetic Resonance Imaging), do not detect
neural activity directly, but rather changes in the blood ﬂow and oxygenation in neighbouring blood vessels, this being the BOLD (Blood Oxygen Level Dependent) effect. It is still an open question as to how
increased neural activity transmits a signal to nearby arterioles causing them to dilate and thus provide more
oxygenated blood to the region. However, there is an increasing consensus that the type of glial cells known
as astrocytes are vitally involved [1].
We have constructed a computational model of the steps leading from increased glutamate release at neuronal synapses to vasodilation in nearby arterioles. This incorporates a mathematical model of an astrocyte
that we have previously used to account for calcium (Ca 2+ ) waves in two-dimensional arrays of astrocytes
[2]. The steps in the present model are: (1) neural activity leads to glutamate release at synapses; (2) glutamate overspill at these synapses acts on metabotropic receptors on astrocyte processes that surround these
synapses, leading to the production and release of EETs (epoxyeicosatrienoic acids) from the astrocytes,
particularly from their endfeet which are in close proximity to arterioles; (3) these EETs diffuse in the extracellular space and act to hyperpolarize the smooth muscle cells that form the walls of the arterioles; (4) this
hyperpolarization propagates electrotonically along the smooth muscle cells of the arteriole; (5) this in turn
causes the closure of L-type Ca2+ channels in the smooth muscle cell walls and the subsequent decrease in
cytosolic Ca2+ results in vasodilation and hence increased blood ﬂow.
The model successfully accounts for the main observed changes in blood ﬂow in cat visual cortex following
stimulation by high-contrast drifting grating [3] and in rat somatosensory cortex following single whisker
stimulation [4]. Using an extension of the balloon model [5] to multiple compartments, we are also able to
predict changes in the BOLD signal that are in agreement with experiment.
Acknowledgments
This work was supported by ARC grant DP0559268.
References
[1] Role of astrocytes in cerebrovascular regulation. R. C. Koehler, D. Gebremedhin, and D. R. Harder.
Journal of Applied Physiology 100:307-317, 2006.
[2] A quantitative model of purinergic junctional transmission of calcium waves in astrocyte networks. M.
R. Bennett, L. Farnell, and W. G. Gibson. Biophysical Journal 89:2235-2250, 2005.
[3] Compartment-resolved imaging of activity-dependent dynamics of cortical blood volume and oximetry.
I. Vanzetta, R. Hildesheim, and A. Grinvald. Journal of Neuroscience 25:2233-2244, 2005.
[4] Columnar speciﬁcity of microvascular oxygenation and volume responses: implications for functional
brain mapping. S. A. Sheth, M. Nemoto, M. Guiou, et al. Journal of Neuroscience 24:634-641, 2004.
[5] A model for the coupling between cerebral blood ﬂow and oxygen metabolism during neural stimulation.
R. B. Buxton and L. R. Frank. Journal of Cerebral Blood Flow and Metabolism 17:64-72, 1997.

229

Saturday PM Talks-3

Cosyne 2008

A Hierarchy of Temporal Receptive Windows in Human Cortex
U. Hasson1, E. Yang1, I. Vallines3 , D. J. Heeger1, N. Rubin1
1

New York University,

2

University of Regensburg

Real-world events unfold at different time scales, and therefore cognitive and neuronal processes must
likewise occur at different time scales. We present a novel procedure that identifies brain regions
responsive to sensory information accumulated over different time scales. We measured fMRI activity
while observers viewed silent films presented forward (the original intact films), backward, or piecewisescrambled in time. We then compared the reliability of the responses in each brain area to the intact films
with that obtained when the temporal structure was disrupted. Early visual areas (e.g., V1 and MT+)
exhibited high response reliability regardless of temporal structure. In contrast, the reliability of responses
in several higher brain areas, including the superior temporal sulcus (STS), precuneus, posterior lateral
sulcus (LS), temporal parietal junction (TPJ) and frontal eye field (FEF), were affected by information
accumulated over longer time scales. These regions showed highly reproducible responses for repeated
forward, but not for backward or piecewise-scrambled presentations. Responses in LS, TPJ and FEF
depended on information accumulated over long durations (~ 36 s), STS and precuneus over intermediate
durations (~12 s), and early visual areas over short durations (< 4 s).
The dependence of the fMRI responses on temporal order could not be attributed to differences in eye
movements. The measured eye positions were independent of temporal order, and were equally reliable
for forward and backward presentations. That is, observers fixated on similar image locations for similar
durations, but in the opposite order, when the films were presented backwards. Moreover, the
reproducibility of the eye movements suggests a comparable level of engagement while observers viewed
the forward and backward films, removing potential concerns that the unreliable responses to the
backward films were because observers paid less attention to them.
Finally, we found a clear dissociation between the reliability of the responses and response amplitudes.
For example, in the LS and TPJ we observed large response amplitudes for all films, but the responses to
the scrambled and time-reversed films were much less reliable than the responses to the intact forward
films. In a separate behavioral study we confirmed that playing the films backward had a great impact on
their intelligibility. We interpret the strong response amplitudes as reflecting incessant processing, aimed
to extract meaningful information from the stimuli. At the same time, the low reliability of the responses
to temporally disrupted movies indicates a failure to attain a consistent/stereotypical sequence of neural
(and cognitive) states.
We conclude that, similar to the known cortical hierarchy of spatial receptive fields, there is a hierarchy
of progressively longer temporal receptive windows in the human brain. Response reliability provides
information about neural processing that is complementary to that derived from more traditional
measurements of response amplitude, and can uncover phenomena that response amplitudes alone do not
reveal, such as the long temporal receptive windows found in this study.
Acknowledgments
We thank R. Blake, P. W. Glimcher, I. Levy, R. Malach, L. Maloney, J. McDermott, J. A. Movshon, Y.
Nir, B. Pesaran, and R. M. Shapley for helpful discussion. Funding was provided by an International
Human Frontier Science Program Organization long-term fellowship (UH), NIH grants R01-EY11794
(DJH) and R01-EY14030 (NR), and the Seaver Foundation.

230

Cosyne 2008

Saturday evening, Poster session III-1

Looking for hallmarks of generative models in the visual cortex
Gergő Orbán1 , Pietro Berkes2 , Máté Lengyel3,4 , and József Fiser1
1

Brandeis University,
3
Collegium Budapest,

2

Gatsby Computational Neuroscience Unit, UCL,
University of Cambridge

4

A recently emerging computational framework of the visual cortex assumes that it implements a generative
model of (natural) visual input [1, 2]. According to this view, the visual cortex implicitly embodies a statistical model of how external causes (the latent variables of the model) combine to form the visual input (the
observed variables of the model). Given a visual stimulus, the cortex inverts the model (according to Bayes’
theorem), and thus infers which causes are likely to underlie it. Many psychophysical and physiological results are consistent with this hypothesis (see [3], for a review). However, testing this general idea directly is
difﬁcult, since it requires the correct speciﬁcation not only of the generative model putatively implemented
by the cortex, but also of its many implementational details.
An alternative approach is to look for fundamental hallmarks of generative models in the cortex that are
not speciﬁc to any particular model, but are characteristic of probabilistic inference and generation and that
require only minimal assumptions about the implementational details. We argue that one such hallmark
of any generative model which adequately represents its input is a direct relationship between the prior
distribution of latent variables,
X, and their posterior distribution given some data present in the observed


variables Y : P (X) = P (X|Y )P (Y )dY . Under the assumption that neural activity in the visual cortex
represents samples from the distribution of latent variables [4], P (X) and P (X|Y ) correspond to two
different forms of V1 activity: that emerging in the absence of visual input (ie, spontaneous activity, SA), and
that evoked by visual stimulus (EA), respectively. Thus, the above equation predicts that the statistics of SA
must be identical to the statistics of EA (the latter integrated over a natural scene ensemble, P (Y )). Indeed,
physiological recordings have shown that the statistics of EA movies in awake animals are remarkably
similar to those recorded during SA [5].
Based on this framework, we analyze the activity of a hierarchical belief network, as a prototypical generative model, in order to identify other statistical hallmarks of generative models that can be found in the
visual cortex. We examine the effects of probabilistic phenomena such as the relation between evoked and
spontaneous activity, explaining away and contextual effects, and the effect of presenting noisy or ambiguous stimuli to the model. We discuss the kind of statistics that could be collected in in-vivo recordings in
order to verify these effects, including measures based on data from a limited number of neurons.
Acknowledgments
This work was supported by a Sloan Swartz postdoctoral fellowship (GO), the Gatsby Charitable Foundation
(PB), the EU Framework 6 (IST-FET 1940) (ML), and the NKTH (NAP2005/KCKHA005) (ML).
References
[1] Probabilistic Models of the Brain: Perception and Neural Function. RPN. Rao, BA Olshausen, and MS Lewicki,
Eds. MIT Press, 2002.
[2] Vision as Bayesian inference: analysis by synthesis? A Yuille, D Kersten, Tr Cogn Sci, 10:301-8, 2006.
[3] Hierarchical Bayesian inference in the visual cortex. TS Lee, D Mumford, J Opt Soc Am A Opt Image Sci Vis,
20:1434-48, 2003.
[4] Interpreting neural response variablity as Monte Carlo sampling of the posterior. PO Hoyer and A Hyvarinen, In
Adv in Neural Inf Proc Systems, 15: 277284. MIT Press, 2003.
[5] Small modulation of ongoing cortical dynamics by sensory input during natural vision. J Fiser, C Chiu, and M
Weliky, Nature 431:573-8, 2004.

231

Saturday evening, Poster session III-2

Cosyne 2008

Across-channel dependencies between local luminance and contrast in natural images
Jussi T. Lindgren1 , Jarmo Hurri1 , and Aapo Hyvärinen1
1

University of Helsinki

Previous research has suggested that visual systems process luminance and contrast separately (eg. [1,2,3]).
This separate processing has been attributed to weak statistical dependencies between these two quantities
in natural images [1,2].
We studied local luminance and contrast in natural images using established measures, and found that when
these two quantities are examined as spatial channels, spatial across-channel dependencies are revealed [4]
that were not apparent in previous pointwise analyses [1,2].
In particular, we computed local luminance as weighted average (LU M = Ew [x]) and local contrast as weighted standard deviation, normalized by the corresponding local luminance (RM S/L =
STDw [x]/(Ew [x] + L0 ), where L0 is a ﬁxed dark-light coefﬁcient). We used raised cosine taper window as the localizing weighting mask w. This processing is identical to the one used in [1,2]. We then
applied these two operators over whole images in convolutive fashion and examined dependencies that hold
between the resulting two (retinotopic) images that we call spatial channels. This split was performed on
4, 000+ natural images to obtain the channel images used in subsequent statistical analyses.
Our main results are as follows. 1) We show empirically that the luminance channel can be used to compute
a correlate of the contrast channel that closely reﬂects the true channel. This shows that there is a strong
redundancy between local luminance and contrast. 2) We demonstrate that relying on higher-order statistics,
Independent Component Analysis learns paired spatial features for luminance and contrast. These features
are shown to share orientation and localization, with the ﬁlters corresponding to the features dependent in
their outputs. 3) We demonstrate that the found dependencies also exist in artiﬁcial images generated from
a dead leaves model, but in a stronger degree. This implies that more complex conditions present in natural
images may work towards decreasing dependencies between local luminance and contrast.
Our results suggest that the separate processing of local luminance and contrast can not be attributed to their
independence in natural images.
References
[1] Independence of luminance and contrast in natural scenes and in the early visual system. V. Mante et al.,
Nature Neuroscience 8(12):1690-1697, 2005.
[2] Local luminace and contrast in natural images. R. A. Frazor and W. S. Geisler, Vision Research
46(10):1585-1598, 2006.
[3] Double dissociation between ﬁrst- and second-order processing. R. Allard and J. Faubert, Vision Research 47(9):1129-1141, 2007.
[4] Spatial dependencies between local luminance and contrast in natural images. J. T. Lindgren et al.,
submitted, 2007.

232

Cosyne 2008

Saturday evening, Poster session III-3

Estimating optimal MRF potentials with Score Matching
Urs Köster, Jussi T. Lindgren and Aapo Hyvärinen
HIIT and Department of Computer Science, University of Helsinki, Finland
Recently there has been an emerging interest in Markov Random Field (MRF) models with learned ﬁlters
rather than hand-picked clique potentials. One such model, dubbed Field of Experts (FoE) by analogy to
the Product of Experts (PoE) model by Osindero et al. was presented by Black and Roth. The difference is
that data is sampled by convolving a sampling window over larger image patches. We refer to this as dense
sampling, which is in contrast to sparse sampling of individual image patches in ICA or PoE models. Here
we argue that ﬁlters learned by Independent Component Analysis (ICA) are optimal clique potentials, and
that a computationally more expensive model, that explicitly takes into account the translation-invariance
of a MRF is not required. We found a simple proof based on invariance properties of natural images, that
the FoE is equivalent to the PoE. The apparent difference is that the image patches are sampled in different
ways, but we show that this difference vanishes asymptotically. Here we show simulations to support this
proof and show that there are no differences in the ﬁlters learned from the PoE and FoE models.
We performed experiments in the two sampling paradigms, but using Score Matching rather than Contrastive
Divergence (CD) as the estimation principle. In the dense sampling paradigm, 8 × 8 patches were sampled
in the 100 different possible positions from 18 × 18 images. 100 such images were taken from a database of
larger natural images, for a total of 10,000 samples. For the experiment with sparse sampling, we randomly
selected 10,000 patches of size 8 × 8 directly from the database. In both cases the data was whitened
and the dimensionality reduced to 60, from which a complete model with 60 ﬁlters was estimated. The
ﬁlters were L2 -normalized. Experiments on ﬁve times overcomplete models produced similar results and
are therefore not shown. We performed experiments both with a product of Student-t distribution used by
Osindero et al. (not shown) and with a log-cosh nonlinearity widely used in ICA models. The obtained
features are very similar visually and quantitatively, as we show by ﬁtting Gabors to the ﬁlters and analyzing
their tuning properties. Below we show (a) features learned in the dense sampling (FoE) paradigm, and
show the (b) aspect ratio and (c) spatial frequency distribution of ﬁtted Gabor functions uperimposed for the
two paradigms. Broken lines indicate sparse sampling, solid lines dense sampling. These results raise the
possibility that the spatially disjoint FoE ﬁlters obtained in previous work are an artifact due to local minima
obtained with the CD algorithm, and that ordinary ICA algorithms are in fact suitable for estimating MRF
models.
20

25
20

15

15
10
10
5

0
0

(a) Filters from dense sampling

5

1

2

3

(b) Aspect ratios

4

0
0

5

10

(c) Frequency Distribution

Urs Köster is supported by the Alfried Krupp von Bohlen und Halbach-Stiftung

233

15

20

Saturday evening, Poster session III-4

Cosyne 2008

Rats learn visual discriminations in the presence of distractors
Erik Flister, Philip Meier, and Pamela Reinagel
University of California, San Diego
We report that rats can rapidly learn and reliably perform two-alternative-forced-choice (2AFC) visual
discriminations, such as discriminating the orientation of a grating. We further show that rats can perform
visual tasks in the presence of distracting visual stimuli. For example rats can indicate which of two
spatial locations contains a grating patch, while ignoring nearby, similar, task-irrelevant stimuli.
Rats were trained using an automated operant chamber for visual behavior which we present here for the
first time. Home cages are modified to allow free access to the operant chamber, which contains 3 beambreak lick ports mounted on a transparent wall adjacent to a CRT monitor. A standard PC running
Matlab, the Psychophysics Toolbox1,2 and our custom code is used to detect licks, present visual stimuli,
provide auditory feedback, and control water rewards. Rats obtain water exclusively by performing trials,
and require no supplementary hydration to maintain health. Task complexity is increased automatically
as subjects achieve performance criteria. The module was designed to use mostly off-the-shelf
components, to have low total cost, and to require minimal human intervention.

Figure 1 Diagram of automated training environment

Using this system, naive Long-Evans rats (P35) typically perform 300 trials/day and reach sustained 85%
correct performance on a baseline 2AFC task in 10 days. Performance is typically above chance after 4
days. We find that rats can learn a variety of visual tasks in this paradigm. Trained rats generalize to new
conditions, such as the addition of distractors or changes in size, location, spatial frequency or contrast of
stimuli. Under some conditions, rats choose stimulus presentations of only 50 ms while maintaining near
perfect performance. We find that rats can also learn visual discriminations using a compact 3 port
lickometer designed to accommodate head-fixed responses, but that learning is 6 times slower compared
to free-running rats with spatially separated ports.
Acknowledgments
The first and second authors contributed equally, and are listed alphabetically. This work was supported
by the James S. McDonnell Foundation, and by fellowships from NSF (PM) and IGERT (EF and PM).
References
[1] The Psychophysics Toolbox. Brainard DH. Spat Vis. 10(4):433-6, 1997.
[2] The VideoToolbox software for visual psychophysics: transforming numbers into movies. Pelli DG.
Spat Vis. 10(4):437-42, 1997.

234

Cosyne 2008

Saturday evening, Poster session III-5

Exploring information combination across local motion detector
channels to model the perception of motion transparency
Andrew I Meso1 and Johannes M Zanker1
1

Royal Holloway University of London, Egham, Surrey TW20 0EX

The visual system’s input – dynamic patterns of light – is transformed into the firing of a large number of
cortical neurons, which act like a cascade of filters with a range of receptive field scales and specificities.
The coherent perception of surfaces and objects in a scene requires the integration of all available cues so
that the activities of a vast population of firing neurons collectively inform perception. Neural methods of
performing such combination are becoming better understood in a growing body of knowledge intimately
linked to an understanding of neural information coding mechanisms.
Perception of motion transparency is a good example for testing information combination across
spatiotemporal channels. Our test stimuli are image sequences in which vertical grating pairs are moving
transparently in the same direction at different speeds. Having rich (broadband) Fourier spectra to
maximize perceived transparency, these gratings elicit responses across a range of channels, whilst
creating a more challenging computational problem of signal combination and separation than gratings
moving in different directions. To address this problem, we develop a global multi-channel motion
detection model, based on local Reichardt detector units1. It simultaneously computes and displays the
local motion responses to input stimuli for 5 different spatial and two different temporal scales. The key
observation made from the initial model outputs is that the local responses across all channels are
strongest at the spatial positions where the harmonics are all locked in phase. These regions within the
output images identify the separate properties of the transparently moving gratings. A combination of
outputs across channels at any particular location will therefore enable the separation of gratings. This
computational architecture is consistent with what we know about neural processing, which exploits the
retinotopic structure of the cortex to combine relevant outputs from columns with receptive fields centered
on shared image locations. Real moving objects in natural scenes cause responses in multiple channels of
motion sensitive neurons as well as neurons with a range of other specificities. The retinotopic
arrangement of a range of properties, such as frequency tuning could therefore be an easily available
substrate to exploit these inherent stimulus phase relationships to combine relevant neural signals.
We test a linear (additive) and a range of compressive nonlinear schemes for combining local Reichardt
detector outputs into local responses which we then pool into global response histograms. Local responses
reveal that all combination mechanisms appear more informative of transparent components than looking
at any individual channels. We generate the global response histograms to estimate the peak separation of
the stimulus components using a k-means cluster algorithm2. We use the separation measure as our
estimate of transparency strength. We find that a combination nonlinearity based on logarithmic addition
best fits psychophysical data, producing histograms in which prominent transparent component peaks are
well separated.
Acknowledgments
This work is funded by an EPSRC & HolViz Ltd CASE studentship (EP/D504538/1)
References
[1] Reichardt, W. (1961). Autocorrelation, a principle for the evaluation of sensory information by the
central nervous system. Sensory Communication. W. A. Rosenblith. Cambridge, MIT Press: 303-317.
[2] Mahani, A., A. Carlsson, et al. (2005). "Motion repulsion arises from stimulus statistics when analyzed
with a clustering algorithm." Biological Cybernetics 92: 288-291.

235

Saturday evening, Poster session III-6

Cosyne 2008

A new masking technique for natural scenes reveals the saliency of
an image
Claudia Wilimzig1, Rufin VanRullen2, and Christof Koch1
1

California Institute of Technology, Pasadena, CA 2CNRS Toulouse, France

Masking is a key experimental tool to precisely control the visibility of visual stimuli. We
explore a new technique for masking images, in particular natural scenes: briefly flashing (e.g.,
10 ms) a natural scene followed by a brief flash of a negative version of the same scene. The
stimulus and its negative version are inversely related like a photo and its negative and can be
obtained by subtracting the stimulus from the maximum palette entry in each respective color
channel.
This technique is related to experiments showing that for two differently colored lights flashed
subsequently, people report a blended version of both stimuli as if the process underlying
perception integrates over both stimuli [1]. If the visual system perfectly averaged over stimulus
and mask, subjects would report seeing a uniform gray patch. Contrariwise, in some areas of the
picture subjects reported deviations from mere gray perception. We show (1.) that these areas are
systematically related to predictions of computational approaches to saliency ([2], e. g.) such that
our masking technique masks everything but the most salient regions of the image; (2.) that this
effect cannot be achieved by using standard masking techniques or no masking; (3.) that this
masking effect is weakened but not abolished in a dichoptic version of the experiment showing
that retinal mechanisms may contribute but are not sufficient to explain this effect.
These results have significant implications for the neuronal coding of saliency and for
computational approaches to saliency. In particular, we show that a simple thresholded wavelet
transform, computing local contrast intensities with positive and negative values corresponding
to two different polarities (ON- and OFF-center cells), corresponds to people’s percepts. Given
the similarities between percepts and saliency maps, this may provide an alternative, and less
costly, tool for computational approaches to saliency.

Acknowledgments
We thank W. Bair and R. Peters for helpful discussions. This work was supported by the ONR, NGA,
and the Alexander von Humboldt foundation.

References
[1] The duration of the present. R. Efron. Annals New York Acad Sci 138: 713-729, 1967.
[2] Computational modeling of visual attention. L. Itti and C. Koch. Nat Rev Neurosci 2:194-204, 2001.

236

Cosyne 2008

Saturday evening, Poster session III-7

Imaging pattern adaptation and the tilt-aftereffect in visual cortex
Andrea Benucci1, Matteo Carandini2
1

Smith-Kettlewell Eye Research Institute, San Francisco, CA

Prolonged adaptation to a visual stimulus affects the perception of subsequently presented patterns. A
prominent example of this pattern adaptation is the tilt aftereffect (TAE), whereby adaptation to an
oriented stimulus shifts the perceived orientation of following stimuli away from the orientation of the
adaptor. There is debate [1,2] as to whether neural correlates of this pattern adaptation are found as early
as in the primary visual cortex (V1).
Pattern adaptation has two main consequences on V1 neurons: (1) a reduction in responsiveness of
neurons tuned for the adapting pattern [3]; (2) a repulsive shift in tuning curves of neurons that prefer
nearby orientations [1]. From these effects, it is not straightforward to predict the impact of adaptation on
a population, and hence on perception. Indeed, the first effect would produce a TAE [4], but the second
goes in the opposite direction [2].
We directly imaged the responses of a large population of V1 neurons responses in anesthetized cats,
using voltage-sensitive dye imaging. We presented contrast-reversing test gratings having random
orientation (2 s duration), preceded by a contrast-reversing adaptor grating with fixed orientation (1 s
duration). In control experiments the adaptor was a blank screen.
Adaptation decreased the responses of all neurons, especially of those whose preferred orientation
matched the adapting orientation. In some experiments, it attracted population responses toward the
adapting orientation (equivalent to a repulsion of single cell tuning curves). In others, it produced bimodal
population profiles with activity peaks on each side of the adapting orientation. A simple winner-take-all
decoding scheme applied to the population responses predicts repulsion in perceived orientation: the
TAE.
We conclude that pattern adaptation profoundly modifies the profile of population activity in V1,
engaging mechanisms that depend on test orientation, preferred orientation, and adaptor orientation. Its
overall impact on population responses accords with the perceptual phenomenon of TAE.
Acknowledgments
Supported by the National Eye Institute and by the McKnight Endowment Fund for Neuroscience.
References
[1] Adaptation-induced plasticity of orientation tuning in adult visual cortex in visual cortex. Dragoi, V.
Sharma, J. Sur M. Neuron 28(1):287-98, Oct, 2000.
[2] Adaptation changes the direction tuning of macaque MT neurons. Kohn, A. Movshon, JA. Nat.
Neurosci. 7(7):764-72, Jul, 2004
[3] Neural correlate of perceptual adaptation to gratings. Maffei, L. Fiorentini, A. Bisti, S. Science.
182(116):1036-8, Dec 7, 1973.
[4] Visual feature-analyzers and after-effects of tilt and curvature. Coltheart, M. Psychol. Rev. 78(2):11421, March, 1971.

237

Saturday evening, Poster session III-8

Cosyne 2008

Identification of retinal and extraretinal contributions to the LGN
by a phenomenological model of retinogeniculate transformation
Xin Wang1, Friedrich T. Sommer2, and Judith A. Hirsch1
1

University of Southern California, 2University of California, Berkeley

Before visual information from the eye reaches the cerebral cortex, it is
processed by the lateral geniculate nucleus (LGN) of the thalamus. Early
physiological studies showed that thalamic relay cells have spatial receptive
fields much like those of presynaptic ganglion cells [1,2]. This similarity
fostered the idea that LGN is a mere relay that does not perform sophisticated
computation. This notion is challenged, however, by work showing that LGN
receptive fields have diverse temporal structures [3]. This temporal diversity is
likely important for the emergence of cortical selectivity to different features
of the stimulus such as direction selectivity.
To achieve a deeper understanding of geniculate computation, we recorded
intracellularly from the LGN of the cat in vivo. Thus, we could extract the
spike trains of postsynaptic relay cells as well as of presynaptic ganglion cells.
We modeled the geniculate output by a linear-nonlinear-Poisson cascade, a
powerful framework for modeling neural responses in the early visual system
[4]
. The linear filters were divided into two sets, corresponding to the two
stages of a sequential dimensionality reduction process. One set consisted of
retinal filters that spanned a stimulus subspace to which the presynaptic
ganglion cell was sensitive. The second set comprised extraretinal filters
which contributed significantly to the postsynaptic but not to the presynaptic
response. We used various means to identify the linear filters, including
spike-triggered-average (STA), spike-triggered-covariance (STC) [5] and
information-theoretic STA and STC (iSTAC) [6] analyses.
In many cases, we found significant extraretinal filters (Figure 1) with
distinct temporal structures. These extraretinal dimensions contributed
substantially to the information of the postsynaptic spike-triggered ensemble
and strongly modulated the geniculate response (Figure 2). The extraretinal
contributions we identified might result from feedforward inputs from other
ganglion cells and local inhibitory neurons, feedback from the cortex and the perigeniculate, and temporal
filtering by synaptic transmission and integration. Our results help to identify and quantitatively describe
the mechanisms the LGN uses to transform the retinal signals it transmits downstream.
Acknowledgments
This work was supported by NIH grant EY09593 and the Redwood Center for Theoretical Neuroscience.
We thank our colleagues Qingbo Wang, Yichun Wei, Vishal Vaingankar for participation in experiments.
References
[1] Single unit activity in lateral geniculate body and optic tract of unrestrained cats. D. H. Hubel, Journal of Physiology 150: 91-104, 1960.
[2] Specificity and strength of retinogeniculate connections. W. M. Usrey, et al., Journal of Neurophysiology 82(6): 3527-3540, 1999.
[3] Temporal diversity in the lateral geniculate nucleus of cat. J. Wolfe, and L. A. Palmer, Visual Neuroscience 15(4): 653-675, 1998.
[4] Do we know what the early visual system does? M. Carandini, et al., Journal of Neuroscience 25(46): 10577-10597, 2005.
[5] Spike-triggered neural characterization. O. Schwartz, et al., Journal of Vision 6(4): 484-507, 2006.
[6] Dimensionality reduction in neural models: an information-theoretic generalization of spike-triggered average and covariance analysis. J. W.
Pillow, and E. P. Simoncelli, Journal of Vision 6(4): 414-428, 2006.

238

Cosyne 2008

Saturday evening, Poster session III-9

Orientation tuning of facilitatory and suppressive signals from the
‘far’-surround of primary visual cortex neurons.
S. Shushruth1,2, J. M. Ichida1, and A. Angelucci1
1

Moran Eye Center, 2 Neuroscience Graduate Program, University of Utah.

Responses of neurons in the primary visual cortex (V1) to stimuli inside their receptive field (RF) can be
modulated by stimuli in the surround. We have previously proposed that divergent and fast-conducting
feedback connections underlie the ‘far’ surround i.e. the surround region beyond the monosynaptic extent
of horizontal connections within V1. We have implemented this idea into a recurrent network model
based on intracortical inhibition having higher threshold and gain than excitation (Schwabe et al. 2006).
This model predicts that, when the RF center and surround are weakly stimulated, the local inhibitory
neurons are inactive, thus feedforward and intracortical inputs sum to facilitate the response of the center
excitatory neurons. Consistent with this model prediction, we recently reported far surround facilitation
when the RF center was driven by an optimally oriented low contrast grating and the far surround by an
iso-oriented thin annular grating (Ichida et al. 2007).
Weak stimulation of a V1 cell’s RF center can also be achieved using gratings of non-optimal orientation
for the cells. We tested our model prediction of far-surround facilitation at sub-optimal center stimulus
orientations. Using single unit recordings in anesthetized macaque V1, here we demonstrate that
facilitation from the far surround can be observed when the RF is weakly activated by a high contrast
grating of sub-optimal orientation. We characterized the orientation tuning of the far surround facilitatory
and suppressive influence. We find that surround suppression and facilitation are tuned to the orientation
of the stimulus being presented in the RF center. Suppression is strongest and facilitation weakest when
the center and surround are of the same orientation, regardless of whether the grating in the RF center is at
the cell’s preferred orientation. We propose that center-surround interactions result from excitatory and
inhibitory mechanisms of similar spatial extent and that changes in the balance of local excitation and
inhibition, elicited by various stimulus configurations, determine whether facilitation or suppression
results.
References
[1] Schwabe L, Obermayer K, Angelucci A, Bressloff PC (2006): The role of feedback in shaping the
extra-classical receptive field of cortical neurons: a recurrent network model. J Neurosci. 26(36): 9117-29
[2] Ichida JM, Schwabe L, Bressloff PC, Angelucci A (2007): Response facilitation from the
“suppressive” receptive field surround of macaque V1 neurons. J Neurophysiol. 98(4):2168-81

239

Saturday evening, Poster session III-10

Cosyne 2008

Estimating the spatial scale of field potentials in visual cortex
Steffen Katzner1, Ian Nauhaus2, Andrea Benucci1, Dario Ringach2, Matteo
Carandini1
1

Smith-Kettlewell Eye Research Institute, San Francisco,
California, Los Angeles

2

University of

Field potentials (FPs) are commonly used to measure responses of local populations of neurons in cortex.
Recent studies have questioned how local these populations are, and have reported that only responses at
high frequencies originate locally [1, 2]. These results, however, were obtained with stimuli eliciting tonic
responses. Under such conditions, the stimulus-related component of the FPs are hard to separate from
ongoing activity, and tuning needs to be inferred indirectly from other response components.
Here, we used stimuli eliciting phasic responses and determined the spatial scale of FPs by measuring
their selectivity for orientation in the primary visual cortex (V1) of anesthetized cats. Selectivity would
indicate that the spatial extent of the population contributing to this measure is less than 300 Pm, the
average distance between columns preferring orthogonal orientations. We first measured the amplitude
spectrum of ongoing activity, recorded during the presentation of a blank screen. This amplitude
decreased markedly with the inverse of frequency, suggesting that visual responses elicited at frequencies
above 10 Hz would be least affected by ongoing activity. We therefore presented random sequences of
gratings, flashed for 32 ms each, having various orientations, spatial frequencies, and spatial phases. After
measuring the map of orientation preference using voltage-sensitive dye optical imaging, we inserted a
10x10 electrode array into the imaged region of cortical tissue, and simultaneously recorded FPs and
multi-unit (MU) signals. Subsequent to the experiment, we determined the precise position of all
electrodes, relative to the map of orientation preference [3]. Stimulus-triggered averaging of the FP
responses revealed a pronounced selectivity for orientation at about half of the recording sites. In the
corresponding amplitude spectrum, differences between the FP responses to orthogonal orientations were
mostly present at low frequencies. Moreover, the preferred orientations computed from selective FPs
were strongly correlated with the preferences according to the MU signals (r = 0.89), and the preferences
according to the optical map (r = 0.69). Finally, a very simple model integrating optical signals within an
area of few hundred Pm radius could predict the tuning of the measured FPs.
We conclude that stimuli eliciting phasic responses can reveal a pronounced orientation selectivity of FPs
in area V1, which is not restricted to responses at high frequencies. These visually driven signals must
therefore originate from local populations, on a spatial scale of less than 300 Pm.
Acknowledgments
This work was supported by NIH grant RO1-EY017396
References
[1] LFP spectra in V1 cortex: The graded effect of stimulus contrast. J. A. Henrie and R. Shapley,
Journal of Neurophysiology 94(1):479-490, July 2005.
[2] Local field potential in cortical area MT: Stimulus tuning and behavioral correlations. J. Liu and W.
T. Newsome, Journal of Neuroscience 26(30): 7779-7790, July 2006.
[3] Precise alignment of micromachined electrode arrays with V1 functional maps. I. Nauhaus and D. L.
Ringach, Journal of Neurophysiology 97(5):3781-3789, March 2007.

240

Cosyne 2008

Saturday evening, Poster session III-11

+/034567/89/:95;73/87;9<=8406>7968945;479?@G9?N@G948<9OQV
N;WX9Y[[;5G9@4/9\X48[G9+X567W689]487;8G948<9^;88;WX9_56WW;8
=>?@QYZ?\^_`{_|}~?{`Y>?}_}@?Z
Q _ \Q`Y}~ _ ^>}?Z _ `{ _ >QY}~ _ YQZ`>ZQZ _ ?> _ \Q _ @?Z}~ _ Z^Z\Q _ }@Q _ ?`Y\}>\ _ `>ZQQ>QZ _ {`Y_
QYQ\?`> _ }> _ Q}@?`Y _ YQ@?`Z _ `Y _ }Z _ QZ\}~?ZQ _ \}\ _ ^>}?Z _ QQ> _ Z^Z\Q}\?}~~^ _ `> _ \Q_
?QY}Y?}~_Y}>_`{_Z`Y\?}~_}>_`Y\?}~_Z\Y\YQZ_}\Q>?QZ_?>YQ}ZQ_^__ZQ_}\_Q}_~Q@Q~_}>_
\Q`Y}~ _ {YQQ>^ _ \`{{Z _ QYQ}ZQ _ `Q@QY _ >` _ `>Q _ \` _ `Y _ >`~QQ _ }Z _ }Q _ } _ Q\}?~Q _ }>_
}>\?\}\?@Q _ `}Y?Z`> _ }Y`ZZ _ ~\?~Q _ }YQ}Z _ `{ _ `\QY _ ?`Y\}>\ _ \Q`Y}~ _ Y`QY\?QZ _ Z _ }Z _ Y}?_
Z\?~Z_}}\}\?`>_}>_?>\QY}\?`>_\?QZ_Q_}@Q_Q}?>Q_YQZ`>ZQ_^>}?Z_?>_\YQQ_~?>Q_`Y\?}~_
}YQ}Z_}~~_>`>_{`Y_\Q?Y_ZQ?}~?}\?`>_{`Y_@?Z}~_`\?`>_\Q_?~Q_\Q`Y}~_}YQ}__\Q_Q?}~_
ZQY?`Y_\Q`Y}~_}YQ}__}>_\Q_@Q>\Y}~_?>\Y}}Y?Q\}~_}YQ}_ ¡_ __Y`¢Q\Z_\`_`\_\Q_`\QY_
}YQ}Z_Z`_`}Y?Z`>_?\_YQ@Q}~_`YYQ~}\QZ_`{_?QY}Y?}~_~Q@Q~_}>_`{_}Y}~~Q~_Y`QZZ?>
`_Q@}~}\Q_\Q_\Q`Y}~_^>}?Z_`{_>QY}~_YQZ`>ZQZ_Q_YQ`YQ_YQZ`>ZQZ_`{_Z?>~Q_>?\Z_?>_}YQ}Z_
 _ _}> _  ¡_ \` _ ~ZQZ _`{ _£¤ _ `QYQ>Q_ Y}>``\_ `\?`>_ ?>_} _Y}>Q _`{ _?YQ\?`>Z _?~Q_
`>Q^Z_}?>\}?>Q_{?}\?`>_¥}_~ZQ_}Z__ZQ_?>_Y}\?`>_}>_¦_ZQ_?>\QY@Q>Q_Q\QQ>_
\Q_Q_{`ZQ_`>_\`_Q`Z_`{_?>\QYQZ\_\Q_Q~?>Q_{Y`_\Q_?>?\?}~_`>\Y}>Z?Q>\_\`_\Q_ZZ\}?>Q_
YQZ`>ZQ _ }> _ \Q _ `Z\Z\?~Z _ Q~?>Q _ \` _ }ZQ~?>Q _ ?{{QYQ>QZ _ Q\QQ> _ }YQ}Z _ QYQ _ Y`>`>Q_
QZQ?}~~^_?>_\Q_`Z\Z\?~Z_^>}?Z_QYQ_`\_QY_}YQ}Z_QYQ_Z~`QY_\`_Q~?>Q_^_`@QY_}_{}\`Y_
`{_¦_Q_ZQ~}\Q_\}\_\QZQ_Z~`QY_^>}?Z_YQ{~Q\_?>\QY}\?`>_Q}>?ZZ_\}\_?\_Q_?`Y\}>\_{`Y_
QYQ\}~_`>Z\}>^_}>_Q?Z?`>}?>_

`>j8/qz;<[0;8W7
?Z_`Y_}Z_Z`Y\Q_^_§¥¡_Y}>\_¥©ª«_}>_ ?Z?`>_|`YQ_|Q>\QY_Y}>\_¥©«£ª_¡_|}~}

241

Saturday evening, Poster session III-12

Cosyne 2008

The effect of asymmetric time ﬁltering on asymmetric signals
Cornelia Fermüller1 and Hui Ji2
1

University of Maryland,

2

National University of Singapore

Static patterns composed of repeating patches of asymmetric intensity proﬁle elicit forceful illusory motion
[1]. Perception of motion depends on the size of the patches and is found to occur in the periphery for larger
patches and closer to the center of the eye for small patches. We propose as main cause for this optical
illusion erroneous estimation of the image motion due to ﬁxational eye movements, in particular the drifts
[2]. The reason is, that image motion is estimated from the spatial and temporal energy of the signal with
ﬁlters which are symmetric in space, but asymmetric (causal) in time. In other words, only the past, but not
the future, is used to estimate the temporal energy. It is shown that such ﬁlters mis-estimate the motion of
locally asymmetric intensity signals for a certain range of spatial frequencies. This mis-estimation predicts
the perceived motion in the different patterns of [1].
In an experiment with human observers we quantitatively compared the perception of three different signals
by nulling the illusory motion with opposing real motion. We found that our model very well predicts the
relative strength of the experimentally obtained perceived illusory motion.

References
[1] Akiyoshi Kitaoka. http://www.ritsumei.ac.jp/ akitaoka/index-e.html, 2007.
[2] I. Murakami, A. Kitaoka, and H. Ashida. A positive correlation between ﬁxation instability and the
strength of illusory motion in a static display. Vision Research, 46:2421–2431, 2006.

242

Cosyne 2008

Saturday evening, Poster session III-13

Response properties of cortical neurons in the absence of On-center
input
Bartlett D. Moore IV1, Daniel L. Rathbun1, and W. Martin Usrey1
1

University of California at Davis

A key feature of the visual system is the early segregation of On and Off channels that begins in the retina
and is maintained in the projections to primary visual cortex. Although it is generally agreed that proper
integration of these channels is necessary for visual function, relatively few studies have investigated the
consequences of disrupting this integration. In the cat, On and Off channels come together in layer 4 of
visual cortex, where the On and Off subfields of simple cells are constructed from the convergence of Onand Off-center LGN inputs.
We measured the response properties of simple cells and complex cells before and after intraocular
injection of DL-2-amino-4-phosphonobutyrate (APB)—a drug that silences On-center neurons in the
retina. The visual responses of cortical neurons were characterized using drifting grating and white-noise
stimuli. Following APB treatment, activity levels decreased for most cells and some cells became nonresponsive to visual stimuli. For those cells with visual responses, we found little effect of APB on
orientation tuning bandwidth. In contrast, we found a variety of effects on direction selectivity. These
results support the view that orientation tuning is invariant under a variety of conditions and
manipulations, while direction selectivity is susceptible to changes in the spatiotemporal patterns of
afferent input.

Acknowledgments
We thank H. Alitto for helpful discussions, and K. Henning for excellent technical support. This work
was supported by McKnight Foundation and NIH grants EY13588 and EY12576

243

Saturday evening, Poster session III-14

Cosyne 2008

Decoding the Neural Response Populations Underlying fMRI signals
in the Human Brain
Christopher W. Tyler and Lora T. Likova
Smith-Kettlewell Eye Research Institute, San Francisco
Introduction. Neural signals exhibit a wide variety of temporal characteristics throughout the cortex, but it
is difficult to measure the flow of information through multiple cortical regions by neurophysiological
methods. Functional magnetic resonance imaging (fMRI) is often considered to exhibit stereotypical
responses, but detailed examination reveals a wide variety of response waveforms to different types of
brief stimulation within the same cortical region (Likova & Tyler, 2007), which can only be explained in
terms of a corresponding variety of response waveforms from the underlying neural populations rather
than hemodynamic factors. To decode the neural signals underlying fMRI response, we propose a
biophysically-based approach that we term Nonlinear Dynamic Forward (NDF) modeling that
incorporates a nonlinear form of convolution of likely neural signals with a metabolic response kernel to
best account for the BOLD waveform.
Methods. FMRI signals were measured throughout the human brain with a GE Signa 3T scanner with a 1
s sampling rate and a randomized event-related design to stimuli consisting of brief luminance and grating
contrast pulses of 100 and 400 ms duration.
Results. A wide variety of BOLD waveforms was found for these basic stimuli, both in primary and
secondary occipital cortex. The waveforms within each cortical region showed large and systematic
differences across stimulus types (Fig. 1), implying that the GLM approach of convolution with a
canonical Hemodynamic Response Function (HRF) is too restrictive to capture much of the activation.
The NDF analysis revealed that most BOLD waveforms could be characterized to >90% of the variance
with predictors based on just two neural components (varying in amplitude and delay parameters). Note
that most of the estimated neural components are too brief for the waveforms to be visible at the scale of
Fig. 1.

Fig.1. Stimulus types,
HRF (inset), BOLD
waveforms (data –
blue, model fit – red)
and estimated neural
response components
underlying the BOLD
responses to four
types
of
brief
stimulation.

Conclusion. Nonlinear dynamic forward modeling allows us to decode the variety of neural signals
underlying BOLD waveforms measured to brief stimulus events in human visual cortex.

244

Cosyne 2008

Saturday evening, Poster session III-15

Capturing Nonlinear Visual Structure by Recursive ICA
Honghao Shan, Garrison W. Cottrell
University of California, San Diego
The efﬁcient coding hypothesis has been a popular theory in explaining the functional role of low-level
vision. In its spirit, the theory suggests that low-level vision captures the statistical structure of the visual
stimuli with minimum energy cost. When independent component analysis (ICA), a linear implementation
of the efﬁcient coding theory, is applied to natural image patches, the resulting basis functions resemble the
V1 simple cells’ receptive ﬁelds [1, 2].
Recently many efforts have been put into building a hierarchical statistical model which captures nonlinear
structures not captured by ICA. Such a model might provide a functional explanation of higher visual layers
in the human visual pathway. Since higher visual layers share similar anatomical structures as the primary
visual cortex, we hypothesize that they might also work under similar computational principles as V1. Based
on this hypothesis, we derived a hierarchical model, the Recursive ICA (RICA) model [3], which captures
nonlinear visual structures of natural images. This model contains a number of layers. ICA is at work on
each layer and coordinate-wise nonlinear activation functions transform the outputs of the lower layer as the
inputs of the higher layer. The higher layer features have larger receptive ﬁelds than the lower layer features,
and capture more complex visual structures.
We have applied the RICA model on different natural image datasets. The visual structures captured are
qualitatively similar. Speciﬁcally, the second layer of the RICA model exhibits properties similar to what
have been observed in V2 neurons. About 70% of the second layer features correspond to angles, curves and
noncartesian gratings. The receptive ﬁelds of the rest 30% features consist of sub-regions tuned to different
orientations of bars/edges. We applied the learned visual structures for different pattern recognition tasks,
and the resulting classiﬁer outperformed most state-of-the-art computer vision techniques.

Acknowledgments
This work was supported by NIMH grant MH57075 to GWC, and NSF Science of Learning Center grant to
the Temporal Dynamics of Learning Center #SBE-0542013 (GWC, PI).
References
[1] Emergence of Simple-cell Receptive Field Properties by Learning a Sparse Code for Natural Images.
Bruno A. Olshausen and David J. Field, Nature 381:607-609, 1996.
[2] The ‘Independent Components’ of Natural Scenes are Edge Filters. Anthony J. Bell and Terrence J.
Sejnowski, Vision Research 37(23):3327-3338, 1997.
[3] Recursive Independent Component Analysis, Honghao Shan, Lingyun Zhang and Garrison W. Cottrell,
NIPS 2006.

245

Saturday evening, Poster session III-16

Cosyne 2008

The Relationship Between Subthreshold and Superthreshold Ocular
Dominance in Cat Visual Cortex
Nicholas J. Priebe1 and David Ferster2
1
Section of Neurobiology, University of Texas, Austin, Texas, 78712, 2Dept. of
Neurobiology and Physiology, Northwestern University, Evanston, IL 60208
Primary visual cortex (V1) is the site at which three dramatic changes in receptive field properties are
known to occur. It is the site where orientation and motion selectivity first emerge, as well as where
information from the right and left eyes is integrated. This integration of ocular information in V1 is the
first step in a cortical pathway that creates a representation for object depth.
The ocularity of individual neurons has previously been measured by comparing the firing rate responses
elicited by stimuli presented separately to each eye. Hubel and Wiesel used this ocular dominance metric
to demonstrate the columnar organization found in cat V1. Neurons within the same column exhibit
similar ocular dominance. Across V1 ocular dominance changes from neurons preferring only right-eye
input to neurons equally responsive to both eyes to neurons preferring only left-eye input. Hubel and
Wiesel also showed that ocular dominance depends on visual experience during the developmental
critical period. During this critical period, disrupting ocular alignment increases the monocularity of
neurons [1] and closing one eye causes a shift in ocular dominance toward the open eye [2]. Measuring
ocular dominance using firing rate responses has thus been useful in observing changes in V1 brought on
by changes in the visual environment.
Ocular dominance has been used to describe the degree of right and left eye input onto a neuron, but is
based on the neurons output, firing rate, instead of the subthreshold membrane potential. Membrane
potential is more closely related to the synaptic inputs onto neurons, since firing rate must pass through
the spike threshold nonlinearity. Spike threshold has been shown to increase direction selectivity, narrow
orientation selectivity, and enhances the differences between simple and complex cells (for review see
[3]). To determine how spike threshold alters ocular dominance, we recorded membrane potential and
firing rate responses to the preferred stimulus in the right and left eye. Ocular dominance based on firing
rate systematically overestimates the degree of monocularity relative to that based on membrane potential.
All neurons in the database depolarized to both the right and left eye, yet not all neurons increased firing
rate to both the right and left eye. A simple model of spike threshold accounts for the discrepancy
between membrane potential and firing rate ocular dominance. This work raises the question of whether
the shifts in firing rate ocular dominance induced by the changes in the visual environment, overestimate
the amount of changes in the inputs to cortical neurons.
Acknowledgments
This work was supported by NEI grant R01 EY04726
References
[1] Binocular interactions in striate cortex of kittens reared with artificial squint. D. H. Hubel and T. N.
Wiesel, J. Neurophysiol. 28:1041-1059, 1965.
[2] Comparison of the effects of unilateral and bilateral eye closure on cortical unit responses in kittens.
T. N. Wiesel and D. H. Hubel, J. Neurophysiol. 28:1029-1040, 1965.
[3] Inhibition, spike threshold and stimulus selectivity in primary visual cortex. N. J. Priebe and D.
Ferster, Neuron, in press.

246

Cosyne 2008

Saturday evening, Poster session III-17

Emergence of tuning to natural stimulus statistics along the central
auditory pathway
J. A. Garcia-Lazaro, Bashir Ahmed and J.W. H. Schnupp
Department of Physiology, Anatomy and Genetics, Sherrington Building, Parks
Road, Oxford
We have previously shown that neurons in primary auditory cortex (A1) respond more strongly and
reliably to dynamic stimuli whose statistics follow “natural” 1/f dynamics than to stimuli exhibiting faster
(1/f 0.5) or slower (1/f 2) modulations. To investigate where along the central auditory pathway this 1/ftuning arises, we extend our study here to the central nucleus of the inferior colliculus (ICC) and the
ventral division of the medial geniculate nucleus of the thalamus (MGV). We found that while the great
majority of ICC recording sites showed a strong preference for the most rapidly varying stimuli, whose
pitch and amplitude profiles changed according to 1/f £ distributions with small values of £, most
recordings in the MGV did not exhibit a marked preference for any particular £ exponent. Only in A1 did
a majority of neurons respond with higher firing rates to stimuli in which £ takes values near 1. Taken
together, these results suggest that, while tuning to 1/f statistics is relatively common and pronounced in
primary auditory cortex, this property is not inherited from lower stations of the auditory pathway, but
only emerges as signals are passed from the ICC, where neurons clearly prefer more rapidly changing
stimuli, through the MGV, where tuning properties seem to lie somewhat in-between those seen in the
ICC or in A1.

247

Saturday evening, Poster session III-18

Cosyne 2008

Beyond what and where: auditory cortical encoding of pitch, timbre
and location cues
J. Bizley, K. Walker, A. King and J. Schnupp
University of Oxford, UK.
We have investigated the cortical encoding of complex sounds in the ferret using band-pass filtered click
trains (“artificial vowels”). The band-pass filters imposed peaks (“formants”) in the energy spectrum of
the stimuli, which determined the stimulus timbre. The click rate determined the pitch of the sound, and
virtual acoustic space was used to reproduce sound-source location cues. Our stimulus set included 64
artificial vowels, which covered a 3-dimensional parameter matrix of 4 pitches, 4 timbres and 4 spatial
locations. These sounds were presented over earphones to 5 anesthetized ferrets, and extracellular spike
responses were recorded in 5 auditory cortical fields.
The responses were subjected to a variance decomposition analysis by summing spike counts into 20 ms
bins and performing a 2-way ANOVA with time bin and stimulus as factors. We then calculated the
proportion of stimulus-induced variance that could be attributed to azimuth, pitch and timbre for each
unit. Many units showed sensitivity for two, or more, of these sound parameters. Neurons in A1 showed
the greatest sensitivity to stimulus location. Pitch sensitivity was restricted mainly to low frequency areas
where the primary and non primary tonotopic fields meet. In contrast, timbre sensitivity was found
throughout both primary fields, A1 and AAF, and, additionally, in the posterior fields. The total amount
of variance that could be explained by the stimulus-time interactions decreased in higher-order cortical
areas. However, responses in the higher-order fields tended to have a greater proportion of their variance
accounted for by interactions between two of the stimulus parameters, which suggests that neurons in
these areas are more selective for specific combinations of stimulus features. In conclusion, while there is
a degree of functional specialization, we do not find evidence for a clear separation of discrete “what” and
“where” processing streams within the 5 cortical areas studied.

248

Cosyne 2008

Saturday evening, Poster session III-19

The statistics of plant echoes as perceived by echolocating bats
Y. Yovel1, M.O. Franz2, P. Stilz1 and H-U. Schnitzler1
1

University of Tübingen, 2University of Applied Sciences, Konstanz,

Introduction: Microchiropteran bats emit ultrasonic pulses and analyze the echoes to perceive their
surroundings, such that they can orientate in space and acquire food in complete darkness.Inspired by
numerous studies on the statistics of natural images, we try to explore the statistics of plant echoes.
Acoustically, plants can be approximated as stochastic arrays of reflectors. Their echoes are thus
superpositions of many reflections, with statistics that reflect their structure and foliage characteristics.

mean power

Methods: Using a bat-mimicking sonar we emitted bat-like, downsweeps (200-0 kHz) of 4ms duration
and recorded the echoes of 4 tree species that are common in the environment of bats in central Europe.
The echoes were first cross-correlated with the emitted signal to obtain the plant’s impulse response. A
Hilbert transform was then used to calculate the envelope of the
0
impulse response. The envelope can be thought of as a one10
dimensional representation of the spatial arrangement of the plant’s
-1
10
reflectors. We next examined the echo’s power spectrum to asses
the frequencies of periodic structure in the plant, similar to what has
-2
10
been done with natural images [2]. In control experiments we
-3
compared the power spectrum of a single leaf, a single branch and a
10
group of branches, and inspected the changes when systematically
-4
ripping off leaves, i.e. when decreasing leaf density. We also
10 0
1
2
136.0
13.6
10
10
10 0.8
created a computer model that simulates plant echoes.
Figure 1

apple tree
beech tree
blackthorn tree
spruce tree
model tree

spatial distance [cm]

Results and Discussion: On a doubly logarithmic plot the averaged
power spectra of all trees have an inverted sigmoid shape with three approximately linear domains in the
low frequencies corresponding to distances of 140 - 0.8 cm (Fig.1). The borders between the domains,
their slopes and the absolute values of the spectra change between species. These domains represent
different scales of structure. We hypothesize that the first domain is mainly influenced by the gross
skeleton of branches, while the other two are associated with smaller scale structures such as twigs and
neighboring leaves. The control experiment supports this hypothesis: The spectrum of a plant with
decreasing leaf density shows a continuous increase in the power density in the first domain (Fig.2). The
differences between different tree species also lead in the same
direction. The spectrum of black thorn, e.g., has less power in the
-1
10
first domain compared to the spectrum of apple, as one would
expect from its bush-like shape and lack of large, spread-out
-2
10
branches. The characteristic inverted sigmoid shape of the observed
spectra can be predicted by a simple model that treats a plant as a
-3
10
3D array of point reflectors distributed with a characteristic distance
between them (Fig.1). In conclusion, our results suggest an
interpretable relation between the power spectra of plant echoes
21.2
4.21
2.12
10
10
envelopes and the spatial statistics of the reflector array of the plant.
spatial distance [cm]
mean power

100% density
55% density
33% density
18% density

Figure 2

References
[1] Modeling the power spectra of natural images: statistics and informationVan der Schaaf A. & Van
Hateren J.H, Vision Res., 36(17): 2759-2770, 1996

249

Saturday evening, Poster session III-20

Cosyne 2008

A role of subplate neurons in early auditory development
Sharba Bandyopadhyay1, Shihab Shamma1, and Patrick Kanold2
1

Institute for Systems Research, University of Maryland, College Park,
Department of Biology, University of Maryland, College Park

2

Several studies show that, manipulations of the acoustic environment such as overexposure or deprivation
during rearing cause profound changes in the organization of the auditory cortex (A1). However, the
specific mechanisms and circuitry involved for normal development and early experience based changes
in A1 are unknown. Subplate neurons (SPNs) present in the developing white matter during critical
period are one of the first postmitotic neurons in the cortex. They form part of a transient circuitry, at least
in the visual cortex, helping in patterned functional segregation of thalamocortical connections. SPNs are
the first to receive and relay information from the thalamus and hence hold a key intermediate position
controlling the information flow into the developing cortex. Although some amount of work exists on the
role of SPNs in setting up visual cortical circuitry, nothing is known about their specific role during
development in the auditory cortex. In this study based on a computational network model of auditory
cortex we investigate the possible role of SPNs during early auditory development. Based on simulations
of the network that develops with spike timing dependent plasticity (STDP) rules we show that the SPNs
may act as teacher neurons and induce tuning in the input layer of the cortex. The underlying principle in
the model is that the SPNs induce and enhance correlated activity between nearby neurons of similar
frequencies. Manipulations of the early acoustic environment such as tone over-exposure, deprivation and
noise rearing and presence and absence of the SPNs are also studied in this contex. Our results show
disruptions of the tonotopic map and the tuning of individual neurons, replicating many of the changes
that are found experimentally after such manipulations.
Thus subplate neurons might play a crucial role in the development of the auditory cortex and its
plasticity during the critical period by controlling correlated activity between thalamus and cortex and
also between cortical columns.

250

Cosyne 2008

Saturday evening, Poster session III-21

Sparse Representation of Sounds in the Unanesthetized Auditory Cortex
Tomáš Hromádka1, Michael R. DeWeese2 , and Anthony M. Zador1
1

Cold Spring Harbor Laboratory, Cold Spring Harbor, NY 11724, 2 Department
of Physics and Helen Wills Neuroscience Institute, University of California,
Berkeley, CA 94720
How do neuronal populations in the auditory cortex represent acoustic stimuli? Although sound-evoked
neural responses in the anesthetized auditory cortex are mainly transient, recent experiments in the unanesthetized preparation have emphasized subpopulations with other response properties. To quantify the relative
contributions of these different subpopulations in the awake preparation, we have estimated the representation of sounds across the neuronal population using a representative ensemble of stimuli.
We used cell-attached recording with a glass electrode, a method for which single unit isolation does not
depend on neuronal activity, to quantify the fraction of neurons engaged by acoustic stimuli (tones, frequency modulated sweeps, white-noise bursts, and natural stimuli) in the primary auditory cortex of awake
head-ﬁxed rats. We ﬁnd that the population response is sparse, with stimuli typically eliciting high ﬁring
rates (>20 spikes/second) in less than 5 % of neurons at any instant. Some neurons had very low spontaneous ﬁring rates (<0.01 spikes/second). At the other extreme, some neurons had driven rates in excess of
50 spikes/second. Interestingly, the overall population response was well described by a lognormal distribution, rather than the exponential distribution that is often reported.
Our results represent the ﬁrst quantitative evidence for sparse representations of sounds in the unanesthetized
auditory cortex. Our results are compatible with a model in which most neurons are silent much of the time,
and in which representations are composed of small dynamic subsets of highly active neurons.

Acknowledgments
T. H. was an Engelhorn Scholar of the Watson School of Biological Sciences at Cold Spring Harbor Laboratory. We gratefully acknowledge support from the National Institutes of Health, the Swartz Foundation,
the Mathers Foundation, and the Sloan Foundation.

251

Saturday evening, Poster session III-22

Cosyne 2008

Schlieren Imaging of Olfactory Search Strategies in Rats
Venkatesh Gopal1, Timur Selimkhanov1, and Mitra JZ Hartmann2
1

Department of Biomedical Engineering, 2Department of Mechanical
Engineering, Northwestern University, 2145 Sheridan Road, Evanston IL 60208
Olfactory localization - following an odor to its source - is a vital behavior for many species, allowing
them to locate prey, mates, and a variety of other resources. Often, these airborne odorants are transported
by turbulent airflows that make the odor plume a ‘patchy’ filamentary structure. Thus, in the absence of
smooth concentration gradients, finding the source of the plume is not simple because the organism has to
solve the dual problems of (1) searching effectively to encounter the plume, and then (2) maximizing the
information that it gains in its momentary contact with the plume.
As part of a broader study of such strategies across different species, we have begun to study how rats
accomplish olfactory source localization. To this end, we have constructed two novel high-speed
schlieren imaging systems that can visualize with high spatial and temporal resolution, both the stimulus
field (the odor plume) and how the rat moves its sensory surfaces (nostrils) through this field. Schlieren
imaging is a technique by which changes in the refractive index of a transparent medium may be
visualized optically. Thus, airborne odor plumes can be made visible by embedding the odorant in a gas
such as CO2, which has a large refractive index contrast with air. Importantly, this eliminates the need to
seed the flow with visible markers such as smoke which can provide the animal with visual cues.
In the first system, called the High Resolution Schlieren System (HRSS), two high-speed cameras are
used to obtain orthogonal views of the rat as it explores an odor plume. By combining these two views,
the three-dimensional trajectory followed by the nostrils through the odor plume is obtained. The HRSS
can image motions within a cubic volume that is 12.5 inches on a side, with a spatial resolution of
approximately 300 Pm/pixel, and a maximum temporal resolution of 1 frame/ms. A second and much
larger system, the Large Area Schlieren System (LASS), has been constructed with a 5ft.×3ft. arena to
image odor following behavior in rats in a more neuroethologically accurate setting. The field of view is
again covered by two high-speed cameras, giving each camera a spatial resolution of 0.9mm/pixel, and
the same maximum time resolution as the HRSS. Data recorded from both these systems will be
presented.
Combining these novel features of our experimental technique, we propose to characterize the spatial
representation of the olfactory scene that is sampled with each sniff and then quantify how these
“snapshots” in time are correlated with movement, and shed light on the neural computations that are
required to implement such search behaviors.
Acknowledgments
We thank the NSF for financial support. Funding for this work was provided by NSF awards IOB0446391 and IIS-0613568 awarded to MJZH
References
[1] Visualizing the invisible: the construction of three low-cost schlieren imaging systems for the
undergraduate laboratory. Venkatesh Gopal, Julian Klosowiak, Robert Jaeger, Timur Selimkhanov and
Mitra J. Z. Hartmann, European Journal of Physics 2007 (Submitted)

252

Cosyne 2008

Saturday evening, Poster session III-23

Local field potential-single unit interactions in awake mouse cortex
Edgar Galindo-Leon1, Robert C. Liu2
1

Department of Biology, Emory University

The use of local field potentials (LFP’s) recorded off extracellular electrodes has been growing in
popularity. From a practical standpoint, LFP’s are quickly and straightforwardly obtained, making them
an attractive measure of neural activity for multielectrode implant paradigms, where single units (SU’s)
can be hard to isolate. Moreover, in anesthetized animals, the LFP is often used to assess the so-called
UP/DOWN state of the neural network [1]. What exactly though are the neural events underlying the LFP
signal, and how do they relate to SU activity, considered the gold standard for electrophysiological
recordings? We explored this issue by examining LFP and SU data taken from a sensory cortical area of
the awake mouse.
LFP and SU signals are derived from the extracellular field potentials by filtering within either a low (e.g.
<100 Hz) or high (e.g. 300 to 6000 Hz) frequency band, respectively. The raw LFP signal is assumed to
represent non-spiking neural activity, such as synaptic potentials, after-potentials of somato-dendritic
spikes and local membrane fluctuations [2]. However, a recent study reported a correlation between the
phase of the LFP signal and multiunit spiking activity in the barrel cortex of anesthetized rats [1]. We
therefore decomposed our LFP traces into both an amplitude and phase component to see which better
reflected the activity of well-isolated SU’s.
We recorded from the auditory cortex of (non-sedated) awake, head-restrained mice listening to speciesspecific, ultrasonic communication calls. LFP’s and SU’s were recorded on split channels from the same
high impedance electrode placed 300 to 700 microns below the cortical surface. Due to the highimpedance, our SU amplitudes were often large, even after low-pass filtering. Thus, we cleaned our LFP
signals using one of two different methods, both with similar success. We then found that during periods
of spontaneous activity (no stimulus), the LFP phase generally showed clearer structure in relation to an
action potential than the LFP amplitude (47/82 SU sites for phase, versus only 14/82 for amplitude). This
phase structure appears to reflect (1) membrane depolarization due to a barrage of synaptic input, (2)
hyperpolarization due to afterpotentials, and (3) SU firing phase-locked to on-going membrane
oscillations, consistent with [2]. Preliminary analysis also suggests that the pattern of LFP-SU correlation
during spontaneous periods may help predict the SU’s firing profile in response to stimuli.
Acknowledgments
This work was supported by NIH grant DC008343 and the NSF Center for Behavioral Neuroscience.
References
[1] Haslinger R. et al, J Neurophysiol. 96(3):1658-63, 2006.
[2] Logothetis NK., J Neurosci. 23(10):3963-71, 2003.

253

Saturday evening, Poster session III-24

Cosyne 2008

Adaptation effects on spike-timing precision within and across
LGN cells
Gaëlle Desbordes1 , Chong Weng2 , Jianzhong Jin2 , Daniel A. Butts3, Garrett
B. Stanley4 , and Jose-Manuel Alonso2
1

2
Harvard University, MA;
State University of New York, NY;
4
Georgia Tech/Emory University, GA
University, NY;

3

Cornell

It is well-known that adaptation to stimulus properties has important effects on the activity of neurons in
sensory systems. Previous single-cell studies found that the spikes from individual retinal, LGN, and V1
cells show more temporal variability across repeats of sub-optimal stimuli, such as low-contrast stimuli
[1,2,3]. However, it is not known how adaptation affects spike-timing precision in a population of cells.
A short movie of a natural scene recorded from a “cat-cam” [4] was presented repeatedly 64 to 120 times
either at high contrast or at low contrast to anesthetized cats while recording extracellular activity of multiple
single units in the lateral geniculate nucleus (LGN). We found that adaptation does not affect the relative
precision of spike timing, within or across LGN neurons. Although groups of spikes (or “events”) are
less precisely time-locked to the visual stimulus at low contrast than at high contrast, the relative timing
precision of spikes in an event is preserved across different levels of contrast, not only within cells but also
across neighboring cells that display correlated activity.
The maintainance of relative precision could have signiﬁcant implications for the visual neural code. We
have recently shown that ﬁne temporal precision is necessary in representing the more slowly-varying natural
environment [5]. Furthermore, closely-timed spikes across geniculate neurons are more effective in driving
a common downstream V1 cell [6,7]. Therefore, it could be advantageous to minimize any degradation of
precision across neurons.
Acknowledgments
NEI grant EY05253, NGIA grant HM1582-05-C-0009, and SUNY Research Foundation.
References
[1] Berry MJ, Warland DK, Meister M (1997). The structure and precision of retinal spike trains. Proc Natl
Acad Sci USA, 94:5411-6.
[2] Kara P, Reinagel P, Reid RC (2000). Low response variability in simultaneously recorded retinal, thalamic, and cortical neurons. Neuron, 27:635–646.
[3] Uzzell VJ and Chichilnisky EJ (2004). Precision of spike trains in primate retinal ganglion cells. J Neurophysiol. 92:780–789.
[4] Kayser C, Einhauser W, König P (2003). Temporal correlations of orientations in natural scenes. Neurocomputing, 52:117–123.
[5] Butts DA et al. (2007). Temporal precision in the neural code and the timescales of natural vision. Nature, 449(7158):92–5.
[6] Alonso JM, Usrey WM, Reid RC (1996). Precisely correlated ﬁring in cells of the lateral geniculate
nucleus. Nature, 383(6603):815–819.
[7] Usrey WM, Reppas JB, Reid RC (1998). Paired-spike interactions and synaptic efﬁcacy of retinal inputs
to the thalamus. Nature, 395(6700):384–387.

254

Cosyne 2008

Saturday evening, Poster session III-25

Markov Chain Monte Carlo Methods for Decoding Neural Spike
Trains
Yashar Ahmadian1 , Jonathan Pillow2 , and Liam Paninski1
1

Columbia University,

2

University College London

Stimulus reconstruction or decoding methods provide an important tool for understanding how sensory and
motor information is represented in neural activity. We address Bayesian decoding methods based on an
encoding generalized linear model (GLM) [1, 2] that accurately describes how stimuli are transformed into
the spike trains of a group of neurons. The log-concave GLM likelihood is combined with a prior distribution
to yield the posterior distribution over the stimuli that possibly generated an observed set of spike responses.
This posterior is log-concave so long as the prior is, meaning that the maximum a posteriori (MAP) stimulus
estimte can be obtained using highly efﬁcient optimization algorithms [3]. Unfortunately, however, the MAP
estimate can have a relatively large average error when the posterior is highly non-Gaussian.
Here we introduce several Markov chain Monte Carlo (MCMC) algorithms that allow for the calculation
of general Bayesian estimators involving posterior expectations (integrals). An efﬁcient version of the “hitand-run” algorithm [4], exploiting the log-concavity property, is shown to be superior to other MCMC
methods when the prior distribution has sharp edges and corners. The Metropolis-adjusted Langevin algorithm (MALA) [5] was signiﬁcantly superior to other MCMC methods for Gaussian priors. Using these
algorithms we show that for the former class of priors the posterior mean estimate can have a considerably
lower average error than MAP, whereas for Gaussian priors the two estimators have equal efﬁciency.
We also consider the effect of uncertainty in the GLM parameters on the posterior estimators. When this
uncertainty increases, the posterior mean shifts towards the prior mean as the Bayesian decoder relies more
heavily on the prior information and less on the observed spikes. Finally, by using MALA to calculate the
mutual information between the stimulus and response exactly, we verify the validity of a computationally
efﬁcient Laplace approximation to this quantity for Gaussian priors in a wide range of model parameters.
This makes direct model-based computation of the mutual information tractable even in the case of large
observed neural populations, where methods based on binning the spike train fail.
Acknowledgments
We Thank M. Kennel, E. Simoncelli, E.J. Chichilnisky, Y. Ding, T. Teravainen, and K. Rahnama Rad for
helpful conversations. LP and YA are supported by NEI R01 EY018003; LP is additionally supported by an
Alfred P. Sloan Research Fellowship. JP is supported by a Royal Society USA/Canada Research Fellowship.
References
[1] McCullagh, P. and Nelder, J. (1989). Generalized linear models. Chapman and Hall, London.
[2] Paninski, L. (2004). Maximum likelihood estimation of cascade point-process neural encoding models.
Network: Computation in Neural Systems, 15:243–262.
[3] Pillow, J. and Paninski, L. (2007). Model-based decoding, information estimation, and change-point
detection in multi-neuron spike trains. Under review, Neural Computation.
[4] Lovasz, L. and Vempala, S. (2004). Hit-and-run from a corner. In Proc. of the 36th ACM Symposium on
the Theory of Computing (STOC ’04), Chicago.
[5] Roberts, G. O. and Tweedie, R. L. (1996). Exponential convergence of langevin diffusions and their
discrete approximations. Biometrika, 2:341–363.

255

Saturday evening, Poster session III-26

Cosyne 2008

Efﬁcient computation of the most likely path in integrate-andﬁre and more general state-space models
Shinsuke Koyama1 , Rob Kass1 , and Liam Paninski2
1

Carnegie Mellon University,

2

Columbia University

A number of important models in neuroscience may be described in “state-space” form with point-process
observations: a hidden state variable evolves according to some continuous Markovian dynamics, and the
rate of the observed spike trains is some function of this underlying hidden state. Examples include the
integrate-and-ﬁre model [4] and models used in a number of spike train decoding applications [1, 6].
It is of interest to compute the most likely (ML) path that the hidden state variable traversed on a given
trial, given the observed spike trains. The point-process ﬁlter algorithm introduced in [1] computes an
approximation to this ML path, but this approximation is rigorously accurate only in the so-called “highinformation” limit (when we have a large number of spikes per unit time, or when the hidden state dynamics
are nearly deterministic). More recently, [5] discussed methods for computing the ML path exactly (and
in more generality than the state-space setting), but the computational complexity of these general methods
scales like O(T 3 ), where T is the trial length, and are therefore inapplicable for long trials.
Here we develop O(T ) methods for computing the exact ML path. In the case of linear Gaussian state
space dynamics, the ML path in continuous time satisﬁes a second-order nonlinear ordinary differential
equation. This equation may be solved numerically via standard O(T ) relaxation methods. More generally, if the dynamics are linear and driven by innovations with a log-concave density, and the point-process
observations satisfy certain standard assumptions, then the optimization problem is strictly concave (guaranteeing a unique ML path), and the Newton-Raphson algorithm may be applied; each Newton update here
requires just O(T ) time, because the Hessian of the loglikelihood is block tridiagonal. In the case of the
integrate-and-ﬁre neuron with a hard voltage threshold, barrier methods may be applied to solve the resulting
constrained optimization problem [3], again in O(T ) time.
We describe several applications of the resulting methods. First, we may compute the (marginal) likelihood
of the observed data via Laplace’s method [2]; thus we may ﬁt the model parameters via directly maximizing
this marginal likelihood, or alternatively via the expectation maximization algorithm. Second, we may apply
similar Laplace approximation methods to develop more efﬁcient proposal densities for sequential Monte
Carlo (“particle ﬁltering”) applications. Finally, the ML path can be used to provide a good initialization
for iterative algorithms (Monte Carlo or expectation propagation) for computing conditional expectations in
these models.
References
[1] E. Brown, L. Frank, D. Tang, M. Quirk, and M. Wilson. Journal of Neuroscience, 18:7411–7425, 1998.
[2] R. Davis and G. Rodriguez-Yam. Statistica Sinica, 15:381–406, 2005.
[3] L. Paninski. Journal of Computational Neuroscience, 21:71–87, 2006.
[4] L. Paninski, S. Iyengar, R. Kass, and E. Brown. In Stochastic Methods in Neuroscience. Oxford University Press, 2008.
[5] J. Pillow and L. Paninski. Under review, Neural Computation, 2007.
[6] W. Truccolo, U. Eden, M. Fellows, J. Donoghue, and E. Brown. Journal of Neurophysiology, 93:1074–
1089, 2005.

256

Cosyne 2008

Saturday evening, Poster session III-27

Different effects of lateral connections on population coding
Masafumi Oizumi1 , Keiji Miura1,2 , and Masato Okada1,2
1

The University of Tokyo,

2

RIKEN BSI

In many neural systems, a population of neurons is considered to encode information in the external world.
In V1, for example, the ﬁring rates of the neuronal population code orientation of bar stimuli via bell-shaped
tuning curves. Of particular interest is how accurately the brain can estimate stimuli with the population
coding scheme. This problem has been intensively studied especiffally in the context of how correlated
noise affects the accuracy of the population coding. We investigated the effects of correlated noise induced
by lateral connections on the population coding, targeting the orientation selectivity in V1 as follows.
We implemented the network model of V1 which consists of three neuronal populations: LGN neurons,
excitatory V1 neurons and inhibitory V1 neurons (Fig.(a)). Both excitatory and inhibitory neurons in V1
receive the LGN afferences which are tuned to the orientation of stimuli. The lateral interactions between
V1 neurons consist of short-range excitation and long-range inhibition. The strength of lateral connections
are determined by a Gaussian function of the difference between the preffered orientations of V1 neurons.
In this model of a hypercolumn in V1, we calculated how much information can be extracted from the
responses of excitatory V1 neurons. The amount of information is measured by the Fisher information.
To estimate the Fisher information in neural network models taking account of correlation structures is
generally very difﬁcult in both terms of analytical calculations and numerical simulations. To overcome this
difﬁculty, we employed an analytically tractable spiking neuron model called spike response model, and
developed a theory in which neural correlations can be analytically calculated (Fig.(b)). Using our theory of
correlations, the Fisher information can be analytically calculated under the assumption that ﬂuctuations of
ﬁring rates are well approximated by a gaussian probability distribution.


C

LGN neurons
Excitation
Inhibition

(KUJGTKPHQTOCVKQP

In the framework described above, ﬁrst, we investigated the effects of recurrent cortical excitation on information encoding by changing the strength of connections between excitatory neurons. We found that the
Fisher information decreases as the strength of recurrent excitatory connections increases. This information
loss is due to the locally positive correlations induced by recurrent excitation. Next, we examined the effects of lateral inhibition by changing the strength of connections between excitatory neurons and inhibitory
neurons. In contrast to recurrent excitation, when connections from excitatory neurons to inhibitory neurons
are strong enough, we found that the Fisher information increases as the strength of inhibitory connections
increases (Fig.(c)). This increase of the Fisher information is caused by negative correlations induced by
lateral inhibition. Figure (b) shows the correlation structure induced by lateral inhibition. Our results suggest that lateral inhibition is utilized for improving the accuracy of the orientation estimate and, on the other
hand, recurrent excitation may be used for another purpose such as stabilizing activity patterns.

D


̂

̂

Excitatory
neurons

Inhibitory
neurons

̂












257






E

+
+UJWHHNGF

Ǎ+UJWHHNGF






5VTGPIVJQHKPJKDKVQT[EQPPGEVKQPU

Saturday evening, Poster session III-28

Cosyne 2008

Spike pattern distributions in model cortical networks
Joanna Tyrcha1 and John Hertz2
1

Stockholm University,

2

Niels Bohr Institute and Nordita

We can learn something about coding in large populations of neurons from models of the spike pattern
distributions constructed from data. In our work, we do this for data generated from computational
models of local cortical networks. This permits us to explore how features of the neuronal and synaptic
properties of the network are related to those of the spike pattern distribution model. We employ the
approach of Schneidman et al [1] and model this distribution by a Sherrington-Kirkpatrick (SK) model:
P[S] = Z-1exp(½ijJijSiSj+ihiSi). In the work reported here, we analyze spike records from a simple
model of a cortical column in a high-conductance state for two different cases: one with stationary tonic
firing and the other with a rapidly time-varying input that produces rapid variations in firing rates. The
average cross-correlation coefficient in the former is an order of magnitude smaller than that in the latter.
To estimate the parameters Jij and hi we use a technique [2] based on inversion of the Thouless-AndersonPalmer equations from spin glass theory. We have performed these fits for groups of neurons of sizes
from 12 to 200 for tonic firing and from 6 to 800 for the case of the rapidly time-varying “stimulus”. The
first two figures show that the distributions of Jij’s in the two cases are quite similar, both growing slightly
narrower with increasing N. They are also qualitatively similar to those found by Schneidman et al and
by Tkaik et al [3] for data from retinal networks. As in their work, it does not appear to be necessary to
include higher order couplings. The means, which are much smaller than the standard deviations, also
decrease with N, and the one for tonic firing is less than half that for the stimulus-driven network.
However, the models obtained never appear to be in a spin glass phase for any of the sizes studied, in
contrast to the finding of Tkaik et al, who reported spin glass behaviour at N=120. This is shown in the
third figure panel. The x axis is 1/J, where J = N1/2std(Jij) and the y axis is H/J, where H is the total
“field” N-1i(hi+jJij‹Sj›). The green curve marks the Almeida-Thouless line separating the normal and
spin glass phases in this parameter plane. All our data, for N 800 (the number of excitatory neurons in
the originally-simulated network), lie in the normal region, and extrapolation from our results predicts
spin glass behaviour only for N>5000.

[1] E. Schneidman et al., Nature 440 1007-1012 (2006)
[2] T. Tanaka, Phys Rev E 58 2302-2310 (1998); H. J. Kappen and F. B Rodriguez, Neural Comp 10
1137-1156 (1998)
[3] G. Tkaik et al., arXiv:q-bio.NC/0611072 v1 (2006)

258

Cosyne 2008

Saturday evening, Poster session III-29

Time delays and synaptic conductances mediate the evitability
of oscillations in a neural network
Stephen P. Womble, Netta Cohen
Biosystems Group, School of Computing, University of Leeds, Leeds LS2 9JT
UK.
In a recurrently balanced network of leaky-integrate-and-ﬁre neurons, we show that synaptic conductance
kinetics mediate the emergence of oscillations in single cells as well as networks. When inhibitory synaptic
conductance kinetics were signiﬁcantly faster than excitatory kinetics, we found that very high frequency
(≈ 200Hz) population-wide oscillations occurred in a network driven by afferent Poissonian spike trains.
Conversely, when inhibitory synaptic conductance kinetics were signiﬁcantly slower than excitatory kinetics, high amplitude lower frequency (≈ 50Hz) oscillations routinely emerged. However when inhibitory
synaptic kinetics moderately lagged excitatory kinetics, oscillations vanished and quasi-stationary behavior
capable of supporting classical distributed population rate-codes was found. Thus, synaptic time scales (and
in particular the synaptic rise time) can modulate the emergence and elimination of oscillations. Speciﬁcally, the fraction of the conductance delivered in the ﬁrst few (3-5) milliseconds after onset of the rise in
the conductance was found to underlie this modulation. Furthermore, the presence and levels of oscillations
were shown to be largely independent of delay times between pre-synaptic spike release and onset of rise.
The above results were found with similarly distributed spike-propagation delay times between all cells. We
also found that reducing the average delay times in the synapses to and from the inhibitory cells, relative to
the delays between excitatory cells, reduced the amplitude of oscillations and broadened the parameter space
in which classical distributed rate-code-like behaviour occurred. This difference in the time delays correlates
well with the reported physiology of recurrent networks in the neocortex and hippocampus, where inhibitory
basket cells are known to have short delay times compared to pyramidal-pyramidal cell connections.
These results demonstrate the importance of considering synaptic conductance properties (in particular the
often neglected properties of synaptic rise times) and time delays when modeling spiking neural networks.
They also suggest a possible role for slow NMDA receptors in modulating the output of a spiking network
from low-frequency oscillations, via a conventional rate code regime, to high-frequency oscillations. Interestingly, previous studies1 have often reported on the oscillatory tendencies of balanced networks, to the
point where the ability to generate stable (irregular tonic) ﬁring patterns has been questioned. The present
work demonstrates that classical rate-code type ﬁring patterns can propagate even through recurrent networks.
Acknowledgments
This work was funded by the EPSRC, grant EP/C513711. NC was also funded by the EPSRC, grant
EP/C011953.
References
[1] On the transmission of rate code in long feedforward networks with excitatory-inhibitory balance. V.
Litvak, H. Sompolinsky, I. Segev, and M. Abeles, The Journal of Neuroscience 23:3006-3015, 2003.

259

Saturday evening, Poster session III-30

Cosyne 2008

A parametric study of decoding Poisson spike trains by Gaussian
filtering
Sidney R. Lehky
The Salk Institute, La Jolla, CA USA
When analyzing neurophysiological data from central sensory structures, a commonplace practice is to
lowpass filter the PSTH by convolving it with a Gaussian kernel to obtain a better estimate of its temporal
waveform. However, there is currently no objective basis on which to choose the Gaussian width , and
in practice it is selected subjectively. At a theoretical level, such neural decoding has been primarily
studied in peripheral sensory structures where the temporal waveforms of the stimulus and neural
responses are highly correlated, with this correlation providing information for estimating a decoding
kernel. Here we are interested in more centrally located sensory areas, such as inferotemporal cortex
during visual object recognition, where after numerous layers of nonlinear recurrent feedback there is
very weak correlation between the temporal modulations of the sensory stimulus and neural responses. In
such case, selecting the smoothing parameter is more dependent on estimates of general statistical
characteristics of the firing rate function such as mean, standard deviation, and frequency bandwidth,
rather than the specifics of particular stimuli.
In this study we examined how the optimal Gaussian  varied within a statistical parameter space
defining a set of Poisson spike trains. Given an ensemble of synthetic Poisson spike trains generated by
an instantaneous firing rate function  (t) , the problem was to recover an estimated ˆ (t) . Through
simulations we calculated mean integrated squared error (MISE) vs.  curves, whose minima gave the
value of interest. The rate functions were colored noise with Fourier spectra proportional to 1/ f  , =1-3,
normalized to a particular mean and standard deviation over a 1 sec observation duration.
Five example MISE curves are shown in Fig. 1, from five “experiments” in which different numbers of
trials per experiment were combined to form a pooled estimate of  (t) . The MISE curves can be
decomposed into the sum of two factors, the variance error V ( ) that decreases as a function of , and
the biased squared error B( ) that asymptotically increases (Fig. 2). V ( ) shifts up and down with minor
change in shape as a function of the mean of  (t) . B( ) shifts up and down as a function of both the
mean and standard deviation of  (t) . The shape of B( ) is strongly affected by the bandwidth of  (t) as
parameterized by the exponent  of the rate function Fourier spectrum, while V ( ) is not.
The optimal  follows a power law
 opt = aI b where I is the pooled mean
interspike interval (pooling spike trains
from multiple replications into one
global spike train). The multiplicative
factor a is inversely proportional to the
coefficient of variation CV of  (t) . The
exponent b is directly proportional to
the Fourier spectrum exponent  of
 (t) . For sufficiently low spike densities (large mean spike intervals), the best estimate of the temporal
waveform of  (t) is closely approximated by a constant equal to mean spike rate.

260

Cosyne 2008

Saturday evening, Poster session III-31

Real Time Spike Sorting with Optimal Multichannel Filters
Felix Franke1,2 , Michal Natora1 , Clemens Boucsein 3,4 , Matthias Munk5 and
Klaus Obermayer1,2
2
Berlin University of Technology,
Bernstein Center for Computational Neu3
roscience, Berlin,
Neurobiology & Biophysics, Albert-Ludwigs-University
4
Freiburg,
Bernstein Center for Computational Neuroscience, Freiburg,
5
Max-Plack-Institute for Brain Research, Frankfurt
1

For the purpose of studying the mechanisms of information processing in the brain, understanding cooperativeness of neuronal ensembles is essential. Therefore, it is necessary to analyze the simultaneous activity of
several neighboring neurons as obtained from extracellular recordings. Such signals usually contain action
potentials from multiple cells which need to be separated by spike sorting in a reliable fashion so that spike
trains from each individual cell can be used for further analysis. Efﬁcient spike-sorting methods should ideally allow for real time detection and classiﬁcation of spikes, for good sorting performance in the presence
of overlaps and low signal-to-noise ratio and for minimal human interaction or supervision.
Linear ﬁltering with optimal multichannel ﬁlters seems to be a promising approach [1,2]. For every recorded
neuron - and its speciﬁc waveform - an optimal multichannel ﬁlter is constructed. This ﬁlter should optimally
have a high output energy for the correct waveform and zero output energy for noise and waveforms of other
neurons. The task of spike detection is reduced to a simple detection of high peaks in the ﬁlter output. Since
every ﬁlter detects only one putative neuron, spike detection and spike clustering are done in the same step.
Overlapping spikes can be disentangled, because both the ﬁlter operation and the superposition of spikes is
linear.
In contrast to [1,2] we use an iterative algorithm to learn a set of multichannel templates autonomously from
a 10 to 30 second piece of recorded data. From the templates a set of optimal discriminative multichannel
ﬁlters is calculated. Thereafter the algorithm can run in realtime. We evaluate the method with simulated
and experimental data. The simulator convolves multichannel waveform templates from real recordings
with Poisson spike trains and adds Gaussian noise. The algorithm is also tested on simultaneous intra- and
extracellular recordings in slices of rat visual cortex as well as on data from macaque prefrontal cortex.
We compare the results to existing spike sorting methods including thresholding combined with principal
component analysis. We conclude that our algorithm is indeed able to successfully resolve overlapping
spikes and outperforms the other methods under realistic signal to noise ratios.
Acknowledgments
We thank Sven Daehne, David Fernengel and Maria Waizel for their help. This work was supported by
BMBF grants 01GQ0743 and 01GQ0410.
References
[1] Optimal ﬁltering for spike sorting of multi-site electrode recordings, R. Vollgraf, M.Munk & K. Obermayer Network: Computation in Neural Systems 16 (1): 85-113, March 2005.
[2] Improved optimal linear ﬁlters for the discrimination of multi-channel waveform templates for spikesorting applications, R. Vollgraf & K. Obermayer IEEE Signal Processing Lett. 16: 121-124, March 2006.

261

Saturday evening, Poster session III-32

Cosyne 2008

Maximally Informative Dimensions Using an Approximate Mutual
Information Measure and Wavelets
Micah Richert1, Bart Krekelberg2, and Tom Albright1
1

Salk Institute,

2

Rutgers University

We propose an alternative measure of mutual information that - for biological data - is highly correlated
with traditional Mutual Information (MI) but it is faster too compute and suffers less from local maxima.
The Maximally Informative Dimensions (MID) analysis was proposed by Sharpee et al. [1] to estimate 1
or more optimal kernels of a linear model. Optimality was defined as the kernels for which the mutual
information between the output of the model and the response of a neuron was maximal. The most
significant advantage of MID is that it can estimate kernels in a way that is less biased by the stimulus
statistics (since most reverse correlation paradigms do not use Gaussian white noise, this is of great
importance). However, the algorithm also has several shortcomings: the complex computation requires a
large amount of time and the search space has many local maxima.
Our new approach uses two heuristic techniques to reduce the computation time and the number of local
maxima. The first technique is to use an approximate measure of MI. To construct this measure, we
project all stimuli onto the current best estimate of the linear kernel and order the neural responses by the
rank of the projection values of the corresponding stimuli; we refer to this as the ranked-response. The
ranked-response is the estimate of the non-linearity of the neuron. The approximate measure of MI is
based on the intuition that for a biologically relevant kernel, the ranked-response should be smooth, but
not flat. Specifically, we define the Non-Binned Approximate Information (NBAI) as the sum of the low
frequencies in the Fourier power spectrum of the ranked-response (excluding DC). Our simulations show
that the NBAI is approximately proportional to MI.
The second technique aims to reduce the dimensionality of the search space without sacrificing
resolution. As receptive fields tend to have spatial structure, the linear kernel should be highly
compressible by using wavelets. Specifically, the linear kernel should be represented well by a relatively
small number of wavelets. Therefore, we can search for the wavelet weights that maximize information,
as compared to searching for pixel values that maximize information as the traditional MID method does.
In simulation and using real neuronal data the algorithm does not appear to get trapped in local maxima.
Furthermore, finding the first or more kernels of length ~1024 can be done in 1 or 2 hours each (running
in Matlab). This compares favorably to the traditional MID analysis which can take a day or longer. This
new technique opens the door for a much broader adoption of MID techniques because it substantially
reduces the computational expense of finding 1 or more linear kernels.
Acknowledgments
We thank Minjoon Kouh from the Sharpee lab for helpful discussions and running the traditional MID
analysis.
References
[1] Analyzing neural responses to natural signals: maximally informative dimensions. T. Sharpee, N.C.
Rust and W. Bialek, Neural Comput 16: 223–250, 2004.

262

Cosyne 2008

Saturday evening, Poster session III-33

Decoding Perceived Natural Scene Categories from
fMRI Activity in PPA and RSC
Dirk B. Walther1, Eamon Caddigan1, Diane Beck1, and Li Fei-Fei2
1

University of Illinois at Urbana-Champaign,

2

Princeton University

Functional magnetic resonance imaging (fMRI) has enabled understanding of many physiological
processes in the brain. Traditionally, general linear models (GLMs) have been used to contrast the
BOLD activation for different experimental conditions [1], treating each voxel independently.
Given the highly interconnected nature of the brain, there is no a-priori reason to assume
independence of voxels other than convenience of data analysis. Indeed, other groups have been
able to use patterns of voxel activity to decode object identity [2, 3], orientation of gratings [4, 5],
or memory recall [6]. All these studies have used variants of discriminant pattern classification
algorithms, such as linear discriminant analysis (LDA) or support vector machines (SVM).
Among the many important research results afforded by traditional univariate analysis is the
discovery of two visual areas with a general preference for scenes compared to faces and other
objects [7]: the parahippocampal place area (PPA) and the retrosplenial cortex (RSC). Using a
generative Gaussian Bayesian Classifier (GBC), we were able to decode the category of visually
presented natural scenes from patterns of fMRI activity in PPA and RSC, thereby demonstrating
that these areas not only respond to scenes in general, but that they contain information about the
specific kind of scene presented. Comparison of decoding performance reveals a clear advantage
of the generative GBC over the discriminative SVM in this scenario.
Adopting classification rate as a measure for the category-specific information represented in
these areas, we demonstrate decreased category-specificity for inverted versus upright images.
This finding is in qualitative agreement with a drop in human categorization performance for
inverted images of natural scenes. We find a similar effect for small versus large images. We
conclude that multivariate analysis of distributed activity patterns in general, and generative
classifiers in particular, can serve as a useful addition to the arsenal of tools for wresting insights
about human brain function from functional magnetic resonance imaging.
Acknowledgments
This work was supported by the UIUC Critical Research Initiative grant to D.B. and L.F.F., a Beckman
Postdoctoral Fellowship to D.B.W., and a Beckman Predoctoral Fellowship to E.C.

References
[1] Friston, K.J., A.P. Holmes, K.J. Worsley, J.-P. Poline, C.D. Frith, and R.S.J. Frackowiak, Statistical parametric
maps in functional imaging: A general linear approach. Human Brain Mapping, 1995. 2(4): p. 189-210.
[2] Cox, D.D. and R.L. Savoy, Functional magnetic resonance imaging (fMRI) "brain reading": detecting and
classifying distributed patterns of fMRI activity in human visual cortex. Neuroimage, 2003. 19(2 Pt 1): p. 261-70.
[3] Haxby, J.V., M.I. Gobbini, M.L. Furey, A. Ishai, J.L. Schouten, and P. Pietrini, Distributed and overlapping
representations of faces and objects in ventral temporal cortex. Science, 2001. 293(5539): p. 2425-30.
[4] Haynes, J.D. and G. Rees, Predicting the orientation of invisible stimuli from activity in human primary visual
cortex. Nat Neurosci, 2005. 8(5): p. 686-91.
[5] Kamitani, Y. and F. Tong, Decoding the visual and subjective contents of the human brain. Nat Neurosci, 2005.
8(5): p. 679-85.
[6] Polyn, S.M., V.S. Natu, J.D. Cohen, and K.A. Norman, Category-specific cortical activity precedes retrieval during
memory search. Science, 2005. 310(5756): p. 1963-6.
[7] Epstein, R. and N. Kanwisher, A cortical representation of the local visual environment. Nature, 1998. 392(6676):
p. 598-601.

263

Saturday evening, Poster session III-34

Cosyne 2008

Finding the optimal sparse, overcomplete model for natural images by model selection
Richard E. Turner, Pietro Berkes, and Maneesh Sahani
Gatsby Computational Neuroscience Unit, UCL
The principles that underlie the structure of receptive ﬁelds in the primary visual cortex are not well understood. One theory is that they emerge from information processing constraints, and that two basic principles
in particular play a key role. The ﬁrst principle is that of sparsity. Both neural ﬁring rates and visual statistics are sparsely distributed, and sparse models for images have been successful in reproducing some of the
characteristics of simple cell receptive ﬁelds (RFs) in V1 [1,2]. The second principle is overcompleteness.
The number of neurons in V1 is 100-300 times larger than the number of neurons in the LGN. It has often
been assumed that sparse, overcomplete codes might lend some computational advantage in the representation and processing of visual information [3,4]. The goal of this work is to investigate this claim, within the
context of the most common form of encoding model.
Many different sparse-overcomplete models for visual processing have been proposed. These have largely
been evaluated on the basis of their correspondance with neural properties (RF frequency, orientation, and
aspect ratio after learning), on their effectiveness in denoising natural images, or on the efﬁciency with
which they can be used to encode natural images. However, only rarely have questions about the degree of
sparsity, the form of the sparsity, as well as the extent of overcompleteness, been addressed.
Here we formalise such questions of optimality in the context of Bayesian model selection, treating both the
degree of sparsity and the extent of overcompleteness as parameters within a probabilistic model, that must
be learnt from natural image data. In the Bayesian framework, models are compared based on their marginal
likelihoods, a measure which reﬂects their ability to ﬁt the data well, but also incoporates a Bayesian equivalent of Occam’s razor by automatically penalizing models with more parameters than are supported by
the data. We compare different sparse coding models and show that the optimal model seems to be indeed
very sparse but, perhaps surprisingly, only modestly overcomplete. Thus, our results suggest that the datamodelling properties of probabilistic linear sparse-coding models are not sufﬁcient to explain the presence
of an overcomplete code in the primary visual cortex.
Acknowledgments
Supported by the Gatsby Charitable Foundation.
References
[1] B.A. Olshausen and D.J. Field (1996) Emergence of Simple-Cell Receptive Field Properties by Learning
a Sparse Code for Natural Images. Nature, 381, 607–609.
[2] A.J. Bell and T.J. Sejnowski (1997) The ’Independent Components’ of natural scenes are edge ﬁlters.
Vision Research, 37, 3327–3338.
[3] B.A. Olshausen and D.J. Field (1997) Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1? Vision Research, 37, 3311–3325.
[4] H. Lee, A. Battle, R. Rajat, and A.Y. Ng (2007) Efﬁcient sparse coding algorithms. NIPS 19.

264

Cosyne 2008

Saturday evening, Poster session III-35

Spatial organization of large-scale concerted activity in primate
retina
J. Shlens1 , G.D. Field1 , J. L. Gauthier1 , M. Greschner1 , M. I. Grivich1 , A.
Sher2 , A. M. Litke2 , E. J. Chichilnisky1
1
2

Salk Institute for Biological Studies, La Jolla, CA
Santa Cruz Institute for Particle Physics, University of California, Santa Cruz, CA

All visual signals in the brain originate in the electrical activity of retinal ganglion cells (RGCs). Standard
models implicitly assume that RGCs signal information independently of one another. However, several
studies have demonstrated signiﬁcant concerted activity in nearby pairs of RGCs which may fundamentally alter visual signals. Very little is known about the spatial structure of this concerted activity in large
populations of RGCs, particularly in the primate retina.
We performed large-scale multi-electrode recordings of the electrical activity of several hundred ON and
OFF parasol (magnocellular-projecting) RGCs in isolated peripheral macaque monkey retina. The regular
mosaic organization of the recorded cells indicated that we recorded from nearly every cell of both types in a
4x8 degree region of the visual ﬁeld. In the presence of steady, spatially uniform photopic illumination, pairs
of RGCs ﬁred synchronously (within 10 ms) several-fold more often than expected by chance, indicating
signiﬁcant network interactions. Synchrony declined with distance between cells, and was universal among
nearby cells of the same type, indicating that it arises from local and highly stereotyped circuitry.
To probe the spatial structure and scale of network interactions, we measured the total number, contiguous
spatial extent, and number of adjacencies in the collection of cells ﬁring within each 10 ms time bin. To
test whether concerted ﬁring can be explained by known interactions between immediate neighbors in the
mosaic, we used a maximum entropy approach borrowed from statistical mechanics to predict the above
measurements based purely on measured pairwise correlations between neighboring cells. The predictions
of this nearest-neighbor Ising model accurately reproduced the data, although systematic departures were
evident in larger populations indicative of non-local interactions. In summary, the spatial structure of spontaneous activity in parasol cells of primate retina can be understood largely on the basis of single cell activity
and neighbor interactions.
Acknowledgments
We thank C. Hulse for technical assistance; D. Petrusca, W. Dabrowski, A. Grillo, P. Grybos, P. Hottowy,
and S. Kachiguine for technical development; H. Fox, M. Taffe, E. Callaway, and K. Osborn for providing
access to retinas; and S. Barry for machining and the San Diego Supercomputer Center for large-scale data
storage. This work was supported by NSF IGERT DGE-033451 (JS), NSF grant PHY-0417175 (AML),
Burroughs-Wellcome CASI (AS), NIH grant EY018003 (EJ), DAAD (MG), NIH NRSA and Chapman
Foundation (JG), Helen Hay Whitney Foundation (GDF).
References
[1] Shlens J, Field GD, Gauthier JL, Grivich MI, Petrusca D, Sher A, Litke AM, Chichilnisky EJ. ”The structure of multi-neuron ﬁring patterns in the primate retina.” Journal of Neuroscience 26(32):8254-66, 2006.
[2] Schneidman E, Berry MJ, Segev R, Bialek W. ”Weak pairwise correlations imply strongly correlated
network states in a neural population.” Nature 440(7087):1007-12, 2006.

265

Saturday evening, Poster session III-36

Cosyne 2008

Maximizing sensory information with neural populations of arbitrary size
Eizaburo Doi1 , Liam Paninski2 , and Eero P. Simoncelli1
1

New York University,

2

Columbia University

One fundamental goal of sensory systems is to transmit sensory information efﬁciently under the condition
that both the input and the neural representation are noisy, and that the number of available neurons is
limited. Previous results based on this idea provide a great deal of insight into the optimality of sensory
coding, in particular with regard to retinal receptive ﬁeld structure and contrast sensitivity [1-4]. These
results relied on the rather strong assumption that the retinal ganglion cell population is shift-invariant (i.e.,
receptive ﬁelds are centered on each photoreceptor and have identical proﬁles, regardless of their location).
But mammalian retinas are clearly not shift-invariant: the cone mosaic is fairly irregular, the mapping from
cones to ganglion cells is often convergent (i.e., the ganglion cells are sampled at lower density), and this
convergence is also irregular and varies substantially with eccentricity. As such, ganglion cell receptive
ﬁelds are highly irregular [5], making it difﬁcult to directly compare the theory with experimental results.
In previous work [6], we relaxed the shift-invariance and the population size assumptions, solving for receptive ﬁelds that minimize the mean squared reconstruction error. Here, we relax those same assumptions
in the context of maximizing the sensory information, deriving a set of conditions that the optimal receptive
ﬁeld populations should satisfy, and compute examples of receptive ﬁeld populations that reach the theoretical limit of information rate. The basic problem setting is the same as the previous models [1-4]: obtain
a population of linear receptive ﬁelds that maximize information transmitted about a signal that is solely
characterized by its covariance. The input and neural noise are both assumed to be additive, independent,
and Gaussian-distributed. The difference is that the individual receptive ﬁelds are allowed to vary, both in
terms of their spatial extent (i.e., the subset of cones from which their responses are constructed), as well as
their tuning properties (i.e., the weights applied to each cone). In addition, the number of ganglion cells is
adjustable, and need not be the same as the number of cones. We are currently working to compare these
results directly to a newly obtained experimental data set in which both cone locations and ganglion cell
receptive ﬁeld properties are known (with Chichilnisky lab). The theory may then be compared with the
data in terms of information preserved, as well as the detailed properties of the receptive ﬁelds.
Acknowledgments
This work was supported by NIH grant RO1-EY018003-01 (ED, LP, EPS) and HHMI (EPS). We would like
to thank EJ Chichilnisky, Jeff Gauthier, and Greg Field for fruitful discussions.
References
[1] An application of the principle of maximum information preservation to linear systems. R. Linsker,
NIPS*1988:186-194, 1989.
[2] Towards a theory of early visual processing. J.J. Atick & A.N. Redlich, Neural Comp. 2:308-320, 1990.
[3] A theory of maximizing sensory information. J.H. van Hateren, Biol. Cybern. 68:23-29, 1992.
[4] Designing receptive ﬁelds for highest ﬁdelity. D.L. Ruderman, Network 5:147-155, 1994.
[5] Fidelity of the ensemble code for visual motion in primate retina. E.S. Frechette, A.Sher, M.I. Grivich,
D.Petrusca, A.M. Litke, and E.J. Chichilnisky, Journal of Neurophysiology 94:119-135, 2005.
[6] A theory of retinal population coding. E. Doi & M. S. Lewicki, NIPS*2006:353-360, 2007.

266

Cosyne 2008

Saturday evening, Poster session III-37

Decoding Spikes without Stimulus Information: Its Implications
on Receptive-Field Learning
Yoonsuck Choe and Huei-Fang Yang
Department of Computer Science, Texas A&M University, College Station, TX
It is well established in the literature that visual cortical receptive ﬁelds have an oriented Gabor shape.
However, why the visual cortex adopted such a representation is a matter of debate. For example, there
are theories based on sparse representations, independent component analysis, information maximization,
efﬁcient encoding, etc. that show the kind of computational principles that may underlie the formation of
the visual receptive ﬁelds.
One question that is not often asked is, how can the output from the visual cortical neurons be interpreted
in subsequent stages in the brain? This question is nontrivial because the subsequent stages only receive the
signal in the form of action potentials, with no direct information from the external visual environment: The
subsequent stages are only looking at an encoded version of the visual signal (i.e., reverse correlation is not
possible). In certain cases, such as in the hippocampus, topological properties in the stimulus space can be
recovered by considering the spikes alone [1], however, for the visual cortex, such an approach may not be
applicable since geometric properties (e.g., orientation) need to be recovered. In previous work, we showed
that this problem can be overcome if we allow the subsequent stages to (1) generate motor outputs that can
cause changes in the incoming (encoded) information and to (2) observe their lawful relationship [2][3].
In this view, the particular motor primitives (stereotypical motor patterns) available to an animal become
important, since the understanding (or decoding) of the encoded sensory message depends on these motor
primitives. If this is the case, it becomes necessary to reevaluate the theories of receptive ﬁeld formation
mentioned above, to incorporate considerations for the decoding aspect, especially the motor aspect (cf. [4]).
Here, we present a computational model based on a simple Hebbian learning rule, combined with standard
reinforcement learning. The combined learning rule enables the discovery of the linkage between sensory
receptive ﬁelds (encoding) and the motor primitives that have similar properties (decoding), while the receptive ﬁelds are forming, concurrently. The results show that principles of receptive ﬁeld formation should
account for the the role of motor primitives in decoding.
References
[1] V. Itskov and C. Curto. From spikes to space: reconstructing features of the environment from spikes
alone. BMC Neuroscience, 8(Suppl2):P158, 2007. (Presented at Computational and Systems Neuroscience
[CoSyNe] 2007.)
[2] Y. Choe, H.-F. Yang, and D. C.-Y. Eng. Autonomous learning of the semantics of internal sensory states
based on motor exploration. International Journal of Humanoid Robotics, 4:211-243, 2007.
[3] Y. Choe and S. K. Bhamidipati. Autonomous acquisition of the meaning of sensory states through
sensory-invariance driven action. Lecture Notes in Computer Science 3141:176–188, 2004.
[4] E. Salinas. How Behavioral Constraints May Determine Optimal Sensory Representations. PLoS Biology, 4:2383–2392, 2006.

267

Saturday evening, Poster session III-38

Cosyne 2008

Is the homunculus ‘aware’ of sensory adaptation?
Peggy Seriès1 , Alan A. Stocker2 and Eero P. Simoncelli2
1

ANC, University of Edinburgh, UK. 2LCV, New York University, USA.

The response properties of sensory neurons change dynamically with the spatial and temporal context. Sensory adaptation, for example, is known to induce a decrease in neurons’ responsivity [1]. How does the
rest of the brain interpret the responses of the adaptation-altered neurons? In particular, does the read-out
of an adapted population undergo complementary changes to compensate for the adaptation? Perceptually,
adaptation leads to distortions in the perception of subsequently presented stimuli [2]. Are these perceptual
effects the signature of a particular type of read-out? Although these questions have been raised in the past
[1-4], the link between the physiological and perceptual effects of adaptation remains unclear.
We explore these issues in the context of motion direction and contrast adaptation. In our framework, perception is modeled as resulting from an encoder-decoder cascade. The encoder is embodied in the mapping
from stimuli to the responses of a population of noisy neurons, which changes adaptively in response to
the recent stimulus history. We choose the simplest models for this change: a decrease in the response
gain of neurons sensitive to the adaptor for direction adaptation, and a shift in contrast-response curves
for contrast adaptation. Several different decoders are considered, which are either ﬁxed and ‘unaware’ of
the adaptation state, or which change dynamically with the encoder, being thus ‘aware’ of the adaptation
state. In each class, the decoders can also either be optimal (e.g. maximum likelihood (ML) and minimum
mean-squared error (MMSE) estimators) or suboptimal (e.g. constrained to be linear (OLE) or based on
the activity of the most active neuron (winner-take-all)). Using Estimation Theory, we systematically compared the predictions of these read-outs with the psychophysical data for estimation (direction after-effect,
changes in apparent contrast) and discrimination tasks. They are also compared with the predictions of
Fisher Information for discriminability.
We ﬁnd that the ‘aware’ decoders that we tested (ML, MMSE with ﬂat prior, OLE) predict direction and contrast estimates that are matched on average to the physical stimulus, inconsistent with the reported perceptual
biases after adaptation. On the other hand, simple models of neural adaptation coupled with ‘unaware’ readouts (population vector, ML optimized for the pre-adaptation responses) account well for both estimation
and discrimination psychophysical performance. We also ﬁnd that Fisher information, while being naturally
linked with the variance of optimal unbiased read-outs, still provides a meaningful lower bound for the discrimination threshold of biased ‘unaware’ read-outs. We discuss the signiﬁcance of having read-outs that
would be ﬁxed on a short time-scale compatible with adaptation as well as and their possible relevance in
other phenomena.
Acknowledgments
This work was primarily funded by the Howard Hughes Medical Institute.
References
[1] Visual adaptation: physiology, mechanisms, and functional beneﬁts. A. Kohn, J Neurophysiol,
97(5):3155-64, 2007.
[2] Perceptual adaptation: motion parallels orientation. C.W. Clifford. TICS, 6(3):136-143, 2002.
[3] Efﬁciency and ambiguity in an adaptive neural code. A. Fairhall et al, Nature, 412:787-92, 2001.
[4] Space and time in visual context. O. Schwartz et al, Nat Rev Neurosci, 8(7):522-35, 2007.

268

Cosyne 2008

Saturday evening, Poster session III-39

Resampling techniques for the statistical investigation of
neurophysiological data
Asohan Amarasingham1, Matthew Harrison2, and György Buzsáki1
1

Center for Molecular & Behavioral Neuroscience, Rutgers University
Department of Statistics, Carnegie Mellon University

2

The uncertainties peculiar to neurophysiological experiments can often be at odds with classical
statistical assumptions, such as that of identically distributed data, Poisson, or Gaussian
distributions. Motivated in part by these these concerns, resampling techniques are being
employed with increasing frequency in neuroscience. Resampling involves the creation of
surrogate data sets which are created from the original data set to reflect statistical hypotheses of
interest. One then compares the surrogate data with the original data set to evaluate these
hypotheses. Some familiar examples include trial shuffling, spike jittering, and bootstrapping
from estimated models, but there are many possible, and practical, variations. Moreover, given
that resampling is appropriate for a hypothesis testing problem, it is straightforward and
automatic to extend the method to control for the effects of multiple hypothesis testing, which
appears to be somewhat overlooked. Such controls are often important, for example, when the
statistics of interest are indexed by continuous variables such as time or space. We argue that
resampling can be particularly appropriate given the peculiarities of spike trains and illustrate
these ideas in the context of several typical problems in multi-unit data analysis, such as i)
establishing differences in time-varying firing rate statistics across populations of neurons and
conditions, ii) separating the effects of firing rate from correlations, and iii) detecting
monosynaptic interactions in a population of cell activity, as well as their variation as a function
of time and/or space.

Acknowledgments
This work was supported by NIH (NS34994, MH54671), and an NSF postdoctoral fellowship in
biological informatics.

269

Saturday evening, Poster session III-40

Cosyne 2008

Optimal visual motion estimation from natural visual streams.
Shiva R. Sinha1, William Bialek3, 4, Rob R. de Ruyter van Steveninck1, 2
1

Department of Physics, 2Program in Neural Science, Indiana University,
Bloomington, IN 47404, 3Lewis–Sigler Institute for Integrative Genomics, 4Joseph
Henry Laboratories of Physics, Princeton University, Princeton, NJ 08544
Organisms, from flies to humans, represent sensory information as a series of action potentials. From this
collection of discrete events, the brain makes reliable inferences about features in the world. One such
feature is wide–field visual motion. The neural events that represent motion are based on estimates of
local motion in the visual field, which are in turn computed from the raw visual input sampled by the
retinal photoreceptors. However the relationship between the neural representation of motion and the
motion in the environment is indirect and ambiguous due to factors such as photon shot noise, finite
sampling, blurring by the optics of the eye or fluctuations in light intensity and contrast. So, while
extracting motion from natural stimuli seems a very natural and elementary process, it is easy to overlook
the challenges posed by this computation, particularly for the case of natural stimuli.
To explore the structure of the motion estimation problem, we have developed a novel camera–gyroscope
system for very high resolution sampling of the joint distribution of input images and rotational motions
during long walks in outdoor environments. This approach allows us to analyze appropriate conditional
distributions of camera and gyrosensor data and derive optimal wide–field motion estimators for a variety
of natural conditions. We find that small patches of the visual world are strikingly uninformative about
motion, conveying on average much less than one bit of information even at unrealistically high signal–
to–noise ratios. Optimally extracting the available information requires a specific computation, whose
structure has much in common with known features of the biological computation, including even
apparently sub–optimal behaviors such as contrast dependent velocity estimation. Our results are in
agreement with a theory of optimal motion estimation, that makes distinct predictions regarding the
relationship between the motion computation and the contrast available in the stimulus. Specifically, the
theory predicts that both correlator models and a ratio of gradients models emerge as opposite limiting
cases of a general estimation problem.
We contrast the characteristics of the optimal motion estimator with the structure of the biological motion
estimation strategy found in the blowfly, Calliphora vicina, a proven model system for investigating both
the neural mechanisms of visual motion processing and for testing theories of sensory encoding and
motion estimation using artificial stimuli. This objective is accomplished by presenting to the fly moving
sine–wave grating, white nose stimuli, and replaying natural motion data streams using a custom–built,
high–speed, high–intensity light emitting (LED) diode array. The LEDs are configured in a hexagonal
array to match the organization of the blowfly’s ommatidia. Neural recordings are made from the
direction–selective, motion–sensitive H1 interneuron using single-unit extracellular recording methods.
The relationship between neural activity and motion in the stimuli will be discussed in the context of
optimal estimation theory.

Acknowledgments
This work was supported by a NIH/NSF Collaborative Research in Computational Neuroscience grant,
and a Metacyte Neuroscience Research grant from Indiana University.

270

Cosyne 2008

Saturday evening, Poster session III-41

A method to estimate statistical properties of input currents and
network structure from dynamics of rate and spike irregularity
Kosuke Hamaguchi1,2, Alexa Riehle3 , Nicolas Brunel1
1
2

UMR 8119, CNRS-Université René Descartes,
3
Amari Research Unit, RIKEN BSI,
INCM UMR 6193, CNRS

Accumulating evidence from intracellular recordings during the UP state, and computational modeling studies indicate that cortical cells are driven by approximately balanced inputs [1, 2, 3, 4]. Such balanced inputs
must be generated at least in part from local interconnected networks of excitatory and inhibitory cells.
However, it is still unclear how the dynamics of input currents to a cell is related to the architecture of the
local cortical circuit in which the cell takes part.
Here, we propose a new method to obtain information about the local network from extracellular electrophysiological recordings. The method relies on the non-stationarity of instantaneous ﬁring rate and an
instantaneous measure of spike train irregularity, such as the CV2 metric [5]. It is composed of three steps:
(i) given a particular single neuron model, one can estimate the dynamics of input current for a measured
spike train by using the inverse map from instantaneous ﬁring rate ν and CV2 to the mean μ and standard
deviation σ of the input.
(ii) given a particular network architecture, one can estimate the possible trajectories in the μ-σ plane, when
the external inputs to the network are varied. These trajectories depend strongly on network parameters,
such as the ones that control the balance between excitation and inhibition.
(iii) one can infer parameters of the local network of a cell by minimizing the distance between the model
trajectory in the μ-σ plane (using step ii) and the one obtained from the electrophysiological measurements
(using step i).
We have applied this strategy to extra-cellular recordings in the motor cortex of behaving Rhesus monkey
during a delay response task [6]. Our analysis (step i) suggests that the estimated input currents from cells
with high spike irregularity (high CV2 ) are driven by sub-threshold input, even when the ﬁring rates are
elevated. Furthermore, it suggests that most irregularly spiking neurons are embedded in an inhibitiondominated network (steps ii-iii). On the opposite, those with regular spike patterns seem to be part of an
excitation-dominated network. This paper provides a simple method to estimate both input statistics of a
cell and parameters of its local network, which could be helpful to further our understanding of local cortical
circuits.
Acknowledgments
KH is supported by JSPS Research Fellowship. AR and NB are supported by ANR grant TIMION.
References
[1] Y. Shu, A. Hasenstaub, and D. A. McCormick. Nature, 423(6937):288–293, 2003.
[2] C. van Vreeswijk and H. Sompolinsky. Science, 274(5293):1724–1726, 1996.
[3] D.J. Amit and N. Brunel. Cereb. Cortex, 7(3):237–252, 1997.
[4] M.N. Shadlen and W.T. Newsome. J. Neurosci., 18:3870–3896, 1998.
[5] G. R. Holt, W. R. Softky, C. Koch, and R. J. Douglas. J. Neurophysiol., 75(5):1806–1814, 1996.
[6] S. Roux, M. Coulmance, and A. Riehle. Eur. J. Neurosci., 18:1011–1016, 2003.

271

Saturday evening, Poster session III-42

Cosyne 2008

Model-based optimal inference of spike times and calcium dynamics given noisy and intermittent calcium-ﬂuorescence imaging
Joshua T. Vogelstein1 and Liam Paninski2
1

Johns Hopkins University,

2

Columbia University

Using calcium sensitive ﬂuorescence to study neural dynamics is becoming increasingly popular both in
vitro and in vivo, at the level of individual spines, dendrites, boutons, neurons, or populations of neurons.
While the data collected from these experiments are time-varying ﬂuorescence images, the signals of interest
are the precise spike times and/or the intracellular calcium concentrations, [Ca2+ ]t , of the observable neurons. Unfortunately, determining the true value of these hidden signals is a difﬁcult problem for a number of
reasons. First, observations are noisy. This is a problem unlikely to be solved in the near future, as a major
source of the noise is photon shot noise, which reﬂects the quantal nature of light. Second, observations may
be undersampled. When using epiﬂuorescence, this problem can be solved with faster cameras. Similarly,
when using confocal microscopy, faster cameras and spinning disk technology enable sufﬁciently fast imaging. However, when using two-photon microscopy (2PM), images are reconstructed from serial scans across
the imaging plane. Array based technologies and faster scanning can help alleviate this problem, but may
also exacerbate the third problem: low signal-to-noise ratio. Fourth, the relationship between ﬂuorescence
observations and [Ca2+ ]t is nonlinear, especially for ﬂuorescent proteins. This has placed undesirable and
unnecessary restrictions on the calcium indicators used for analysis, as the standard analytical tools assume
a linear relationship between [Ca2+ ]t and ﬂuorescence.
We develop a technique based on a ﬁrst-principles approach incorporating a well deﬁned probabilistic
“forward-model” of the signals of interest and the imaging process. In particular, we assume a biophysically based nonlinear model governing spiking activity, [Ca2+ ]t , and ﬂuorescence observations. We then
develop a Sequential-Monte-Carlo Expectation-Maximization (SMC-EM) algorithm, designed to optimally
infer the [Ca2+ ]t and spike times, given the observed ﬂuorescence signals. This strategy allows us to infer
[Ca2+ ]t using only a single trial. By considering empirically derived noise distributions, the algorithm provides errorbars on the inference of the [Ca2+ ]t and spike trains, thus acknowledging the inherent uncertainty
of these estimates, given the nature of the data. Further, this framework provides a natural way of incorporating a stimulus (if present) to improve the accuracy of the inference. Finally, because our approach ﬁts the
model parameters directly to each observed cell’s ﬂuorescence data, we do not need to train the algorithm
using simultaneously acquired intracellular electrophysiology and [Ca2+ ]t data for each cell, but rather the
parameters are automatically recovered.
Simulated results indicate that this approach can successfully infer spike times from a range of noisy, intermittent data, even as the ﬂuorescence signal saturates, or with multiple spikes per image frame. Experimental
veriﬁcation of this approach using different indicators is in progress.
Acknowledgments
The authors would like to thank Q. Huys, M. Nikitchenko, K. Svoboda, B. Watson, R. Yuste, D. Greenberg, D. Yue, M. Tadross, E. Young, and S. Mihalas for helpful discussions. Support for JV was provided
by NIDCD DC00109. LP is supported by an NSF CAREER award and by an Alfred P. Sloan Research
Fellowship.

272

Cosyne 2008

Saturday evening, Poster session III-43

Parallel stochastic networks approximate large-scale interconnected populations of spiking neurons
Terence D. Sanger1
1

Stanford University Dept. Neurology and Neurological Sciences

Simulations of massively-interconnected networks of spiking neurons require computation of the membrane
potentials of individual neurons and transmission of spikes between neurons. The membrane computations
scale linearly with the number of neurons, but for fully-interconnected networks the number of transmissions
between neurons scales as the square of the number of neurons. Approximations that reduce the requirement
for transmission could facilitate such simulations.
Consider a network that consists of 2 distinct populations, each with N neurons. Suppose that each neuron
n2,i in population 2 is fully connected to all neurons n1,j in population 1 through synapses with transmission
probabilities wij . Then if s
2,i ∈ {0, 1} indicates the presence of absence of a spike in neuron n2,i , the
expected value is E[s2,i ] = j wij E[s1,j ]. If all the neurons in each population obey the same ﬁrst-order
statistics and all the synaptic weights are equal, then E[s2,i ] = N wE[s1,j ] ∀i, j. Therefore the expected
value of the output population is unchanged if each output neuron is connected to only a single input neuron.
The approximation consists of a set of parallel circuits, where each circuit contains only a single neuron
from each population. For example, consider a network of 600 Izhikevich neurons arranged in 6 populations, roughly corresponding to pyramidal tract neurons, anterior horn cells, and dorsal horn cells for the
agonist and antagonist muscles about a single joint. The interconnections form a feedback controller, so that
the pyramidal cells specify the reference position of the joint, the dorsal horn cells provide proprioceptive
feedback, and the anterior horn cells provide the driving signal to muscle (simulated using a Hill model). To
approximate this network, a set of 100 parallel independent micro-networks is created, each with 6 neurons
(one of each type). The networks are linked only through their outputs to common muscles. Each micronetwork provides a very poor feedback controller, but the average behavior of the 100 controllers provides
good step tracking and disturbance rejection, as shown in the ﬁgure.
100

100

50

50

0

0

500

1000

1500

2000

0

100

100

50

50

0

0

500

1000

1500

2000

0

100

100

50

50

0

0

500

1000

1500

2000

0

500

1000

1500

2000

0

0

500

1000

1500

2000

0

500

1000

1500

2000

0

500

1000

1500

2000

Figure: Left: agonist, Right: antagonist, Row1:
pyramidal neurons, Row2: anterior horn cells,
Row3: Ia afferents, Row4: reference trajectory and
tracking response, for a reference step at 1000msec
and torque disturbances at 500 and 1500msec.

1
0.5
0

273

Saturday evening, Poster session III-44

Cosyne 2008

Effects of dendritic compartmentalization on the property of spatial
working memory
Kenji Morita1
1

RIKEN Brain Science Institute

In working memory tasks, pyramidal cells in the relevant cortical circuit receive both sensory feedforward inputs and recurrent inputs to shape stimulus-selective sustained activity. Recent studies have
revealed that each dendritic branch of pyramidal cells functions as a compartmentalized integration
subunit [1]. Since significant part of the inputs are applied onto dendritic branches, such
compartmentalization could affect the computational property of the memory circuit. We addressed this
issue by constructing a model of neural circuit for spatial working memory, incorporating pyramidal
dendritic arborization and branch-specific nonlinear input integration (Fig. A). Specifically, based on the
architecture of existing models [2], we additionally incorporated multiple dendritic branches with the
assumption that feed-forward inputs converge onto some portion of the branches in each pyramidal cell
while recurrent inputs are evenly distributed. We show that the model forms accurate memory when the
contrast of the feed-forward input exceeds a certain level, whereas memory is not induced when the
contrast is low (Fig. Ba). Even when the input intensity is increased (Fig. Bb), low-contrast input does not
induce inaccurate memory, which can be formed more easily in models without dendritic branching
unless additional normalization process is explicitly incorporated. We explored how dendritic
compartmentalization enables such contrast-dependent memory formation. When the low-contrast input is
presented, majority of the dendritic branches that do not receive significant feed-forward input hardly
contribute to somatic firing, because the total input in those branches does not reach the presumed branchspecific threshold, even in the cell receiving the strongest feed-forward inputs in total (Fig. C). Thereby
cell-to-cell difference in the amount of feed-forward excitation is largely canceled at the level of somatic
impact, preventing winner-take-all competition that leads to the formation of inaccurate memory. This
mechanism holds in either case with somatically or dendritically mediated recurrent inhibition; however,
for a wider range of conditions in the dendritic inhibition case. Provided the input contrast represents the
certainty of stimulus location, such contrast-dependent accurate memory formation might serve for
animal's appropriate decision making based on the memory. In terms of modeling, our results indicate the
necessity and usefulness of considering a reduced neuron model with a few 'parallel' compartments, each
of which receives inputs from different sources, instead of or in addition to tandem ones.

Acknowledgments
This work was supported by Grant-in-Aid for Young Scientists (B) 19700310 from MEXT, Japan
References
[1] Dendrites: bug or feature? M. Häusser and B. Mel, Curr Opin Neurobiol 13, 372-383, 2003.
[2] A neural circuit basis for spatial working memory. C. Constantinidis and X.-J. Wang, Neuroscientist
10, 553-565, 2004.

274

Cosyne 2008

Saturday evening, Poster session III-45

In vivo electrophysiological identification of Channelrhodopsin2tagged neuronal subpopulations
Susana Q. Lima, Tomáš Hromádka and Anthony M. Zador
Cold Spring Harbor Laboratory, Cold Spring Harbor, New York 11724, USA
Neural circuits consist of a heterogeneous mixture of neurons with different patterns of molecular
expression and neuroanatomical projections. Recordings of neural activity in behaving animals reveal
tremendous functional heterogeneity as well: nearby neurons often respond very differently to the same
stimulus or action. However, little is known about how this structural circuit-level heterogeneity
contributes to function, in part because of the technical difficulty of identifying neurons during in vivo
recordings in behaving animals.
To overcome this difficulty, we have developed a technique that allows us to “tag” subpopulations of
neurons for identification during in vivo electrophysiological recordings. The tag is a light-gated ion
channel—the algal protein channelrhodopsin-2 (ChR2)—whose expression can be genetically restricted
to a subpopulation of neurons. In the subpopulation of neurons expressing ChR2, a brief flash of blue
light triggers a single action potential with millisecond precision [1].
We are using this approach to test the hypothesis that neuroanatomical connectivity represents one main
structural correlates of the functional diversity in the rodent cortex. To do this, we restrict ChR2
expression to subsets of neurons in the rat auditory cortex (ACx). ACx pyramidal neurons project to
multiple brain regions, including the amygdala, the posterior parietal cortex or the contralateral ACx, and
presumably carry different information about auditory stimuli to these centers. To target ACx neurons
specifically based on their projection pattern we inserted the ChR2 coding region into a herpes simplex
virus (HSV). The HSV travels in a retrograde fashion through the axons of infected neurons. ChR2tagged neurons can be identified by their low-latency and reliable response to a brief light flash. Thus, for
example, we have used this approach to identify the subpopulation of layer 5 ACx neurons that project to
the contralateral cortex.
This approach is general, in that any population to which expression of ChR2 can be genetically
restricted, can be tagged. Promising future applications include tagging of different subpopulations of
neurons based on promoters, like for example subclasses of interneurons, and tagging of neurons in
different cortical layers,

1.Boyden, E.S., et al., Millisecond-timescale, genetically targeted optical control of neural activity. Nat
Neurosci, 2005. 8(9): p. 1263-8.

FUNDING: Patterson Trust Postdoctoral Fellowship

275

Saturday evening, Poster session III-46

Cosyne 2008

Training Chaotic Networks
David Sussillo and L.F. Abbott
Department of Neuroscience, Columbia University, New York, NY
Previous work [1,2] has shown that otherwise silent recurrent networks can be induced to generate complex temporal output patterns by adding external feedback loops (upper ﬁgure below).
In this previous work, the recurrent network itself was inactive, and the
feedback loop was broken and replaced by a direct injection of the target waveform during training. Both of these features make the training
procedure simpler. In fact, other work [3] has pointed out that severe
impediments to learning may exist if the recurrent network has, by itself, non-trivial dynamics. On the other hand, biological networks exhibit complex dynamics even when not actively engaged in a task, and
breaking of feedback loops and injection of desired waveforms are not
biologically realistic manipulations. We address the problem of using external feedback loops to control recurrent networks that by themselves exhibit chaotic dynamics, and training these networks without breaking
feedback or injecting the desired function into the network.
We ﬁnd that chaotic networks can be trained to generate complex, repeatable and controlled outputs.
We use a simple online, local learning rule based only on 1) the presynaptic ﬁring rate and 2) a global
error signal deﬁned as the difference between the target function and the network output. We show
that this synaptic update rule is an approximation of the full gradient descent learning rule for RNNs
[4], and explore conditions under which the approximate learning rule converges. We provide a bound
on the learning rate for this rule based on network properties, under which the network error is reduced. An additional bound on the learning rate assures that the network is successfully learning as
opposed to simply reducing the network error via continuous change of the synaptic weights. As indicated by the lower right ﬁgure, we ﬁnd that, after learning, a single chaotic network can generate multiple time-dependent output waveforms through the individual activation of multiple feedback pathways.

Acknowledgments
We thank T. Toyoizumi for helpful discussions. Supported by
NIH Pioneer Award 5-DP1-OD114-02.
References
[1] Harnessing nonlinearity: Predicting chaotic systems and
saving energy in wireless communications. H. Jaeger and
H. Haas, Science 78 (2004); 304.
[2] Computational aspects of feedback in neural circuits.
W. Maass, P. Joshi and E. Sontag, PLoS Comp Biol 3:15-34.
[3] Bifurcations in the Learning of Recurrent Neural Networks.
K. Doya, IEEE International Symposium on Circuits and Systems
1992 2777-2780.
[4] Experimental Analysis of the Real-time Recurrent Learning
Algorithm. R. Williams and D. Zipser, Connection Science
1:87-111.

276

Cosyne 2008

Saturday evening, Poster session III-47

Frontal eye field input neurons have higher spontaneous firing rates
and narrower action potentials than output neurons
SooYoon Shin and Marc A. Sommer
Dept. of Neuroscience and the Center for the Neural Basis of Cognition, University
of Pittsburgh, Pittsburgh, PA 15213
The frontal eye field (FEF) is reciprocally connected with the superior colliculus (SC), and we previously
identified two critical types of FEF neurons in this circuit. In response to a brief pulse of current in the
SC, FEF input neurons are orthodromically activated while FEF output neurons are antidromically
activated. The route of orthodromic activation of FEF input neurons, however, is controversial. We have
presented many lines of evidence indicating that the input neurons are driven through an SC-thalamusFEF pathway, but an alternative explanation is that they are activated via collaterals of corticotectal
neurons (discussed in Sommer & Wurtz 1998, 2004). Here we test our hypothesis in a new way. The
hypothesis implies that FEF input neurons primarily reside in thalamic-recipient layer IV, which contains
a large proportion of interneurons. One feature of interneurons is that they exhibit higher spontaneous
firing rates and narrower action potentials than pyramidal neurons (e.g. Constantinidis and GoldmanRakic 2002; Rao et al. 1999; Simons 1978; Swadlow 1995). FEF output neurons, in contrast, are entirely
pyramidal and reside in layer V (Fries 1984). Our hypothesis would be supported if FEF input neurons,
which may consist largely of interneurons, exhibit higher spontaneous firing rates and narrower action
potentials than FEF output neurons, which are homogeneously pyramidal. We studied 27 FEF input
neurons and 67 FEF output neurons from four monkeys. To test the hypothesis, we 1) measured each
neuron's spontaneous firing rate and action potential width and 2) categorized each FEF input neuron as a
putative inhibitory interneuron or a putative pyramidal neuron by plotting spontaneous firing rate vs.
action potential width. We found, first, on average, FEF input neurons did have higher average
spontaneous firing rate and narrower action potential widths. Second, we found that about a half of the
FEF input neurons reliably fell outside of a calibration cluster of pyramidal neurons in the plot. In
conclusion, many FEF input neurons have higher spontaneous firing rates and narrower action potentials
than FEF output neurons. These findings suggest that FEF input neurons may consist largely of
interneurons, supporting our hypothesis that FEF input neurons reside in layer IV and are driven via
thalamic input.

277

Saturday evening, Poster session III-48

Cosyne 2008

Inhibitory stabilization of the cortical network underlies visual
surround suppression
Evan S. Schaffer1, Hirofumi Ozeki2, Ian M. Finn2, David Ferster2, and
Kenneth D. Miller1
1

Columbia University,

2

Northwestern University

Lateral inhibition is thought to be a universal process that sharpens the tuning of sensory neurons and
enhances apparent contrast. According to this principle, a stimulus lateral to a neuron’s preferred
stimulus, either in physical space or in feature space, is expected to increase inhibition onto that neuron.
In visual cortex, one form of lateral inhibition is surround suppression: stimuli in the receptive field
surround suppress the response to stimuli in the receptive field center [1-6]. Unlike the classical examples
of lateral inhibition, we find from in vivo intracellular recordings that suppressive stimuli evoke only a
transient increase in synaptic inhibition, after which both inhibition and excitation decrease to below their
initial levels. From theoretical considerations, these observations suggest that cortex forms an inhibitionstabilized network (ISN) [7], in which strong, recurrent excitatory connections are unstable on their own,
and require inhibition for stabilization. The strong excitatory recurrence in ISNs allows the networks to
perform complex computations, and yet remain stable enough to respond in a graded way to stimuli of
different strengths [8]. Lateral inhibition may thus be implemented through a network mechanism that
leads to a reduction in local recurrent excitation together with a paradoxical decrease in synaptic
inhibition, rather than the expected increase in inhibition.
Acknowledgments
We thank N. Priebe, C.E. Boudreau, J. Yu, H. Sato, L.F. Abbott, M. Eisele and T. Toyoizumi for
comments. This work was supported by grants from the National Institutes of Health (D.F. and K.D.M.).
H.O. was partly supported by a JSPS postdoctoral fellowship.
References
[1] Length and width tuning of neurons in the cat's primary visual cortex. DeAngelis, G. C., Freeman, R.
D. & Ohzawa, I. J. Neurophysiol. 71: 347-374, 1994.
[2] Extensive integration field beyond the classical receptive field of cat's striate cortical neurons-classification and tuning properties. Li, C. Y. & Li, W. Vision Res. 34: 2337-2355, 1994.
[3] Contrast dependence of contextual effects in primate visual cortex. Levitt, J. B. & Lund, J. S. Nature
387: 73-76, 1997.
[4] Membrane potential and conductance changes underlying length tuning of cells in cat primary visual
cortex. Anderson, J. S., Lampl, I., Gillespie, D. C. & Ferster, D. J. Neurosci. 21: 2104-2112, 2001.
[5] Selectivity and spatial distribution of signals from the receptive field surround in macaque V1
neurons. Cavanaugh, J. R., Bair, W. & Movshon, J. A. J. Neurophysiol. 88: 2547-2556 (2002).
[6] Relationship between excitation and inhibition underlying size tuning and contextual response
modulation in the cat primary visual cortex. Ozeki, H. et al. J. Neurosci. 24: 1428-1438, 2004.
[7] Paradoxical effects of external modulation of inhibitory interneurons. Tsodyks, M. V., Skaggs, W. E.,
Sejnowski, T. J. & McNaughton, B. L. J. Neurosci. 17: 4382-4388, 1997.
[8] Computing and stability in cortical networks. Latham, P. E. & Nirenberg, S. Neural Comput.
16: 1385-1412, 2004.

278

Cosyne 2008

Saturday evening, Poster session III-49

Poster withdrawn

279

Saturday evening, Poster session III-50

Cosyne 2008

Eye-Head Coordination Obeys Minimal-Effort Rule
Andreas A. Kardamakis1 and Adonis Moschovakis1
1
Institute of Applied and Computational Mathematics, FORTH, Crete, Greece
Gaze shifts are combined eye-head movements consisting of coordinated eye saccades and rapid head
movements. These two-segment movements have an extra degree of freedom that allows the gaze system
additional flexibility when programming eye and head commands for reorienting the line of sight.
However, behavioral observations show that the eye and head contributions are systematically
constrained in that larger gaze shifts rely on larger head components, whereas smaller ones consist mainly
of eye movements. Furthermore, eye contributions do not exceed amplitudes of 30-35º even for gaze
shifts as large as 75º and eye velocity profiles are less and less symmetrical as the amplitude of the gaze
shifts increases. Optimal control theory suggests that these data could be accounted for if the brain
followed the principle of minimal effort to program horizontal gaze shifts. The optimal set of control
signals is obtained by relating the dynamics of the controlled eye/head components to the criterion
function. The performance objective is to minimize the squared sum of eye/head torque signals, integrated
in time for each coordinated movement (minimal-effort). By applying Pontryagin’s Maximum Principle,
we obtained analytical expressions for eye/head control signals and from these we acquired the optimal
trajectories of the eyes and the head by solving the deterministic two-point boundary-value problem. To
qualitatively represent extraocular muscle length-tension curves we introduced an eye position-varying
weight expressed as a polynomial function. This accounts for the active elastic restoration forces that pull
the eye towards the central position. A second weight corresponding to the optimal head control signal
was used to express the large inertial difference relative to the eye. In this context, eye/head commands
are programmed to follow a performance trade-off between viscoelastic and inertial forces.
Optimal trajectories were obtained for rightward horizontal centripetal and centrifugal gaze shifts ranging
from 5º to 75º while initial eye positions varied from 30º to the left to 10º to the right of straight ahead.
Our model accurately predicted the amplitude of eye and head components relative to the amplitude of the
gaze shifts as well as the fact that ocular movements do not exceed 30º. Furthermore, the model accounts
for the quantitative relationships between eye and head contributions as a function of initial eye positions.
The head contributed progressively less (and the eyes more) to gaze shifts of the same size as initial eye
position deviated further in the contralateral direction. The greater the size of the gaze shift, the steeper
the slope of these relationships. Perhaps the most striking feature of the model is its ability to generate eye
velocity profiles that closely match those of animals. Lower peak velocities and dual-peak velocity
profiles emerge as a result of the minimal-effort principle. Simulations of optimal head-restrained
saccades also show realistic unimodal velocity profiles with shorter acceleration and longer deceleration
phases, and provide an explanation of the main sequence relationship. Reproduction of the major
kinematic features of head-fixed and head free movements implies that the time course of force
development was successfully modelled. Ultimately, the motoneuronal commands sent to the eye and the
head must in turn be consistent with the signals predicted by the minimal-effort rule. Our results suggest
that the minimal-effort rule provides a rationale for the adoption of the cross-talk mechanism between eye
and head control signals, which belongs to the independent eye/head control class of neural models.
Acknowledgments
Financial support of grant 03ED803 from the Secretariat of Research and Technology is gratefully
acknowledged. We deeply thank Dr. Tsakiris for his valuable assistance.

280

Cosyne 2008

Saturday evening, Poster session III-51

Cortical Electrical Stimulation Parameters that Elicit Excitation
and Suppression in a Target Region of Rat Motor Cortex
Azadeh Yazdan-Shahmorad1, Mark J. Lehmkuhle1 and Daryl R. Kipke1
1

Department of Biomedical Engineering, University of Michigan

Cortical electrical stimulation (CES) techniques are practical tools in neurorehabilitation that are currently
being used to test models of functional recovery after neurological injury [1]. Results from studies using
CES suggest that electrical stimulation increases cortical excitability and intracortical facilitation while
decreasing intracortical inhibition that outlasts the period of stimulation. These changes in cortical
excitability are influenced by GABAergic neurotransmission and may involve long term potentiation-like
mechanisms [2]. Here we investigate the effects of different CES parameters on unit activity in a target
region of rat motor cortex.
Three normal Long-Evans rats were implanted with a penetrating single shank silicon microelectrode
array in motor cortex [3] along with three cortical bone screws that were used to record the
electrocorticogram and a single bone screw for delivering cortical electrical stimulation. Unit activity and
local field potentials were recorded across cortical layers beneath the stimulating bone screw in awake
animals. Constant current CES pulses consisted of either cathodic-first or anodic-first square pulses (100
Psec) followed by an exponentially decaying second phase to balance charge at frequencies of 25, 50,
100, 250 and 500 Hz. The experiment was designed to randomly deliver 25, 50, 75 or 100% of the current
needed to elicit a movement threshold for each frequency in separate “anodic” and “cathodic” sessions.
The stimulation pattern consisted of a 1 second train of stimulation pulses followed by 4 seconds of
recording. This pattern was repeated 25 times for each frequency/movement threshold combination.
Recording sessions without CES were made before and after each session and each CES parameter set.
Opposite effects were observed following cathodic and anodic stimulation by comparing the average unit
activity 2 seconds after CES with the preceding 2 seconds. Increases in cortical activation were observed
following low and high frequency CES for cathodic pulses while mid-range CES frequencies showed a
decrease in activation. Conversely, mid-range CES frequencies showed a significant increase in unit
activity for anodic pulses compared to other frequencies. The highest increase in unit activity was seen at
the 50% of movement threshold for anodic CES and there were no significant differences between
different amplitudes of cathodic CES.
Acknowledgments
Thanks to T.Marzullo for his help with the surgeries. This work was supported by NorthStar
Neuroscience, Seattle, WA, USA.
References
[1] Motor Cortex Stimulation for the Enhancement of Recovery from Stroke. J. A. Brown, H. L. Lutsep,
M. Weinand, and S. C. Cramer, Neurosurgery 58: 464-471, March 2006.
[2] Noninvasive Cortical Stimulation in Neurorehabilitation: A Review. M. L. Harris-Love and L. G.
Cohen, Archives of physical medicine and rehabilitation 87: 84-93, December 2006.
[3] Chronic neural recording using silicon-substrate microelectrode arrays implanted in cerebral cortex. R.
J. Vetter, J. C. Williams, J. F. Hetke, E. A. Nunamaker, and D. R. Kipke, IEEE Transactions on
Biomedical Engineering 51:896-904 , June 2004.

281

Saturday evening, Poster session III-52

Cosyne 2008

Single-trial representation of uncertainty about reach goals in macaque PMd
Z. RIVERA ALVIDREZ1, R. S. KALMAR2, A. AFSHAR1,3, G. SANTHANAM1, B. M. YU1,2,5, S. I.
RYU4, K. V. SHENOY1,2;
1
Dep.of Elec. Eng., 2Neurosci. Prog., 3Sch. of Med., 4Dep. of Neurosurg., Stanford University, Stanford,
CA, 5Gatsby Comp. Neurosci. Unit, UCL, UK.
Animals are often faced with uncertainty about which actions will be the most beneficial. When presented
with two possible movements the nervous system could withhold planning; however, previous studies
have suggested that monkeys represent plans to both options simultaneously (Cisek & Kalaska, Neuron,
2005) or alternate plans between locations (Horwitz & Newsome, J. Neurophys., 2001). Presumably,
these strategies allow for faster movements than not planning at all, though the behavioral benefit of these
strategies has not been explored. Here, we recorded from ensembles of neurons to directly observe which
strategy is adopted when a monkey must make fast and accurate reaches given uncertainty about target
location. We trained monkeys to perform three versions of a delayed-reach task. In the first version
(delayed-reach task), one target was presented during a delay period, after which the monkey was
rewarded for a reach to that target. The second version (distractor task) was nearly the same as the first,
but a distractor (different color) was presented in the location opposite the target. In the third version
(delayed-reach task with ambiguous delay) two possible targets (same color) were presented during the
delay period, after which the correct target was identified (by a change of color). All three trial types were
interleaved.
Simultaneous responses of 82-102 neurons were recorded using a microelectrode array implanted in
dorsal premotor cortex (PMd), and we used linear methods such as factor analysis and principal
components analysis to reduce the dimensionality of the neural responses. Using the first few dimensions
of the low-D projections of these responses, we examined single-trial trajectories representing the
evolution of delay-period activity. These plan trajectories suggested that on some trials monkeys
alternated planning to the two possible reach goals, planned reaches to only one of the targets, or formed
plans not directly corresponding to any of the targets. To increase confidence that these results were due
to coordinated neural activity rather than spurious spiking noise, we computed the fraction of each
trajectory's path that lay closer to the target zones than to the baseline (no plan) zone. Bootstrap analysis
revealed far greater amounts of plan trajectory residing close to the single-target planning zones than
expected by chance (p<0.001, comparison with trial-shuffled data, three datasets). To our knowledge this
is the first evidence that in a task with 2 target options, a variety of strategies is used on different trials.
Fig. 1. 3D representation of data in factor
(a)
analysis space. (a) Delayed-reach task.
(b)
Green points represent plans to target 1
(t1); blue points represent plans to target 2
(t2); red points represent neural state at
target onset. Gray lines are plan
trajectories for all delayed-reach trials.
Two trajectories are highlighted in
green/blue as examples of typical plan
trajectories for t1/t2. (b) Delayed-reach
task with ambiguous delay period. One example trajectory is shown, illustrating a type of complex plan
dynamics seen during the 400 ms delay period. Blue/green/red points as in (a).
Support: BioX, BWF, CIS, CRF, MSTP, NDSEG, NIH, NSF, ONR, SGF, Sloan, Whitaker

282

Cosyne 2008

Saturday evening, Poster session III-53

Optimizing memory capacity in the hippocampus by memory
storage in the dentate gyrus and adult neurogenesis
Sen Cheng1, Friedrich T. Sommer2 and Loren M. Frank1
1

Dept of Physiology, UC San Francisco, 2Redwood Center, HWNI, UC Berkeley

The hippocampus is required for the formation of episodic memories, but the neural mechanisms
underlying memory storage in the hippocampus remain unclear. Currently, the dominant hypothesis is
that memory patterns are stored as attractor states in the recurrent CA3 network1. To minimize
interference between similar memories, input patterns to CA3 are thought to be separated by the dentate
gyrus (DG)1. Additionally, several groups have suggested that adult neurogenesis in the DG might
improve pattern separation2.
Here we show that this scheme is relatively inefficient, and that memory storage in the DG coupled with
adult neurogenesis can yield much higher memory capacity than storage in CA3. We therefore propose
that episodic memories are more likely to be stored in the DG. Our model assumes that episodic
memories are represented as sparse activity patterns in the hippocampus. We estimated the maximum
number of memory patterns that share a single neuron to measure the potential of interference among
stored memories. Based on data from the rat, our analysis revealed that if memories were stored in CA3,
the interference would be much larger than if memories were stored in the DG. Moreover, if neurogenesis
in DG is included in the model, the interference is further reduced by roughly an order or magnitude.
Using the Willshaw model3, we tested whether the interference among memories is indeed detrimental for
cued memory retrieval. We calculated the maximum rate at which CA3 or DG can store memories while
maintaining a high retrieval quality. If associations were stored in CA3, the maximum storage rate would
be much lower than if associations were stored in the static DG. Furthermore, with the addition of
neurogenesis, the DG can store associations at a rate roughly an order of magnitude higher. This increased
memory capacity is due to neuronal turnover and comes at the cost of forgetting old memories more
quickly, as described previously4. Thus, memory storage in the DG would be temporary, consistent with
experimental evidence for a time-limited role of the hippocampus in learning and memory.
We further studied the case in which young DG neurons are preferentially recruited to encode episodic
memories5. While such a mechanism reduces the overlap among memory patterns further, the
improvement is small (~25%) – much less significant than the order-of-magnitude improvement
introduced by neuronal turnover. Finally, our model allows us to explore when neurogenesis does not
improve memory capacity. As an example, an animal with a rat-sized hippocampus, and a much shorter
life span or a much lower rate of episodic memory formation would not benefit from neurogenesis. These
results suggest that the DG may be specialized for the rapid storage of new associations, leaving CA3
available to process perhaps temporal aspects of episodic memory.
Acknowledgments
This work was supported by the Hawkins-Strauss Trust, the John Merck Scholars Program, the McKnight
Scholars Program and NIH grants MH059733, MH077970 and MH080283.
References
[1] Treves A and Rolls ET, Hippocampus 4:374, 1994. O'Reilly RC and McClelland JL, ibid 4:661, 1994.
[2] Becker S, ibid 15:722, 2005. Wiskott L, et al., ibid. 16:329, 2006.
[3] Willshaw DJ, et al. Nature 222:960, 1969.
[4] Deisseroth K, et al, Neuron 42:535, 2004. Chambers RA, et al, Neuropsychopharmacol. 29:747, 2004.
[5] Kee N, et al., Nat Neurosci. 10:355, 2007.

283

Saturday evening, Poster session III-54

Cosyne 2008

Temporal Integration in a Network of Conductance-based Model
Neurons with Dendritic Bistability
Lee M1, Levine JH2, Bomash I1, Molinelli E1, Aksay E1, Goldman MS3,4
1
Weill Medical College of Cornell University, 2M.I.T., 3Wellesley College, 4UC
Davis
Temporal integration in a network has been identified as an essential computation for motor control,
working memory, and decision making. When presented with a transient input, integrating networks
accumulate the information over time and output persistent changes in neuronal firing. Classically, it has
been hypothesized that these networks use synaptic feedback to give rise to persistent firing. We recently
tested this hypothesis in the oculomotor integrator through inactivation experiments that perturbed
feedback. We found deficits in persistent firing, suggesting a critical role for feedback, but these deficits
appeared only at times when the inactivated cells would have fired at elevated rates. Thus, we suggested
that the traditional model of feedback-based integration should be modified to include the principle that
firing rates of integrator neurons must exceed a threshold level before they affect their postsynaptic
targets. One way in which thresholds on synaptic inputs may be realized is through the presence of
bistable postsynaptic functions. Such bistable elements could arise from active dendritic properties that
permit two stable levels of membrane voltage, a depolarized ‘on’ state and a resting ‘off’ state.
To understand how neurons with bistability in the dendrites could be incorporated into a realistic network
capable of temporal integration, we have begun to develop a conductance-based network of spiking
neurons in which each cell has multiple bistable dendritic compartments. Each compartment contains leak
and voltage-gated conductances which combine to yield a N-shaped current-voltage relationship; in the
presence of synaptic current, two stable fixed points are present. The soma is modeled as an integrateand-fire compartment with leak, saccadic, and noise currents, as well as input from the dendrites.
Parameters are tuned to yield a threshold-linear relationship between injected current and firing rate as
seen experimentally. The connectivity matrix for the network is chosen to be rank one; that is, each
dendrite receives input from a particular presynaptic neuron, and each weight equals the product of a
unique dendritic gain times a global somatic gain scaling all inputs. Somatic gains and compartmental
impedances were constrained by experimental measurements. This procedure leaves, for a system of N
dendrites, N free parameters in the determination of the weights. To enable the model to integrate, we
implement a tuning procedure for balancing the leak with network and intrinsic feedback. Two key
approximations are made. First, we treat the current from a dendritic compartment to the soma as arising
from a single effective conductance. Second, we describe somatic responses as rate functions. These two
approximations allow us to establish a self-consistency relationship between ideal and realized feedback;
minimization of the difference with a constrained least squares fit determines the N free parameters.
The tuned model exhibits multiple fixed points that lie on a line in firing rate state space, with fixation
rates in the spiking model close to those predicted from the reduced model. Further, the network shows an
insensitivity to small inputs, as indicated by the existence of a threshold value below which external input
cannot activate plateau potentials in the dendrites; thus the network remains at a fixed point unless it is
perturbed by a sufficiently large input. Ongoing work is focused on making the network fully bilateral,
incorporating realistic morphological data, and testing the model with smooth inputs that drive the
vestibuloocular reflex. The completed model will match many experimental features and may help
explain common mechanisms underlying diverse processes such as motor control, working memory, and
decision making.

284

Cosyne 2008

Saturday evening, Poster session III-55

Combining Multielectrode Recording and Opto-genetic Control
of Hippocampal Neurons in Awake, Behaving Rats.
Caleb Kemere1 , Feng Zhang2 , Karl Deisseroth2 , and Loren Frank1
1
2

Keck Center for Integrative Neuroscience and Department of Physiology, UCSF,
Department of Bioengineering, Stanford University

Synaptic plasticity in the hippocampus is thought to be responsible for the acquisition of new semantic
and episodic memories. In rat hippocampus, artiﬁcially induced LTP interferes with spatial learning and
learning interferes with the ability to potentiate synapses artiﬁcially. Current approaches for probing synaptic plasticity within and between regions of the hippocampus rely on electrical stimulation, which presumably activates both excitatory principal cells and inhibitory interneurons. In addition to this lack of
speciﬁcity, electrical stimulation induces artifacts that make simultaneous recording of neural activity during and immediately following stimulation difﬁcult. Viral transfection of the light-sensitive ion channel,
channelrhodopsin-2 (ChR-2), combined with in-vivo light delivery, provides an elegant method for selectively activating particular classes of cells without electrical artifacts.
We have developed a combined multi-electrode/ﬁber optic system that allows us to selectively activate principal cells in the hippocampus of an awake, behaving animal following infection with a lentivirus expressing
ChR-2 under the CaMKIIα promoter. Using this system, we can record ﬁeld potentials and isolated individual units during optical stimulation of excitatory neurons. After transfecting cells and recording activity in
area CA1, we could reliably evoke a population response with light pulses as short as 2 ms. Furthermore, in
contrast to in vitro experiments [1], we evoked this response with a short latency of about 5 ms. Additionally, we found that simultaneously recorded neurons were silenced for about 20 ms following the evoked
population response, indicating that this technique can be used to brieﬂy interfere with ongoing activity.
Following transfection of cells in CA3 and the dentate gyrus and targeting CA3 with an optical ﬁber, we
were able to evoke responses in both areas CA3 and CA1. We found that stimulation using a sequence
of 5 ms optical pulses resulted in a frequency-dependent short-term facilitation of the population response.
Speciﬁcally, the evoked local ﬁeld potential following the second pulse in the sequence increased with
increasing frequency to a maximum of about 30 Hz. This peak frequency is consistent with the previously
reported characteristics of the ChR-2 ion channel [1]. Interestingly, the response of area CA3 to repeated
pulses saturated more rapidly than in CA1; at 8Hz, maximal response was achieved after 3-4 pulses, but
continued to rise in area CA1 for 8 pulses. These differences suggest that short term plasticity in CA3
recurrent collaterals has different properties than that at the CA3-CA1 synpase. The ability to selectively
activate excitatory neurons and record the local and downstream effects offers a new and potentially very
powerful tools for probing the properties of neural activity and plasticity in-vivo.
Acknowledgments
This work was supported by the Swartz and Sloan Foundations, the John Merck Scholars Program, the
McKnight Scholars Program and NIH grants MH077970 and MH080283.
References
[1] Millisecond-timescale, genetically targeted optical control of neural activity. E. S. Boyden, F. Zhang, E.
Bamberg, G. Nagel, and K. Deisseroth, Nature Neuroscience 8(9):1263–1268, Sept. 2005.

285

Saturday evening, Poster session III-56

Cosyne 2008

Calcium, synaptic plasticity and intrinsic homeostasis in Purkinje
neuron models
Pablo Achard1,2, and Erik De Schutter1,3
1

Antwerp University,
Technology

2

Brandeis University,

3

Okinawa Institute for Science and

Activity homeostasis designates bio-mechanisms that regulate the activity of a neuron through the
dynamic expression of ion channels or synapses [1]. We have recently reproduced the complex electrical
activity of a Purkinje cell (PC) with very different combinations of ionic channel maximum conductances
[2], suggesting that a large parameter space is available to homeostatic mechanisms. Some models [3,4]
have hypothesized that one such mechanism could work via the regulation of the average cytoplasmic
calcium concentration. While this hypothesis is attractive for rhythm generating neurons, it raises many
questions for PCs since in these neurons calcium is supposed to play a very important role in the
induction of synaptic plasticity [5]. To address this question, we generated 148 new PC models. In these
models the somatic membrane voltages are stable, but the somatic calcium dynamics are very variable, in
agreement with experimental results [6]. Conversely, the calcium signal in spiny dendrites is robust.
Using a PC spine model of calcium signal transduction pathways [7], we demonstrate that the induction
of long-term depression is preserved for all models. We conclude that calcium is unlikely to be the sole
activity-sensor in this cell but that there is a strong relationship between activity homeostatis and synaptic
plasticity.

Acknowledgments
We thank T Doi, S Kuroda, T Michikawa, M Kawato and I Ogasawara for the availability of their model
and the kind help they provided us to run it.
References
[1] Neural response variability. A. Scientist and O. Colleague, Nature Neuroscience 4(23):1800-1810,
May 2008.
[1] Variability, compensation and homeostasis in neuron and network function. E. Marder and J.-M.
Goaillard. Nat Rev Neurosci 7: 563-574, 2006.
[2] Complex parameter landscape for a complex neuron model. P. Achard and E. De Schutter. PLOS
Comput Biol 2: e94, 2006.
[3] A model neuron with activity-dependent conductances regulated by multiple calcium sensors. Z. Liu,
J. Golowasch, E. Marder and L.F. Abbott. J Neurosci 18: 2309-2320, 1998.
[4] Activity-dependent regulation of conductances in model neurons. G. LeMasson, E. Marder and L.F.
Abbott. Science 259: 1915-1917, 1993.
[5] Cerebellar long-term depression: characterization, signal transduction, and functional roles. M. Ito.
Physiol Rev 81: 1143-1195, 2001.
[6] Robustness of burst firing in dissociated purkinje neurons with acute or long-term reductions in
sodium conductance. A.M. Swensen and B.P. Bean. J Neurosci 25: 3509-3520, 2005.
[7] Inositol 1,4,5-trisphosphate-dependent Ca2+ threshold dynamics detect spike timing in cerebellar
Purkinje cells. T. Doi, S. Kuroda, T. Michikawa and M. Kawato. J Neurosci 25: 950-961, 2005.

286

Cosyne 2008

Saturday evening, Poster session III-57

A uniﬁed voltage-based model for STDP, LTP and LTD
Claudia Clopath1 , Lars Buesing1,2 , Eleni Vasilaki1 and Wulfram Gerstner1
1

2

LCN – EPFL, Lausanne

IGI – TU Graz

Standard models for Spike Timing-Dependent Plasticity (STDP) take into account pair interactions of presynaptic and postsynaptic spike times [1, 2]. However, a number of pairing experiments [3, 4] show that pair
interactions are not sufﬁcient to capture Long-Term Potentation and Depression (LTP/LTD). Moreover, by
construction, simple spike-based model fail in voltage clamp experiments, where LTP or LTD can be induced by coincidence of presynaptic spikes arrival with depolarization of the postsynaptic membrane [4],
[5]. Here we present a triplet model which takes into account the presynaptic spike times and the postsynaptic membrane potential, ﬁltered with three different time constants. For spike induced experiments, this
model can formally be reduced to the triplet rule proposed by Pﬁster et al. [6], and yields similar results
to, for example, frequency dependent pairing experiment by Sjöström et al. [4] (Fig 1). Moreover, it also
reproduces the behavior of the ABS rule [5], i.e. no synaptic changes are observed under presynaptic stimulation when the postsynaptic potential is hyperpolarized; while small depolarization leads to LTD and strong
depolarization to LTP (Fig 2). Additionally, this model can describe the hybrid experiment by Sjöström et
al. where low-frequency potentiation that is normally absent in pair STDP experiments is rescued by depolarization [4]. This novel model can be used with hard bound, soft bound or any other arbitrary weight
dependence.
1/ ρ

Δw

Δw

0

-0.2 post before
pre
-50

0
T [ms]

50

0..
8
0.6 10ms pre before post
0.4
0.2
0
0.2
10ms
post before pre
0.4
0
10
20
30
40
50
ρ[Hz]

0.6
0.5
0.4
Δw

T
pre before
post

0.2

0.3
0.2
0.1
0

−0.1
−80

-70
-60
-50
membrane potential [mV]

Figure 1: Our model (lines) describes the timing (left) and

Figure 2: Our model reproduces

the pairing frequency dependence (right) of STDP as measured by Sjöström et al. (total weight change after 60 pairings, dots with error bars) [4].

the ABS behavior where the voltage is clamped at different values
during presynaptic stimulation [5].

Acknowledgments
This work was supported by the European grant FACETS and the Swiss National Foundation.

References
[1] Gerstner, W., Kempter, R., van Hemmen, L., and Wagner, H. (1996) Nature 383, 76–78.
[2] Song, S., Miller, K. D., and Abbott, L. F. (2000) Nat. Neurosci. 3, 919–926.
[3] Wang, H., Gerkin, R., Nauen, D., and Bi, G. (2005) Nat. Neurosci. 8(2), 187–93.
[4] Sjöström, P. J., Turrigiano, G. G., and Nelson, S. B. (2001) Neuron 32, 1149–1164.
[5] Artola, A., Bröcher, S., and Singer, W. (1990) Nature 347, 69–72.
[6] Pﬁster, J. P. and Gerstner, W. (2007) J. Neursci. 26, 9673–9682.

287

Saturday evening, Poster session III-58

Cosyne 2008

Sequential Effects: Annoying Quirk or Adaptive Behavior?
Angela J. Yu

Jonathan D. Cohen

Center for the Study of Brain, Mind, and Behavior, Princeton University
Just as visual illusions reveal the principles and mechanisms underlying natural visual processing, ”statistical
illusions” provide similar insight into statistical inference and learning in natural behavior. In a variety of
behavioral tasks, subjects show a sequential effect: they respond faster and more accurately to a stimulus
if it is consistent with the recent trend of stimuli, compared to when it violates the trend. This is true even
if the experiment has a randomized design such that stimulus identity cannot be predicted from previous
history. In this work, we use a normative Bayesian framework that examines the hypothesis that such
quirky idiosyncrasies reﬂect adaptive mechanisms important for learning about changing statistics in the
natural environment. We show that a prior belief in non-stationarity can induce an otherwise Bayes-optimal
algorithm to exhibit a pattern of sequential effects similar to human behavior. The Bayesian algorithm is
shown to be well approximated by linear-exponential ﬁltering of past observations, something also observed
in human and animal behavior in response to true changes in experimental contingencies – this serves as
further evidence that randomized experimental design nevertheless taps into statistical learning mechanisms
appropriate for non-stationary environments. We explicitly derive the approximately linear relationship
between the assumed rate of change in the Bayesian generative model, and the time constant of exponential
discounting in the linear-exponential ﬁlter. We show how neurons implementing standard leaky integration
dynamics, with appropriate tuning, can easily implement these computations. We also show that nearoptimal tuning of the leaky-integration process is possible, using stochastic gradient descent based only
on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal
prediction based on a history of noisy inputs using standard neuronal dynamics, but that they can also learn
to tune the processing parameters without explicitly representing probabilities. We verify the validity of our
model by comparing its predictions with human and animal behavior in both stationary and non-stationary
task settings.

Acknowledgments
We thank Peter Dayan and Philip Holmes for helpful discussions.

288

Cosyne 2008

Saturday evening, Poster session III-59

Spike Timing Dependent Plasticity with a Shifted Temporal
Window
Baktash Babadi and L.F. Abbott
Department of Neuroscience, Columbia University, New York, NY
Synaptic modiﬁcation due to spike-timing dependent plasticity (STDP) has been observed in many parts
of the nervous system. STDP involves potentiation of a synapse when the corresponding presynaptic spike
precedes a postsynaptic spike, and depression otherwise [1]. STDP has been shown to endow the incoming
synapses to a neuron with a competition needed for a variety of neuro-computational problems ranging
from development of cortical maps to sequence learning [2]. However, STDP has an inherent instability that
requires bounds to be placed on synaptic strength to avoided uncontrolled growth.
Here, we show that with a slight rightward shift of the STDP window (ﬁgure, left inset), STDP can be
intrinsically stable, not requiring synaptic strength bounds. By numerical simulations, as well as solving
the steady-state solution of the Fokker-Planck equation governing the distribution of the weights of the incoming synapses to a single integrate-and-ﬁre neuron, we show that any positive shift of the STDP window,
no matter how small, can stabilize the distribution of the synaptic weights (upper left panel of ﬁgure).
Assuming that the incoming spike trains to the
neuron are Poisson, the ﬁnal synaptic weights follow a gamma distribution, that can be calculated
analytically (ﬁgure and right inset, solid and dotted curves - analytic, shading and bars - simulations ). If a subset of the incoming spike trains
is correlated, a competition takes place between
their corresponding synapses and those of the uncorrelated spike trains. The outcome of the competition depends on the relative LTP/LTD balance
in the STDP window; if LTP exceeds LTD the
uncorrelated synapses are strengthened, while in
the preponderance of LTD the correlated synapses
gain strength. Alternatively, the net amount of inhibition to the neuron can govern the outcome of the competition; when the inhibition is higher than a certain threshold, the correlated synapses are strengthened and
vice versa. We conclude that the STDP window with a slight shift, which is beyond the resolution of current experiments, is not only stable but can implement both Hebbian and anti-Hebbian learning, depending
on the LTP/LTD or the net excitation/inhibition balance. Numerical simulations also show that the shifted
STDP results in a rich temporal dynamics at the network level.
Acknowledgments
Supported by NIH Pioneer Award 5-DP1-OD114-02 and grant MH-58754.
References
[1] Spike timing-dependent plasticity: from synapse to perception. Y Dan & MM Poo, Physiol Rev. 86:103348. Jul 2006.
[2] Timing in synaptic plasticity: from detection to integration. G-Q Bi & J Rubin, Trends Neurosci. 28:2228. May 2005.

289

Saturday evening, Poster session III-60

Cosyne 2008

Specificity versus associativity from correlation-based plasticity.
Mark Bourjaily1 and Paul Miller1
1

Volen Center for Complex Systems and Dept of Biology, Brandeis University.

Many cognitive tasks require association of two stimuli to produce a response that differs from the
response to either stimulus alone. For example, in the first phase of an associative transitive inference task
[1], rats are presented with containers identifiable by a single cue odor (“A” or “X”) then a choice of two
containers with odors (“B” or “Y”). If they dig in the container with odor “B” following cue “A” they find
a reward, as they do if they dig in the container with odor “Y” following cue “X”. Thus an association
must be learnt that “A” predicts “B” and “X” predicts “Y”. To successfully make the correct response of
“dig” in the container whose odor matches the one predicted for reward, but to “switch” to the other
container if it does not match, the rat must respond using logic equivalent to exclusive-or (XOR). A single
set of connections from inputs to outputs cannot be used to solve such logic --- a “hidden” layer is
essential. In the case of odor association, neurons responsive to a specific pairing, “A and B” must arise in
this intermediate hidden layer. It is important that such neurons, which can then activate a “dig”
response, do not respond to “A and Y” or “X and B”. In this paper we analyze how an initially randomly
connected network of spiking neurons can develop the necessary associativity and specificity to enable
correct responses to the task. For example, we find that spike-timing dependent plasticity (STDP) tends to
“over-associate” so cells responding to a combination of inputs (“A and B”) later respond also to “A”
alone. However, the more recently characterized long-term potentiation of inhibition (LTPi), counters
such “A” to “AB” excitation (see Figure), by generating cross-inhibition that is essential to produce and
maintain specificity in the circuitry. We discuss how the results of these different plasticity mechanisms
can be distinguished in the development of neural activity patterns during training in vivo.
Figure: Final pooled connectivity and example spike patterns following unsupervised learning via STDP
and LTPi in a random network with the inputs shown. A to AB excitation is countered by inhibition.

References
[1] Conservation of hippocampal memory function in rats and humans. M. Bunsey and H. Eichenbaum,
Nature 379(6562):255-257, Jan 1996.

290

Cosyne 2008

Saturday evening, Poster session III-61

Learning temporal representations through reward dependent
expression of synaptic eligibility traces in recurrent networks.
Jeff Gavornik1,2, Marshall G. Shuler3, Yonatan Loewenstein4, and Harel Z.
Shouval1
1

Department of Neurobiology and Anatomy, The University of Texas Medical School at
Houston, 2Department of Electrical and Computer Engineering, The University of Texas at
Austin, 3Department of Neuroscience, The Johns Hopkins University, 4Department of
Neurobiology, Department of Cognitive Science and the Interdisciplinary Center for Neural
Computation, The Hebrew University of Jerusalem.
Experimental recordings have demonstrated that neurons in the primary visual cortex are able to learn to
express temporal intervals as a function of reward timing [1]. The form of this expression is a transient
period of stimulus evoked activity that persists until the expected reward time. It is not known how
biological networks are able to learn temporal representations with time scales that are orders of
magnitude larger than the intrinsic excitability time constants of individual neurons. We have previously
developed a synaptic plasticity rule that allows a network with recurrent lateral connections to learn
synaptic weights sufficient to set network decay times corresponding to trained temporal intervals. Our
learning rule works by expressing plasticity eligibility traces, called proto-weights, as permanent lateral
recurrent synaptic weights. We demonstrate that a Hebbian implementation of our rule is able to encode
multiple intervals in both deterministic networks of passive integrator neurons and noisy stochastic
networks of non-linear integrate and fire neurons. We show that an extension to our rule, similar to the
covariance rule, allows training in networks with non orthogonal inputs and that our rule works with
sparse, non-symmetric connection matrices. Synaptic expression in our model is modulated by an external
reward signal that is inhibited by activity at either the network or single neuron level. We examine this
distinction and its experimentally verifiable implications for possible biological mechanisms. We
demonstrate that time-estimate error in our trained network increases linearly with encoded-time in
accordance with the Weber rule and make experimentally testable predictions based on the temporal
structure of both noise and stimulus evoked correlations.

Acknowledgments
This work is supported by NSF CRCNS grant 0515285.
References
[1] Reward timing in primary visual cortex. M.G. Shuler and M.F. Bear. Science 311(5767):1606-1609,
March 2006.

291

Saturday evening, Poster session III-62

Cosyne 2008

Environmental modifications of hippocampal theta-cycle
compressed cell assembly representations
Kamran Diba1 and György Buzsáki1
1

Center for Molecular and Behavioral Neuroscience, Rutgers University, Newark,
NJ 07102
Hippocampal place cells on a linear track can be used to define a temporally ordered event sequence.
These sequences are observed on the order of seconds during track running, and on the order of tens of
milliseconds during theta oscillations and sharp-wave ripples (in forward and in reverse). We tested the
plasticity of these temporal sequences, in CA1 and CA3 regions of the rat hippocampus, by shrinking the
length of the linear track. We varied the track length only once, rather than from trial to trial (Gothard et.
al. 2006). We observed that the sequence representation changed within a single trial, to reflect the
modified environment. The modified representation involved the disappearance of some place-fields, the
emergence of other new place-fields, and the shrinkage and shifting of the remaining place-fields. These
place-field modifications simultaneously reflect the adaptability and the rigidity of cell assembly
representations in the hippocampus.

References
[1] Binding of hippocampal CA1 neural activity to multiple reference frames in a landmark-based
navigation task.
K.M. Gothard, W.E. Skaggs, K.M. Moore and B.L. McNaughton, Journal of
Neuroscience 16(2):823-835, Jan 2006.

292

Cosyne 2008

Saturday evening, Poster session III-63

A computational analysis of changes in sensory representation
versus read-out underlying perceptual learning
Ching-Ling Teng and Joshua I. Gold
Department of Neuroscience, University of Pennsylvania, Philadelphia, PA 19104
Performance on perceptual tasks improves with training. Despite the prevalence of this phenomenon,
called perceptual learning, our understanding of the underlying neural changes remains incomplete. For
example, there is evidence that changes can occur either at early stages of sensory processing1,2 or at later
stages that convert sensory input into a categorical choice3,4. However, little is known about the
conditions that give rise to these different forms of plasticity or their implications for perceptual abilities
in general. We use a simple model5 that separates the sensory representation from the population readout
to gain insights into how perceptual decisions can be influenced by changes in each process alone or
interactions between the two. The model allows us to examine changes in tuning curves of individual
neurons, cortical magnification in the sensory representation and selective pooling or population read-out
of task-specific sensory information. We consider three types of perceptual task: stimulus estimation, fine
discrimination and detection.
Our preliminary results indicate that for stimulus estimation tasks, once the readout is optimized for
performance, further refinement of the sensory representation can lead to erroneous estimates of stimulus
parameters. In contrast, for fine discrimination tasks, once the read-out is optimized, a further narrowing
of tuning curves in the sensory representation can improve discriminability. However, this effect also has
limits, and performance is compromised when the tuning curves become too narrow relative to the current
read-out. Moreover, the benefit to performance is larger for changes in narrowly versus broadly tuned
neurons. However, the range of changes in sensory tuning that can give rise to improved performance
without changing the read-out is greater for more broadly tuned neurons. For fine discrimination and
detection but not for stimulus estimation, once the readout is optimized for performance, further
increasing the ratio of tuned versus spontaneous activity in the sensory representation can improve
performance substantially without changing the read-out. By focusing our results to a suitable
physiological range of neuronal firing properties, we hope to explain and further predict the underlying
neural mechanisms for perceptual learning for different neural populations and task-specific demands.
Acknowledgments
We thank Prof. Haim Sompolinsky for helpful discussions. Supported by R01-EY015260, P50MH062196, the Burroughs-Wellcome Fund, the McKnight Foundation and the Sloan-Swartz Foundation.
Representative References
[1] The neural basis of perceptual learning. Gilbert CD, Sigman M, Crist RE. Neuron 31:681–697, 2001.
[2] Plasticity in the frequency representation of primary auditory cortex following discrimination training
in adult owl monkeys. Recanzone GH, Schreiner CE, Merzenich MM. J Neurosci.13(1):87-103, 1993.
[3] Mechanisms of perceptual learning. Dosher BA, Lu ZL. Vision Res 39: 3197–3221, 1999.
[4] Physiological correlates of perceptual learning in monkey areas MT and LIP. Law C, Gold JI Soc For
Neuro Abs #621.15, 2005.
[5] Simple models for reading neuronal population codes. Seung HS and Sompolinsky H. PNAS.
90(22):10749-53, 1993

293

Saturday evening, Poster session III-64

Cosyne 2008

Calcium-based concurrent coincidence detectors reproduce various timing-dependent synaptic plasticity
Hiroki Kurashige1 , and Yutaka Sakai1
1

Tamagawa University

In the conventional view of spike-timing-dependent plasticity (STDP), the synaptic change depends on
the relative timing between the presynaptic and postsynaptic spikes. However, recent experimental
study suggests that the spike-timing-dependence also depends on the strength of presynaptic input[1].
Brieﬂy, using weak presynaptic input, paired spiking in
Δw
pre-before-post order cannot induce any synaptic change,
while using strong presynaptic input, LTP can be induced.
Whereas, paired spiking in post-before-pre order can induce LTD in the both cases. The main aim of this study
tpost − tpre [ms]
is to compose the uniﬁed model for synaptic plasticity
e τCa = 50, AP = 1.0
which can reproduce this aspect of STDP. Recent experτCa = 80, AP = 1.0
imental studies suggest that the induction of spike-timing τCa = 50, AP = 1.8
dependent LTD requires the coexistence of Ca2+ increase
τCa = 80, AP = 1.8
and the activation of metabotropic glutamate receptors
Figure 1: STDPs at several patterns of AP
(mGluR). Karmarkar and Buonomano’s model is in line
and τCa values induced by a single pairing
with the suggestions[2]. Their model incorporates two
stimulation, which approximates a situation
coincidence detections through NMDA receptors for LTP
of sufﬁciently low frequency pairings.
and mGluRs for LTD, and can reproduce the spike-timingdependence only in the case of strong presynaptic input,
but, cannot reproduce the dependence on the presynaptic input strength and several other important properties
Δw
of STDP. On the basis of their idea, we propose a model
stimulus frequency [Hz]
having two coincidence detectors incorporating the mech tpost − tpre = +5
anisms of the detection of lasting high Ca2+ concentratpost − tpre = −5
tion for LTP and of the coexistence of moderate preceding
Ca2+ elevation and following mGluR activity, the depressFigure 2: Pairing-frequency dependence of
ing synapses and the NMDAR suppression by voltage deSTDP induced in the present model. The
pendent Ca2+ channel activity. In this study, the situation
change of synaptic strength Δw induced by
of strong presynaptic input is represented by using large
ten times of pairing stimulations. The control
parameters for the Ca2+ decay time constant (τCa ) and the
parameters are set as τCa = 50 and AP = 1.
efﬁcacy of action potential (AP ) because strong input can
practically make the parameters be large through the induction of the dendritic spikes and slow Ca2+ extrusion. Then, our model can reproduce the dependence on the presynaptic input strength (Fig. 1). Also, our
model can reproduce the pairing-frequency dependence[1] which is important properties of STDP using the
weak presynaptic input (Fig. 2), the nonlinear effect of triplet stimulation[3] and the location dependence[4].
Acknowledgments Supported by xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx. Refs [1] P.J.
Sjöström et al., Neuron 32 (2001) 1149–1164. [2] U.R. Karmarkar et al., J. Neurophysiol. 88 (2002)
507–513. [3] R.C. Froemke et al., Nature 416 (2002) 433–438. [4] P.J. Sjöström et al., Neuron 51 (2006)
227–238.

294

Cosyne 2008

Saturday evening, Poster session III-65

A Line-Attractor Network Model of Multiple Choice Decisions
Moran Furman1 and Xiao-Jing Wang1
1

Department of Neurobiology, Yale University School of Medicine

Decision making with multiple alternatives has been subject of many psychological studies and mathematical modeling (e.g. Usher & McClelland, 2001). It is generally observed that a decision is harder, and
its reaction time is longer, when there are more choice options to consider. Recently, to investigate the
neural basis of multiple-choice decisions, Churchland et al. (2005a, 2005b) trained monkeys to perform a
visual motion direction discrimination task with multiple choice targets. They collected behavioral data and
recorded from the lateral-intraparietal (LIP) area, and found that changing the number of choices affected
both behavior and neural activity in several ways. For nontrivial discriminations (when the motion coherence is not too high), an increase in the number of choices resulted in lower monkey’s performance (the
rate of correct responses), longer reaction times, and reduced LIP neural activity during the early stages of
decision formation.
Here we propose a spiking network model for decision making in the multiple-choice motion discrimination task, using a continuous line-attractor circuit (Compte et al., 2000) which is capable of representing
all possible directions of a motion stimulus. We extended the model to a ‘double-ring’ structure with two
interconnected neural circuits. One module represents the cortical area LIP in which accumulation of information takes place; and the other represents the superior-colliculus (SC) where burst neurons are capable
of triggering saccadic responses. Our simulation protocol incorporates visual inputs to LIP that represent
both the targets and the motion stimulus, presumably coming from different visual pathways. The LIP units
accumulate sensory information, while SC bursts indicate threshold-crossing of LIP activity and termination of the decision process (Lo & Wang, 2006). Simulation results show that the model accounts for a
wide-range of behavioral and physiological data for 2, 4, and 8 choices. Speciﬁcally, the model reproduces
the performance and reaction-times as a function of motion coherence; longer reaction times in error trials
than in correct trials; and reduced neural responses during targets presentation with an increased number of
choices. We analyzed the synaptic currents to LIP model neurons and found that the reduction in the early
neural response for a larger number of choices results from a larger recruitment of the inhibitory neurons
in the network. Interestingly, the model shows a higher probability of erronous choices for a target if the
latter is closer to the correct target, thereby predicting a speciﬁc dependence of error rate on the similarity
between the chosen target and the correct one. This prediction is speciﬁc to a continuous network model in
contrast to a discrete network model (Wang, 2002) and experimentally testable.
Acknowledgments
Supported by The Swartz Foundation.
References
[1] Usher M and McClelland JL (2001). Psychological Review, 108:550-592.
[2] Churchland AK, Kiani R, Tam M, and Shadlen MN (2005a). Cosyne 2005 Abstracts.
[3] Churchland AK, Kiani R, Tam M, Palmer J, and Shadlen MN (2005b). Soc. Neurosci. Abstracts.
[4] Compte A, Brunel N, Goldman-Rakic PS, and Wang X-J (2000). Cerebral Cortex, 10:910-923.
[5] Lo C-C and Wang X-J (2006). Nature Neuroscience, 9(7):956-963.
[6] Wang X-J (2002). Neuron, 36:955-968.

295

Saturday evening, Poster session III-66

Cosyne 2008

Value comparisons as a mechanism at the neural basis of emotions
Nienke Korsten1, and John G. Taylor1
1

Department of Mathematics, King’s College London

Appraisal theory [1] states that the emergence of emotions is the result of a series of assessments of
incoming stimuli in the brain, the result of which determines which emotion (if any) we feel. Pinpointing
the neural correlates of appraisal in general has proven difficult, but there is compelling evidence for the
existence of neural correlates of stimulus value assessments, from rodent and primate single cell research
[2] as well as neuroimaging in humans [3]. Cells in the amygdala code for the direct reward value of the
stimulus, whereas the function of the orbitofrontal cortex appears to be more predictive [2].
As a neural site for monitoring these conflicting values, we suggest the anterior cingulate cortex (ACC).
This area has a conflict monitoring function in other neural processes [4] and was activated most often
across different emotions according to a metastudy of human neuroimaging research on emotions [5]. It is
connected to other brain areas implicated in value assessment [6], and has been shown to malfunction in
depression [7]. When a value conflict occurs, the ACC initiates an emotional response.
We propose a neural model encompassing a representation of these areas, in which we suggest that
emotions arise from comparisons/conflict between three instances of a particular perceived value (e.g.
food reward): the ‘actual’ value (the amount of food reward possessed at the moment), the ‘expected’
value (the amount of food reward expected to be possessed) and the ‘normal’ value (the amount of food
reward that we would normally, on average, expect to be possessing). When a discrepancy between two
of these instances is large enough (we are receiving a larger reward than we were expecting), an
emotional response emerges (joy, surprise). Through comparisons of these different value instances, we
can differentiate between the basic emotions of joy, anger, hope, fear, sadness, disappointment and relief.
Acknowledgments
We thank Nikos Fragopanagos for helpful comments. This work was supported by the HUMAINE EC
Network of Excellence.
References
[1] A systems approach to appraisal mechanisms in emotion. D. Sander, D. Grandjean and K.R. Scherer,
Neural Networks 18: 317-352, 2005
[2] Emotion and motivation: the role of the amygdala, ventral striatum, and prefrontal cortex. R.N.
Cardinal et al., Neuroscience and Biobehavioral Reviews 26:321-352, 2002
[3] Dissociable contributions of the human amygdala and orbitofrontal cortex to incentive motivation and
goal selection. F.S. Arana et al., The Journal of Neuroscience 23(29):9632-9638, October 2003
[4] Conflict monitoring and cognitive control. M.M. Botvinick et al., Psychological Review 108(3):624652, 2001
[5]Functional neuroanatomy of emotion: a meta-analysis of emotion activation studies in PET and fMRI.
K.L. Phan et al., NeuroImage 16:331-348, 2002
[6] Mapping the functional connectivity of the anterior cingulate cortex. D.S. Margulies et al.,
NeuroImage 37:579-588, May 2007
[7] Deep brain stimulation for treatment resistant depression. H.S. Mayberg et al., Neuron 45:651-660,
March 2005

296

Cosyne 2008

Saturday evening, Poster session III-67

Role of Medial Prefrontal Cortex and Secondary Motor Cortex in
Withholding Impulsive Action
Masayoshi Murakami1 and Zachary F. Mainen1
1

Cold Spring Harbor Laboratory

Withholding impulsive responding for short term gains to achieve long-term goals is an important aspect
of goal-directed behavior. Although it has been extensively studied how the brain evaluates rewards
available at different time points, few studies have investigated how brain withholds response for short
term gains to achieve a more valuable long-term goals. In this study, we investigate the role of medial
prefrontal cortex (mPFC) and secondary motor cortex (M2) in withholding impulsive responding in an
inter-temporal choice task.
In our task, there are two nose poke ports: one is for waiting, and the other is for delivering water reward.
If a rat succeeds in keeping its nose inside the waiting port for 400 ms, a tone is played. If the rat goes to
the reward port after this tone, it receives a small amount of water reward. If the rat waits longer in the
waiting port, a second tone is played after a random delay period (exponential distribution with 700 ms at
the earliest and around 2000 ms mean). If the rat goes to the reward port after the second tone, it receives
a large reward (3 to 4 times the small reward).
Rats learn to withhold responding for a small reward to obtain a large reward. We found that the time a
subject is willing to wait for a given set of reward amounts and delays varies randomly from trial to trial.
Thus, in combination with electrophysiological recording, we can compare the activity of neurons in trials
with different waiting times. We hypothesized that neuronal activity in those brain areas responsible for
suppressing impulsive behavior should be able to predict how long a rat will withhold a response.
We made recordings from mPFC/M2 neurons using implanted movable tetrodes during this task. We
found that activity of subset of mPFC/M2 neurons can indeed predict how long a rat will withhold a
response, that is, the firing rate of these neurons during a time window before the end of waiting was
significantly correlated with waiting time of the rat. Individual neurons showed both positive and
negative correlations with waiting time. Predictive activity occured at different time points during the
waiting period and even before waiting period.
These results support a possible role of mPFC/M2 in withholding impulsive choice in inter-temporal
choice task and may help to constrain circuit models of action withholding. In the future, we plan to
further differentiate the role of specific subregions of mPFC/M2 and to examine the interactions of these
areas with other cortical and subcortical areas implicated in response inhibition and timing.
Acknowledgments
We thank members of the Mainen laboratory for helpful discussions. This work was supported by Uehara
Memorial Foundation for Studying Abroad.

297

Saturday evening, Poster session III-68

Cosyne 2008

Prefrontal control of goal-directed behavior: a Bayesian model
Matthew Botvinick & Andrew Ledvina
Princeton University
Behavioral and neuroscientific research, in both animals and humans, increasingly suggests that
action selection occurs within two anatomically and computationally distinct systems. In one
system, actions are selected based on pre-established situation-action associations or habits. In
the other, actions are chosen in a goal-directed or purposive manner, based on a comparison of
the predicted consequences, costs and benefits of available lines of behavior. Goal-directed
action selection is believed to depend critically on both dorsolateral and orbital prefrontal cortex.
However, the computations performed by these areas in the generation of purposive action are
not yet well understood.
Raw materials for a computational account can be found in operations research. In particular,
work on dynamic programming offers a set of techniques for solving sequential decision
problems, given a transition function (a model of action-outcome contingencies) and a reward
function (a representation of the incentive value attached to different environmental states). One
particularly germane approach characterizes decision-making as a form of probabilistic
inference. Here, the transition and reward functions are understood as conditional probability
distributions, and a behavioral policy is chosen by inverting these two functions using Bayes’
rule. In intuitive terms, the algorithm asks ‘Given a particular initial state, if I adopt the
assumption that my actions will yield maximum utility, what are those actions most likely to be?’
We propose a policy induction model of prefrontal function,
which builds on this computational foundation. The model
takes the form of a probabilistic graphical model, encoding the
dependencies among three sets of variables: decision variables
(d), which, like some dorsolateral prefrontal neurons, code for
behavioral decision rules; utility variables (u), which, like some
orbitofrontal neurons, code for incentive value; and state
variables (s), which together encode a causal model of the
environment. Applying standard procedures for Bayesian
inference, networks of this kind can be proven to converge on
optimal behavioral policies. More importantly, the framework
provides an account for numerous empirical phenomena,
including latent learning, detour behavior, and effects of
devaluation and effort on instrumental choice.
The resulting theory complements and extends pioneering work
by Daw, Niv & Dayan [1] characterizing goal-directed behavior
as model-based tree search, and by Rao [2] modeling planning
in terms of probabilistic inference. It also relates closely to
computational work on goal-directed behavior by Hasselmo [3],
linking this with research characterizing neural computation in
terms of Bayesian inference [4].

A sequential decision problem
(from [5]) and its network
instantiation.

References
[1] Daw, Niv, & Dayan (2005). Nat. Neurosci., 8, 1704–1711. [2] Verma & Rao (2006). Proc. of
IEEE/RSJ International Conf. On Intelligent Robots and Systems. [3] Hasselmo (2005). J. Cog. Neuro..
17,1115-1129. [4] Knill & Pouget (2004). Trends Neurosci.. 27, 712-9. [5] Niv, Joel & Dayan, (2006).
Trends Cog. Sci., 10, 375-381.

298

C o s y ne 2 0 0 8

S a turda y e ve ning, P os te r s e s s ion I I I -6 9

Somatosensory decision-making in the head-fixed mouse
Daniel H. O’Connor and Karel Svoboda
Janelia Farm Research Campus, Howard Hughes Medical Institute.
We developed a simple somatosensory discrimination task for head-fixed mice. Mice used their whiskers
to judge the absolute distance to a small pole presented on one side of their head. Mice were trained to
indicate a target (“go”) position by licking for a water reward. When the pole was in a distracter (“no-go”)
position mice had to withhold a licking response to avoid an aversive airpuff.
After optimization of training parameters all mice (n=7) reached criterion performance (85% correct) in
7-14 daily sessions. Learning occurred both within and between sessions. Once trained, mice performed
about 200-500 trials per session (~50 min); individual mice performed at high levels (> 90%) for months.
To facilitate high-speed whisker tracking, we trimmed whiskers down to a single row or fewer. Mice
performed at high levels even with a single whisker. Stimulus sampling was associated with stereotyped
and asymmetric whisking patterns.
With all whiskers trimmed, trained mice performed at chance levels (51 ± 2% correct, n=3 mice),
indicating that mice solved the task with their whiskers rather than relying on other sensory cues.
We varied the difficulty of the task by decreasing the distance between the go and no-go positions,
obtaining psychometric-style curves. Mice with full whisker fields (n=3 mice) or a single row (n=1
mouse) were able to determine distance to better than 0.95 mm (~6 degrees of whisker angle).
Silencing contralateral barrel cortex using muscimol injections reversibly reduced performance to chance
levels (53 ± 2% correct, n=3 mice); silencing of the visual cortex had little effect (95 ± 0.2% correct, n=2
mice). Furthermore, ablation lesions of somatosensory cortex contralateral to the whisker stimulus
brought performance to chance levels (50 ± 3% correct, n=3 mice), whereas performance remained
unaffected with a similar lesion applied to the ipsilateral hemisphere. These experiments indicate that our
discrimination task depends critically on somatosensory cortex.
Acknowledgments
We thank Daniel Huber and Takaki Komiyama for critical suggestions on the behavior and surgeries, and
Takashi Sato, Chris Harvey and Leopoldo Petreanu for helpful discussions.

299

Saturday evening, Poster session III-70

Cosyne 2008

Inhibition and control in depression
Quentin JM Huys1,2 , Hanneke EM Den Ouden3 , and Peter Dayan2
Center for Theoretical Neuroscience, Columbia University, 2 Gatsby Computational Neuroscience Unit and 3 Wellcome Trust Centre for Neuroimaging, UCL

1

It is a commonplace that psychiatric disorders such as depression and schizophrenia involve substantial disturbances to affectively-charged decision-making. Although our understanding of the latter has substantially
advanced, with theoretical concepts drawn from reinforcement learning (RL) providing a normatively-sound
framework within which to link psychological, neurobiological and pharmacological results, there have hitherto been only a few applications to psychiatry. Here, we use RL to identify components of decision-making
that are key in depression, the psychiatric disease that is one of the world’s biggest scourges.
We focused on the central ﬁnding in depression of a vicious cycle, with impairments in decision-making apparently themselves contributing to the worsening of the disorder. For instance, stress is a main aetiological
factor in the development of depression, but a signiﬁcant fraction of the stress associated with depression
is self-generated. Humans with depression self-select into high-risk environments, i.e. they fail to choose
to avoid situations that control populations successfully avoid 2 , and are frequently involved in aggressive
acts 5 , potentially because they fail to choose to terminate aggressive behaviour before escalation 4 .
In RL terms, such vicious cycles can arise from forms of meta-learning, when moderately ineffective interactions in one environment (perhaps caused organically) lead to settings of expectations or priors that
generalize to other environments, and make interactions there even worse. In the case of depression, we
suggest the involvement of two such factors: variously associated with model-free, neuromodulatory, Pavlovian control, and model-based, instrumental control. With others 1 , we argue that an important function of
serotonin is reﬂexive, Pavlovian, behavioural inhibition in the face of anticipated punishment. Any lapse
in this inhibitory crutch will lead to defective avoidance of negatively valenced (stressful) states. This will
lead to excess punishment, and thus inﬂuence the environmental priors acquired by the model-based controller. Such priors will suggest that the subject is occupying environments that are more negative and less
controllable than the norm, and therefore lead subjects to increasingly inefﬁcient interactions.
We analyze this cycle, and use the deep understanding of controllability and learned helplessness in depression 3 to make a connection to tonic levels of dopamine in the disease. We also present preliminary data
from depressed and control subjects’ performance on a decision-making task developed to probe subjects’
prior beliefs about controllability and outcome bias along with their sensitivity to rewards and punishments.
Acknowledgments
Funded by: UCL Bogue Fellowship; Center for Theoretical Neuroscience; Gatsby Charitable Foundation;
Wellcome Trust.
References
[1] J. A. Gray. CUP, 1991.
[2] K. S. Kendler et al. Am. J. Psychiatry, 156:837–41, 1999.
[3] S. F. Maier and L. R. Watkins. Neurosci. Biobehav. Rev., 29(4-5):829–41, 2005.
[4] P. T. Mehlman et al. Am J Psychiatry, 151(10):1485–1491, Oct 1994.
[5] J. Monahan et al. OUP, 2001.

300

Cosyne 2008

Saturday evening, Poster session III-71

Prior probabilities and decision formation in LIP.
Vinod Rao1, Lawrence H. Snyder1, and Gregory C. DeAngelis2
1

Washington University in St. Louis,

2

University of Rochester

Prior expectations can alter one’s decisions about a stimulus. This prior may modify sensory encoding of
the stimulus, and/or may alter how the decision emerges from sensory evidence. Since area LIP has been
proposed as a site for evidence accumulation in a direction discrimination task [1], we examined how
manipulation of the prior might modify activity in LIP. In our task, an arrow-shaped cue, pointing in one
of two opposite directions, is presented for 200ms at the fovea. Following a 150 to 800ms delay, a foveal
random-dot motion stimulus is displayed for 250ms. The stimulus contained variable-coherence global
motion either along or opposite the cued direction [2]. The monkey is rewarded for making a saccade in
the direction of the motion stimulus. The cue’s direction matched that of the motion on 67% of the trials.
The monkey learned to significantly but incompletely bias his choices towards the cued direction,
especially on trials with weak, low-coherence motion. This bias suggests that the monkey integrates its
prior expectation with the motion information to improve its performance. Further, although the motion
was fixed-duration, the monkey responded significantly slower to stimuli whose motion direction
conflicts with the cue direction. LIP neurons were selected based on spatial tuning and maintained activity
in a memory saccade task. In our discrimination task, the motion axis and target locations were chosen so
that one of the saccade targets (T1) lay within the neuron’s response field (RF). Consistent with prior
work, LIP responses ramped up before T1 choices, and the rate of ramping activity increased with motion
coherence.
The main finding was that LIP responses were elevated when the monkey expected motion towards the
RF. Specifically, shortly after the onset of the cue, the neural response was increased on trials with cues
towards the RF, relative to trials with cues pointing away from the RF. This shift in activity persisted
during the motion stimulus and into the period where LIP activity would ramp up before a T1-choice. In
addition, we tested some cells with a motion stimulus lasting a full 1000ms (instead of 250ms). We
observed a much smaller, but significant increase in LIP activity starting around the motion onset on trials
with cues towards the RF. Because there is more motion signal to evaluate on these long-duration trials,
the prior has a weaker influence on the monkey’s choices (an average validity effect ~40% of the effect
on short-duration trials), and this is reflected in the LIP activity. It is notable that the slope of the ramping
activity before T1 choices was not altered by the direction of the prior cue. This is consistent with our
previous finding [3] that the cue had essentially no effect on responses in area MT. Together, these
findings suggest that the prior in our task acts on the computation of decision variables rather than on the
sensory representation of motion.
Acknowledgments
This work was supported by NIH grants EY013644 and MH077368.
References
[1] Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction
time task. J.D. Roitman and M.N. Shadlen, J. Neuroscience 22(21): 9475-89, Nov 2002.
[2] The analysis of visual motion: a comparison of neuronal and psychophysical performance. K.H.
Britten, M.N. Shadlen, W.T. Newsome, and J.A. Movshon, J. Neuroscience 12(12): 4745-65, Dec 1992.
[3] Expectation effects on motion perceptions and MT responses. V. Rao and G.C. DeAngelis,
Neuroscience Meeting Planner. Atlanta GA: Society for Neuroscience, 2006.

301

Saturday evening, Poster session III-72

Cosyne 2008

Dynamic, context-dependant spike response of visual neurons reflects inference and
learning in natural movies
Sophie Deneve, Timm Lochmann, Udo Ernst

Group for Neural Theory, Ecole Normale Supérieure, Collège de France, CNRS.
Experimental studies have demonstrated that stimuli outside the classical receptive field (cRF) of a
neuron can nevertheless strongly modulate the cell's response to stimuli inside the cRF. These nonclassical receptive field (ncRF) properties can drastically change the neuron's behaviour, e.g. its preferred
orientation. They also act differently depending on stimulus contrast, enhancing the firing rate when the
contrast inside the cRF is low, and suppressing the activity when the contrast is high. The extent and
influence of the cRFs and ncRFs varies over time: RF sharpen and preferred spatial frequency increases at
longer delays. These observations hint at a coarse to fine coding continuum as more information becomes
available: At low contrast or for short presentations, neurons integrate information from a wider range of
channels, decreasing detection threshold at the cost of a lower precision. At high contrast and longer
presentations, the summation field sharpens as neurons achieve higher precision. Several mechanistic
models have been proposed to account for the observed phenomena. However, few studies have
investigated the computational role of ncRFs. Here we show that ncRF properties and their contextual
dependencies emerge from natural visual statistics, more precisely, from detecting the presence of
dynamic, independent features in natural movies. Therefore, by drawing a parallel between the spatiotemporal statistics of the visual input and the plasticity and dynamics of a spiking neural networks, this
computational model proposes a new implementation for ncRFs modulatory effects.
The assumption that neural systems learn the underlying cause of their sensory input have successful
accounts for linear visual response properties, e.g. for the shape of cRF of V1 simple cells. However,
sensory systems have to estimate the state of constantly changing variables (i.e. the presence of oriented
edges in a movie) from noisy streams of stochastic data (photons, spikes), rather than static images.
Natural visual input consists of objects that all appear and disappear randomly and independently, and
superpose each other. When learning such a generative model, local spatial and temporal correlations in
natural movies lead to center surround, and simple cell-like RFs. Moreover, due to the temporal dynamics
embedded in the network, the predicted cRFs and ncRFs vary as a function of time and contrast, and
account for a very large family of contextual effects in LGN and V1. Interestingly, such a model can be
implemented in a spiking neural network, composed of adapting leaky integrate and fire neurons with
feedforward connections and lateral divisive inhibition. The resulting model also accounts for detailed
properties of spike response functions, such as the Poisson-like statistics, the transient nature of visual
responses, and the dependency of the spike-triggered average on stimulus contrast. This model is not
based on rate coding units, but on the assumption that spikes signal an increase in the probability of
presence of the neuron’s preferred object, and thus, extract and transmit the maximal amount of
information about the objects composing a scene.
This approach provides several new insights about the nature of visual processing. First it suggest that
visual neurons are better described by their “causal field” (i.e. the predicted consequence of the
appearance of their preferred object in the scene, which is invariant) rather than their RF. Second, by
taking time into account, it supports the idea that neural processing is adjusted to the spatial and temporal
statistics of its input and involved in probabilistic inference and decision making at the level of single
spikes. Finally, divisive inhibition (as opposed to subtractive inhibition) emerges as an essential
component of visual processing to implement explaining away, i.e. solve ambiguities between different
possible interpretations of a visual scene, the strongest inhibition occurring when the center and surround
stimuli are identical, rather than being a non-selective form of gain modulation.

302

Cosyne 2008

Saturday evening, Poster session III-73

Probabilistic Inference Using Stored Examples
Lei Shi, Thomas Grifﬁths
Helen Wills Neuroscience Institute, Department of Psychology, UC Berkeley
Perception involves inferring the state of the world from sensory evidence, considering a potentially unlimited state space. Human behavior is consistent with the optimal statistical solution to this problem in
many tasks, including cue combination [1] and sensorimotor learning [2]. Understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously
challenging. Here we propose a simple mechanism that can account for such behavior which only involves
averaging over a few feature detection neurons which ﬁre at a rate determined by their similarity to a sensory stimulus. We show that this mechanism is equivalent to a Monte Carlo method known as importance
sampling, and describe a basic neural circuit that can implement this algorithm.
Speciﬁcally, assume that a sensory system consists of a population of neurons, each of which ﬁres in response to a stimulus x at a rate proportional to a tuning curve f (x, θ), where θ are parameters describing the
response properties of that neuron (e.g., the stimulus of maximal response). Then, an average in which each
θ receives weight equal to the number of spikes produced by the corresponding neurons will approximate
the expectation of θ over the posterior distribution p(θ|x) produced by applying Bayes’ rule with likelihood
p(x|θ) proportional to f (x, θ) and prior p(θ) proportional to the number of neurons described by parameters
θ.
Simulation of a human sensorimotor learning task [2] shows a population of as few as 50 such neurons
accounts well for human behavior. This model can also be easily applied to the case of multiple sensory
modalities, such as haptic-visual cue combination [1]. In this case, each neuron combines information from
both sensory modalities, in a similar manner to [3]. The weighted sum using the spikes produced by these
neurons approximates the expectation of the posterior distribution over the state of the world (such as the
length of a block) responsible for producing a stimulus. Simulations show that as few as 50 such neurons
can also approximate human haptic-visual cue combination data [1].
The notion of using a population of neurons to represent a probability density is not new [3]. However, we
extend this observation in two ways. First, we show that a small subset of neurons, whose tuning curves
represent a small set of typical examples from sensory experience, is sufﬁcient for Bayesian inference.
Second, our theoretical analysis shows that this mechanism corresponds to a Monte Carlo sampling method.
Importance sampling enables one to estimate the expectation of a random variable which has a density
function hard to sample from by generating samples from a surrogate distribution and weighting these
samples by their “importance”. When the surrogate distribution is the prior, importance sampling takes the
same form as our model.

References
[1] Marc O Ernst and Martin S Banks. Humans integrate visual and haptic information in a statistically optimal
fashion. Nature, 415(6870):429–433, 2002.
[2] K. Körding and D. M. Wolpert. Bayesian integration in sensorimotor learning. Nature, 427:244–247, 2004.
[3] W. J. Ma, J. Beck, P. Latham, and A. Pouget. Bayesian inference with probabilistic population codes. Nature
Neuroscience, 9:1432–1438, 2006.

303

Saturday evening, Poster session III-74

Cosyne 2008

Minimal Assumption ROI-based fMRI Classification Analysis
Joset A. Etzel1, Valeria Gazzola1, and Christian Keysers1
1

School of Behavioral and Cognitive Neurosciences; Neuroimaging Center; University Medical Center
Groningen; University of Groningen; 9713AW Groningen, The Netherlands
Multivoxel pattern classification analysis of fMRI (functional magnetic resonance imaging) data has
many applications [1], but methods of determining significance of the observed accuracies are not well
developed. Also, many fMRI research hypotheses are ROI (region of interest)-based, which can be
difficult to address with existing paradigms. This work describes a set of methods for determining the
significance of the classification accuracy of discrete ROIs in the context of multivoxel fMRI data
classification. The methods do not answer the question, “is there significant activation anywhere in the
brain?” but rather “does the activity in these ROIs classify these conditions?” The method proposes
evaluating the significance and validity of the observed classification accuracy by means of three
procedures: a permutation test, a localization test, and random subsets.
The permutation test gives a significance level for the null hypothesis of arbitrary labeling (voxel activity
patterns unrelated to condition label) for each ROI. The localization test estimates the likelihood of a ROI
with the observed accuracy by calculating the classification accuracy of size-matched random subsets of
brain structures not expected to classify. The localization test thus provides an indication of the expected
range of accuracies for a specific number of voxels in the absence of condition-related activity, providing
a check for systematic errors and differences in ROI size. Finally, determining the classification accuracy
of random subsets of the voxels in each ROI provides another evaluation of the impact of ROI size upon
classification accuracy, as well as the importance of individual voxels for classifier performance.
The method was used to evaluate fMRI data collected by Gazzola, Aziz-Zadeh et al. [2] to investigate the
human auditory mirror neuron system. The subjects listened to five types of sounds: hand actions, mouth
actions, environmental, scrambled hand actions, and scrambled mouth actions. Above-chance
classification of the volumes into hand or environmental categories was possible for voxels in bilateral
primary somatosensory cortex as well as the left secondary somatosensory, primary motor, and premotor
cortices. Classification of the sounds into mouth or hand categories was less accurate, but was performed
bilaterally by the premotor cortex as well as the primary and secondary somatosensory cortices.
The use of the permutation test, the localization test, and random subsets thus creates a logical framework
allowing evaluation of the observed classification accuracy of particular ROIs without requiring complex
distributional models or assumptions.
Acknowledgments
The research was supported by a Nederlandse Organisatie voor Wetenschappelijk Oderzoek vidi and a
Marie Curie Excellence grant to C.K.
References
1.
Haynes, J.D. and G. Rees, Decoding mental states from brain activity in humans. Nat Rev
Neurosci, 2006. 7(7): p. 523-34.
2.
Gazzola, V., L. Aziz-Zadeh, and C. Keysers, Empathy and the Somatotopic Auditory Mirror
System in Humans. Current Biology, 2006. 16(18): p. 1824-1829.

304

Cosyne 2008

Saturday evening, Poster session III-75

A computational model of the basal ganglia fitted to LFP activity
points to enhanced synaptic efficacy as a cause of Parkinson’s
disease pathology
George L. Tsirogiannis1, Dimitrios Perakis1, George A. Tagaris2, Damianos
Sakas3 and Konstantina S. Nikita1
1

Biomedical Simulations and Imaging Laboratory, National Technical University
of Athens, 2 Department of Neurology, “G. Gennimatas” Athens General Hospital,
3
Department of Neurosurgery, University of Athens, “Evangelismos” Hospital
The prevailing view for the cause of Parkinson’s disease is the death of nigral dopaminergic cells. The
subsequent dopamine depletion is thought to crucially alter the functionality of cortico-basal ganglionic
circuits. It has been reported that under loss of dopamine, both inhibitory and excitatory synaptic efficacy
is enhanced. The objective of the present work is to show, through a population level computational
model of the basal ganglia, that the post synaptic potentials (PSPs) generating local field potentials
(LFPs) in the subthalamic nucleus (STN) exhibit an increase in both height and width, leading to the
appearance of the characteristic parkinsonian beta band peak in the LFP recordings from the STN.

The population level model is based on the formulation originally developed by Lopes da Silva [1] and it
is adapted to the specific architecture of the basal ganglia. According to a recent study [2], this kind of
models can reproduce field potential activity. In the present work, the output of the model is considered to
be LFP activity, like that being acquired during typical microelectrode recordings. The model comprises
three populations, the STN projection neurons, the projection neurons of the external segment of the
globus pallidus (GPe) and the striatal subpopulation that expresses D2 dopamine receptors (STR D2).
Cortical activity is modeled as a random process and is the source of two projections, to the STR D2 and
to the STN. Other connections that are included in the model are the projection of STR D2 to the GPe, the
reciprocal connections between GPe and STN and also an internal inhibitory connection within the GPe.
The model is fitted to real data which comprise the power spectral density of LFP recordings, derived
from Parkinson’s disease patients and characterized by a prominent beta band peak. From the fitting
procedure, the values of the free model parameters that minimize the error, corresponding to the height
and width of the PSPs, point to higher than normal values for peak voltage and duration. These results
provide indication that the enhanced synaptic efficacy, which could have been evoked by dopamine
depletion, leads to changes in the PSPs responsible for the LFP recordings in the STN and might be the
root cause for the appearance of the beta band peak in the LFP spectrum, thought to be a characteristic
expression of Parkinson’s disease.
Acknowledgments
This work was supported by the Greek Secretariat of Research and Technology.
References
[1] Models of neuronal populations: The basic mechanisms of rhythmicity. F.H. Lopes da Silva, A. van
Rotterdam, P. Barts, E. van Heusden and W. Burr, Prog. Brain Res.45: 281-308, 1976.
[2] Relevance of nonlinear lumped-parameter models in the analysis of depth-EEG epileptic signals. F.
Wendling, J.J. Bellanger, F. Bartolomei and P. Chauvel, Biol. Cybern. 83: 367-378, 2000.

305

Saturday evening, Poster session III-76

Cosyne 2008

	
			
			

	 	!
	





	


 


!"
  
	 
 # 
  
# 
 

$






	

	

	


%& '()$

	
    
    


  *  
	    &  

  
  
  
	  
  $  
#        


	
	$
!#




#

      
+  
	  
  
	     	    
	  


 


(,)-

	



	


  $    
            
    
  


  	    #    
          
	  

$
	+#
	
$
.



	
(/)#

$0	

	#

    
    
1  $  2    
  

      
          

  	    
	  


$*
1
#

$	1
	

1


  	+      
  
    3    $   

#
#
	



$




	
#


$


 #

#
#







$
"#	
	&   #
1  #  
      &  4
	  %5

6
-7899:/;<4=9>


¤6/'
	¥4
%	


 
¦2§¦;¦'$

	
() ©$.$7ª$.$«1©$«<$41	
$%%-,,¬,¦¦;
(,) !$<!$7		&$-/,;//9,¦¦=
(,) ­$7®!$<!$7ª$2
	&&'-¬¯=;,¦¦=

306

Cosyne 2008

Saturday evening, Poster session III-77

Efﬁcient estimation of two-dimensional ﬁring rate surfaces via
Gaussian process methods
Kamiar Rahnama Rad and Liam Paninski
Columbia University
Often we would like to estimate some two-dimensional ﬁring rate function from limited spike train observations.
We may consider the following examples:1) We observe a spatial point process whose rate is given by λ(r) =

f (z(r) + i ai gi (r)), where z(r) = z(x, y) represents an unknown two-dimensional function, gi (r) is some
set of known basisfunctions and ai isa set of scalar weights. 2) We observe a temporal process whose rate is
given by λ(t) = f z(r(t)) + k T y (t) , where r(t) is some known time-varying path through space — e.g., the
time-varying position of a rat in a maze [1] or the hand position in a motor experiment [2,3] — y (t) is a vector
of fully-observed covariates (possibly including spike history effects), and k is a vector of weights. 3) We make
repeated observations of a temporal point process whose mean rate function may change somewhat from trial
to trial1 ; in this case we may model the rate as λ(t, i), where t denotes the time within a trial and i denotes the
trial number [4].
In each case, the function f (.) is assumed to be convex and log-concave. Now we further assume that z is
generated by sampling from a Gaussian process with covariance function C(r, r ). This allows us to employ
standard regularization methods to estimate z, with no fear of any bad local maxima. An important feature of
these methods is that, unlike standard linear smoothing techniques, these Gaussian process methods automatically adjust their smoothness depending on how much data is available at a given spatial location r: the more
data we have near r, the less smooth we have to assume z is at r in order to obtain reliable estimates. See also
[5,6] for discussion of related Gaussian process-based approaches.
Here we show that, by making certain assumptions on the covariance function C (namely, that the inverse of C
has a certain block-tridiagonal form), we can compute the maximum likelihood estimate for z in O(d3/2 ) time
where d = dim(z). This may potentially be sped up to O(d) if more specialized multigrid algorithms are used.
We may also optimize the parameters deﬁning C (by maximizing the marginal likelihood, either directly or via
EM) in O(d2 ) time. Since typically d > 104 , this gain in computational efﬁciency is quite signiﬁcant.
Acknowledgments
This work was supported by an NSF CAREER award and a Sloan Research Fellowship to LP.
References
[1] A statistical paradigm for neural spike train decoding applied to position prediction from ensemble ﬁring
patterns of rat hippocampal place cells. Brown EN. et al. J. Neurosci. 18:7411-7425, 1998.
[2] Probabilistic inference of arm motion from neural activity in motor cortex. Gao Y. et al. NIPS 2002.
[3] Superlinear population encoding of dynamic hand trajectory in primary motor cortex. Paninski L. et al. J.
Neurosci. 24: 8551-8561, 2004.
[4] A Dynamic Analysis of Neuronal Spiking Activity In The Primate Hippocampus. Czanner et al. SFN 2005.
[5] Spatiotemporal prediction for log-Gaussian Cox processes. Brix and Diggle J. Roy. Stat Soci B 63 (4),
823-841, 2001.
[6] Inferring neural ﬁring rates from spike trains using Gaussian processes. Cunningham et al. NIPS 2007.
1

Thanks to C. Shalizi for pointing out this example.

307

Saturday evening, Poster session III-78

Cosyne 2008

Subjective contextual statistics
Adam N. Sanborn and Peter Dayan
Gatsby Computational Neuroscience Unit, University College London
Parallel to the substantial studies into the statistics of characteristics of natural scenes are recent attempts to
use experimental methods to elicit the actual prior distributions over such characteristics that are employed
by psychophysical subjects [1]. Iterated learning [2] is a powerful and general new method for doing just
this, in which subjects act a little like a Markov-chain Monte Carlo stochastic process whose stationary
distribution is the prior.
More particularly, subjects are asked to generate stimuli for themselves, ostensively based on feedback as
to what these stimuli should have been. In doing this, the inﬂuence of the feedback is inevitably tempered
by the inﬂuence of their prior distributions. Following a block of such trials, subjects repeat the same task,
except the feedback signals in the new block are taken as their guesses from the previous block. Kalish et
al [2] showed that after a number of iterations, this procedure is equivalent to producing samples from the
prior distribution alone for a set of stimuli. Intuitively, the inﬂuence of the initial feedback (whatever it may
be) becomes weaker on each iteration until the responses subjects produce only reﬂect the prior distribution.
To test and prove these ideas, we used the case of contextual effects associated with visual orientation. This
is an attractive domain, because the actual natural scene statistics have been well studied, and much is known
both neurophysiologically and psychophysically about contextual processing. In particular, phenomena such
as the tilt illusion prove that the stimuli in the surround of a target oriented bar exert a critical inﬂuence on
the perception that bar; further, the normative basis of this inﬂuence, and its relationship to input statistics
or subjective priors, are all hotly debated.
We adapted iterated learning to this case by presenting subjects with surround stimuli at various orientations,
positions, and contrasts, and providing them with a constrained ability to generate target stimuli. We present
preliminary results from this procedure.

Acknowledgments
We thank Joshua Solomon for help developing the stimuli. ANS was supported by a Royal Society USA
Research Fellowship and PD was supported by the Gatsby Charitable Foundation, the BBSRC, EPSRC and
the Wellcome Trust.
References
[1] Stocker, A.A. & Simoncelli, E.P. (2006) Noise characteristics and prior expectations in human visual
speed perception. Nature Neuroscience 9(4):578-585.
[2] Kalish, M. L., Grifﬁths, T. L., & Lewandowsky, S. (2007). Iterated learning: Intergenerational knowledge
transmission reveals inductive biases. Psychonomic Bulletin and Review, 14, 288-294.

308

Cosyne 2008

Saturday evening, Poster session III-79

Response of pyramidal cells to dynamic inputs: role of the axon
initial segment and implications of back-propagating action
potentials
Yuguo Yu, Yousheng Shu and David A. McCormick
Department of Neurobiology, Kavli Institute for Neuroscience, Yale
University School of Medicine, New Haven, CT 06520
Action potentials in cortical pyramidal cells are initiated in the axon initial segment (AIS) and
back-propagate through the soma and dendrites. Previous investigations of the interactions of
ongoing synaptic inputs and spike threshold have suggested that there is a wide variance in spike
threshold and that spike threshold is determined by the amplitude-time course of the membrane
potential just prior to the spike. However, these experiments were performed with recordings at a
distance (e.g. in the soma or proximal dendrites) of the spike initiation zone. By recording
simultaneously from the cell body and axon of cortical pyramidal cells, we have re-examined the
relationship between membrane potential trajectory and spike discharge properties. Our
recordings and models reveal that the relationship between spike threshold and membrane
potential in the soma is correlated, but variable, in comparison to that at the point of spike
initiation: the axon initial segment. Thus, recordings from the soma may reveal a biased and
noisy relationship between membrane potential trajectories and spike threshold properties that is
different from the axon initial segment.
Furthermore, our investigations of the
electrophysiological properties of cortical axons reveal that they are significantly different from
those of the cell body and dendrites. In particular, cortical axons contain a high density of the
low threshold and slowly inactivation K+ current known as the d-current. This current enables
cortical pyramidal cells to integrate synaptic events over a long time frame (seconds), allowing
for the performance of temporal integration. In addition, we found that at the axon initial
segment, spike threshold exhibits some degree of adaptation to specific properties of the stimulus,
resulting in increased sensitivity to transient inputs, facilitating the detection of synchronous
synaptic activity.
Our results reveal that the relationship between membrane potential fluctuations and spike
threshold is complex and varies in different neuronal compartments: axon initial segment, soma,
and dendrites. Future studies will need to take into consideration the complex
electrophysiological properties of axons to fully understand the input-output relationships of
cortical neurons.

309

Saturday evening, Poster session III-80

Cosyne 2008

Song Discrimination Using a Biologically Plausible Circuit That
Implements a Spike Distance Metric.
Eric Larson, Cyrus P. Billimoria, and Kamal Sen
Hearing Research Center and the Center for Biodynamics, Dept. of Biomedical
Engineering, Boston University
Recent work has shown that spike trains from individual sensory neurons can be used to discriminate between and classify stimuli [1]. Multiple groups have developed spike (dis)similarity metrics to quantify
the differences between spike trains. Using a nearest neighbor approach the spike similarity metrics can
be used to classify the stimuli into groups used to evoke the spike trains. The nearest prototype spike train
to the tested spike train can then be used to identify the stimulus. However, how biological circuits might
perform such computations remains unclear. Elucidating this question would facilitate the experimental
search for such circuits in biological systems, as well as the design of artiﬁcial circuits that can perform
such computations.
Here we present a biologically plausible model for discrimination inspired by a spike distance metric [2]
using a network of integrate and ﬁre model neurons. We then apply this model to the birdsong system in the
context of song discrimination and recognition. We show that the model circuit is effective at recognizing
individual songs, based on experimental input data from the ﬁeld L region (homologous to primary auditory
cortex). We also compare the performance and robustness of this model to two alternative models of song
discrimination: a model based on coincidence detection and a model based on ﬁring rate.

Acknowledgments
Thanks to Rajiv Narayan for the neural data and Ross Maddox for helpful discussions.
References
[1] Cortical Discrimination of Complex Natural Stimuli: Can Single Neurons Match Behavior? L. Wang, R.
Narayan, G. Grana, M. Shamir, and K. Sen, J. Neurosci. 27(3):582 - 589, January 17, 2007.
[2] A Novel Spike Distance. M. C. W. van Rossum, Neural Comp. 13:751-763, 2001.

310

Cosyne 2008

Saturday evening, Poster session III-81

Stiffness and damping of the neuro-musculoskeletal system cannot
be estimated adequately using a stiffness-damping-inertia model
DA Kistemaker1,2, LA Rozendaal2, PL Gribble1
1
2

Dept. of Psychology, University of Western Ontario, London, Canada,
Fac. of Human Movement Science, VU University, Amsterdam, The Netherlands

INTRODUCTION
Visco-elastic properties of the (neuro-)musculoskeletal system play an important role in the control of
posture and movement. Often, these properties are estimated in vivo using 2ndorder stiffness-dampinginertia (KBI) models. In such an approach, perturbations are applied to the musculoskeletal system and
subsequently parameters of the KBI model are optimized to obtain a best fit between simulated and
experimentally observed responses1. However, in the real musculoskeletal system the skeleton interacts
with a visco-elastic contractile element (CE) in series with an elastic tendon (SE); a system of at least 3rd
order. This raises three questions: i) can a KBI model adequately describe the behavior of the higher order
musculoskeletal system, ii) what information about the underlying musculoskeletal system is captured by
the estimated stiffness and damping, and iii) how sensitive are those parameters to measurement errors?
METHODS
A 3rdorder 1DF musculoskeletal model (MSM) actuated by a Hilltype muscle (Fig. 1A), was used to simulate responses to moment
perturbations. Subsequently, parameters of the KBI model (Fig.
1B) were determined to reproduce the MSM’s response. Analytical
approximations of the optimal fit were derived to investigate the
relation between fitted and actual dynamical parameters. A
sensitivity analysis was used to assess the range of KBI parameter
values that would still yield an adequate fit of the MSM response.

Fig 1. Models used. KCE = CE stiffness, BCE = CE
damping, KSE = SE stiffness, K = stiffness, B =
damping, and I = inertia.

RESULTS and DISCUSSION
The KBI model was not always capable of adequately describing the
behavior of the MSM; not even for time windows as small as 100 ms at
moderate levels of KCE, BCE and KSE (see Fig. 2), as often suggested in
the literature2. Simulations and analytical analyses showed that the
optimally estimated stiffness and damping depended not only on a nonlinear mixture of all dynamic parameters of the MSM, but also on the
frequency content of the perturbations applied (see Table 1). Perhaps
most problematic, it was found that estimated stiffness and damping
were very sensitive to small measurement and/or optimization errors.
These results show that 2ndorder KBI-models do not adequately
characterize the behavior of the 3rdorder MSM, and this will be all the
more true for the real neuro-musculoskeletal system as it is of even
higher order. This suggests that great care should be taken when
interpreting and using stiffness and damping values obtained by fitting
KBI models.

Fig 2. Responses of the MSM (gray) and
optimized KBI-model (black) for different
values of KCE, BCE and KSE.

K
K

low freq.
K SE
K CE
K CE  K SE
§
·
K SE
BCE ¨
¸
© K CE  K SE ¹

high freq.
K
2

B

I MSM K SE
BCE

Table 1. Analytical approximations of
stiffness and damping for low and high
frequency perturbations.

References
[1] Gomi H Kawato M Science 272:117-20, 1996. [2] Hunter IW Kearney RE J Biomech 15:747-2, 1982.

311

K SE

Saturday evening, Poster session III-82

Cosyne 2008

	







 !"#$#%&"#'"#( 
"
)) 
"



	?	
		
9		?
	
	

		@			


		


	
	

	

	
	
		

		 
  
 		 
 		 
  
 	 
 		 
 	 
  
 	 
 	 
 	 
 		 
 



	
	
	
	



		


		

	
	

 
  
 		 
 
 		 
 		 
 	 
 			 
  
  
  
 	 
  
 



	

	
	
		

	
	
	
	
		
 


	



		
	 
	
	!


		


"	


	
#		
		


			
		
	


				


		
	
			


	
	
		

$	#		


	

	
			


	

	
	
	
	
		


	
		
		
	


	

 
  
 	 
 	 
  	 
  
 	 
 	 
 			 
 	 
 	 
  
  
 	 
 		 
 		 
 

	 
	
	

%	
	
	


		
		

	!

		

	

	 
 	 
  
  
 		 
 	 
 	 
 		 
  
  
 	 
 	 
  
 		 
 		


		
	
			
			

$
	
		
		


	

	

	
	

	


	
	
		

	
	

		
		
	

	


		




				
	!


&
	

		
	


	

		 

	
	
	
	
	
		
	 
		

		 

		
	
	



		
	

		
	
	

	

		
	
		
			

	

		

	

	
				
&
			

		


	
		
		




	
			

		
	
		
	

				
		

	
		

		
	
		

		

			
		
	


	
	
	
	



 
 	 
 '	 
 () 
 $			 
 * 
 "	 
 	! 
 
 +
 	
 
  
 		

	

			
	

		
	
			

	
	



		

	
		

		


	
			
	
	
			
	


		
		



		
	
	
		

	
	

			

	



,
		
	
		
	


		
				

	
	

-	






		

			


		



	
	

 
  
  
  
 	 
  
  
  
 	 
 	 
 	 
  
  
  
 	 
 

		

%


	
	
	


	
	

	
		

	

	
	

	
	
		
		!
	

		
	

.
	

		

			
	
	
	

	
	
				



	
	

		 
  
  
 			 
 	 
  
 	 
  
  
  
 		 
 	 
 					
			
		

	


		



312

Cosyne 2008

Saturday evening, Poster session III-83

Modelling a duration-tuned neural circuit.
Brandon Aubie, Paul A. Faure, and Suzanna Becker
McMaster University, Hamilton ON, Canada
Signal duration is critical for many aspects of auditory processing (e.g. human speech). In vivo electrophysiology data from a variety of preparations suggest that duration selectivity is produced by a combination of
excitatory and inhibitory synaptic inputs to a neuron that are offset in time. This results in a range of duration selectivity that is tolerant to changes in signal amplitude [1]. Our research focuses on modelling neural
mechanisms of duration selectivity in the auditory midbrain (inferior colliculus) to better understand sensory temporal processing. Experiments using paired tone stimulation in big brown bats (Eptesicus fuscus)
have revealed that duration-tuned neurons receive an onset-evoked inhibition that precedes an onset-evoked
excitation, and that the inhibiton is sustained for at least the duration of the stimulus [2]. The current hypothesis our model implements is that duration-tuned neurons respond after this inhibition is released if a
ﬁxed latency sub-threshold excitatory post synpatic potential is received at the same time.
(a)
(b)
(c)
Stimulus Duration (ms)

25

20

15

10

5

0
0

5

10

15

20

25

Time (ms)

30

35

40

(a) Model duration-tuned circuit. Solid lines are excitatory and dashed lines are inhibitory synapses.
(b) Model response to a range of input durations that resembles in vivo single-unit responses from a durationtuned neuron (c) [2].
Here we use an Adaptive Exponential Integrate-and-Fire [3] model to explore mechanisms of duration selectivity. Duration-tuned neuron E is inhibited by B for the same duration as the stimulus. A rapidly adapting
cell C inhibits D for a ﬁxed duration after stimulus onset. The adaptation causes spiking to quickly diminish,
thus releasing D from inhibition. This causes a post-inhibitory rebound in D that projects sub-threshold excitation to E. Upon stimulus offset, E is released from inhibition and evokes a sub-threshold post-inhibitory
rebound. When this rebound excitation coincides with the the ﬁxed latency sub-threshold excitation from D,
the membrane potential of cell E surpasses spiking threshold and responds with a burst of action potentials.
Our model suggests that post-inhibitory rebound timing has a large effect on ﬁrst spike latency. Unlike a
simple energy integration circuit, the present model shows resilience to various input current amplitudes.
References
[1] Duration selectivity of neurons in the inferior colliculus of the big brown bat: tolerance to changes in
sound level. Fremouw T, Faure PA, Cassedy JH & Covey E, J Neurophysiology 94(3):1869-1878, 2005.
[2] Temporal masking reveals properties of sound-evoked inhibition in duration-tuned neurons in the inferior
colliculus. Faure PA, Fremouw T, Casseday JH & Covey E, J Neuroscience 23(7):3052-3065, 2003.
[3] Adaptive exponential integrate-and-ﬁre model as an effective description of neuronal activity. Brette R
and Gerstner W, J Neurophysiology 94(5):3637-3642, 2005.

313

Saturday evening, Poster session III-84

Cosyne 2008

Automated sorting of intracellular calcium signals for large-scale
imaging studies of neuronal and glial populations
Eran A. Mukamel, Axel Nimmerjahn, and Mark J. Schnitzer
James H. Clark Center, Stanford University
Spike sorting has long been an important aspect of the analysis of extracellular electrophysiological
recordings, prompting the development of automated techniques for assigning spikes into trains from
individual neurons. However, despite the recent growth of optical methods for recording cellular activity,
automated algorithms do not yet exist for identifying cellular signals from large-scale optical imaging
data. To address this need, we created and validated a computational approach to cell sorting based on
spatio-temporal independent component analysis (ICA) [1-3]. Our analysis method can automatically
extract neuronal and glial Ca2+ activity signals from optical imaging data sets. Such cell sorting requires
minimal human supervision, and thus scales favorably to large data sets. We validated each stage of our
cell sorting protocol by benchmarking its performance in analyzing artificial data sets with known signal
sources and noise statistics. For comparison, we used standard region of interest (ROI) analysis to extract
estimates of cellular signals from the same artificial movies. The performance of cell sorting was
quantified using signal fidelity, defined as normalized correlation between the true and extracted signals.
These studies revealed a graceful degradation in the performance of cell sorting as the signal/noise ratio in
the movie data decreased. Although fewer cells were recovered from noisy data sets, the cells that were
identified maintained high signal fidelity. By contrast, signals could be extracted with high fidelity by
semi-automated ROI analysis only from movies with low noise and relatively restricted fields of view.
To test our approach under experimental conditions, we analyzed two-photon fluorescence Ca2+-imaging
data recorded in mouse cerebellar cortex. Subjects were head-fixed but awake and free to run on a
trackball. We extracted up to 100 individual Purkinje cell complex spike trains from movies with a 500
¢m field of view. In these recordings, cell sorting also extracted up to 25 distinct active glial cell regions,
revealing transient activations of individual Bergmann glial fibers and different epochs of wave-like
events involving multiple glial cells. Despite the wide separation of time scales between fluorescence
signals arising from neuronal events, which last ~250 ms, and glial activations (~4 s), as well as the
distinct morphologies of these cell types, a unified cell sorting protocol was able to extract signals of both
types. An automated approach to analysis of large-scale optical recordings will be increasingly important
as the use of in vivo cellular level imaging techniques becomes widespread.
FIGURE 1. One glial two neuronal [Ca2+] signals in
an artificial data set (gray traces and spike rasters) are
compared with the cor-responding signals extracted
by region of interest analysis (a) and ICA cell sorting
(b) (colored traces and rasters). The spatial filters
(left) corresponding to each extracted signal show
that ICA cell sorting unmixes signals from
overlapping cells to prevent crosstalk and accurately estimate neuronal spike times.
References
[1] Stone, J.V., et al., Neuroimage, 15(2):407-21, 2002; [2] Hyvarinen, A. and Karthikesh, R.
Neurocomputing, 49:151–162, 2002; [3] Reidl, J., et al., Neuroimage, 34(1):94-108, 2007.

314

Cosyne 2008

Saturday evening, Poster session III-85

Plasticity and Population Coding
Bryan Tripp and Chris Eliasmith
Centre for Theoretical Neuroscience, University of Waterloo
Synaptic plasticity and population coding are key topics in the theory of large neuronal circuits, but the relationships between them are not well understood. For example, little is known about the effects of plasticity
on the dimension of the space that a neural population encodes, or the distribution through this space of
neuron preferred direction vectors (a dynamic property1 ). Here we describe a method of interpreting the
effects of synaptic plasticity on preferred directions, and on decoding and mapping of population activity.
This method builds on a theory of population coding which has been described previously.2 This theory
exploits the fact that the activity of neurons in a population can often be described in terms of a lowdimensional represented variable x. For example in cosine-tuned neurons, the activity ai of the ith neuron
T
in a population is ai = G(φ̃i x), where φ̃i is the neuron’s preferred direction. Functions of x can then

be decoded as fˆ(x) = i (φfi ai ), with appropriate decoding vectors φfi . The synaptic weights wji which
can communicate fˆ(x) to a second population (with neuron indices j) can then be found analytically2 as
T
wji = φ̃j φfi . Notably, the dimensions of the vectors that are represented by the pre- and post-synaptic
populations bound the rank of the resulting synaptic weight matrix. Plasticity, on the other hand, can create
synaptic weight matrices of arbitrary rank. To relate plasticity to the above framework, we perform singular
value decomposition of the weight matrix. This provides the dimension of the space that is encoded postsynaptically, along with the preferred direction and decoding vectors. This allows us to track changes in
these vectors as a network learns, revealing the effects of plasticity on the population code. Interestingly,
plasticity can change the dimension of a population, corresponding to the gradual emergence or disappearance of singular values. If the rank of the weight matrix is low compared to the number of neurons, this
indicates that the post-synaptic population represents low-dimensional information in a redundant population code. If the rank is high, further analysis can distinguish between several possibilities, including: 1)
different post-synaptic neurons decode essentially the same functions in different ways, and 2) the populations represent high-dimensional information.
We illustrate this approach with two examples. First, we analyze the synaptic weights that arise from a
learning rule that produces a scalar communication channel.2 We verify that the resulting encoding is onedimensional. Secondly, we compare various lossy compression networks that represent reduced information
redundantly. The networks differ in their distributions of encoding vectors in the reduced space. This inﬂuences the functions that can be extracted by downstream neurons, and the manner in which these functions
can be learned. The theory presented previously2 allows us to ﬁnd families of networks with given high-level
information-processing properties and dynamics. The present extension provides an opportunity to work in
the other direction, beginning with the low-level details of a network and inferring the high-level function.
Acknowledgments
This work was supported by NSERC (261453-05 and CGS-D), CFI (3358401), OIT (3358501), and CRC.
References
[1] Motor Learning with Unstable Neural Representations. U. Rokni, A.G. Richardson, E. Bizzi, and H.S.
Seung, Neuron 54:653-666, 2007. [2] Neural Engineering. C. Eliasmith and C. Anderson, MIT Press, 2003.

315

Saturday evening, Poster session III-86

Cosyne 2008

Timing errors in sequence experiments: Implications for models
of temporal processing
J. Haß1,2,3, S. Blaschke1,2 , T. Rammsayer1,4 and J. M. Herrmann1,2,3
1

2
Bernstein Center for Computational Neuroscience Göttingen,
University of
3
4
MPI for Dynamics and Self-Organization,
University of Bern
Göttingen,

From the execution of complex motor plans to the perception and generation of speech, our brain depends
on reliable representations of time. Currently, it is not easy to decide which of the many existing theories
and models decribes best the neural processes underlying these representations. We have conducted psychophysical experiments in combination with detailed computational modeling in order to critically evaluate
existing theories and to ﬁnd a neuronal model which is in accordance with the psychophysical results.
One testable prediction that seperates two different classes of models is about the inﬂuence of context information on timing errors. If participants are presented with a sequence of intervals of constant length (CI),
followed by an interval of a slightly varied length (VI), one might expect that this interval is easier to detect
the more CIs are previously presented. However, evidence for this effect is ambiguous and there are both
models that predict an augmented discrimination and those which deny such an effect.
In our experiments, the participants listened to sequences of isochronous tone intervals and had to detect a
temporally deviating interval VI that could occur at different positions within the sequence. We found that
the position of the VI inﬂucenced the discrimination threshold, so we could rule out the class of models
that do not predict an adaptation effect. The results also question models based on oscillatory processes, as
phase information did not effect the performance. Furthermore, we varied both the duration of the standard
intervals and the number of presented intervals in the sequence. While the discrimination threshold increases
with the duration of the intervals, indicating impaired performance, the number of intervals has no effects.
This implies that the sequence is not processed as a whole. In this case, additional intervals would increase
the total length of the sequence and decrease the performance, just as changing the length of the intervals
does. Thus, we conclude that each interval must be processed individually in a ﬁrst stage and the effects of
the context information can be separated from those of the interval duration.
Finally, we present a computational model of the stage that is directly concerned with temporal processing
and which explains the dependence of temporal variablity on the length of the intervals. While it is known
that timing errors increase monotonically with the duration of the interval to be processed, current theories
also postulate that they obey Weber’s law, which states that this increase is linear. This view is challenged
by recent experimental ﬁndings which report deviations from linearity at longer intervals. Our model, which
is based on a set of synﬁre chains with different speeds of transmission, explains the complete form of the
error course as the optimal solution under limited resources. Furthermore, we discuss a mechanism based
on spike-timing dependant plasticity which implements the neuronal selection of the optimal chain for each
given interval of time. This mechanism also integrates the output from the various chains into a unique
representation of time, and provides an explanation for training effects on temporal processing.
Acknowledgment
This work was supported by a grant from the BMBF in the framework of the Bernstein Center for Computational Neuroscience Göttingen, grant number 01GQ0432.

316

Cosyne 2008

Saturday evening, Poster session III-87

	

			

	
		
/

	
		


                          
!!  !      	    !  !            !
!"# $ 	!
!%!!&" !
& 	 	
&!!"
	! 
! ! &"

&  	
 !!	! 		"
"
   "
!'(#  )* "
 ! !!%
+ " !%
"!	 (	
    &    ,-(    -(	        ,              
!"! 	!
 ! 	
	  ".!
 ! !!!	
!  !"(	!&! 
          !  !  "  
        !
%!" "


  /!$!+
%       !   '(#"  0          !      
    !  !      !    1!         2"  0  
          !   !     !	    &    
1 !2 !	 !!
!!!  "

317

Saturday evening, Poster session III-88

Cosyne 2008

The use of temporal data mining
to model the C. elegans neuronal network
Patrick Butler1 , Debprakash Patnaik1 , Casey Diekman2 ,
Naren Ramakrishnan1 , and K.P. Unnikrishnan3
1

Virginia Tech,

2

University of Michigan,

3

General Motors R&D

Bioinformatics methods have become immensely popular in the past decade to study properties of molecular
networks underlying protein-protein interactions, transcriptional regulation, and phenotype proﬁles (e.g., see
[1]). With the availability of the synaptic connectivity of the C. elegans nematode, and large-scale datasets
such as from multi-electrode arrays (MEAs), similar ‘network biology’ approaches to computational neuroscience are gaining interest. For instance, Reigl, Alon, and Chklovskii [2] have demonstrated the existence
of 2-, 3-, and 4-neuron motifs that might form building blocks of larger computational modules. Recently,
motifs in the C. elegans network have also been used as starting points to understand the neuronal basis
of higher-level behaviors such as spatial orientation [3]. Here, we present the use of temporal data mining
methods to gain insight into the neuronal network of C. elegans. Using an event sequence model formulated
over the C. elegans neuronal system, we show how temporal patterns of ﬁrings can be captured using the
notion of ‘frequent episodes’ [4] popular in the data mining literature. We identify many motifs embedded
in the neuronal network that exhibit both structural and temporal (dynamic) properties and which can serve
to summarize the state of the network as a whole. We deﬁne measures of network reconstruction that help
characterize the sensitivity and speciﬁcity of our approach and show how the data mining algorithm can be
steered to guarantee given constraints on reconstruction effectiveness.
References
[1] Network motifs: simple building blocks of complex networks, Milo, R., Shen-Orr, S., Itzkovitz, S.,
Kashtan, N., Chklovskii, D.B. and Alon, U., Science 298(5594): 824–927, Oct 2002.
[2] Search for computational modules in the C. elegans brain, Reigl, M., Alon, U. and Chklovskii, D.B.,
BMC Biology 2(1): 25, 2004.
[3] Circuit motifs for spatial orientation behaviors identiﬁed by neural network optimization, Dunn, N.A.,
Conery, J.S., and Lockery, S.R., J. Neurophysiol 98: 888–897, 2007.
[4] Discovering patterns in multi-neuronal spike trains using the frequent episode method, Patnaik, D., Sastry, P.S., and Unnikrishnan, K.P., arXiv.org url http://arxiv.org/abs/0709.0566v2.

318

Cosyne 2008

Saturday evening, Poster session III-89

Reconstructing functional connectivity from multi-neuronal
spike train datasets using probabilistic graphical models
Patrick Butler1 , Naren Ramakrishnan1 , and K.P. Unnikrishnan2
1

Virginia Tech,

2

General Motors R&D

Many statistical and information-theoretic approaches [1] have been proposed for analyzing time series
datasets from multi-neuronal arrays to infer functional connectivity. These methods typically are devoted to
studying all-pairs correlations [2] or, recently, relations between two spike trains conditional on a third [3].
We present an approach to reconstructing arbitrary-degree functional connectivities that does not require all
possible combinations to be tested. Rather than test for dependence, our approach assesses independence
(and conditional independence) between spike trains. Given two spike trains, we assess the minimal set of
other spike trains necessary to ‘decouple’ the given trains. Such decoupling relationships are summarized in
the form of a Markov network, or an undirected probabilistic graphical model. The network encodes conditional independence relationships using its edges: a node (neuron/channel) is conditionally independent of
all other nodes given its neighbors. It serves multiple uses: predictive (e.g., assessing the dynamics of stimulated versus spontaneous ﬁring), diagnostic (e.g., determining what will induce a given ﬁring sequence),
and abductive (e.g., assessing the result of ‘blocking’ a given path in a ﬁring circuit). The networks are
reconstructed by generalizing the classic methods of Perkel, Gerstein, and Moore [4] to assess conditional
independence between spike trains, and utilizing the notion of d-separation from probabilistic reasoning [5]
to lower the number of conditional independence tests performed. Together, our methods help avoid the
large number of false positives obtained by approaches that directly assess dependence and which are hence
unable to distinguish between direct and indirect relationships. To track developing cultures, we present a
differential approach that helps distinguish relationships underlying growing circuits from transient ones.
We argue that our graphical models make explicit the constraints underlying the functional connectivity by
identifying a small set of couplings that explain the data nearly as well as the complete set. Furthermore,
their rigorous probabilistic semantics enables prediction and inference. Experimental results are illustrated
using cortical networks cultured on multi-electrode arrays.
References
[1] Multiple neural spike train data analysis: state-of-the-art and future challenges. Brown, E.N., Kass, R.E.,
and Mitra, P.P., Nature Neuroscience 7(5):456-461, May 2004.
[2] Weak pairwise correlations imply strongly correlated network states in a neural population. Schneidman,
E., Berry II, M.J., Segev, R. and Bialek, W., Nature 440:1007–1012, 2005.
[3] The functional structure of cortical neuronal networks grown in vitro. Bettencourt L.M.A., Stephens
G.J., Ham M.I., Gross G.W., Phys Rev E 75: 021915, 2007.
[4] Neuronal spike trains and stochastic point processes II: simultaeneous spike trains, Perkel D.H., Gerstein,
G.L., and Moore, G.P., Biophys J. 7(4): 419–440, 1967.
[5] Probabilistic reasoning in intelligent systems, Pearl J., 552 pages, Morgan Kaufmann, 1988.

319

Saturday evening, Poster session III-90

Cosyne 2008

Using transfer entropy to unveil the effective connectivity for
MEG measurements: an application to a Simon task.
Raul Vicente1,2 , Michael Wibral3 , Jochen Triesch2 , and Gordon Pipa1,2,4
1

Department of Neurophysiology, Max-Planck Institute for Brain Research,
2
Frankfurt Institute for Advanced Studies
Frankfurt am Main, Germany,
3
(FIAS), Frankfurt am Main, Germany,
Brain Imaging Center, Frankfurt am
4
Massachusetts Institute of Technology (MIT) and MasMain, Germany,
sachusetts General Hospital (MGH), USA.
The functional connectivity of the brain describes the network of statistically correlated activities of different
brain areas. However, as it is well known correlation does not imply causality and most of synchronization
measures are not able to distinguish context-dependent causal interactions among remote brain areas, i.e.
to determine the so-called effective connectivity [1]. This type of effective or causal brain networks is a
fundamental step in unveiling the neural circuitry of brain areas and its directed interactions involved in the
processing of information.
In this work we make use of the transfer entropy concept to establish the effective connectivity of healthy
human brains under a simple Simon task from MEG measurements with a view to test the validity of this
approach in neuroimaging techniques. The formalism of transfer entropy, based on information theory,
has recently been proposed as a rigorous quantiﬁcation of the information ﬂow among several systems in
interaction and it is the natural generalization of the well-known mutual information [2]. In the experiment
the subjects are presented with an ”L” or ”R” letter in either the left or the right side of a screen and
instructed to press a left-side key in response to the ”L” letter and the right-side key in response to the ”R”
letter, independently of the spatial location of the stimulus. The data were collected with a whole-head 275
channels CTF-MEG scanner at a temporal resolution of 1.2 kHz.
After appropriate preprocessing and artifact rejection of the MEG data, the transfer entropy between all
possible pairs of channels was computed with a Kozachenko-Leonenko estimator while the statistical signiﬁcance of the causal interactions was determined by a non-parametric permutation test. The ﬂow of
information revealed that different neuronal circuits (including long-range causal inﬂuences as interhemispheric temporal lobe interactions) are recruited for the four different experimental conditions (”L” letter on
the left side, ”R” letter on the right side, ”L” letter on the right side, and ”R” letter on the left side) during
the processing of the visual stimulus and production of the appropriate motor response, in accordance to the
physiologically expected predictions. The effective networks of Simon versus non-Simon effect conditions
(the ﬁrst two conditions versus the last two) are also compared and analyzed in basis to advance in the use
of effective connectivity as a tool to better understand and classify different cognitive processes.
Acknowledgments
We thank Mario Chavez and Ernesto Pereda for helpful discussions. This work was partially supported by
the Hertie Stiftung and the European Project GABA (FP6-NEST contract 043309).
References
[1] Functional and effective connectivity in neuroimaging: a synthesis. K.J. Friston, Hum. Brain Mapping.
(2):56-78, 1994. [2] Measuring Information Transfer. T. Schreiber, Phys. Rev. Lett. (85):461-465, 2001.

320

Cosyne 2008

Saturday evening, Poster session III-91

Accounting for noise when estimating Variance Explained
Ralf M. Haefner & Bruce G. Cumming
Laboratory for Sensorimotor Research, National Eye Institute, NIH
Constructing models of biological systems, e.g. in systems neuroscience, mostly aims at providing functional descriptions, not fundamental physical laws. It seems likely that any parametric model of signal
processing in single neurons can be ruled out given a sufﬁcient amount of data. Rather than only testing the
statistical validity of a particular mathematical formulation against data, e.g. by using a χ2 -test, it is equally
important to know how much of the signal, or variance, in the data is explained by the model. This is commonly measured by Variance Explained (VE). A fundamental problem of the traditional estimator for VE is
its bias in the presence of measurement noise in the data: since the total variance in the data consists of the
true underlying variance and that due to noise, the traditional estimator yields a systematic underestimation
of the true VE of the model in the absence of noise (David & Gallant 2005, Sahani & Linden 2003).
We derive an analytical expression for the dependence of VE on sampling noise and present a more accurate
estimator of the true VE in the presence of noise. We tested it in a wide range of simulations ﬁtting various
functions to simulated data with varying amounts of noise. The data were drawn from models that were
identical to the one ﬁtted (true VE = 100%) or different (true VE < 100%). For our new estimator we ﬁnd
that the root-mean-square error (RMSE, of the difference between the true VE in the absence of noise and
the estimated VE in the presence of noise) is reduced by up to 80% in typical applications and never worse
than the RMSE of the original, widely-used formula.
We compare the new estimator with the traditional formula by testing the performance of the binocular energy model in explaining the responses of disparity-selective neurons in primate V1. Our comprehensive
data set currently includes 51 neurons for each of which we measured responses to a) disparity in drifting
gratings with frequencies covering most of the spatial frequency tuning curve and b) correlated and anticorrelated random dot stereograms. Based on these data we have concluded (Haefner & Cumming 2006) that
about 70% of our neurons deviate statistically signiﬁcantly from the binocular energy model3 . However,
using our new estimator we ﬁnd that in 70% of neurons the energy model explains more than 80% of the
variance. This suggests that many of the signiﬁcant deviations are nonetheless small. Using the traditional
formula, we ﬁnd that the energy model explains less than 80% of the variance in 75% of our neurons. Thus,
a more accurate estimator for VE can lead to qualitatively different conclusions.
Acknowledgments
We thank C. Quaia for helpful discussions. This work was supported by the Intramural Research Program
of the NIH, National Eye Institute.
References
[1] How Linear are Auditory Cortical Responses? M. Sahani and J.F. Linden, In: Advances in Neural
Information Processing Systems, vol. 15, p. 301-308, 2003
[2] Predicting neuronal responses during natural vision. S.V. David and J.L. Gallant, Network 16(2-3):23960, Jun-Sep 2005
[3] Responses to binocular gratings and RDS imply summation of multiple energy units within macaque
V1. R.M. Haefner and B.G. Cumming, SfN Abstract, 2006

321

Saturday evening, Poster session III-92

Cosyne 2008

Maximally reliable Markov chains under energy constraints
Sean Escola, Michael Eisele, and Liam Paninski
Columbia University
Signal to noise ratios in physical systems can be signiﬁcantly degraded if the latency or processing time of
the system is highly variable. Molecular computations where precise temporal procession is a necessary
feature (e.g. the ion channel dynamics involved in action potential generation and membrane refraction)
appear to have reduced their processing time variabilities by employing multiple steps. Numerical studies
of the activation time of rhodopsin molecules, for example, have revealed that the low variability observed
experimentally can only be accounted for by assuming that the molecules shut off by means of a multi-step
process, and that, if implemented by the retina, a single-step system would lead to much poorer vision [1].
We generalize this theoretical result and prove that the reliability of the passage time from the ﬁrst state to
the last state in a multi-state system with no memory (i.e. a Markov chain) is maximal if and only if the
system proceeds irreversibly through each state from ﬁrst to last with identical transition rates between all
consecutive pairs of states. Speciﬁcally, we show that for such a linear Markov chain, the following relation
2
holds: var(t) = NT−1 , where t is the passage time through the system, T is the mean passage time (which
we consider to be ﬁxed), and N is the number of states. The variance of the passage time exceeds this value
for any other network topology. It is thus clear that by increasing the number of states, it is possible to
arbitrarily decrease the variance of the passage time or, equivalently, increase the reliability of the system.
In a physical system, however, irreversible transitions are implemented by maintaining large energy drops
between states. As the number of states in a linear Markov chain increases, the total energy across the length
of the chain also increases, and thus inﬁnitely long chains (which would be perfectly reliable) would require
inﬁnite energies. To study the effect of energy constraints on Markov chain reliability, we numerically
optimized the transition rates between all pairs of states in an N -state Markov chain to minimize the variance
of the passage time while holding T and the total energy Etot constant, where transition rates of zero and
inﬁnity correspond to energies of inﬁnity and zero respectively. Thus transition rates near zero (needed to
prevent reverse transitions in a linear chain) are only possible if Etot is permitted to be large. We found that
as Etot is decreased, the variance increases as expected, but also that at certain points the linear structure
becomes unstable and that states merge to meet the more stringent energy constraint, decreasing the effective
number of states. Eventually, at low available energy values, all intermediate states merge with the ﬁrst,
and the system becomes a one-step (i.e. minimally reliable) process. These results suggest that there is a
fundamental tradeoff in physical systems between reliability (preferring large, linear Markov chains) and
conservation of energy resources (preferring small, fully-connected chains).
Acknowledgments
We thank W. Bialek for helpful discussions. This work was supported by the NIH Medical Scientist Training
Program, the Columbia University MD-PhD Program, an NSF CAREER award, and an Alfred P. Sloan
Research Fellowship.
References
[1] On the reproducibility of single photon responses (SPRs): the gordian knot of rod phototransduction
perseveres. R. Hamer, S. Nicholas, D. Tranchina, and P. Liebman, J. Vis. 2(10):113-113, Dec. 2002.

322

Cosyne 2008

Saturday evening, Poster session III-93

Exact and approximate solutions for marginalization with
probabilistic population codes
Jeffrey M. Beck1,2, Wei Ji Ma1, Vidhya Navalpakkam , and Alexandre Pouget1
University of Rochester, 2California Institute of Technology

1

In many tasks, human behavior has been shown to be nearly optimal in a Bayesian sense. This implies
that neural computation is capable of optimally manipulating representations of probability distributions
which represent both incoming evidence and prior knowledge. In particular, neurons must be able to
perform two classes of Bayesian inference: evidence accumulation and marginalization (integration over
hidden variables). Several theoretical studies have sought representations of probability distributions that
could implement these inferences using neurons whose responses are proportional to probability or log
probability. The main problem with these approaches is that they fail to account for the variability in
neural responses, and in particular the Poisson-like variability that has been reported in cortex.
We show here that the Probabilistic Population Coding (PPC) framework [1] can provide an optimal
solution to both evidence accumulation and marginalization, while accounting for neural variability.
Specifically, we show that in the high-information limit, a linear PPC may be marginalized using only
linear operations, a quadratic point non-linearity, and divisive normalization, operations that are believed
to be used in real neural circuits. Additionally, we show that even when the large information assumption
is not valid, the same set of neural operations can still leads to near-optimal levels of performance. This
results from the fact that the PPC framework allows us to approximate the generative model when either
(1) it induces no measurable information loss about the stimuli of interest or (2) there is insufficient
observational data to distinguish between the approximate statistical model and the ground truth.
These results are illustrated in a neural implementation of a Bayesian model of a visual search task (See
Poster #). The task is to detect and localize a target among distractors. Input patterns of activity obey
statistics that correspond to a linear PPC and represent the values of a particular feature at each of several
different spatial locations. The goal of the network is then to construct from the input pattern of activity,
rin, a second population pattern of activity, rout, which also represents a linear PPC but for the probability
that the target is present at each location. Since a line attractor can be used to optimally decode a linear
PPC, we conclude that the operation rout=f(rin) along with the line attractor can be used to generate a
saccade in the direction which maximizes the likelihood of fixating on the target or, alternatively, can
generate a no-go signal when it is most likely that the target is not present. Thus, this three-layer network
would generate an optimal sensorimotor transformation.
The Bayes-optimal computation for this task requires a series of operations which include exponentiation,
and sumation, and log transformation. Motivated by the structure of the optimal computation in the largeinformation limit, we show that a network which constructs rout using only linear operations, a quadratic
point non-linearity, and divisive normalization can be made to yield a linear PPC which performs both
target detection and target localization at levels which are nearly indistinguishable from optimal. We also
show that when rout is constructed from either just linear combinations of rin or just linear and quadratic
terms but without divisive normalization, a significant degradation in performance results.
Reference
[1] Ma, W.J., Beck, J.M., Latham, P.E., and Pouget, A. (2006). Bayesian inference with probabilistic
population codes. Nature Neuroscience 9 (2), 1432-8.

323

Saturday evening, Poster session III-94

Cosyne 2008

Molecular crowding and binding regulate glutamate receptor
synaptic trafficking at the post-synaptic density: a modeling study
Fidel Santamaria1, George J. Augustine2, and Sridhar Raghavachari2
1
The University of Texas at San Antonio, San Antonio, TX 78249, 2Duke
University Medical Center, Durham, NC 27710
In general, synaptic plasticity is, in part, the long term change in the concentration of ionotropic receptors
at the post-synaptic side of a synapse. Although, much progress has been made, the biophysical
mechanisms that are responsible for the increase and decrease of such receptors are not well understood.
We used a computer model to investigate the biophysical mechanisms of ionotropic receptor movement in
and around the post-synaptic density (PSD). The PSD is a large membrane associated protein complex
composed of scaffolding molecules and signaling proteins. We particularly studied the movement of
alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid receptors (AMPARs). AMPARs follow a
random walk when moving in the extra-synaptic membrane and a more restricted diffusion process in the
PSD [1]. We simulated AMPARs random walk with a Monte Carlo algorithm over a rectangular mesh,
tracking their spread for over 5000 ms. In most cases, we performed multiple independent simulations (n
= 1000) and calculated the mean square displacement (MSD) of the ensemble, and in others, we counted
the concentration of molecules inside the PSD. The simulations were fully characterized by the time step
and the diffusion coefficient of AMPAR (Dfree = 0.08 Pm2/s). In the simulations, AMPAR could freely
diffuse, bounce off anchored proteins (obstacles), or bind to anchored proteins (traps). The time a single
AMPAR was bound to a PSD molecule was stochastically determined using an exponential distribution
of binding energies. Using the model, we investigated (1) the effects obstacles on AMPAR movement; (2)
the mechanism of increasing and decreasing the concentration of AMPAR in the PSD with respect to the
surrounding membrane; and (3) induction of long term synaptic plasticity changes.
We found that (1) AMPARs diffusion over obstacles result in a different type of process that strongly
resembles the experimental results. Instead of having a linear relationship between the MSD and time
(MSD~t), we found a power-law dependency (MSD~ta). The value of a was less than 1 and depended on
the obstacle density. Such type of sub-diffusion process is referred to as anomalous diffusion and the
effect of obstacles is called molecular crowding. (2) AMPARs remained trapped inside a small PSD
surrounded by un-obstructed membrane due to molecular crowding, the molecules could move within the
PSD but the probability of escaping was very low. (3) Instead of increasing the concentration of
AMPARs, low binding energies over randomly selected PSD molecules resulted in allowing AMPAR
molecules inside the PSD to escape to the surrounding membrane. (4) Long term potentiation and
depression, including spike-timing synaptic plasticity, could be implemented under this framework
Our results show that the biophysical properties of molecular diffusion and reaction results in nonintuitive mechanisms of AMPAR retention and regulation in the PSD, and suggest an important role for
molecular crowding of PSD molecules in maintaining long term levels of AMPAR concentration.
Acknowledgments
This work was supported by NIH-RCMI grant 5 G12 RR13646-08.
References
[1] Diffusional Trapping of GluR1 AMPA Receptors by Input-Specific Synaptic Activity.
Ehlers et al., Neuron 54(3):447-460, May 2007.

324

Michael D.

Cosyne 2008

Saturday evening, Poster session III-95

Scaling relationships for neuronal circuit wiring set by noise and
metabolic cost
A. Aldo Faisal1, 2, Simon B. Laughlin 2
1

University of Cambridge, Dept. of Engineering, Comput & Biol. Learning group
University of Cambridge, Dept. of Zoology

2

The development of information processing devices, such as computer chips has been profoundly
influenced by two basic physical constraints: noise and energy. Here we relate these two constraints to the
wires of neural circuits – the axons. Axons carry the fundamental signal of the nervous systems, the
action potential (AP) to allow neurons to communicate fast and reliably [1]. Neurons in cerebral cortex
achieve axonal wiring densities of 4 km (!) per mm3, by using unmyelinated axons of 0.3 ¢m average
diameter for local cortical, account for about 40% of resting metabolic consumption [2]. Although, as in
computer chips, wire miniaturization economizes on space and energy [3,4], it increases the effects of
noise introduced by thermodynamic fluctuations in a neuron's “protein transistors,” voltage-gated ion
channels – so called channel noise. We previously showed that channel noise causes AP communication
to break down in axons and cell bodies below 0.1 ¢m and 3 ¢m diameter respectively – a universal limit
to neuron size matched by anatomical data across species [1]. In the many thin axons operating close to
this limit we demonstrated several mechanisms through which channel noise can destroy information
encoded in the timing of APs, by randomly varying the speed of conduction in the order of milliseconds
[5].
Here, we explore the basic relationships imposed by noise and energy on axonal connectivity. First, we
conducted detailed Monte Carlo simulations of thin axons using the Modigliani stochastic simulator
(www.modigliani.co.uk, see [1,5]), which allowed us to establish how the effects of axonal variability
scales with respect to axonal geometry. We find that spike time reliability increases as a power-law of
axon diameter and decreases as a power-law of axon length (or synaptic distance). This yields a basic
relationship between axon diameter, synaptic distance and the achievable synaptic spike time reliability.
We match these results to the few existing anatomical data points (maximum synaptic distance, average
diameter in cortex and cerebellum) and make testable predictions on cortical axonal geometry and
associated synapses (NMDA/AMPA). Second, we use a simple model of metabolic cost, based on an
axonal membrane surface area argument[2] which shows that increasing axon diameter or axon length
linearly increases the metabolic cost of axons. Thus, choosing a particular combination of axon diameter
and axon length/synaptic distance sets up a trade-off between noise and energy in wiring cortical circuits
that can be assayed with anatomical data.
Acknowledgments
AAF was supported by Studienstiftung des Deutschen Volkes, Boehringer-Ingelheim Fonds and the
BBSRC.
References
[1] AA Faisal, JA White and SB Laughlin (2005), Curr Biol 15, 1143-1149
[2] D Attwell and SB Laughlin (2001), J Cereb Blood Flow & Metab 21, 1133-1145
[3] BL Chen, DH Hall and DB Chklovskii (2006), PNAS 103, 4723-4728
[4] C Cherniak (1994), J Neurosci, 14, 2418–2427
[5] AA Faisal and SB Laughlin (2007), PLOS Comp Biol 3, e79

325

Saturday evening, Poster session III-96

Cosyne 2008

Task-dependent suppression of evoked responses in auditory cortex
Gonzalo H. Otazu1 and Anthony M. Zador1
1

Cold Spring Harbor Laboratory

Attention is a cognitive process in which a subset of sensory input relevant to behavior is subjected to
further processing. Neural correlates of attention in both auditory and visual cortex typically take the form
of an enhancement of neural activity to the attended stimulus. Here we have compared neural activity
elicited by sounds while rats performed a two-alternative choice auditory task (“engaged” condition) with
those elicited by identical stimuli while subjects were awake but not performing a task (“idle” condition).
Surprisingly, we found that cortical responses were consistently suppressed rather than enhanced in the
engaged condition (see fig a,b,c). To probe the mechanisms underlying this cortical suppression, we
recorded in the auditory thalamus, which provides input to the cortex. We found that although thalamic
evoked responses were identical in the two conditions, spontaneous rates in the thalamus were higher in
the engaged condition (see fig d,e,f), consistent with a mechanism involving synaptic depression at the
thalamocortical inputs. These results demonstrate that in the auditory cortex, engaging in an auditory task
can induce a powerful and robust suppression distinct from, and with the opposite sign as, previously
characterized forms of auditory attentional modulation, the function of which may be to reduce activity in
neurons projecting to targets not involved in this task. Our results represent a first step toward
understanding the synaptic and circuit mechanisms by which this suppression occurs.

a. Example PSTH of multiunit cortical activity showing suppressed responses in the engaged condition compared to
the idle condition. b. Most of the recorded sites (2 animals, 111 sites) showed suppressed responses during the task,
without changes in the spontaneous activity. c. Quantification using suppression index showed suppression of
evoked responses without changes in spontaneous activity. d-f. Similar figures for auditory thalamus (2 animals, 91
sites) showing an increase of spontaneous activity, without changes in the evoked activity

326

Cosyne 2008

Saturday evening, Poster session III-97

A behavioral paradigm for probing the auditory system of freely
moving rats under different behavioral contexts
Santiago Jaramillo and Anthony M. Zador
Cold Spring Harbor Laboratory, 1 Bungtown Rd., Cold Spring Harbor, NY 11724
The response characteristics of neurons in sensory cortex has been shown to depend on the behavioral
context of the organism. We use rodents as a model system for the study of the mechanisms underlying
these phenomena in auditory cortex. A relative low cost and the applicability of molecular tools currently
unavailable for higher mammals, are some of the advantages offered by rodents as a model system. An
essential step in our studies of the context-dependent modulation of activity in the auditory system of rodents
is the design of appropriate behavioral paradigms.
We have implemented a two-alternative choice task in which the same probe stimulus was presented to a
freely moving animal under two different behavioral contexts. Rats were required to perform discrimination
of cue stimuli preceded by irrelevant probe stimuli. The animal initiated each trial by poking its nose into
a central port (from an arrangement of three ports). After a random delay of 250-350 msec, the probe
stimulus was presented. This probe stimulus allows for the characterization of the response of a cell being
recorded, and as such it may contain series of tones, chords, or more complex stimuli. The probe stimulus
was designed to last for 800-1000 msec after which the cue sound was presented for 300 msec or less. The
animal was required to remain in the center port during the presentation of probe and cue stimuli and then
move to the appropriate side port (left or right) for water reward.
Each behavioral context corresponds to a block of trials (100-150) in which only a subset of cue sounds was
presented. We have trained animals to perform a frequency discrimination task where either low-frequency
cue sounds (below 14 kHz) or high-frequency (above 14 kHz) cue sounds were presented in each block.
When using probe sounds consisting of trains of tones (50 ms separated by 100 ms), and cue sounds consisting of frequency modulated sounds (centered at 6.5 kHz or 11 kHz for one context, and 18 kHz or 31 kHz
for the second context), animals achieved above-chance performance in less than 20 sessions. Performance
continued to increase asymptotically with further sessions.
The behavioral paradigm described above is being used in our laboratory with simultaneous tetrode recordings in auditory cortex.

Acknowledgments
This work was supported by the Swartz Foundation.

327

Saturday evening, Poster session III-98

Cosyne 2008

Beyond the limits of feed-forward processing
in visual object recognition
Albert L. Rothenstein, John K. Tsotsos
York University, Toronto, Canada
The limits of feedforward processing in recognition tasks with images containing multiple objects have been
explored theoretically [2], through computational modeling [5], and experimentally [1]. All these diverse
methods show that performance degrades when increasing the number of objects.
We will present the results of a computational modeling study of the role visual attention plays in primate
object recognition, and in particular in recognition beyond the ﬁrst feed-forward pass. The main condition
for multi-pass recognition is the recovery of spatial information from high level, abstract, and invariant representations. This is equivalent to a solution to the binding problem. We present a particular solution within
the context of the Selective Tuning model of visual attention [3, 4]. An object detection and recognition
system based on this is implemented, demonstrating the important contribution of top-down object-based
attention in improving the ability of the system to eliminate the inﬂuence of distractors.
The network consists of two parallel pathways, one detects and categorizes objects in input images, while the
second pathway implements an object recognition system. Similar to primate visual performance, detection
and categorization are fast and parallel, while detailed recognition is slow and serial. The pathways are
trained using standard supervised learning. Image processing starts with the presentation of the stimulus and
the feedforward propagation of the information along parallel visual pathways. Of interest in the context of
this work are representations for the categorization and recognition of stimuli. The power of the feedforward
pass in creating detailed and accurate representations of the input stimuli has been extensively explored.
Object based attention is deployed at the level of categoric representations, and triggers a feedback cascade
of attentional modulations moving back towards early visual areas and inhibiting distractors. Attentional
selection and the inhibition of distracters allows the visual system to reevaluate the stimuli present in the
input image, but due to the need to eliminate interference, this process needs to be serial.
References
[1] G. Kreiman, T. Serre, and T Poggio. On the limits of feed-forward processing in visual object recognition. In COSYNE, 2007.
[2] J. K. Tsotsos. Analyzing vision at the complexity level. Behavioral and Brain Sciences, 13(3):423–445,
1990.
[3] J. K. Tsotsos, S. M. Culhane, W. Y. K. Wai, Y. H. Lai, N. Davis, and F. Nuﬂo. Modeling visual-attention
via selective tuning. Artiﬁcial Intelligence, 78(1-2):507–545, 1995.
[4] J. K. Tsotsos, A.-J. Rodrı́guez-Sánchez, A. L. Rothenstein, and E. Simine. Different binding strategies for the different stages of visual recognition. In 2nd Int. Symposium Brain, Vision and Artiﬁcial
Intelligence, 2007.
[5] D. Walther, L. Itti, M. Riesenhuber, T. Poggio, and C. Koch. Attentional selection for object recognition
- a gentle way. Biologically Motivated Computer Vision, Proceedings, 2525:472–479, 2002.

328

Cosyne 2008

Saturday evening, Poster session III-99

Attention-dependent reductions in response variability in macaque
area V4 can be accounted for by changes in spike power spectra and
burstiness
Jude F. Mitchell, Kristy A. Sundberg, and John H. Reynolds
Systems Neurobiology Lab, The Salk Institute, La Jolla, CA
We recently reported that directing attention to a sustained stimulus in the receptive field of macaque area
V4 neurons reduces their response variability, as measured by the Fano Factor (Mitchell et. al., Neuron,
2007). Here we examine several properties of neuronal spiking that could be altered by attention to cause
the observed changes in response variability including: 1) changes in the firing rate from trial to trial, 2)
increases in firing rate with attention that impose regularity in spiking due to the spike refractory period,
and 3) fluctuations in firing rate within trials as indicated by changes in power spectrum. The first two
factors account for only a modest fraction of the reduction in variability. The third factor, fluctuations of
rate within trials at frequencies <10Hz , accounts for most of the observed attention-dependent reduction
in variability. Additionally, there is a significant reduction in the spike-LFP coherence at the lower
frequencies, suggesting that these reductions reflect reductions in synchronous low frequency oscillations
across the population.
We also examined how attention dependent reductions in variability differ between cell classes. Studies in
cortical slices have found that parvalbumin-expressing inhibitory interneurons can be distinguished from
pyramidal neurons on the basis of their shorter action potential duration. As reported in Mitchell et al.
(2007), the distribution of action potential widths in macaque area V4 is clearly bimodal. Narrow spiking
neurons, putative interneurons, exhibit a reduction in response variability that is more than twofold
stronger than broad spiking neurons, putative pyramidal neurons. Correspondingly, narrow spiking
neurons also show a stronger reduction in the low frequency spike power. The two classes also differ in
the degree to which they exhibit burstiness in spiking. Broad spiking neurons more frequently exhibit
bursts as indexed by larger peaks at short inter-spike intervals in their spike auto-correlation (ISI’s <
8ms). Among those neurons with burst firing, there is a significant reduction in the burstiness of firing
when attention is directed to the stimulus in the receptive field.

329

Saturday evening, Poster session III-100

Cosyne 2008

Attentional Processes in Correspondence-Based
Object Recognition
Philipp Wolfrum and Christoph von der Malsburg
Frankfurt Institute for Advanced Studies, Germany
We show here that a model for invariant object
recognition described earlier [1] naturally displays
effects of space based and object based attention.
The network consists of three functional layers: an
input layer for image representation, an intermediate assembly layer for recurrent information integration, and a gallery layer for memory storage (see
ﬁgure). Each layer consists of cortical columns as
functional building blocks that are modeled in accordance with recent experimental ﬁndings. Connections between input and assembly layer are modulated by control units on a fast time scale to allow
detection of objects in the input image and position
invariant mapping to the normalized assembly layer.

Recurrent information exchange between assembly
and gallery layer leads to recognition. Tests on standard benchmark databases show competitive performance for face recognition.

Here, we prepare the system to recognize faces of 1000 different identities invariantly of position. It is then
confronted with a large test image containing several of those faces (under different expression). We show
how by pre-activating a subset of the control units in a top-down manner (supposedly from prefrontal cortex),
attention of the system can be directed to a certain part of visual space, resulting in preferred processing and
recognition of objects at that position. This bias can be neutralized by very salient stimuli at other positions
of the visual ﬁeld. Similarly, top-down activation of speciﬁc subsets of the memory domain (e.g. female
faces or a single identity) results in object priming: Among several faces in the visual ﬁeld, the system will
preferentially detect and subsequently recognize those faces indicated by the subset.
In accord with [2], selection of a location in the input domain is not achieved by increased activity at the input
level but by selective routing to higher domains, resulting in effectively shifting receptive ﬁelds of neurons
at higher stages. In distinction to previous work (e.g. [3]), our system keeps track of spatial relationships
between features and is therefore able to distinguish between objects of similar qualitative structure (i.e.
containing many similar features).
Acknowledgments
This work was supported through EU project FP6-2005-015803 (“Daisy”) and by the Hertie Foundation.

References
[1] P. Wolfrum, J. Lücke, and C. von der Malsburg. In Proc. Int. Conference on Computer Vision Theory
and Applications, 2008. Accepted.
[2] S. J. Luck, L. Chelazzi, S. A. Hillyard, and R. Desimone. J Neurophysiol, 77(1):24–42, 1997.
[3] G. Deco and E. T. Rolls. Vision Res, 44(6):621–642, 2004.

330

Cosyne 2008

Sunday AM Talks-1

From Neuronal Circuit Reconstructions to Principles of Brain
Design
Dmitri “Mitya” Chklovskii
Janelia Farm, Howard Hughes Medical Institute
How does electrical activity in neuronal circuits give rise to intelligent behavior? We believe that this
question is impossible to answer without a comprehensive description of the connectivity in neuronal
networks. Such a description may be a wiring diagram, which catalogs all neurons and synaptic
connections between them. In collaboration with several laboratories, we are reconstructing vertebrate
and invertebrate wiring diagrams from electrophysiological, light, and electron microscopical data. To
gain insight into brain function from the wiring diagrams, we formulate engineering principles of brain
design and test them experimentally. By focusing initially on explaining the function of simpler
organisms, we are assembling a theoretical framework and accumulating experience necessary to
understand more complex systems, such as the mammalian neocortex.

331

Sunday AM Talks-2

Cosyne 2008

Neuronal ensemble bursting in the basal forebrain encodes salience
irrespective of valence
Shih-Chieh Lin1, 2, Miguel A.L. Nicolelis1-5
1 Dept. of Neurobiology, Duke University Medical Center, Durham, NC, USA
2 Center for Neuroengineering, Duke University, Durham, NC, USA
3 Dept. of Biomedical Engineering, Duke University, Durham, NC, USA
4 Dept. of Psychological and Brain Sciences, Duke University, Durham, NC, USA
5 Edmond and Lily Safra International Institute of Neuroscience of Natal, Natal RN, Brazil

The main goal of animal behavior is to maximize reward and avoid punishment. To achieve this goal,
animals similarly attend to both types of motivationally salient events despite their opposite hedonic
valence. Recent evidence suggests that the opposite hedonic valence of reward and punishment are
processed by different and possibly opposing neural systems. However, it remains unknown whether
motivational saliency of reward and punishment is processed by the same valence-specific neural systems,
or alternatively, is encoded separately as a distinct and valid neurobiological construct.
Here we show that motivational saliency is encoded by ensemble bursting of basal forebrain (BF) neurons
in behaving rats. We observed that motivationally salient sensory cues that predicted either sucrose or
quinine delivery in a Go/Nogo task elicited a similar brief bursting response in many BF neurons. This
bursting response occurred irrespective of the cue’s sensory modality, the associated motor response and
the hedonic valence of the expected outcome. BF ensemble bursting emerged as cues acquired
motivational saliency and predictive ability through associative learning and diminished after the cueoutcome associations underwent extinction. The same BF neurons also responded to both primary reward
(sucrose) and punishment (quinine) with highly similar bursting patterns. These salience-encoding BF
neurons represented a homogeneous subset of BF non-cholinergic neurons because they did not change
their average firing rate across wake-sleep states [1] and their firing properties were consistent with in
vitro characterizations of BF non-cholinergic neurons [2]. Finally, the relationship between BF ensemble
bursting and behavioral performance was documented by showing that BF bursting responses predicted
successful detection of near-threshold tones in a tone detection task.
Our results point to the existence of an independent salience-encoding system, mediated by ensemble
bursting of BF neurons. This discovery suggests that the valence and salience of attended stimuli are
encoded by two major neuromodulatory systems – the midbrain dopaminergic neurons and BF noncholinergic neurons – using similar bursting responses. Contrary to the traditional view that BF functions
are mediated mostly via cholinergic neurons, our findings provide the first evidence regarding the in vivo
functions of the poorly understood BF non-cholinergic neurons in behavioral contexts. The encoding of
motivational saliency by BF ensemble bursting may improve behavioral performance by transiently
enhancing cortical gamma oscillation power [3] and, therefore, mediating the influences of attention on
cortical processing. Together, our results support the hypothesis that BF ensemble bursting represents a
novel candidate mechanism for attention.
[1] Lee, M.G., et al., Cholinergic Basal Forebrain Neurons Burst with Theta during Waking and Paradoxical Sleep.
J. Neurosci., 2005. 25(17): p. 4365-4369.
[2] Alonso, A., et al., Differential oscillatory properties of cholinergic and noncholinergic nucleus basalis neurons
in guinea pig brain slice. Eur J Neurosci, 1996. 8(1): p. 169-82.
[3] Lin, S.C., D. Gervasoni, and M.A. Nicolelis, Fast modulation of prefrontal cortex activity by basal forebrain
noncholinergic neuronal ensembles. J Neurophysiol, 2006. 96(6): p. 3209-19.

332

Cosyne 2008

Sunday AM Talks-3

The Hemo-Neural Hypothesis: A Proposed Role for Blood Flow in
Neuromodulation and Information Processing
Christopher I. Moore1,2, Ulf Knoblich1,2, Rosa Cao1,2, Jessica Cardin1,2, Bryan
Higashikubo, and Joshua C. Brumberg3
1

McGovern Institute for Brain Research, Cambridge, MA,
MA, 3Queens College, CUNY, Flushing, NY

2

MIT, Cambridge,

Hemodynamics are typically considered to serve as a metabolic support system, or to play another
physiologic role that has no direct impact on neural excitability. In contrast to this canonical view, we
recently proposed the Hemo-Neural hypothesis (Moore and Cao, 2007). This hypothesis predicts that
functional hyperemia, enhanced blood flow to a discrete brain region evoked by neural activity (the effect
measured in the BOLD fMRI response), impacts neural information processing. We predict that
hyperemia modulates the excitability of sensory cortical neurons, altering their gain and spatio-temporal
receptive field structure.
Hemodynamics may impact neural activity through direct and indirect mechanisms. Direct mechanisms
include delivery of diffusible blood-borne messengers, and mechanical and thermal modulation of neural
activity. Indirect mechanisms are proposed to act through hemodynamic modulation of astrocytes, which
can in turn regulate neural activity.
To test this hypothesis, we induce hyperemia with a pharmacological agent (Pinacidil) that causes
vasodilation by relaxation of smooth muscles on the neocortical surface. We simultaneously conduct
neurophysiological recordings. Our preliminary data indicate that Pinacidil is selective, as predicted by
the absence of its receptor in neurons and astrocytes and as confirmed by intracellular in vitro slice data
showing an absence of effects on membrane and firing properties (N=26 neurons). Intracellular in vivo
recordings in Barrel cortex indicate that hyperemia is correlated with neuron (N=6/9) and glia (N=4/5)
depolarization, enhancing sensory transmission (see Knoblich et al., this meeting). Ongoing studies
include 2-photon imaging during hyperemia induction.
This hypothesis has implications for understanding computation in the brain, as it proposes an entirely
novel form of neuromodulation and ‘state’-dependent regulation of sensory representation. If
hemodynamics modulate neurons, this finding is also important for interpretation of widely-used
functional imaging signals (e.g., the BOLD response), as functional imaging is probing a part of the
process of computation, and not just a derivative marker of activity. This hypothesis also has potential
clinical implications. If functional hyperemia is a neuromodulator, then this discovery may inform the
understanding, diagnosis and treatment of neurological and psychiatric diseases with vascular
involvement.

333

Sunday AM Talks-4

Cosyne 2008

Spatially inhomogeneous processing of visual motion by Drosophila
Michael B. Reiser
Janelia Farm Research Campus, Howard Hughes Medical Institute, Ashburn, VA
Over 50 years ago Bernard Hassenstein and Werner Reichardt carried out a series of elegant experiments
on the optomotor reactions of the beetle Clorophanus, and produced a computational model for insect
motion detection that has demonstrated remarkable explanatory power for a large range of behavioral and
electrophysiological results [1]. Despite the success of this model, there remain several aspects of
visually-guided behaviors that cannot be explained by a simple application of an array of HassensteinReichardt (HR) elementary motion detections (EMDs) driving an optomotor response. I will present the
results of some recent experiments that aim to unify the predictions of optomotor reactions of tethered
flying insects with the more elaborate visual processing that is often suggested by free-flight behavioral
experiments.
The experiments consist of rapidly interleaved presentations of pattern motion at different temporal
frequencies presented to restricted frontal and lateral eye regions of a tethered Drosophila flying in a
flight simulator. The results suggest that the direction and magnitude of the turning response is dependent
on the location, speed, and direction of the motion stimulus (as had been previously shown [2]). A novel
finding is that the responses show a temporal frequency tuning that is not constant across the eye. One
suggestion is that the sides of the Drosophila eye are tuned for longer integration of visual motion.
Speculations about the computational utility of these responses will be presented. The result that identical
motion stimuli presented to different parts of the eye elicit strikingly different amplitude turning
responses is surprising and not at all predicted by the optics of the Drosophila compound eye [3] (which
unlike the eyes of many other flying insects does not contain substantial spatial heterogeneity such as an
acute, high resolution zone). The most parsimonious interpretation of these results is that the turning
behavior is controlled by a system fed by two non-overlapping arrays of HR EMDs that contain motion
detectors with two different time constants.
These results corroborate earlier findings showing that a centering-like turning behavior is primarily
elicited by motion processed by the lateral regions of the eye [4], a feature common to the well-studied
centering response of honeybees [5]. Taken together, these results suggest that differential tuning of
motion detection is a sensible strategy for visually-guided navigation, and may allow flies to overcome
some of the limitations of seeing the world through an array of HR motion detectors. Finally, I will
present the results of related experiments on turning responses that assess the ability of Drosophila to
discriminate bilaterally asymmetric motion stimuli.
References
[1] Hassenstein, B. and Reichardt, W. (1956). Z. Naturforsch., 11(9-10):513–524.
[2] Heisenberg, M. and Wolf, R. (1984). Vision in Drosophila. Springer Verlag, Berlin.
[3] Buchner, E. (1984). In Ali, M., editor, Photoreception and Vision in Invertebrates, pages 561– 621.
Plenum, New York.
[4] Reiser, M. B. (2006). PhD dissertation, California Institute of Technology.
[5] Srinivasan, M. V., Lehrer, M., Kirchner, W. H., and Zhang, S. W. (1991). Visual Neurosci., 6(5):519–
535.

334

Cosyne 2008

Sunday AM Talks-5

Oscillations organize internally advanced cell ensembly sequences
György Buzsáki
Center for Molecular and Behavioral Neuroscience, Rutgers,
The State University of New Jersey, Newark, NJ 07102
How does the brain orchestrate perceptions, thoughts and actions from the spiking activity of its neurons?
Previous single neuron recording research has regarded spike pattern variability as noise that should be
averaged out to reveal the brain’s representation of invariant input. An alternative view is that variability
of spikes is centrally coordinated and that this brain-generated ensemble pattern in cortical structures is a
potential source of cognition. Large-scale recordings from neuronal ensembles now offer opportunities
for challenging and testing these competing theoretical frames. A postulated signature of the cell
assembly is that its participants show a higher probability of spiking together than with members of other
assemblies, even in the absence of external inputs. Interactions among parallel-recorded hippocampal
neurons revealed a consistent temporal structure beyond that predicted from the environmental inputs. We
find that prediction of spike times of hippocampal pyramidal neurons is improved using the spike times of
simultaneously recorded neurons, over prediction from the animal’s trajectory in space, or a spatiallydependent theta phase modulation. Thus, we suggest that the assembly organization arises from the
internal dynamics of neuronal circuits. Assemblies are organized most efficiently within 10-30ms,
suggesting that cell assemblies are synchronized at this timescale (gamma cycles). Seven to nine cell
assemblies form assembly sequences within a theta cycle. The most active assemble occupies the trough
of theta representing “here and now”, flanked by representation of past and future events on the
descending and ascending phase of theta, respectively. The “lifetime” of an assembly in the dorsal
hippocampus is 1-2 sec, corresponding to 7 to 14 cycles of theta. In the absence of environmental inputs,
the hippocampus continues to generate perpetually changing assembly sequences, which may represent
the neuronal substrate of episodic memory. Interference with assembly organization by drugs, such as
cannabinoids, affects episodic memory. Thus, assembly-based approach can provide an insight into
centrally-organized (cognitive) events without reference to introspection and may be a powerful method
for evaluating drugs affecting cognitive abilities.

References
[1] Organization of cell assemblies in the hippocampus. Harris, K. D., Csicsvari, J., Hirase, H., Dragoi,
G.. and Buzsáki, G. , Nature 424:552-556, 2003.
[2] Neuronal oscillations in cortical networks. Buzsaki, G. and Draguhn, A. , Science 304: 1926-1929,
2004.
[3] Buzsaki G. Rhythms of the Brain. Oxford University Press, 2007
[4] Forward and reverse hippocampal place-cell sequences during ripples, Diba K, Buzsaki G.. Nature
Neurosci. 10:1241-2.

335

Sunday AM Talks-6

Cosyne 2008

Quantitative Single-Neuron Modeling: Competition 2008
Richard Naud1 , Thomas Berger1, Laurent Badel1 , Arnd Roth2 and Wulfram
Gerstner1
1
2

Ecole Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland,
University College London (UCL), London, UK.

How well are single-cell properties reproduced by the present-day neuronal models? Recently, several labs
have approached this question by assessing the quality of the models with respect to spike timing prediction
[1, 2] or characteristic features of the voltage trace [3]. So far, every modeler used his own preferred performance measure on his own data set. The ‘Quantitative Single-Neuron Modeling Competition’ offers a
coherent framework to compare neuronal models with four different experiments on layer V pyramidal neurons of the somatosensory cortex under somatic and dendritic stimulation. Speciﬁc performance measures
are used on these four experiments to quantify speciﬁc performances of neuronal models under somatic and
dendritic stimulation, e. g.: spike timing (Fig. 1A, C-D), inter-spike intervals (FIG. 1B), or subthreshold
waveform (Fig 1B, D). Expert and novice modelers have been invited to submit their prediction of neuronal behavior in any or all of these four different experimental setups. Here we present the results of this
competition. Along with submissions from other labs, we show the performance of a stochastic Poisson
neuron model and the adaptive Exponential Integrate-and-Fire model. The results are valuable for network
modelers interested in ﬁnding the best trade-off between biological relevance and simplicity.

A

V(t)

V(t)

g(t)

C

B

D

V(t)
I(t)

I(t)

I(t)

Figure 1: Four different experimental setups deﬁne four separated challenges (schematic): A) Somatic
injection of noisy conductance, B) somatic injection of step currents, C) somatic and dendritic injection of
noisy current, D) complex somatic and dendritic injections and recordings.
Acknowledgments
We thank Matthew Larkum for providing the multi-electrode data.
References
[1] R. Jolivet, R. Kobayashi, A. Rauch, R. Naud, S. Shinomoto and W. Gerstner, Journal of Neuroscience
Methods, in Press.
[2] J. W. Pillow, L. Paninski, V. J. Uzzell, E. P. Simoncelli, E. J. Chichilnisky, Journal of Neuroscience
23(47): 11003-11013, 2005.
[3] S. Druckmann, Y. Banitt, A. A. Gidon, F. Schürmann, H. Markram and I. Segev, Frontiers in
Neuroscience 1(1): 7-18, 2007.

336

Cosyne 2008

Sunday AM Talks-7

Neural mechanisms of speech processing: time warp invariance
by adaptive integration time
R. Gütig1,2 and H. Sompolinsky1−3
1
3

Racah Institute of Physics and 2 ICNC, Hebrew University of Jerusalem, Israel.
Center for Brain Science, Harvard University, Cambridge, USA.

In speech processing, perceptually relevant temporal cues can require resolution of spectral transitions with
millisecond precision. However, dynamic variations in speaking rate on a scale of hundreds of milliseconds
introduce time warp of spoken syllables, ranging from two fold compression to two fold dilation. Although
the auditory systems of adult humans, pre-articulate infants and also non-human primates have been shown
to readily normalize temporal cues underlying phonetic category boundaries for speaking rate effects [1],
the neural mechanisms subserving this time warp invariance have remained mysterious. In addition to
time warp robustness, the neural hardware must also support processing of temporal features over different
relative scales, e.g. slow formant transitions vs. fast voice on- and offset times.
To resolve these computational challenges by means of a biologically plausible spiking neuronal network,
we studied a neural model of spike-based learning of auditory discrimination tasks, extending our recent
tempotron model [2]. In our present model, synaptic inputs are modeled as conductances rather than current
pulses, such that the time scale of the resulting voltage dynamics is governed by the total synaptic conductance. As a result, the statistics of inputs as well as the synaptic learning rule, interact with the time scale of
neural information processing by changing the effective integration time constant of the cell. For instance,
by balancing excitatory and inhibitory synaptic conductances a neuron can adjust its effective integration
time scale to match the temporal extent of its acoustic target feature. Moreover, since in a high conductance
state the effective membrane time constant of a neuron scales inversely with its total conductance, the membrane voltage exhibits a high degree of time warp invariance. Warping of input spike trains which increases
the total conductance, such as time compression, or decrease it when time is stretched, are canceled by the
corresponding change in the membrane integration time.
We have applied our model to speech recognition problems. Using a simple model of the auditory periphery,
sound signals were converted into spike patterns by thresholding their power-spectral densities, and fed
into small populations of conductance-based LIF neurons. Surprisingly, using our tempotron learning, as
few as 15 neurons were sufﬁcient to achieve perfect word recognition on the TI46 [3] digit recognition
task, outperforming complex state-of-the-art hidden-markov-model word recognition systems (e.g. Spinx
4). Analysis of the spectro-temporal receptive ﬁelds of the trained neurons revealed the neural target features
solving the digit recognition task. Our results demonstrate the powerful role of synaptic conductances in
spike-timing based neural processing.
Acknowledgments
This work was supported by the Minerva Foundation and the Israeli Science Foundation.
References
[1] Miller JL (1981) Some effects of speaking rate on phonetic perception. Phonetica 38:159. [2] Gütig R,
Sompolinsky H (2006) The tempotron: a neuron that learns spike timing-based decisions. Nat Neurosci 9:
420-428. [3] Liberman M et al. (1993) TI 46-Word. Linguistic Data Consortium, Philadelphia.

337

Sunday PM Talks-1

Cosyne 2008

Olfactory processing in a tiny microcircuit
Rachel I. Wilson
Harvard Medical School
Each first-order olfactory receptor neuron (ORN) expresses just a single odorant receptor gene, which
confers upon it a particular odor response profile. All the ORNs that express the same odorant receptor
gene project their axons the same compartment of neuropil (“glomerulus”) in the brain. Each secondorder neuron receives direct ORN input from a single glomerulus. An odorant typically activates multiple
types of ORNs, and thus olfactory information is represented as a distributed population code. Glomeruli
are linked by local interneurons, but the function of inter-glomerular connections is unclear. Because the
vertebrate olfactory system contains ~1000 glomeruli, each with a unique response profile, and with no
simple spatial relationship between glomeruli, the logic of this system has been difficult to unravel.
My lab has been investigating olfactory processing in a more tractable system, the Drosophila antennal
lobe. The basic organization of the fly antennal lobe is similar to that of the vertebrate olfactory lobe, but
numerically this system is much simpler, with only ~50 glomeruli. Moreover, genetic tools allow us to
label first- and second-order neurons corresponding to identified glomeruli. Extracellular and intracellular
recordings from these neurons allow us to monitor their odor responses in vivo. We also use genetic tools
and pharmacology to probe functional interactions between neurons in this circuit.
What transformations are occurring as olfactory information passes from ORNs to second-order neurons
(called “projection neurons” or PNs)? We find that the odor responses of a PN show higher spike-count
reliability than the responses of a cognate presynaptic ORN, likely reflecting the fact that PNs pool
signals from many homotypic ORNs. We also find that weak ORN odor responses are amplified in
postsynaptic PNs, but strong ORN responses are not amplified to the same degree. This mainly reflects
synaptic and cellular nonlinearities intrinsic to each glomerulus. This transformation produces a type of
reformatting that Simon Laughlin has called “histogram equalization”, with the result that distances
between odor representations become more uniform. As a consequence, a linear discriminator classifies
odors more accurately using PN spike trains as compared to an equivalent number of ORN spike trains.
What is the function of inter-glomerular connections? We were surprised to find that inter-glomerular
input to PN dendrites is predominantly excitatory. The function of these excitatory connections is still
something of a mystery. We were then again surprised to find that the net effect of inter-glomerular input
to a glomerulus can actually be inhibitory. This is because there are inter-glomerular circuits that suppress
neurotransmitter release from ORN axon terminals. So while the net effect of inter-glomerular input to a
PN is generally excitatory, presynaptic inhibition generally dominates over postsynaptic excitation. The
odor tuning of this inter-glomerular presynaptic inhibition is roughly similar across glomeruli, and is
strongly correlated with the total ORN activity evoked by each odor. Thus, we propose that interglomerular presynaptic inhibition acts as a spatially diffuse gain control signal.

338

Cosyne 2008

Sunday PM Talks-2

Modeling Olfactory Discrimination in Drosophila
Sean X. Luo, Richard Axel and L.F. Abbott
Department of Neuroscience, Columbia University, New York, NY
Fruit ﬂies are capable of distinguishing many odors with a remarkable degree of speciﬁcity. Recently, a
series of experiments [1, 2, 3] has documented odor-evoked response patterns in both the ﬁrst order neurons,
the olfactory receptor neurons (ORNs), and the second order neurons, the projection neurons (PNs) of the
antennal lobe. A single odorant can elicit activity in multiple ORNs and their corresponding PNs, and a
signiﬁcant degree of overlap of PN activity exists for different odors. Moreover it has been shown that the
ﬁring patterns in the third order neurons in the mushroom body is sparse [4, 5]. How is the overlapping,
combinatorial representation in the antennal lobe transformed into a sparse representation in higher olfactory
centers?
Here we show that a transformation to a sparse code can be accomplished in the olfactory system by direct
feedforward connections between the PNs and the third order neurons (3Ns). Determining the correct PN to
3N synaptic weights is equivalent to ﬁnding a linear discriminator that classiﬁes two labeled sets of vectors,
one representing the odors to which the 3N responds, and the other the odors to which it does not respond.
Because 3Ns respond sparsely, one of these sets has only a few examplars. The capacity of such a sparse
discriminator increases exponentially as a function of the number of dimensions [6], which, in this instance,
is the number of olfactory receptors.
Model 3Ns can be constructed to respond to only one out of an experimental data set of 110 odors with
high reliability. All model 3Ns have the same threshold, and hence do not require individual ﬁne-tuning of
intrinsic properties for speciﬁcity. Furthermore, speciﬁcity is maintained at different concentrations and with
different degrees of connectivity between PNs and 3Ns. This model thus represents a plausible mechanism
by which combinatorial codes can be read out for the purpose of sparse odor discrimination.
Acknowledgments
Supported by NIH Pioneer Award 5-DP1-OD114-02.
References
[1] Hallem, E. A., and Carlson, J. R. (2006). Coding of odors by a receptor repertoire. Cell 125, 143-160.
[2] Bhandawat, V., Olsen, S. R., Gouwens, N. W., Schlief, M. L., and Wilson, R. I. (2007). Sensory processing in the Drosophila antennal lobe increases reliability and separability of ensemble odor representations.
Nat Neurosci 10, 1474-1482.
[3] Root CM, Semmelhack JL, Wong AM, Flores J, Wang JW. (2007). Propagation of olfactory information
in Drosophila. Proc Natl Acad Sci U S A. 104, 11826-31
[4] Wang, Y., Wright, N. J., Guo, H., Xie, Z., Svoboda, K., Malinow, R., Smith, D. P., and Zhong, Y. (2001).
Genetic manipulation of the odor-evoked distributed neural activity in the Drosophila mushroom body. Neuron 29, 267-276.
[5] Perez-Orive, J., Mazor, O., Turner, G. C., Cassenaer, S., Wilson, R. I., and Laurent, G. (2002). Oscillations and sparsening of odor representations in the mushroom body. Science 297, 359-365.
[6] Itskov, V. and Abbott, L.F. (2007) Capacity of a percepton for sparse discrimination. (unpublished).

339

Sunday PM Talks-3

Cosyne 2008

High-speed depth-targetable control of genetically defined neurons
in freely moving mammals: technology development and
neuropsychiatry application
Karl Deisseroth
Depts of Bioengineering and Psychiatry, Stanford University
We have developed both microbial opsin-based genetic tools and solid-state optical approaches
to allow specific cell types, even deep within the brain, to be controlled with millisecond
precision in freely behaving mammals [1-4]. Use of a fiberoptic approach allows depth targeting
of hypothalamic cells (here, the hypocretin cells in the lateral hypothalamus), establishing for the
first time a causal relationship between frequency-dependent activity of a genetically defined
neural circuit and a specific complex mammalian behavior central to clinical conditions like
narcolepsy. The fiberoptic approach also is compatible with targeting superficial structures like
M2 motor cortex, and together with a custom commutator, optical control of locomotion in freely
moving and even rotating mammals is achieved. Finally we demonstrate integration of
millisecond-scale optical control with simultaneous millisecond-scale optical imaging, designed
to be adaptable to intact-circuit preparations. Together these neuroengineering advances raise the
prospect of determining the role of activity in specific cell types in neuropsychiatric disease.
Acknowledgments
This work was supported by NARSAD and the Culpeper, Coulter, Klingenstein, Whitehall, McKnight,
Kinetics, NIMH, NIDA, CIRM, and the NIH Director's Pioneer Award Program.
References
[1] Gradinaru V, Thompson KR, Zhang F, Mogri M, Kay K, Schneider MB, and Deisseroth K. (2007).
Targeting and readout strategies for fast optical neural control in vitro and in vivo. J.Neurosci.27:14231-8.
[2] Airan RD, Hu ES, Vijaykumar R, Roy M, Meltzer LA, and Deisseroth K. (2007). Integration of lightcontrolled neuronal firing and fast circuit imaging. Current Opinion in Neurobiology 17:587-592.
[3] Adamantidis A., Zhang F., Aravanis A. M., Deisseroth K., and de Lecea L (2007). Neural substrates
of awakening probed with optogenetic control of hypocretin neurons. Nature, 450:420-424.
[4] Zhang F., Wang, L.P., Brauner M., Liewald J.F., Kay K., Watzke N., Wood P.G., Bamberg E., Nagel
G., Gottschalk A., and Deisseroth K. (2007). Multimodal fast optical interrogation of neural circuitry.
Nature 446, 633-639.

340

Author index

Cosyne 2008

Benucci, Andrea, 22, 237, 240
Berger, Daniel, 202
Berger, Thomas , 336
Bergmann, Urs, 124
Berkes, Pietro, 147, 231, 264
Berry, Michael, 31, 36, 201, 207
Bialek, William, 31, 270
Biederlack, Julia, 143
Billimoria, Cyrus, 28, 310
Biot, Claire, 196
Bizley, Jennifer, 248
Blanche, Timothy, 19, 133
Blanke, Olaf, 169
Blaschke, Stefan, 316
Bologna, Luca Leonardo, 85
Bomash, Illya, 284
Bonjean, Maxime, 103
Bosman, Conrado, 161, 167
Bossaerts, Peter, 76
Botvinick, Matthew, 298
Boucsein, Clemens, 261
Bourjaily, Mark, 290
Bradley, David, 14
Bradshaw, Alexis, 30
Bressler, Steve, 108
Briggs, Farran, 114
Britten, Kenneth, 241
Brody, Carlos, 154, 185, 205
Brown, Emery, 44, 197
Brown, Ethan, 158
Brumberg, Joshua, 50, 333
Brunel, Nicolas, 123, 212, 271
Buesing, Lars, 287
Buia, Calin, 215
Buonomano, Dean, 175, 180
Burak, Yoram, 91
Butera, Robert, 86
Butler, Patrick, 318, 319
Butts, Daniel, 254
Butz, Markus, 65, 102
Buzsaki, Gyorgy, 49, 64, 172, 269, 292
Buzsaki, Gyuri, 335
Caddigan, Eamon, 263
Cadieu, Charles, 226
Calford, Mike, 67
Callaway, Ed, 111

Index
Abbott, Larry, 164, 276, 289, 339
Achard, Pablo, 286
Adolphs, Ralph, 12
Afshar, Afsheen, 282
Aguiar, Paulo, 141
Ahammad, Parvez, 92
Ahmadian, Yashar, 255
Ahmed, Bashir, 247
Ahrens, Misha, 27, 112
Aihara, Kazuyuki, 142
Aimone, James, 171
Aksay, Emre, 284
Albright, Tom, 262
Alonso, Jose-Manuel, 254
Amarasingham, Asohan, 49, 269
Amodei, Dario, 207
Andermann, Mark, 10
Andersen, Richard, 110
Anderson, Emily, 132
Angelucci, Alessandra , 16, 239
Aonishi, Toru, 39
Asari, Hiroki, 196
Atallah, Bassam, 53
Aubie, Brandon, 313
Augustine, George, 324
Axel, Richard, 339
Babadi, Baktash, 198, 289
Badel, Laurent, 336
Bajcsy, Ruzena , 92
Bakolitsa, Constantina, 100
Ballard, Dana, 69
Bandettini, Peter, 45
Bandyopadhyay, Sharba, 250
Barak, Omri, 82
Barbieri, Riccardo, 197
Bart, Evgeniy, 121
Battaglia, Demian, 123
Battaglia, Peter, 99, 210
Beck, Diane, 263
Beck, Jeffrey, 200, 323
Becker, Sue, 184, 313
Beggs, John, 52
Bennett, Max, 229
Bensmaia, Sliman, 159

341

Author index

Cosyne 2008

Dean, Isabel, 27
Dean, Mark, 77
Deisseroth, Karl, 285, 340
Deneve, Sophie, 302
den Ouden, Hanneke, 63, 193, 300
den Nijs, Marcel, 165
de Ruyter van Steveninck, Rob, 270
de Sa, Virginia, 134
Desbordes, Gaelle, 254
Desimone, Robert, 161, 167
Deweese, Michael, 251
DiCarlo, James, 13, 125, 199, 227, 312
DiGiovanna, Jack, 170
Diba, Kamran, 64, 292
Ding, Long, 222
Dinse, Hubert, 223
Doi, Eizaburo, 266
Doukhan, David, 199, 312
Dounskaia, Natalia, 279
Dreher, Bogdan, 67
Eftychios, Pnevmatikakis, 35
Egger, Seth, 241
Eisele, Michael, 322
Elhilali, Mounya, 218
Eliasmith, Chris, 315
Elliott, Terry, 191
Engelbrecht, Jan, 209
Erdi , Peter , 214
Erdi, Peter, 213
Erlich, Jeffrey, 154, 205
Ernst, Udo, 302
Escola, Sean, 322
Eskandar, Emad, 44
Etzel, Joset, 304
Evans, Nathan, 169
Fairhall, Adrienne, 47, 162
Faisal, Aldo, 96, 221, 325
Farajidavar, Aydin, 204
Farnell, Les, 229
Faure, Paul, 313
Fei-Fei, Li, 263
Felsen, Gidon, 57
Fermuller, Cornelia, 242
Ferreira, Daniel, 192
Ferster, David, 246, 278
Field, Greg, 265

Cao, Rosa, 50, 333
Caplin, Andrew, 77
Carandini, Matteo, 22, 237, 240
Cardin, Jessica, 50, 333
Carlsson, Gunnar, 40
Carvalho, Tiago, 175
Casile, Antonino, 93
Cassenaer, Stijn, 166
Castellanos Pérez-Bolde, Lucia, 32
Casti, Alexander, 198
Chandrasekaran, Chandramouli, 138
Chastain, Erick, 189
Chen, Jen-Yung, 83
Chen, Nan-Hui, 143
Chen, Yanqing, 105
Cheng, Ming, 44
Cheng, Sen, 283
Chi, Zhiyi, 225
Chiappalone, Michela, 85
Chichilnisky, EJ, 42, 100, 265
Chklovskii, Dimitri, 331
Choe, Yoonsuck, 267
Clark, Andrew, 14
Clopath, Claudia, 287
Cohen, Jonathan, 288
Cohen, Netta, 259
Cohn, Anthony, 116
Connor, Charles, 15
Coombes, Stephen, 79
Corda, Benoit, 199, 312
Cortes, Jesus , 149
Cottrell, Garrison, 219, 245
Cox, David, 125, 199, 312
Cronin, Beau, 181
Cumming, Bruce, 188, 321
Cuntz, Hermann , 101, 195
D'Esposito, Mark, 216
Dabaghian, Yuri, 116
Das, Sandhitsu, 148
Daunizeau, Jean, 63, 193
Davidson, Thomas, 173
Daw, Nathaniel, 115
Dawson, Geraldine, 162
Dayan, Peter, 72, 146, 178, 183, 300, 308
De Schutter, Erik, 286
DeAngelis, Gregory, 301

342

Author index

Cosyne 2008

Gribble, Paul, 59, 311
Griffiths, Thomas, 303
Grill-Spector, Kalanit, 122
Grivich, Matthew, 265
Groh, Jennifer, 139
Gruen , Sonja, 130, 157
Guetig, Robert, 337
Guillaume, Masson, 75
Gundlfinger, Anja, 34
Gur, Moshe, 126
Gustafson, Nicholas, 115
Gutmann, Michael, 142
Haas, Julie, 71
Haefner, Ralf, 321
Hakim, Vincent, 212
Hamaguchi, Kosuke, 271
Hamker, Fred H, 84, 129, 174
Hansel, David, 123
Harper, Nicol, 27
Harrison, Matthew, 269
Hartmann, Mitra, 113, 252
Hasenstaub, Andrea, 111
Haslinger, Robert, 194
Hass, Joachim, 316
Hatsopoulos, Nicholas , 155
Havenith, Martha, 143
Hedges, James, 21
Heeger, David, 216, 230
Hegdé, Jay, 121
Hehrmann, Phillipp, 27
Herreros-Alonso, Ivan, 95
Herrmann, Christoph, 48
Herrmann, J. Michael, 316
Hertz, John, 258
Higashikubo, Bryan, 50, 333
Hirsch, Judith, 128, 238
Histed, Mark, 55
Holzman, Lars, 187
Hong, Sungho, 162
Howard , Matthew, 12
Hromadka, Tomas, 251, 275
Hsiao, Steven, 15
Hu, Rollin, 44
Hugues, Etienne, 107
Huh, Ben Dongsung, 56
Hurri, Jarmo, 232

Finkel, Leif, 148
Finn, Ian, 278
Fischer, Ingo , 97
Fiser, Jozsef, 231
Fisher, Dimitri, 68
Fleischer, Falk, 93
Flister, Erik, 234
Flynn, Mark, 158
Frank, Loren, 116, 283, 285
Franke, Felix, 261
Franz, Matthias , 249
Fremouw, Thane, 29
Fries, Pascal, 17, 161, 167
Friston, Karl, 63, 193
Froemke, Robert, 66
Fruend, Ingo, 48
Fujisawa, Shigeyoshi, 49
Fukai, Tomoki, 80, 186
Furman, Moran, 295
Gage, Fred, 171
Galindo-Leon, Edgar, 70, 253
Ganmor, Elad, 54
Gao, Dashan, 88
Garcia-Lazaro, Jose , 247
Gastpar, Michael, 203
Gauthier, Jeff, 265
Gavornik, Jeffrey, 291
Gazzola, Valeria , 304
Geffen, Maria, 25
Gerstner, Wulfram, 287, 336
Gewaltig, Marc-Oliver, 160
Ghazanfar, Asif, 138
Gibson, Bill, 229
Giese, Martin, 93
Gill, Patrick, 29, 203
Glimcher, Paul, 77, 118
Goble, Jacob, 279
Gold, Joshua, 222, 293
Goldman, Mark, 60, 284
Gollo, Leonardo, 97
Gopal, Venkatesh, 252
Graham, Brett, 131
Grattan, Lauren, 118
Graña, Gilberto, 28
Greschner, Martin, 265
Grey, Charles, 108

343

Author index

Cosyne 2008

Klinkner, Kristina, 194
Kloosterman, Fabian, 173
Knoblauch, Andreas, 160
Knoblich, Ulf, 50, 333
Knudsen, Eric, 135
Knudsen, Phyllis, 135
Koch, Christof, 74, 236
Koepsell, Kilian, 19, 133
Kolodziejski, Christoph, 176
Kopec, Charles, 205
Kording, Konrad, 181
Korsten, Nienke, 296
Koyama, Shinsuke, 32, 256
Krekelberg, Bart, 262
Kriegeskorte, Nikolaus, 45
Kristina, Visscher, 217
Krueger, Kai, 178
Kuang, Xutao, 191
Kulkarni, Jayant, 155
Kupper, Rüdiger, 160
Kurashige, Hiroki, 294
Körner, Edgar, 160
Körner, Ursula, 160
Köster, Urs, 233
Lalazar, Hagai, 144, 168
Lange, Joachim, 17
Larson, Eric, 310
Latham, Peter, 38, 61
Laudanski, Jonathan, 79
Laughlin, Simon, 325
Laurent , Perrinet, 75, 179
Laurent, Gilles, 152, 166
Lauritzen, Thomas, 216
Lazar, Aurel A., 35
Ledvina, Andrew, 298
Lee, Melanie, 284
Lehky, Sidney, 260
Lehmkuhle, Mark, 281
Leibold, Christian, 34, 64
Lenggenhager, Bigna, 169
Lengyel, Mate, 231
Leopold, David, 104
Levine, Joseph, 284
Lewallen, Sam , 91
Lewi, Jeremy, 86
Li, Nuo, 227

Huys, Quentin, 63, 300
Hyvärinen, Aapo, 142, 232, 233
Häusser, Michael, 101, 195, 220
Ichida, Jennifer, 16, 239
Ishkanov, Tigran, 40
Istvan , Ulbert, 214
Ito, Junji, 130
Jaramillo, Santiago, 327
Ji, Hui, 242
Jin, Dezhe, 62
Jin, Jianzhong, 254
Jingling, Li , 9
Jitsev, Jenia, 11
Johansson, Roland, 136
Johnson, Hope, 180
Johnson, Jeffery , 87
José, Jorge, 107
Jurjut, Ovidiu, 41
Kaiser, Katharina, 26
Kalmar, Rachel, 282
Kanold, Patrick, 250
Kardamakis, Andreas, 280
Karthiek, C, 317
Kaschube, Matthias, 120
Kass, Robert, 32, 256
Kastner, Sabine, 228
Katahira, Kentaro, 150
Katzner, Steffen, 240
Kawasaki, Hiroto, 12
Kawato, Mitsuo, 117
Kazama, Hokto, 137
Keil, Wolfgang, 120
Kemere, Caleb, 285
Kempter, Richard, 34, 64, 306
Kenyon, Garrett, 158
Kepecs, Adam, 73
Kerlin, Aaron, 10
Kersten, Daniel, 121
Keysers, Christian , 304
Kiani, Roozbeh, 45
Kim, Sung Soo , 159
King, Andrew, 248
Kipke, Daryl, 281
Kiss, Tamas, 213
Kistemaker, Dinant, 311
Klein, Robert, 57

344

Author index

Cosyne 2008

Miller, Kenneth, 182, 278
Miller, Paul, 290
Ming, Vivienne, 208
Mirasso, Claudio, 97
Mirollo, Renato, 209
Mitchell, Jude, 132, 329
Miura, Keiji, 257
Miyakawa, Hiroyoshi, 39
Moazzezi, Reza, 146
Molinelli, Evan, 284
Monaci, Gianluca, 90
Mongillo, Gianluigi, 82
Mooney, Marie, 87
Moore IV, Bartlett, 243
Moore, Christopher, 30, 50, 333
Morita, Kenji, 274
Moschovakis, Adonis, 280
Mukamel, Eran, 314
Munk, Matthias, 261
Mur, Marieke, 45
Murakami, Masayoshi, 297
Muresan, Raul, 41
Murias, Michael, 162
Natora, Michal, 261
Naud, Richard, 336
Nauhaus, Ian, 22, 240
Navalpakkam, Vidhya, 74, 200, 323
Nguyen, David, 197
Nicolelis, Miguel, 332
Nienborg, Hendrikje, 188
Nikita, Konstantina, 305
Nikolic, Danko, 41, 143
Nimmerjahn, Axel, 314
Nishikawa, Jun, 150, 153
Niv, Yael, 183
Niven, Jeremy, 221
Norcia, Anthony, 18
Northmore, David, 131
Nowotny, Thomas , 71
O'Connor, Daniel, 299
O'Doherty, John, 183
Obermayer, Klaus, 37, 67, 261
Oertelt, Nadja, 125
Ohl, Frank, 48
Oizumi, Masafumi, 257
Ojemann, Jeffrey, 165

Likova, Lora, 244
Lima, Susana, 275
Lin, Shih-Chieh, 332
Lindgren, Jussi, 232, 233
Lisberger, Stephen, 127
Litke, Alan, 42, 52, 265
Liu, Robert, 70, 253
Lochmann, Timm, 302
Loewel, Siegrid, 120
Loewenstein, Yonatan, 291
Louie, Kenway, 118
Loureiro, Oscar, 187
Lundstrom, Brian, 47
Luo, Sean, 339
Ma, Wei Ji, 24, 200, 323
Macke, Jakob, 36
Maddox, Ross, 28
Magnasco, Marcelo, 25
Magnasco, Marcelo , 98
Mahadevan, Vijay, 88
Mahmoudi, Babak, 170
Maier, Joost, 139
Mainen, Zachary, 57, 73, 297
Major, Guy, 60
Makeig, Scott, 163
Maldonado, Pedro, 130
Maloney, Laurence T., 78
Margoliash, Daniel, 225
Marinazzo , Daniele , 149
Marks, Tim, 219
Martinoia, Sergio, 85
Massobrio, Paolo, 85
Matthew, Tong, 219
McAlpine, David, 27, 224
McCool, Christin, 241
McCormick, David, 309
McPeek, Robert M., 58
Meier, Philip, 234
Meliza, Daniel, 225
Memoli, Facundo, 40
Merolle, Mauro, 203
Merzenich, Michael, 66
Meso, Andrew, 235
Metzler, Dirk, 41
Miller, Amit, 61
Miller, Kai, 165

345

Author index

Cosyne 2008

Rajan, Kanaka, 164
Ramakrishnan, Naren, 318, 319
Rammsayer, Thomas, 316
Ramos, Raddy, 50, 333
Rao, Rajesh, 189
Rao, Vinod, 301
Rathbun, Daniel, 243
Ray, Debajyoti, 72
Rehn, Martin, 23
Reid, R. Clay, 10, 55
Reinagel, Pamela, 234
Reiser, Michael, 334
Reynolds, John, 132, 329
Richert, Micah, 262
Richter, Craig, 108
Riehle, Alexa, 271
Rinberg, Dima, 156
Ringach, Dario, 22, 40, 240
Ritt, Jason, 30
Rivera Alvidrez, Zuley, 282
Rogers, Steve, 221
Roth, Arnd, 101, 336
Rothenstein, Albert, 328
Rothkopf, Constantin, 69
Rozendaal, Leonard, 311
Rucci, Michele, 20
Rust, Nicole, 13
Rutledge, Robb, 77
Ryu, Stephen, 282
Saal, Hannes, 136
Sadeghi, Kolia, 201
Saeb Taheri, Sohrab, 204
Sahani, Maneesh, 27, 106, 112, 140, 264
Sajda, Paul, 46
Sakai, Yutaka, 186, 294
Sakas, Damianos, 305
Salazar, Rodrigo, 108
Sanborn, Adam, 308
Sanchez, Justin , 170
Sanger, Terence, 273
Santamaria, Fidel, 324
Santhanam, Gopal, 282
Sapiro, Guillermo, 40
Sarma, Sridevi, 44
Sastry, P.S., 81, 318
Sastry, Shankar , 92

Okada, Masato, 39, 150, 257
Okanoya, Kazuo, 150, 153
Olshausen, Bruno, 19, 133, 208, 211, 226
Omori, Toshiaki, 39
Onken, Arno, 37
Onton, Julie, 163
Oostenveld, Robert, 17, 161
Orban, Gergo, 231
Osborne, Leslie, 127
Ostojic, Srdjan, 212
Ota, Keisuke, 39
Otazu, Gonzalo, 326
Otte, Stephani, 111
Ozeki, Hirofumi, 278
Pai, Shraddha, 185
Palmer, Alan, 79
Palmer, Stephanie, 31
Paninski, Liam, 33, 42, 86, 155, 192, 198,
255, 256, 266, 272, 307, 322
Papadopoulou, Maria, 152
Park, Joonkoo, 206
Parra, Lucas, 24
Pasquale, Valentina, 85
Pastalkova, Eva, 172
Patnaik, Debprakash, 81, 318
Perakis, Dimitrios, 305
Perona, Pietro, 74
Pessiglione, Mathias, 193
Phoka, Elena, 101
Pillow, Jonathan, 38, 42, 100, 147, 255
Pinto, Nicolas, 199, 312
Pipa, Gordon, 97, 157
Pipa, Gordon , 320
Pissadaki, Eleftheria Kyriaki, 145
Poggio, Tomaso, 8
Poirazi, Panayiota, 145
Poletti, Martina, 20
Porr, Bernd, 176
Porter, Kristin, 139
Pouget, Alexandre, 323
Preuschoff, Kerstin, 76
Priebe, Nicholas, 246
Principe, Jose , 170
Radwan, Basma, 125
Raghavachari, Sridhar , 324
Rahnama Rad, Kamiar, 33, 307

346

Author index

Cosyne 2008

Silver, Michael, 216
Simen, Patrick, 43
Simon, Jonathan, 218
Simoncelli, Eero, 21, 42, 100, 190, 266, 268
Singer, Wolf, 41, 143
Singh, Gurjeet, 40
Sinha, Shiva, 270
Sjöström, Per Jesper, 195
Skowronski-Lutz, Ethan, 30
Smith, Anne, 89
Smith, Katy, 104
Smith, Peter, 89
Snodderly, Max, 126
Snyder, Lawrence, 301
Sohl-Dickstein, Jascha, 211
Sommer, Friedrich, 23, 90, 128, 160, 238,
283
Sommer, Marc , 277
Somogyvari , Zoltan , 214
Sompolinsky, Haim, 91, 164, 337
Song, Joo-Hyun, 58
Soto Sanchez, Cristina, 128
Sporns, Olaf, 52
Sripati, Arun , 159
Stanley, Garrett, 151, 254
Stephan, Klaas Enno, 63, 193
Stevens, Charles , 51
Stilz, Peter , 249
Stocker, Alan, 21, 190, 268
Sumner, Christian , 79
Sundberg, Kristy, 132, 329
Sur, Mriganka, 181
Sussillo, David, 276
Suzuki, Wendy, 119
Svoboda, Karel, 299
Swindale, Nicholas, 19, 133
Tagaris, George, 305
Taillefumier, Thibaud, 98
Tang, Aonan, 52
Tank, David, 60
Taylor, John, 296
Teeter, Corinne, 51
Teng, Ching-Ling, 293
Teramae, Jun-nosuke, 80
Tetzlaff, Christian, 102
Theunissen, Frédéric, 29, 203

Sato, Yasuomi Daishin, 11
Savin, Cristina, 177
Sayres, Rory, 122
Scanziani, Massimo, 53
Schaette, Roland, 306
Schaffer, Evan, 278
Schmidt, Robert, 34, 64
Schmitz, Dietmar , 34, 64
Schneidman, Elad, 54
Schnitzer, Mark, 314
Schnitzler, Hans-Ulrich, 249
Schnobrich, Charlene , 60
Schnupp, Jan, 247, 248
Schrater, Paul, 99, 210
Schreiner, Christoph, 66
Schwabe, Lars, 16, 169
Schwartz, Gregory, 36, 207
Segev, Ronen, 54
Sejnowski, Terrence, 103, 111, 149
Sekuler, Robert, 217
Selimkhanov, Timur, 252
Sen, Kamal, 28, 310
Sereno, Martin, 134
Series, Peggy , 149, 268
Seutin, Vincent, 103
Shadbolt, Nigel, 191
Shahar, Maayan, 144, 168
Shalizi, Cosma, 32, 194
Shamma, Shihab, 218
Shamma, Shihab , 250
Shan, Honghao, 219, 245
Shenoy, Krishna, 109, 282
Shenoy, Pradeep, 189
Sher, Alexander, 42, 52, 265
Shi, Jianing, 46
Shi, Lei, 303
Shiau, LieJune, 94
Shin, SooYoon, 277
Shlens, Jonathon, 42, 265
Shouval, Harel, 291
Shpigelman, Lavi, 144
Shu, Yousheng, 309
Shuler, Marshall, 291
Shushruth, S, 16, 239
Shusterman, Roman, 156
Siegelmann, Hava, 187

347

Author index

Cosyne 2008

Wang, Jimmy, 208
Wang, Qi, 151
Wang, Xiao-Jing, 295
Wang, Xin, 128, 238
Warland, David, 23
Watanabe, Shigeo, 39
Watt, Alanna, 195
Weng, Chong, 254
Whiteley, Louise, 106
Wibral, Michael, 320
Wiegrebe, Lutz, 26
Wielaard, Jim, 46
Wiles, Janet, 171
Wilimzig, Claudia, 223, 236
Wilke, Melanie, 104
Williams, Ziv, 44
Wilson, Elizabeth, 59
Wilson, Matthew, 173, 197
Wilson, Rachel, 137, 338
Wilson, Robert, 148
Wiltschut, Jan, 174
Witten, Ilana, 135
Woergoetter, Florentin, 65, 102
Wolf, Fred, 120
Wolfrum, Philipp, 330
Wolpert, Daniel, 96
Womble, Stephen, 259
Womelsdorf, Thilo, 161
Womelsdorf, Thilo , 167
Wong, Jeremy, 59
Wood, Andy, 79
Wood, Frank, 147
Woolley, Sarah, 29
Wu, Shih-Wei, 78
Wu, Wei , 155
Wörgötter, Florentin, 176
Xiang, Juanjuan, 218
Xiao, Youping , 198
Yang, Huei-Fang, 267
Yau, Jeffrey, 15
Yazdan-Shahmorad, Azadeh, 281
Young, Joshua, 67
Yovel, Yossi, 249
Yu, Angela, 288
Yu, Byron, 282
Yu, Hsin-Hao, 134

Thurley, Kay, 34
Tiesinga, Paul, 215
Tochiki, Keri, 60
Todorov, Emanuel, 56
Torres, Elizabeth, 110
Toulouse, Trent, 184
Toyoizumi, Taro, 33, 182
Triesch, Jochen, 177, 320
Tripp, Bryan, 315
Tsang, Sabrina, 125
Tse, Susan , 60
Tsirogiannis, George, 305
Tsodyks, Misha, 68, 82
Tsotsos, John, 328
Tsuchiya, Naotsugu, 12
Turchi, Janita, 104
Turner, Glenn , 152
Turner, Richard, 140, 264
Tyler, Christopher, 244
Tyrcha, Joanna, 258
Uchida, Naoshige, 73
Ujfalussy, Balazs, 213
Unnikrishnan, K.P., 81, 318, 319
Usrey, W Martin, 114, 243
Vaadia, Eilon, 144, 168
Vaingankar, Vishal, 128
van Ooyen, Arjen, 102
van Rossum, Mark, 149
van Vreeswijk, Carl , 157
VanRullen, Rufin, 236
Vasconcelos, Nuno, 88
Vasilaki, Eleni , 287
Vasudevan, Ramanarayan, 92
Venkateswaran, Nagarajan, 317
Verschure, Paul F. M. J., 95
Vicente, Raul, 97, 320
Vijayakumar, Sethu, 136
Vinck, Martin, 161
Vogelstein, Joshua, 272
von der Malsburg, Christoph, 11, 124, 330
Wang, Linli, 62
Wade, Alex, 18
Waleszczyk, Wioletta, 67
Walker, Kerry, 248
Walther, Dirk, 263
Wang, Chun, 67

348

Author index

Cosyne 2008

Yu, Shan, 143
Yu, Yuguo, 309
Zador, Anthony, 196, 251, 275, 326, 327
Zanker, Johannes, 235
Zauner, Klaus-Peter, 191
Zhan, Jiening, 203
Zhang, Feng, 285
Zhang, Jun, 206
Zhang, Lingyun, 219
Zhang, Tao, 241
Zhaoping , Li, 9
Zhou, Xiang, 24
Ziesche, Arnold, 84
Zimmerli, Lukas, 95
Zirnsak, Marc, 129
Zoccolan, Davide, 125

349

Notes

Cosyne 2008

350

Cosyne 2008

Notes

351

Notes

Cosyne 2008

352

